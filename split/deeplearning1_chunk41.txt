ct demonstrated for the log-likelihood in equation 8.5 and equation 8.6; we observe now that this holds for other functions L
besides the likelihood. A similar result can be derived when x and y are continuous,
under mild assumptions regarding pdata and L.
Hence, we can obtain an unbiased estimator of the exact gradient of the
generalization error by sampling a minibatch of examples {x (1) , . . . x(m)} with corresponding targets y(i) from the data generating distribution pdata , and computing
the gradient of the loss with respect to the parameters for that minibatch:
gÌ‚ =

î?˜
1
L(f (x(i); Î¸), y (i)).
âˆ‡Î¸
m
i

(8.9)

Updating Î¸ in the direction of gÌ‚ performs SGD on the generalization error.
Of course, this interpretation only applies when examples are not reused.
Nonetheless, it is usually best to make several passes through the training set,
unless the training set is extremely large. When multiple such epochs are used,
only the ï¬?rst epoch follows the unbiased gradient of the generalization error, but
281

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

of course, the additional epochs usually provide enough beneï¬?t due to decreased
training error to oï¬€set the harm they cause by increasing the gap between training
error and test error.
With some datasets growing rapidly in size, faster than computing power, it
is becoming more common for machine learning applications to use each training
example only once or even to make an incomplete pass through the training
set. When using an extremely large training set, overï¬?tting is not an issue, so
underï¬?tting and computational eï¬ƒciency become the predominant concerns. See
also Bottou and Bousquet (2008) for a discussion of the eï¬€ect of computational
bottlenecks on generalization error, as the number of training examples grows.

8.2

Challenges in Neural Network Optimization

Optimization in general is an extremely diï¬ƒcult task. Traditionally, machine
learning has avoided the diï¬ƒculty of general optimization by carefully designing
the objective function and constraints to ensure that the optimization problem is
convex. When training neural networks, we must confront the general non-convex
case. Even convex optimization is not without its complications. In this section,
we summarize several of the most prominent challenges involved in optimization
for training deep models.

8.2.1

Ill-Conditioning

Some challenges arise even when optimizing convex functions. Of these, the most
prominent is ill-conditioning of the Hessian matrix H. This is a very general
problem in most numerical optimization, convex or otherwise, and is described in
more detail in section 4.3.1.
The ill-conditioning problem is generally believed to be present in neural
network training problems. Ill-conditioning can manifest by causing SGD to get
â€œstuckâ€? in the sense that even very small steps increase the cost function.
Recall from equation 4.9 that a second-order Taylor series expansion of the
cost function predicts that a gradient descent step of âˆ’î€?g will add
1 2 î€¾
î€? g Hg âˆ’ î€?gî€¾g
2

(8.10)

to the cost. Ill-conditioning of the gradient becomes a problem when 12 î€?2g î€¾Hg
exceeds î€?gî€¾ g. To determine whether ill-conditioning is detrimental to a neural
network training task, one can monitor the squared gradient norm g î€¾g and
282

16

1 .0

14

0 .9

Classiï¬?cation error rate

Gradient norm

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

12
10
8
6
4
2
0
âˆ’2
âˆ’50

0

50 100 150 200 250

Training time (epochs)

0 .8
0 .7
0 .6
0 .5
0 .4
0 .3
0 .2
0 .1

0

50

100

150

200

250

Training time (epochs)

Figure 8.1: Gradient descent often does not arrive at a critical point of any kind. In this
example, the gradient norm increases throughout training of a convolutional network used
for object detection. (Left)A scatterplot showing how the norms of individual gradient
evaluations are distributed over time. To improve legibility, only one gradient norm
is plotted per epoch. The running average of all gradient norms is plotted as a solid
curve. The gradient norm clearly increases over time, rather than decreasing as we would
expect if the training process converged to a critical point. (Right)Despite the increasing
gradient, the training process is reasonably successful. The validation set classiï¬?cation
error decreases to a low level.

the gî€¾Hg term. In many cases, the gradient norm does not shrink signiï¬?cantly
throughout learning, but the g î€¾Hg term grows by more than an order of magnitude.
The result is that learning becomes very slow despite the presence of a strong
gradient because the learning rate must be shrunk to compensate for even stronger
curvature. Figure 8.1 shows an example of the gradient increasing signiï¬?cantly
during the successful training of a neural network.
Though ill-conditioning is present in other settings besides neural network
training, some of the techniques used to combat it in other contexts are less
applicable to neural networks. For example, Newtonâ€™s method is an excellent tool
for minimizing convex functions with poorly conditioned Hessian matrices, but in
the subsequent sections we will argue that Newtonâ€™s method requires signiï¬?cant
modiï¬?cation before it can be applied to neural networks.

8.2.2

Local Minima

One of the most prominent features of a convex optimization problem is that it
can be reduced to the problem of ï¬?nding a local minimum. Any local minimum is
283

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

guaranteed to be a global minimum. Some convex functions have a ï¬‚at region at
the bottom rather than a single global minimum point, but any point within such
a ï¬‚at region is an acceptable solution. When optimizing a convex function, we
know that we have reached a good solution if we ï¬?nd a critical point of any kind.
With non-convex functions, such as neural nets, it is possible to have many
local minima. Indeed, nearly any deep model is essentially guaranteed to have
an extremely large number of local minima. However, as we will see, this is not
necessarily a major problem.
Neural networks and any models with multiple equivalently parametrized latent
variables all have multiple local minima because of the model identiï¬?ability
problem. A model is said to be identiï¬?able if a suï¬ƒciently large training set can
rule out all but one setting of the modelâ€™s parameters. Models with latent variables
are often not identiï¬?able because we can obtain equivalent models by exchanging
latent variables with each other. For example, we could take a neural network and
modify layer 1 by swapping the incoming weight vector for unit i with the incoming
weight vector for unit j , then doing the same for the outgoing weight vectors. If we
have m layers with n units each, then there are n!m ways of arranging the hidden
units. This kind of non-identiï¬?ability is known as weight space symmetry.
In addition to weight space symmetry, many kinds of neural networks have
additional causes of non-identiï¬?ability. For example, in any rectiï¬?ed linear or
maxout network, we can scale all of the incoming weights and biases of a unit by
Î± if we also scale all of its outgoing weights by 1Î±. This means thatâ€”if the cost
function does not include terms such as weight decay that depend directly on the
weights rather than the modelsâ€™ outputsâ€”every local minimum of a rectiï¬?ed linear
or maxout network lies on an (m Ã— n )-dimensional hyperbola of equivalent local
minima.
These model identiï¬?ability issues mean that there can be an extremely large
or even uncountably inï¬?nite amount of local minima in a neural network cost
function. However, all of these local minima arising from non-identiï¬?ability are
equivalent to each other in cost function value. As a result, these local minima are
not a problematic form of non-convexity.
Local minima can be problematic if they have high cost in comparison to the
global minimum. One can construct small neural networks, even without hidden
units, that have local minima with higher cost than the global minimum (Sontag
and Sussman, 1989; Brady et al., 1989; Gori and Tesi, 1992). If local minima
with high cost are common, this could pose a serious problem for gradient-based
optimization algorithms.
It remains an open question whether there are many local minima of high cost
284

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

for networks of practical interest and whether optimization algorithms encounter
them. For many years, most practitioners believed that local minima were a
common problem plaguing neural network optimization. Today, that does not
appear to be the case. The problem remains an active area of research, but experts
now suspect that, for suï¬ƒciently large neural networks, most local minima have a
low cost function value, and that it is not important to ï¬?nd a true global minimum
rather than to ï¬?nd a point in parameter space that has low but not minimal cost
(Saxe et al., 2013; Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska
et al., 2014).
Many practitioners attribute nearly all diï¬ƒculty with neural network optimization to local minima. We encourage practitioners to carefully test for speciï¬?c
problems. A test that can rule out local minima as the problem is to plot the
norm of the gradient over time. If the norm of the gradient does not shrink to
insigniï¬?cant size, the problem is neither local minima nor any other kind of critical
point. This kind of negative test can rule out local minima. In high dimensional
spaces, it can be very diï¬ƒcult to positively establish that local minima are the
problem. Many structures other than local minima also have small gradients.

8.2.3

Plateaus, Saddle Points and Other Flat Regions

For many high-dimensional non-convex functions, local minima (and maxima)
are in fact rare compared to another kind of point with zero gradient: a saddle
point. Some points around a saddle point have greater cost than the saddle point,
while others have a lower cost. At a saddle point, the Hessian matrix has both
positive and negative eigenvalues. Points lying along eigenvectors associated with
positive eigenvalues have greater cost than the saddle point, while points lying
along negative eigenvalues have lower value. We can think of a saddle point as
being a local minimum along one cross-section of the cost function and a local
maximum along another cross-section. See ï¬?gure 4.5 for an illustration.
Many classes of random functions exhibit the following behavior: in lowdimensional spaces, local minima are common. In higher dimensional spaces, local
minima are rare and saddle points are more common. For a function f : R n â†’ R of
this type, the expected ratio of the number of saddle points to local minima grows
exponentially with n. To understand the intuition behind this behavior, observe
that the Hessian matrix at a local minimum has only positive eigenvalues. The
Hessian matrix at a saddle point has a mixture of positive and negative eigenvalues.
Imagine that the sign of each eigenvalue is generated by ï¬‚ipping a coin. In a single
dimension, it is easy to obtain a local minimum by tossing a coin and getting heads
once. In n-dimensional space, it is exponentially unlikely that all n coin tosses will
285

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

be heads. See Dauphin et al. (2014) for a review of the relevant theoretical work.
An amazing property of many random functions is that the eigenvalues of the
Hessian become more likely to be positive as we reach regions of lower cost. In
our coin tossing analogy, this means we are more likely to have our coin come up
heads n times if we are at a critical point with low cost. This means that local
minima are much more likely to have low cost than high cost. Critical points with
high cost are far more likely to be saddle points. Critical points with extremely
high cost are more likely to be local maxima.
This happens for many classes of random functions. Does it happen for neural
networks? Baldi and Hornik (1989) showed theoretically that shallow autoencoders
(feedforward networks trained to copy their input to their output, described in
chapter 14) with no nonlinearities have global minima and saddle points but no
local minima with higher cost than the global minimum. They observed without
proof that these results extend to deeper networks without nonlinearities. The
output of such networks is a linear function of their input, but they are useful
to study as a model of nonlinear neural networks because their loss function is
a non-convex function of their parameters. Such networks are essentially just
multiple matrices composed together. Saxe et al. (2013) provided exact solutions
to the complete learning dynamics in such networks and showed that learning in
these models captures many of the qualitative features observed in the training of
deep models with nonlinear activation functions. Dauphin et al. (2014) showed
experimentally that real neural networks also have loss functions that contain very
many high-cost saddle points. Choromanska et al. (2014) provided additional
theoretical arguments, showing that another class of high-dimensional random
functions related to neural networks does so as well.
What are the implications of the proliferation of saddle points for training algorithms? For ï¬?rst-order optimization algorithms that use only gradient information,
the situation is unclear. The gradient can often become very small near a saddle
point. On the other hand, gradient descent empirically seems to be able to escape
saddle points in many cases. Goodfellow et al. (2015) provided visualizations of
several learning trajectories of state-of-the-art neural networks, with an example
given in ï¬?gure 8.2. These visualizations show a ï¬‚attening of the cost function near
a prominent saddle point where the weights are all zero, but they also show the
gradient descent trajectory rapidly escaping this region. Goodfellow et al. (2015)
also argue that continuous-time gradient descent may be shown analytically to be
repelled from, rather than attracted to, a nearby saddle point, but the situation
may be diï¬€erent for more realistic uses of gradient descent.
For Newtonâ€™s method, it is clear that saddle points constitute a problem.
286

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

J(Î¸ )

P ro je c

tion 1

P ro

of Î¸

je c

t

2
io n

of

Î¸

Figure 8.2: A visualization of the cost function of a neural network. Image adapted
with permission from Goodfellow et al. (2015). These visualizations appear similar for
feedforward neural networks, convolutional networks, and recurrent networks applied
to real object recognition and natural language processing tasks. Surprisingly, these
visualizations usually do not show many conspicuous obstacles. Prior to the success of
stochastic gradient descent for training very large models beginning in roughly 2012,
neural net cost function surfaces were generally believed to have much more non-convex
structure than is revealed by these projections. The primary obstacle revealed by this
projection is a saddle point of high cost near where the parameters are initialized, but, as
indicated by the blue path, the SGD training trajectory escapes this saddle point readily.
Most of training time is spent traversing the relatively ï¬‚at valley of the cost function,
which may be due to high noise in the gradient, poor conditioning 
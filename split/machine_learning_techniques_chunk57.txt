  of  disk  space!).  The  list  includes  image  datasets,  text  datasets
(including  translation  datasets),  and  audio  and  video  datasets.  You  can  visit  https://
homl.info/tfds to view the full list, along with a description of each dataset.

TFDS  is  not  bundled  with  TensorFlow,  so  you  need  to  install  the  tensorflow-
datasets  library  (e.g.,  using  pip).  Then  call  the  tfds.load()  function,  and  it  will
download  the  data  you  want  (unless  it  was  already  downloaded  earlier)  and  return
the data as a dictionary of datasets (typically one for training and one for testing, but
this depends on the dataset you choose). For example, let’s download MNIST:

import tensorflow_datasets as tfds

dataset = tfds.load(name="mnist")
mnist_train, mnist_test = dataset["train"], dataset["test"]

You  can  then  apply  any  transformation  you  want  (typically  shuffling,  batching,  and
prefetching), and you’re ready to train your model. Here is a simple example:

mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)
for item in mnist_train:
    images = item["image"]
    labels = item["label"]
    [...]

The  load()  function  shuffles  each  data  shard  it  downloads  (only
for the training set). This may not be sufficient, so it’s best to shuf‐
fle the training data some more.

Note that each item in the dataset is a dictionary containing both the features and the
labels. But Keras expects each item to be a tuple containing two elements (again, the
features and the labels). You could transform the dataset using the map() method, like
this:

The TensorFlow Datasets (TFDS) Project 

| 

441

mnist_train = mnist_train.shuffle(10000).batch(32)
mnist_train = mnist_train.map(lambda items: (items["image"], items["label"]))
mnist_train = mnist_train.prefetch(1)

But  it’s  simpler  to  ask  the  load()  function  to  do  this  for  you  by  setting  as_super
vised=True (obviously this works only for labeled datasets). You can also specify the
batch size if you want. Then you can pass the dataset directly to your tf.keras model:

dataset = tfds.load(name="mnist", batch_size=32, as_supervised=True)
mnist_train = dataset["train"].prefetch(1)
model = keras.models.Sequential([...])
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd")
model.fit(mnist_train, epochs=5)

This  was  quite  a  technical  chapter,  and  you  may  feel  that  it  is  a  bit  far  from  the
abstract beauty of neural networks, but the fact is Deep Learning often involves large
amounts  of  data,  and  knowing  how  to  load,  parse,  and  preprocess  it  efficiently  is  a
crucial  skill  to  have.  In  the  next  chapter,  we  will  look  at  convolutional  neural  net‐
works, which are among the most successful neural net architectures for image pro‐
cessing and many other applications.

Exercises

1. Why would you want to use the Data API?

2. What are the benefits of splitting a large dataset into multiple files?

3. During  training,  how  can  you  tell  that  your  input  pipeline  is  the  bottleneck?

What can you do to fix it?

4. Can  you  save  any  binary  data  to  a  TFRecord  file,  or  only  serialized  protocol

buffers?

5. Why would you go through the hassle of converting all your data to the Example

protobuf format? Why not use your own protobuf definition?

6. When  using  TFRecords,  when  would  you  want  to  activate  compression?  Why

not do it systematically?

7. Data  can  be  preprocessed  directly  when  writing  the  data  files,  or  within  the
tf.data pipeline, or in preprocessing layers within your model, or using TF Trans‐
form. Can you list a few pros and cons of each option?

8. Name  a  few  common  techniques  you  can  use  to  encode  categorical  features.

What about text?

9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a train‐
ing  set,  a  validation  set,  and  a  test  set;  shuffle  the  training  set;  and  save  each
dataset  to  multiple  TFRecord  files.  Each  record  should  be  a  serialized  Example
protobuf with two features: the serialized image (use tf.io.serialize_tensor()

442 

| 

Chapter 13: Loading and Preprocessing Data with TensorFlow

  to  serialize  each  image),  and  the  label.11  Then  use  tf.data  to  create  an  efficient
dataset for each set. Finally, use a Keras model to train these datasets, including a
preprocessing  layer  to  standardize  each  input  feature.  Try  to  make  the  input
pipeline as efficient as possible, using TensorBoard to visualize profiling data.
10. In this exercise you will download a dataset, split it, create a tf.data.Dataset to
load  it  and  preprocess  it  efficiently,  then  build  and  train  a  binary  classification
model containing an Embedding layer:

a. Download  the  Large  Movie  Review  Dataset,  which  contains  50,000  movies
reviews from the Internet Movie Database. The data is organized in two direc‐
tories, train and test, each containing a pos subdirectory with 12,500 positive
reviews  and  a  neg  subdirectory  with  12,500  negative  reviews.  Each  review  is
stored in a separate text file. There are other files and folders (including pre‐
processed bag-of-words), but we will ignore them in this exercise.

b. Split the test set into a validation set (15,000) and a test set (10,000).

c. Use tf.data to create an efficient dataset for each set.
d. Create a binary classification model, using a TextVectorization layer to pre‐
process each review. If the TextVectorization layer is not yet available (or if
you like a challenge), try to create your own custom preprocessing layer: you
can  use  the  functions  in  the  tf.strings  package,  for  example  lower()  to
make  everything  lowercase,  regex_replace()  to  replace  punctuation  with
spaces, and split() to split words on spaces. You should use a lookup table to
output word indices, which must be prepared in the adapt() method.

e. Add an  Embedding layer and compute the mean embedding for each review,
multiplied by the square root of the number of words (see Chapter 16). This
rescaled mean embedding can then be passed to the rest of your model.

f. Train the model and see what accuracy you get. Try to optimize your pipelines

to make training as fast as possible.

g. Use TFDS to load the same dataset more easily: tfds.load("imdb_reviews").

Solutions to these exercises are available in Appendix A.

11 For large images, you could use tf.io.encode_jpeg() instead. This would save a lot of space, but it would

lose a bit of image quality.

Exercises 

| 

443

CHAPTER 14
Deep Computer Vision Using
Convolutional Neural Networks

Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐
parov back in 1996, it wasn’t until fairly recently that computers were able to reliably
perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing
spoken words. Why are these tasks so effortless to us humans? The answer lies in the
fact that perception largely takes place outside the realm of our consciousness, within
specialized  visual,  auditory,  and  other  sensory  modules  in  our  brains.  By  the  time
sensory information reaches our consciousness, it is already adorned with high-level
features; for example, when you look at a picture of a cute puppy, you cannot choose
not to see the puppy, not to notice its cuteness. Nor can you explain how you recog‐
nize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective expe‐
rience: perception is not trivial at all, and to understand it we must look at how the
sensory modules work.

Convolutional neural networks (CNNs) emerged from the study of the brain’s visual
cortex, and they have been used in image recognition since the 1980s. In the last few
years, thanks to the increase in computational power, the amount of available training
data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐
aged to achieve superhuman performance on some complex visual tasks. They power
image  search  services,  self-driving  cars,  automatic  video  classification  systems,  and
more. Moreover, CNNs are not restricted to visual perception: they are also successful
at many other tasks, such as voice recognition and natural language processing. How‐
ever, we will focus on visual applications for now.

In  this  chapter  we  will  explore  where  CNNs  came  from,  what  their  building  blocks
look like, and how to implement them using TensorFlow and Keras. Then we will dis‐
cuss  some  of  the  best  CNN  architectures,  as  well  as  other  visual  tasks,  including

445

object detection (classifying multiple objects in an image and placing bounding boxes
around  them)  and  semantic  segmentation  (classifying  each  pixel  according  to  the
class of the object it belongs to).

The Architecture of the Visual Cortex
David  H.  Hubel  and  Torsten  Wiesel  performed  a  series  of  experiments  on  cats  in
19581 and 19592 (and a few years later on monkeys3), giving crucial insights into the
structure of the visual cortex (the authors received the Nobel Prize in Physiology or
Medicine  in  1981  for  their  work).  In  particular,  they  showed  that  many  neurons  in
the visual cortex have a small local receptive field, meaning they react only to visual
stimuli  located  in  a  limited  region  of  the  visual  field  (see  Figure  14-1,  in  which  the
local receptive fields of five neurons are represented by dashed circles). The receptive
fields of different neurons may overlap, and together they tile the whole visual field.

Moreover, the authors showed that some neurons react only to images of horizontal
lines,  while  others  react  only  to  lines  with  different  orientations  (two  neurons  may
have  the  same  receptive  field  but  react  to  different  line  orientations).  They  also
noticed that some neurons have larger receptive fields, and they react to more com‐
plex  patterns  that  are  combinations  of  the  lower-level  patterns.  These  observations
led to the idea that the higher-level neurons are based on the outputs of neighboring
lower-level neurons (in Figure 14-1, notice that each neuron is connected only to a
few neurons from the previous layer). This powerful architecture is able to detect all
sorts of complex patterns in any area of the visual field.

1 David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats,” The Journal of Physiology 147

(1959): 226–238.

2 David H. Hubel and Torsten N. Wiesel, “Receptive Fields of Single Neurons in the Cat’s Striate Cortex,” The

Journal of Physiology 148 (1959): 574–591.

3 David H. Hubel and Torsten N. Wiesel, “Receptive Fields and Functional Architecture of Monkey Striate Cor‐

tex,” The Journal of Physiology 195 (1968): 215–243.

446 

| 

Chapter 14: Deep Computer Vision Using Convolutional Neural Networks

Figure 14-1. Biological neurons in the visual cortex respond to specific patterns in small
regions of the visual field called receptive fields; as the visual signal makes its way
through consecutive brain modules, neurons respond to more complex patterns in larger
receptive fields.

These  studies  of  the  visual  cortex  inspired  the  neocognitron,4  introduced  in  1980,
which  gradually  evolved  into  what  we  now  call  convolutional  neural  networks.  An
important  milestone  was  a  1998  paper5  by  Yann  LeCun  et  al.  that  introduced  the
famous  LeNet-5  architecture,  widely  used  by  banks  to  recognize  handwritten  check
numbers. This architecture has some building blocks that you already know, such as
fully  connected  layers  and  sigmoid  activation  functions,  but  it  also  introduces  two
new building blocks: convolutional layers and pooling layers. Let’s look at them now.

Why  not  simply  use  a  deep  neural  network  with  fully  connected
layers  for  image  recognition  tasks?  Unfortunately,  although  this
works  fine  for  small  images  (e.g.,  MNIST),  it  breaks  down  for
larger  images  because  of  the  huge  number  of  parameters  it
requires. For example, a 100 × 100–pixel image has 10,000 pixels,
and if the first layer has just 1,000 neurons (which already severely
restricts the amount of information transmitted to the next layer),
this means a total of 10 million connections. And that’s just the first
layer. CNNs solve this problem using partially connected layers and
weight sharing.

4 Kunihiko Fukushima, “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern

Recognition Unaffected by Shift in Position,” Biological Cybernetics 36 (1980): 193–202.

5 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,

no. 11 (1998): 2278–2324.

The Architecture of the Visual Cortex 

| 

447

Convolutional Layers
The most important building block of a CNN is the convolutional layer:6 neurons in
the first convolutional layer are not connected to every single pixel in the input image
(like they were in the layers discussed in previous chapters), but only to pixels in their
receptive  fields  (see  Figure  14-2).  In  turn,  each  neuron  in  the  second  convolutional
layer is connected only to neurons located within a small rectangle in the first layer.
This architecture allows the network to concentrate on small low-level features in the
first  hidden  layer,  then  assemble  them  into  larger  higher-level  features  in  the  next
hidden layer, and so on. This hierarchical structure is common in real-world images,
which is one of the reasons why CNNs work so well for image recognition.

Figure 14-2. CNN layers with rectangular local receptive fields

All the multilayer neural networks we’ve looked at so far had layers
composed  of  a  long  line  of  neurons,  and  we  had  to  flatten  input
images to 1D before feeding them to the neural network. In a CNN
each  layer  is  represented  in  2D,  which  makes  it  easier  to  match
neurons with their corresponding inputs.

6 A convolution is a mathematical operation that slides one function over another and measures the integral of
their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform
and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very
similar to convolutions (see https://homl.info/76 for more details).

448 

| 

Chapter 14: Deep Computer Vision Using Convolutional Neural Networks

A neuron located in row i, column j of a given layer is connected to the outputs of the
neurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,
where  fh  and  fw  are  the  height  and  width  of  the  receptive  field  (see  Figure  14-3).  In
order for a layer to have the same height and width as the previous layer, it is com‐
mon  to  add  zeros  around  the  inputs,  as  shown  in  the  diagram.  This  is  called  zero
padding.

Figure 14-3. Connections between layers and zero padding

It is also possible to connect a large input layer to a much smaller layer by spacing out
the  receptive  fields,  as  shown  in  Figure  14-4.  This  dramatically  reduces  the  model’s
computational complexity. The shift from one receptive field to the next is called the
stride. In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4
layer, using 3 × 3 receptive fields and
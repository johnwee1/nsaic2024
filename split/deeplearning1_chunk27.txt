escent because the formula for the
log-likelihood of the Gaussian distribution parametrized by Î² involves only multiplication by Î² i and addition of log Î²i . The gradient of multiplication, addition,
and logarithm operations is well-behaved. By comparison, if we parametrized the
output in terms of variance, we would need to use division. The division function
becomes arbitrarily steep near zero. While large gradients can help learning,
arbitrarily large gradients usually result in instability. If we parametrized the
output in terms of standard deviation, the log-likelihood would still involve division,
and would also involve squaring. The gradient through the squaring operation
can vanish near zero, making it diï¬ƒcult to learn parameters that are squared.
188

CHAPTER 6. DEEP FEEDFORWARD NETWORKS

Regardless of whether we use standard deviation, variance, or precision, we must
ensure that the covariance matrix of the Gaussian is positive deï¬?nite. Because
the eigenvalues of the precision matrix are the reciprocals of the eigenvalues of
the covariance matrix, this is equivalent to ensuring that the precision matrix is
positive deï¬?nite. If we use a diagonal matrix, or a scalar times the diagonal matrix,
then the only condition we need to enforce on the output of the model is positivity.
If we suppose that a is the raw activation of the model used to determine the
diagonal precision, we can use the softplus function to obtain a positive precision
vector: Î² = Î¶(a). This same strategy applies equally if using variance or standard
deviation rather than precision or if using a scalar times identity rather than
diagonal matrix.
It is rare to learn a covariance or precision matrix with richer structure than
diagonal. If the covariance is full and conditional, then a parametrization must
be chosen that guarantees positive-deï¬?niteness of the predicted covariance matrix.
This can be achieved by writing Î£(x) = B (x)Bî€¾ (x) , where B is an unconstrained
square matrix. One practical issue if the matrix is full rank is that computing the
likelihood is expensive, with a d Ã— d matrix requiring O(d3 ) computation for the
determinant and inverse of Î£(x) (or equivalently, and more commonly done, its
eigendecomposition or that of B (x)).
We often want to perform multimodal regression, that is, to predict real values
that come from a conditional distribution p(y | x) that can have several diï¬€erent
peaks in y space for the same value of x. In this case, a Gaussian mixture is
a natural representation for the output (Jacobs et al., 1991; Bishop, 1994).
Neural networks with Gaussian mixtures as their output are often called mixture
density networks. A Gaussian mixture output with n components is deï¬?ned by
the conditional probability distribution
p(y | x) =

n
î?˜
i=1

p(c = i | x)N (y ; Âµ(i)(x), Î£(i) (x)).

(6.35)

The neural network must have three outputs: a vector deï¬?ning p( c = i | x), a
matrix providing Âµ (i)(x) for all i, and a tensor providing Î£ (i)( x) for all i. These
outputs must satisfy diï¬€erent constraints:
1. Mixture components p(c = i | x): these form a multinoulli distribution
over the n diï¬€erent components associated with latent variable1 c, and can
1

We consider c to be latent because we do not observe it in the data: given input x and target
y , it is not possible to know with certainty which Gaussian component was responsible for y, but
we can imagine that y was generated by picking one of them, and make that unobserved choice a
random variable.
189

CHAPTER 6. DEEP FEEDFORWARD NETWORKS

typically be obtained by a softmax over an n-dimensional vector, to guarantee
that these outputs are positive and sum to 1.
2. Means Âµ(i)(x): these indicate the center or mean associated with the i-th
Gaussian component, and are unconstrained (typically with no nonlinearity
at all for these output units). If y is a d-vector, then the network must output
an n Ã— d matrix containing all n of these d-dimensional vectors. Learning
these means with maximum likelihood is slightly more complicated than
learning the means of a distribution with only one output mode. We only
want to update the mean for the component that actually produced the
observation. In practice, we do not know which component produced each
observation. The expression for the negative log-likelihood naturally weights
each exampleâ€™s contribution to the loss for each component by the probability
that the component produced the example.
3. Covariances Î£ (i)(x): these specify the covariance matrix for each component
i. As when learning a single Gaussian component, we typically use a diagonal
matrix to avoid needing to compute determinants. As with learning the means
of the mixture, maximum likelihood is complicated by needing to assign
partial responsibility for each point to each mixture component. Gradient
descent will automatically follow the correct process if given the correct
speciï¬?cation of the negative log-likelihood under the mixture model.
It has been reported that gradient-based optimization of conditional Gaussian
mixtures (on the output of neural networks) can be unreliable, in part because one
gets divisions (by the variance) which can be numerically unstable (when some
variance gets to be small for a particular example, yielding very large gradients).
One solution is to clip gradients (see section 10.11.1) while another is to scale
the gradients heuristically (Murray and Larochelle, 2014).
Gaussian mixture outputs are particularly eï¬€ective in generative models of
speech (Schuster, 1999) or movements of physical objects (Graves, 2013). The
mixture density strategy gives a way for the network to represent multiple output
modes and to control the variance of its output, which is crucial for obtaining
a high degree of quality in these real-valued domains. An example of a mixture
density network is shown in ï¬?gure 6.4.
In general, we may wish to continue to model larger vectors y containing more
variables, and to impose richer and richer structures on these output variables. For
example, we may wish for our neural network to output a sequence of characters
that forms a sentence. In these cases, we may continue to use the principle
of maximum likelihood applied to our model p( y; Ï‰(x)), but the model we use
190

y

CHAPTER 6. DEEP FEEDFORWARD NETWORKS

x

Figure 6.4: Samples drawn from a neural network with a mixture density output layer.
The input x is sampled from a uniform distribution and the output y is sampled from
pmodel (y | x ). The neural network is able to learn nonlinear mappings from the input to
the parameters of the output distribution. These parameters include the probabilities
governing which of three mixture components will generate the output as well as the
parameters for each mixture component. Each mixture component is Gaussian with
predicted mean and variance. All of these aspects of the output distribution are able to
vary with respect to the input x, and to do so in nonlinear ways.

to describe y becomes complex enough to be beyond the scope of this chapter.
Chapter 10 describes how to use recurrent neural networks to deï¬?ne such models
over sequences, and part III describes advanced techniques for modeling arbitrary
probability distributions.

6.3

Hidden Units

So far we have focused our discussion on design choices for neural networks that
are common to most parametric machine learning models trained with gradientbased optimization. Now we turn to an issue that is unique to feedforward neural
networks: how to choose the type of hidden unit to use in the hidden layers of the
model.
The design of hidden units is an extremely active area of research and does not
yet have many deï¬?nitive guiding theoretical principles.
Rectiï¬?ed linear units are an excellent default choice of hidden unit. Many other
types of hidden units are available. It can be diï¬ƒcult to determine when to use
which kind (though rectiï¬?ed linear units are usually an acceptable choice). We
191

CHAPTER 6. DEEP FEEDFORWARD NETWORKS

describe here some of the basic intuitions motivating each type of hidden units.
These intuitions can help decide when to try out each of these units. It is usually
impossible to predict in advance which will work best. The design process consists
of trial and error, intuiting that a kind of hidden unit may work well, and then
training a network with that kind of hidden unit and evaluating its performance
on a validation set.
Some of the hidden units included in this list are not actually diï¬€erentiable at
all input points. For example, the rectiï¬?ed linear function g (z) = max{0, z} is not
diï¬€erentiable at z = 0. This may seem like it invalidates g for use with a gradientbased learning algorithm. In practice, gradient descent still performs well enough
for these models to be used for machine learning tasks. This is in part because
neural network training algorithms do not usually arrive at a local minimum of
the cost function, but instead merely reduce its value signiï¬?cantly, as shown in
ï¬?gure 4.3. These ideas will be described further in chapter 8. Because we do not
expect training to actually reach a point where the gradient is 0, it is acceptable
for the minima of the cost function to correspond to points with undeï¬?ned gradient.
Hidden units that are not diï¬€erentiable are usually non-diï¬€erentiable at only a
small number of points. In general, a function g(z) has a left derivative deï¬?ned
by the slope of the function immediately to the left of z and a right derivative
deï¬?ned by the slope of the function immediately to the right of z. A function
is diï¬€erentiable at z only if both the left derivative and the right derivative are
deï¬?ned and equal to each other. The functions used in the context of neural
networks usually have deï¬?ned left derivatives and deï¬?ned right derivatives. In the
case of g(z) = max{0, z} , the left derivative at z = 0 is 0 and the right derivative
is 1. Software implementations of neural network training usually return one of
the one-sided derivatives rather than reporting that the derivative is undeï¬?ned or
raising an error. This may be heuristically justiï¬?ed by observing that gradientbased optimization on a digital computer is subject to numerical error anyway.
When a function is asked to evaluate g(0), it is very unlikely that the underlying
value truly was 0. Instead, it was likely to be some small value î€? that was rounded
to 0. In some contexts, more theoretically pleasing justiï¬?cations are available, but
these usually do not apply to neural network training. The important point is that
in practice one can safely disregard the non-diï¬€erentiability of the hidden unit
activation functions described below.
Unless indicated otherwise, most hidden units can be described as accepting
a vector of inputs x, computing an aï¬ƒne transformation z = W î€¾x + b, and
then applying an element-wise nonlinear function g(z). Most hidden units are
distinguished from each other only by the choice of the form of the activation
function g(z ).
192

CHAPTER 6. DEEP FEEDFORWARD NETWORKS

6.3.1

Rectiï¬?ed Linear Units and Their Generalizations

Rectiï¬?ed linear units use the activation function g (z ) = max{0, z }.

Rectiï¬?ed linear units are easy to optimize because they are so similar to linear
units. The only diï¬€erence between a linear unit and a rectiï¬?ed linear unit is
that a rectiï¬?ed linear unit outputs zero across half its domain. This makes the
derivatives through a rectiï¬?ed linear unit remain large whenever the unit is active.
The gradients are not only large but also consistent. The second derivative of the
rectifying operation is 0 almost everywhere, and the derivative of the rectifying
operation is 1 everywhere that the unit is active. This means that the gradient
direction is far more useful for learning than it would be with activation functions
that introduce second-order eï¬€ects.
Rectiï¬?ed linear units are typically used on top of an aï¬ƒne transformation:
h = g (W î€¾x + b).

(6.36)

When initializing the parameters of the aï¬ƒne transformation, it can be a good
practice to set all elements of b to a small, positive value, such as 0.1. This makes
it very likely that the rectiï¬?ed linear units will be initially active for most inputs
in the training set and allow the derivatives to pass through.
Several generalizations of rectiï¬?ed linear units exist. Most of these generalizations perform comparably to rectiï¬?ed linear units and occasionally perform
better.
One drawback to rectiï¬?ed linear units is that they cannot learn via gradientbased methods on examples for which their activation is zero. A variety of
generalizations of rectiï¬?ed linear units guarantee that they receive gradient everywhere.
Three generalizations of rectiï¬?ed linear units are based on using a non-zero
slope Î± i when zi < 0: hi = g(z , Î± )i = max(0, zi) + Î±i min (0, zi ). Absolute value
rectiï¬?cation ï¬?xes Î± i = âˆ’1 to obtain g(z) = |z |. It is used for object recognition
from images (Jarrett et al., 2009), where it makes sense to seek features that are
invariant under a polarity reversal of the input illumination. Other generalizations
of rectiï¬?ed linear units are more broadly applicable. A leaky ReLU (Maas et al.,
2013) ï¬?xes Î±i to a small value like 0.01 while a parametric ReLU or PReLU
treats Î±i as a learnable parameter (He et al., 2015).
Maxout units (Goodfellow et al., 2013a) generalize rectiï¬?ed linear units
further. Instead of applying an element-wise function g(z ), maxout units divide z
into groups of k values. Each maxout unit then outputs the maximum element of
193

CHAPTER 6. DEEP FEEDFORWARD NETWORKS

one of these groups:
g(z )i = max zj
jâˆˆG(i)

(6.37)

where G(i) is the set of indices into the inputs for group i, { (i âˆ’ 1)k + 1, . . . , ik}.
This provides a way of learning a piecewise linear function that responds to multiple
directions in the input x space.
A maxout unit can learn a piecewise linear, convex function with up to k pieces.
Maxout units can thus be seen as learning the activation function itself rather
than just the relationship between units. With large enough k, a maxout unit can
learn to approximate any convex function with arbitrary ï¬?delity. In particular,
a maxout layer with two pieces can learn to implement the same function of the
input x as a traditional layer using the rectiï¬?ed linear activation function, absolute
value rectiï¬?cation function, or the leaky or parametric ReLU, or can learn to
implement a totally diï¬€erent function altogether. The maxout layer will of course
be parametrized diï¬€erently from any of these other layer types, so the learning
dynamics will be diï¬€erent even in the cases where maxout learns to implement the
same function of x as one of the other layer types.
Each maxout unit is now parametrized by k weight vectors instead of just one,
so maxout units typically need more regularization than rectiï¬?ed linear units. They
can work well without regularization if the training set is large and the number of
pieces per unit is kept low (Cai et al., 2013).
Maxout units have a few other beneï¬?ts. In some cases, one can gain some statistical and computational advantages by requiring fewer parameters. Speciï¬?cally,
if the features captured by n diï¬€erent linear ï¬?lters can be summarized without
losing information by taking the max over each group of k features, then the next
lay
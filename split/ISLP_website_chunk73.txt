received a transplant by
time t, and xi (t) = 0 otherwise.

11.7.4

Checking the Proportional Hazards Assumption

We have seen that Cox’s proportional hazards model relies on the proportional hazards assumption (11.14). While results from the Cox model tend
to be fairly robust to violations of this assumption, it is still a good idea to
check whether it holds. In the case of a qualitative feature, we can plot the
log hazard function for each level of the feature. If (11.14) holds, then the
log hazard functions should just differ by a constant, as seen in the top-left
panel of Figure 11.4. In the case of a quantitative feature, we can take a
similar approach by stratifying the feature.

11.7.5

Survival Trees

In Chapter 8, we discussed flexible and adaptive learning procedures such as
trees, random forests, and boosting, which we applied in both the regression
and classification settings. Most of these approaches can be generalized to
the survival analysis setting. For example, survival trees are a modification
survival
of classification and regression trees that use a split criterion that maximizes trees

11.8 Lab: Survival Analysis

489

the difference between the survival curves in the resulting daughter nodes.
Survival trees can then be used to create random survival forests.

11.8

Lab: Survival Analysis

In this lab, we perform survival analyses on three separate data sets. In
Section 11.8.1 we analyze the BrainCancer data that was first described
in Section 11.3. In Section 11.8.2, we examine the Publication data from
Section 11.5.4. Finally, Section 11.8.3 explores a simulated call-center data
set.
We begin by importing some of our libraries at this top level. This makes
the code more readable, as scanning the first few lines of the notebook tell
us what libraries are used in this notebook.
In [1]: from matplotlib.pyplot import subplots
import numpy as np
import pandas as pd
from ISLP.models import ModelSpec as MS
from ISLP import load_data

We also collect the new imports needed for this lab.
In [2]: from lifelines import \
(KaplanMeierFitter ,
CoxPHFitter)
from lifelines.statistics import \
(logrank_test ,
multivariate_logrank_test )
from ISLP.survival import sim_time

11.8.1

Brain Cancer Data

We begin with the BrainCancer data set, contained in the ISLP package.
In [3]: BrainCancer = load_data('BrainCancer ')
BrainCancer.columns
Out[3]: Index (['sex', 'diagnosis ', 'loc', 'ki', 'gtv', 'stereo ',
'status ', 'time '],
dtype='object ')

The rows index the 88 patients, while the 8 columns contain the predictors
and outcome variables. We first briefly examine the data.
In [4]: BrainCancer['sex']. value_counts ()
Out[4]: Female
45
Male
43
Name: sex , dtype: int64
In [5]: BrainCancer['diagnosis ']. value_counts ()

490

11. Survival Analysis and Censored Data

Out[5]: Meningioma
42
HG glioma
22
Other
14
LG glioma
9
Name: diagnosis , dtype: int64
In [6]: BrainCancer['status ']. value_counts ()
Out[6]: 0
53
1
35
Name: status , dtype: int64

Before beginning an analysis, it is important to know how the status
variable has been coded. Most software uses the convention that a status
of 1 indicates an uncensored observation (often death), and a status of 0
indicates a censored observation. But some scientists might use the opposite
coding. For the BrainCancer data set 35 patients died before the end of the
study, so we are using the conventional coding.
To begin the analysis, we re-create the Kaplan-Meier survival curve
shown in Figure 11.2. The main package we will use for survival analysis is lifelines. The variable time corresponds to yi , the time to the ith lifelines
event (either censoring or death). The first argument to km.fit is the event
time, and the second argument is the censoring variable, with a 1 indicating an observed failure time. The plot() method produces a survival curve
.plot()
with pointwise confidence intervals. By default, these are 90% confidence
intervals, but this can be changed by setting the alpha argument to one
minus the desired confidence level.
In [7]: fig , ax = subplots(figsize =(8 ,8))
km = KaplanMeierFitter ()
km_brain = km.fit(BrainCancer['time '], BrainCancer['status '])
km_brain.plot(label='Kaplan Meier estimate ', ax=ax)

Next we create Kaplan-Meier survival curves that are stratified by sex, in
order to reproduce Figure 11.3. We do this using the groupby() method of
.groupby()
a dataframe. This method returns a generator that can be iterated over in
the for loop. In this case, the items in the for loop are 2-tuples representing
the groups: the first entry is the value of the grouping column sex while
the second value is the dataframe consisting of all rows in the dataframe
matching that value of sex. We will want to use this data below in the logrank test, hence we store this information in the dictionary by_sex. Finally,
we have also used the notion of string interpolation to automatically label
string
the different lines in the plot. String interpolation is a powerful technique interpolation
to format strings — Python has many ways to facilitate such operations.
In [8]: fig , ax = subplots(figsize =(8 ,8))
by_sex = {}
for sex , df in BrainCancer.groupby('sex'):
by_sex[sex] = df
km_sex = km.fit(df['time '], df['status '])
km_sex.plot(label='Sex=%s' % sex , ax=ax)

As discussed in Section 11.4, we can perform a log-rank test to compare
the survival of males to females. We use the logrank_test() function from logrank_
test()

11.8 Lab: Survival Analysis

491

the lifelines.statistics module. The first two arguments are the event
times, with the second denoting the corresponding (optional) censoring
indicators.
In [9]: logrank_test(by_sex['Male ']['time '],
by_sex['Female ']['time '],
by_sex['Male ']['status '],
by_sex['Female ']['status '])
Out[9]:

t_0
null_distribution
degrees_of_freedom
test_name
test_statistic
p
1.44 0.23

-1
chi squared
1
logrank_test
-log2(p)
2.12

The resulting p-value is 0.23, indicating no evidence of a difference in
survival between the two sexes.
Next, we use the CoxPHFitter() estimator from lifelines to fit Cox
CoxPHFitter()
proportional hazards models. To begin, we consider a model that uses sex
as the only predictor.
In [10]: coxph = CoxPHFitter # shorthand
sex_df = BrainCancer [['time ', 'status ', 'sex']]
model_df = MS(['time ', 'status ', 'sex'],
intercept=False).fit_transform(sex_df)
cox_fit = coxph ().fit(model_df ,
'time ',
'status ')
cox_fit.summary [['coef ', 'se(coef)', 'p']]
Out[10]:

covariate
sex[Male]

coef

se(coef)

p

0.407667

0.342004

0.233263

The first argument to fit should be a data frame containing at least the
event time (the second argument time in this case), as well as an optional censoring variable (the argument status in this case). Note also that
the Cox model does not include an intercept, which is why we used the
intercept=False argument to ModelSpec above. The summary() method delivers many columns; we chose to abbreviate its output here. It is possible
to obtain the likelihood ratio test comparing this model to the one with no
features as follows:
In [11]: cox_fit. log_likelihood_ratio_test ()
Out[11]: null_distribution
degrees_freedom
test_name
test_statistic
p
1.44 0.23

chi squared
1
log -likelihood ratio test
-log2(p)
2.12

492

11. Survival Analysis and Censored Data

Regardless of which test we use, we see that there is no clear evidence for
a difference in survival between males and females. As we learned in this
chapter, the score test from the Cox model is exactly equal to the log rank
test statistic!
Now we fit a model that makes use of additional predictors. We first note
that one of our diagnosis values is missing, hence we drop that observation
before continuing.
In [12]: cleaned = BrainCancer.dropna ()
all_MS = MS(cleaned.columns , intercept=False)
all_df = all_MS.fit_transform(cleaned)
fit_all = coxph ().fit(all_df ,
'time ',
'status ')
fit_all.summary [['coef ', 'se(coef)', 'p']]
Out[12]:

covariate
sex[Male]
diagnosis[LG glioma]
diagnosis[Meningioma]
diagnosis[Other]
loc[Supratentorial]
ki
gtv
stereo[SRT]

coef

se(coef)

p

0.183748
-1.239541
-2.154566
-1.268870
0.441195
-0.054955
0.034293
0.177778

0.360358
0.579557
0.450524
0.617672
0.703669
0.018314
0.022333
0.601578

0.610119
0.032454
0.000002
0.039949
0.530664
0.002693
0.124660
0.767597

The diagnosis variable has been coded so that the baseline corresponds to
HG glioma. The results indicate that the risk associated with HG glioma
is more than eight times (i.e. e2.15 = 8.62) the risk associated with meningioma. In other words, after adjusting for the other predictors, patients
with HG glioma have much worse survival compared to those with meningioma. In addition, larger values of the Karnofsky index, ki, are associated
with lower risk, i.e. longer survival.
Finally, we plot estimated survival curves for each diagnosis category,
adjusting for the other predictors. To make these plots, we set the values of
the other predictors equal to the mean for quantitative variables and equal
to the mode for categorical. To do this, we use the apply() method across
rows (i.e. axis=0) with a function representative that checks if a column
is categorical or not.
In [13]: levels = cleaned['diagnosis ']. unique ()
def representative(series):
if hasattr(series.dtype , 'categories '):
return pd.Series.mode(series)
else:
return series.mean ()
modal_data = cleaned.apply(representative , axis =0)

We make four copies of the column means and assign the diagnosis
column to be the four different diagnoses.
In [14]: modal_df = pd.DataFrame(
[modal_data.iloc [0] for _ in range(len(levels))])
modal_df['diagnosis '] = levels
modal_df

11.8 Lab: Survival Analysis
Out[14]:

sex
Female
Female
Female
Female

diagnosis
Meningioma
HG glioma
LG glioma
Other

loc
Supratentorial
Supratentorial
Supratentorial
Supratentorial

ki
80.920
80.920
80.920
80.920

gtv
8.687
8.687
8.687
8.687

stereo
SRT
SRT
SRT
SRT

493

...
...
...
...
...

We then construct the model matrix based on the model specification

all_MS used to fit the model, and name the rows according to the levels of
diagnosis.
In [15]: modal_X = all_MS.transform(modal_df)
modal_X.index = levels
modal_X

We can use the predict_survival_function() method to obtain the esti- .predict_
mated survival function.
survival_
In [16]: predicted_survival = fit_all. predict_survival_function (modal_X)
predicted_survival
Out[16]:

Meningioma
HG glioma
0.070
0.998
0.982
1.180
0.998
0.982
1.410
0.996
0.963
1.540
0.996
0.963
...
...
...
67.380
0.689
0.040
73.740
0.689
0.040
78.750
0.689
0.040
82.560
0.689
0.040
85 rows × 4 columns

LG glioma
0.995
0.995
0.989
0.989
...
0.394
0.394
0.394
0.394

Other
0.995
0.995
0.990
0.990
...
0.405
0.405
0.405
0.405

This returns a data frame, whose plot methods yields the different survival
curves. To avoid clutter in the plots, we do not display confidence intervals.
In [17]: fig , ax = subplots(figsize =(8, 8))
predicted_survival .plot(ax=ax);

11.8.2

Publication Data

The Publication data presented in Section 11.5.4 can be found in the
ISLP package. We first reproduce Figure 11.5 by plotting the Kaplan-Meier
curves stratified on the posres variable, which records whether the study
had a positive or negative result.
In [18]: fig , ax = subplots(figsize =(8 ,8))
Publication = load_data('Publication ')
by_result = {}
for result , df in Publication.groupby('posres '):
by_result[result] = df
km_result = km.fit(df['time '], df['status '])
km_result.plot(label='Result =%d' % result , ax=ax)

As discussed previously, the p-values from fitting Cox’s proportional hazards model to the posres variable are quite large, providing no evidence
of a difference in time-to-publication between studies with positive versus
negative results.

function()

494

11. Survival Analysis and Censored Data

In [19]: posres_df = MS(['posres ',
'time ',
'status '],
intercept=False).fit_transform(Publication)
posres_fit = coxph ().fit(posres_df ,
'time ',
'status ')
posres_fit.summary [['coef ', 'se(coef)', 'p']]
Out[19]:

covariate
posres

coef

se(coef)

p

0.148076

0.161625

0.359578

However, the results change dramatically when we include other predictors in the model. Here we exclude the funding mechanism variable.
In [20]: model = MS(Publication.columns.drop('mech '),
intercept=False)
coxph ().fit(model.fit_transform(Publication),
'time ',
'status ').summary [['coef ', 'se(coef)', 'p']]
Out[20]:

covariate
posres
multi
clinend
sampsize
budget
impact

coef

se(coef)

p

0.570774
-0.040863
0.546180
0.000005
0.004386
0.058318

0.175960
0.251194
0.262001
0.000015
0.002464
0.006676

1.179606e-03
8.707727e-01
3.710099e-02
7.506978e-01
7.511276e-02
2.426779e-18

We see that there are a number of statistically significant variables, including whether the trial focused on a clinical endpoint, the impact of the
study, and whether the study had positive or negative results.

11.8.3

Call Center Data

In this section, we will simulate survival data using the relationship between
cumulative hazard and the survival function explored in Exercise 8. Our
simulated data will represent the observed wait times (in seconds) for 2,000
customers who have phoned a call center. In this context, censoring occurs
if a customer hangs up before his or her call is answered.
There are three covariates: Operators (the number of call center operators
available at the time of the call, which can range from 5 to 15), Center
(either A, B, or C), and Time of day (Morning, Afternoon, or Evening). We
generate data for these covariates so that all possibilities are equally likely:
for instance, morning, afternoon and evening calls are equally likely, and
any number of operators from 5 to 15 is equally likely.
In [21]: rng = np.random.default_rng (10)
N = 2000
Operators = rng.choice(np.arange (5, 16),
N,
replace=True)

11.8 Lab: Survival Analysis

495

Center = rng.choice (['A', 'B', 'C'],
N,
replace=True)
Time = rng.choice (['Morn.', 'After.', 'Even.'],
N,
replace=True)
D = pd.DataFrame ({'Operators ': Operators ,
'Center ': pd.Categorical(Center),
'Time ': pd.Categorical(Time)})

We then build a model matrix (omitting the intercept)
In [22]: model = MS(['Operators ',
'Center ',
'Time '],
intercept=False)
X = model.fit_transform(D)

It is worthwhile to take a peek at the model matrix X, so that we can be
sure that we understand how the variables have been coded. By default,
the levels of categorical variables are sorted and, as usual, the first column
of the one-hot encoding of the variable is dropped.
In [23]: X[:5]
Out[23]:

0
1
2
3
4

Operators
13
15
7
7
13

Center[B]
0.0
0.0
1.0
0.0
0.0

Center[C]
1.0
0.0
0.0
1.0
1.0

Time[Even .]
0.0
1.0
0.0
0.0
1.0

Time[Morn .]
0.0
0.0
1.0
1.0
0.0

Next, we specify the coefficients and the hazard function.
In [24]: true_beta = np.array ([0.04 , -0.3, 0, 0.2, -0.2])
true_linpred = X.dot(true_beta)
hazard = lambda t: 1e-5 * t

Here, we have set the coefficient associated with Operators to equal 0.04;
in other words, each additional operator leads to a e0.04 = 1.041-fold increase in the “risk” that the call will be answered, given the Center and
Time covariates. This makes sense: the greater the number of operators at
hand, the shorter the wait time! The coefficient associated with Center ==
B is −0.3, and Center == A is treated as the baseline. This means that the
risk of a call being answered at Center B is 0.74 times the risk that it will
be answered at Center A; in other words, the wait times are a bit 
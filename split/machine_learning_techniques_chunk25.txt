e. This
problem is often referred to as the curse of dimensionality.

Fortunately, in real-world problems, it is often possible to reduce the number of fea‐
tures considerably, turning an intractable problem into a tractable one. For example,
consider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐
ders  are  almost  always  white,  so  you  could  completely  drop  these  pixels  from  the
training  set  without  losing  much  information.  Figure  7-6  confirms  that  these  pixels
are utterly unimportant for the classification task. Additionally, two neighboring pix‐
els are often highly correlated: if you merge them into a single pixel (e.g., by taking
the mean of the two pixel intensities), you will not lose much information.

Reducing  dimensionality  does  cause  some  information  loss  (just
like  compressing  an  image  to  JPEG  can  degrade  its  quality),  so
even  though  it  will  speed  up  training,  it  may  make  your  system
perform  slightly  worse.  It  also  makes  your  pipelines  a  bit  more
complex  and  thus  harder  to  maintain.  So,  if  training  is  too  slow,
you  should  first  try  to  train  your  system  with  the  original  data
before considering using dimensionality reduction. In some cases,
reducing  the  dimensionality  of  the  training  data  may  filter  out
some noise and unnecessary details and thus result in higher per‐
formance, but in general it won’t; it will just speed up training.

Apart  from  speeding  up  training,  dimensionality  reduction  is  also  extremely  useful
for data visualization (or DataViz). Reducing the number of dimensions down to two
(or three) makes it possible to plot a condensed view of a high-dimensional training

213

set on a graph and often gain some important insights by visually detecting patterns,
such as clusters. Moreover, DataViz is essential to communicate your conclusions to
people who are not data scientists—in particular, decision makers who will use your
results.

In  this  chapter  we  will  discuss  the  curse  of  dimensionality  and  get  a  sense  of  what
goes on in high-dimensional space. Then, we will consider the two main approaches
to  dimensionality  reduction  (projection  and  Manifold  Learning),  and  we  will  go
through three of the most popular dimensionality reduction techniques: PCA, Kernel
PCA, and LLE.

The Curse of Dimensionality
We are so used to living in three dimensions1 that our intuition fails us when we try
to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to
picture in our minds (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a
1,000-dimensional space.

Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2

It turns out that many things behave very differently in high-dimensional space. For
example, if you pick a random point in a unit square (a 1 × 1 square), it will have only
about a 0.4% chance of being located less than 0.001 from a border (in other words, it
is very unlikely that a random point will be “extreme” along any dimension). But in a
10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most
points in a high-dimensional hypercube are very close to the border.3

1 Well, four dimensions if you count time, and a few more if you are a string theorist.

2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd‐

Boy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.

3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put

in their coffee), if you consider enough dimensions.

214 

| 

Chapter 8: Dimensionality Reduction

Here  is  a  more  troublesome  difference:  if  you  pick  two  points  randomly  in  a  unit
square, the distance between these two points will be, on average, roughly 0.52. If you
pick two random points in a unit 3D cube, the average distance will be roughly 0.66.
But what about two points picked randomly in a 1,000,000-dimensional hypercube?
The average distance, believe it or not, will be about 408.25 (roughly  1, 000, 000/6)!
This is counterintuitive: how can two points be so far apart when they both lie within
the same unit hypercube? Well, there’s just plenty of space in high dimensions. As a
result,  high-dimensional  datasets  are  at  risk  of  being  very  sparse:  most  training
instances  are  likely  to  be  far  away  from  each  other.  This  also  means  that  a  new
instance will likely be far away from any training instance, making predictions much
less reliable than in lower dimensions, since they will be based on much larger extrap‐
olations.  In  short,  the  more  dimensions  the  training  set  has,  the  greater  the  risk  of
overfitting it.

In theory, one solution to the curse of dimensionality could be to increase the size of
the  training  set  to  reach  a  sufficient  density  of  training  instances.  Unfortunately,  in
practice,  the  number  of  training  instances  required  to  reach  a  given  density  grows
exponentially  with  the  number  of  dimensions.  With  just  100  features  (significantly
fewer  than  in  the  MNIST  problem),  you  would  need  more  training  instances  than
atoms  in  the  observable  universe  in  order  for  training  instances  to  be  within  0.1  of
each  other  on  average,  assuming  they  were  spread  out  uniformly  across  all  dimen‐
sions.

Main Approaches for Dimensionality Reduction
Before we dive into specific dimensionality reduction algorithms, let’s take a look at
the  two  main  approaches  to  reducing  dimensionality:  projection  and  Manifold
Learning.

Projection
In most real-world problems, training instances are not spread out uniformly across
all dimensions. Many features are almost constant, while others are highly correlated
(as discussed earlier for MNIST). As a result, all training instances lie within (or close
to) a much lower-dimensional subspace of the high-dimensional space. This sounds
very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D dataset repre‐
sented by circles.

Main Approaches for Dimensionality Reduction 

| 

215

Figure 8-2. A 3D dataset lying close to a 2D subspace

Notice that all training instances lie close to a plane: this is a lower-dimensional (2D)
subspace  of  the  high-dimensional  (3D)  space.  If  we  project  every  training  instance
perpendicularly onto this subspace (as represented by the short lines connecting the
instances  to  the  plane),  we  get  the  new  2D  dataset  shown  in  Figure  8-3.  Ta-da!  We
have just reduced the dataset’s dimensionality from 3D to 2D. Note that the axes cor‐
respond to new features z1 and z2 (the coordinates of the projections on the plane).

Figure 8-3. The new 2D dataset after projection

216 

| 

Chapter 8: Dimensionality Reduction

However, projection is not always the best approach to dimensionality reduction. In
many cases the subspace may twist and turn, such as in the famous Swiss roll toy data‐
set represented in Figure 8-4.

Figure 8-4. Swiss roll dataset

Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of
the Swiss roll together, as shown on the left side of Figure 8-5. What you really want is
to unroll the Swiss roll to obtain the 2D dataset on the right side of Figure 8-5.

Figure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll
(right)

Main Approaches for Dimensionality Reduction 

| 

217

Manifold Learning
The  Swiss  roll  is  an  example  of  a  2D  manifold.  Put  simply,  a  2D  manifold  is  a  2D
shape that can be bent and twisted in a higher-dimensional space. More generally, a
d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally
resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it
locally resembles a 2D plane, but it is rolled in the third dimension.

Many dimensionality reduction algorithms work by modeling the manifold on which
the  training  instances  lie;  this  is  called  Manifold  Learning.  It  relies  on  the  manifold
assumption,  also  called  the  manifold  hypothesis,  which  holds  that  most  real-world
high-dimensional  datasets  lie  close  to  a  much  lower-dimensional  manifold.  This
assumption is very often empirically observed.

Once again, think about the MNIST dataset: all handwritten digit images have some
similarities.  They  are  made  of  connected  lines,  the  borders  are  white,  and  they  are
more  or  less  centered.  If  you  randomly  generated  images,  only  a  ridiculously  tiny
fraction  of  them  would  look  like  handwritten  digits.  In  other  words,  the  degrees  of
freedom available to you if you try to create a digit image are dramatically lower than
the  degrees  of  freedom  you  would  have  if  you  were  allowed  to  generate  any  image
you wanted. These constraints tend to squeeze the dataset into a lower-dimensional
manifold.

The manifold assumption is often accompanied by another implicit assumption: that
the task at hand (e.g., classification or regression) will be simpler if expressed in the
lower-dimensional space of the manifold. For example, in the top row of Figure 8-6
the  Swiss  roll  is  split  into  two  classes:  in  the  3D  space  (on  the  left),  the  decision
boundary  would  be  fairly  complex,  but  in  the  2D  unrolled  manifold  space  (on  the
right), the decision boundary is a straight line.

However, this implicit assumption does not always hold. For example, in the bottom
row of Figure 8-6, the decision boundary is located at x1 = 5. This decision boundary
looks very simple in the original 3D space (a vertical plane), but it looks more com‐
plex in the unrolled manifold (a collection of four independent line segments).

In short, reducing the dimensionality of your training set before training a model will
usually speed up training, but it may not always lead to a better or simpler solution; it
all depends on the dataset.

Hopefully you now have a good sense of what the curse of dimensionality is and how
dimensionality  reduction  algorithms  can  fight  it,  especially  when  the  manifold
assumption holds. The rest of this chapter will go through some of the most popular
algorithms.

218 

| 

Chapter 8: Dimensionality Reduction

Figure 8-6. The decision boundary may not always be simpler with lower dimensions

PCA
Principal Component Analysis (PCA) is by far the most popular dimensionality reduc‐
tion algorithm. First it identifies the hyperplane that lies closest to the data, and then
it projects the data onto it, just like in Figure 8-2.

Preserving the Variance
Before  you  can  project  the  training  set  onto  a  lower-dimensional  hyperplane,  you
first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐
sented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes).
On the right is the result of the projection of the dataset onto each of these axes. As
you can see, the projection onto the solid line preserves the maximum variance, while
the  projection  onto  the  dotted  line  preserves  very  little  variance  and  the  projection
onto the dashed line preserves an intermediate amount of variance.

PCA 

| 

219

Figure 8-7. Selecting the subspace to project on

It  seems  reasonable  to  select  the  axis  that  preserves  the  maximum  amount  of  var‐
iance, as it will most likely lose less information than the other projections. Another
way to justify this choice is that it is the axis that minimizes the mean squared dis‐
tance between the original dataset and its projection onto that axis. This is the rather
simple idea behind PCA.4

Principal Components
PCA identifies the axis that accounts for the largest amount of variance in the train‐
ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the
first  one,  that  accounts  for  the  largest  amount  of  remaining  variance.  In  this  2D
example there is no choice: it is the dotted line. If it were a higher-dimensional data‐
set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,
a fifth, and so on—as many axes as the number of dimensions in the dataset.

The  ith  axis  is  called  the  ith  principal  component  (PC)  of  the  data.  In  Figure  8-7,  the
first  PC  is  the  axis  on  which  vector  c1  lies,  and  the  second  PC  is  the  axis  on  which
vector  c2  lies.  In  Figure  8-2  the  first  two  PCs  are  the  orthogonal  axes  on  which  the
two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane.

4 Karl Pearson, “On Lines and Planes of Closest Fit to Systems of Points in Space,” The London, Edinburgh, and

Dublin Philosophical Magazine and Journal of Science 2, no. 11 (1901): 559-572, https://homl.info/pca.

220 

| 

Chapter 8: Dimensionality Reduction

For each principal component, PCA finds a zero-centered unit vec‐
tor  pointing  in  the  direction  of  the  PC.  Since  two  opposing  unit
vectors  lie  on  the  same  axis,  the  direction  of  the  unit  vectors
returned  by  PCA  is  not  stable:  if  you  perturb  the  training  set
slightly and run PCA again, the unit vectors may point in the oppo‐
site  direction  as  the  original  vectors.  However,  they  will  generally
still lie on the same axes. In some cases, a pair of unit vectors may
even rotate or swap (if the variances along these two axes are close),
but the plane they define will generally remain the same.

So  how  can  you  find  the  principal  components  of  a  training  set?  Luckily,  there  is  a
standard  matrix  factorization  technique  called  Singular  Value  Decomposition  (SVD)
that can decompose the training set matrix X into the matrix multiplication of three
matrices U Σ V⊺, where V contains the unit vectors that define all the principal com‐
ponents that we are looking for, as shown in Equation 8-1.

Equation 8-1. Principal components matrix

V =

∣
∣
c1 c2
∣
∣

∣
⋯ cn
∣

The following Python code uses NumPy’s svd() function to obtain all the principal
components of the training set, then extracts the two unit vectors that define the first
two PCs:

X_centered = X - X.mean(axis=0)
U, s, Vt = np.linalg.svd(X_centered)
c1 = Vt.T[:, 0]
c2 = Vt.T[:, 1]

PCA assumes that the dataset is centered around the origin. As we
will  see,  Scikit-Learn’s  PCA  classes  take  care  of  centering  the  data
for you. If you implement PCA yourself (as in the preceding exam‐
ple),  or  if  you  use  other  libraries,  don’t  forget  to  center  the  data
first.

Projecting Down to d Dimensions
Once  you  have  identified  all  the  principal  components,  you  can  reduce  the  dimen‐
sionality  of  the  dataset  down  to  d  dimensions  by  projecting  it  onto  the  hyperplane
defined by the first d principal components. Selecting this hyperplane ensures that the
projection will preserve as much variance as possible. For example, in Figure 8-2 the
3D  dataset  is  projected  down  to  the  2D  plane  defined  by  the  first  two  principal

PCA 

| 

221

components, preserving a large part of the dataset’s variance. As a result, the 2D pro‐
jection looks very much like the original 3D dataset.

To project the training set onto the hyperplane and
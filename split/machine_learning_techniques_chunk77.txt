on of all the activations in the coding layer (left) and distribution
of the mean activation per neuron (right)

Sparse Autoencoders 

| 

585

Variational Autoencoders
Another  important  category  of  autoencoders  was  introduced  in  2013  by  Diederik
Kingma  and  Max  Welling  and  quickly  became  one  of  the  most  popular  types  of
autoencoders: variational autoencoders.7

They are quite different from all the autoencoders we have discussed so far, in these
particular ways:

• They are probabilistic autoencoders, meaning that their outputs are partly deter‐
mined  by  chance,  even  after  training  (as  opposed  to  denoising  autoencoders,
which use randomness only during training).

• Most importantly, they are generative autoencoders, meaning that they can gener‐

ate new instances that look like they were sampled from the training set.

Both these properties make them rather similar to RBMs, but they are easier to train,
and the sampling process is much faster (with RBMs you need to wait for the network
to  stabilize  into  a  “thermal  equilibrium”  before  you  can  sample  a  new  instance).
Indeed,  as  their  name  suggests,  variational  autoencoders  perform  variational  Baye‐
sian  inference  (introduced  in  Chapter  9),  which  is  an  efficient  way  to  perform
approximate Bayesian inference.

Let’s  take  a  look  at  how  they  work.  Figure  17-12  (left)  shows  a  variational  autoen‐
coder. You can recognize the basic structure of all autoencoders, with an encoder fol‐
lowed by a decoder (in this example, they both have two hidden layers), but there is a
twist: instead of directly producing a coding for a given input, the encoder produces a
mean  coding  μ  and  a  standard  deviation  σ.  The  actual  coding  is  then  sampled  ran‐
domly from a Gaussian distribution with mean μ and standard deviation σ. After that
the  decoder  decodes  the  sampled  coding  normally.  The  right  part  of  the  diagram
shows  a  training  instance  going  through  this  autoencoder.  First,  the  encoder  pro‐
duces μ and σ, then a coding is sampled randomly (notice that it is not exactly located
at  μ),  and  finally  this  coding  is  decoded;  the  final  output  resembles  the  training
instance.

7 Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv preprint arXiv:1312.6114

(2013).

586 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

Figure 17-12. Variational autoencoder (left) and an instance going through it (right)

As you can see in the diagram, although the inputs may have a very convoluted distri‐
bution, a variational autoencoder tends to produce codings that look as though they
were sampled from a simple Gaussian distribution:8 during training, the cost function
(discussed  next)  pushes  the  codings  to  gradually  migrate  within  the  coding  space
(also called the latent space) to end up looking like a cloud of Gaussian points. One
great consequence is that after training a variational autoencoder, you can very easily
generate  a  new  instance:  just  sample  a  random  coding  from  the  Gaussian  distribu‐
tion, decode it, and voilà!

Now, let’s look at the cost function. It is composed of two parts. The first is the usual
reconstruction loss that pushes the autoencoder to reproduce its inputs (we can use
cross entropy for this, as discussed earlier). The second is the latent loss that pushes
the autoencoder to have codings that look as though they were sampled from a simple
Gaussian distribution: it is the KL divergence between the target distribution (i.e., the
Gaussian  distribution)  and  the  actual  distribution  of  the  codings.  The  math  is  a  bit
more complex than with the sparse autoencoder, in particular because of the Gaus‐
sian  noise,  which  limits  the  amount  of  information  that  can  be  transmitted  to  the
coding  layer  (thus  pushing  the  autoencoder  to  learn  useful  features).  Luckily,  the

8 Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions.

Variational Autoencoders 

| 

587

equations  simplify,  so  the  latent  loss  can  be  computed  quite  simply  using  Equation
17-3:9

Equation 17-3. Variational autoencoder’s latent loss

ℒ = −

K

1
2 ∑

i = 1

1 + log σi

2 − σi

2 − μi

2

In this equation, ℒ is the latent loss, n is the codings’ dimensionality, and μi and σi are
the mean and standard deviation of the ith component of the codings. The vectors μ
and  σ  (which  contain  all  the  μi  and  σi)  are  output  by  the  encoder,  as  shown  in
Figure 17-12 (left).

A common tweak to the variational autoencoder’s architecture is to make the encoder
output γ = log(σ2) rather than σ. The latent loss can then be computed as shown in
Equation 17-4. This approach is more numerically stable and speeds up training.

Equation 17-4. Variational autoencoder’s latent loss, rewritten using γ = log(σ2)

ℒ = −

K

1
2 ∑

i = 1

1 + γi − exp γi − μi

2

Let’s  start  building  a  variational  autoencoder  for  Fashion  MNIST  (as  shown  in
Figure 17-12, but using the γ tweak). First, we will need a custom layer to sample the
codings, given μ and γ:

class Sampling(keras.layers.Layer):
    def call(self, inputs):
        mean, log_var = inputs
        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean

This Sampling layer takes two inputs: mean (μ) and log_var (γ). It uses the function
K.random_normal()  to  sample  a  random  vector  (of  the  same  shape  as  γ)  from  the
Normal distribution, with mean 0 and standard deviation 1. Then it multiplies it by
exp(γ / 2) (which is equal to σ, as you can verify), and finally it adds μ and returns the
result. This samples a codings vector from the Normal distribution with mean μ and
standard deviation σ.

Next, we can create the encoder, using the Functional API because the model is not
entirely sequential:

9 For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch’s

great tutorial (2016).

588 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

codings_size = 10

inputs = keras.layers.Input(shape=[28, 28])
z = keras.layers.Flatten()(inputs)
z = keras.layers.Dense(150, activation="selu")(z)
z = keras.layers.Dense(100, activation="selu")(z)
codings_mean = keras.layers.Dense(codings_size)(z)  # μ
codings_log_var = keras.layers.Dense(codings_size)(z)  # γ
codings = Sampling()([codings_mean, codings_log_var])
variational_encoder = keras.Model(
    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])

Note  that  the  Dense  layers  that  output  codings_mean  (μ)  and  codings_log_var  (γ)
have the same inputs (i.e., the outputs of the second Dense layer). We then pass both
codings_mean  and  codings_log_var  to  the  Sampling  layer.  Finally,  the  varia
tional_encoder  model  has  three  outputs,  in  case  you  want  to  inspect  the  values  of
codings_mean and codings_log_var. The only output we will use is the last one (cod
ings). Now let’s build the decoder:

decoder_inputs = keras.layers.Input(shape=[codings_size])
x = keras.layers.Dense(100, activation="selu")(decoder_inputs)
x = keras.layers.Dense(150, activation="selu")(x)
x = keras.layers.Dense(28 * 28, activation="sigmoid")(x)
outputs = keras.layers.Reshape([28, 28])(x)
variational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])

For  this  decoder,  we  could  have  used  the  Sequential  API  instead  of  the  Functional
API, since it is really just a simple stack of layers, virtually identical to many of the
decoders we have built so far. Finally, let’s build the variational autoencoder model:

_, _, codings = variational_encoder(inputs)
reconstructions = variational_decoder(codings)
variational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])

Note  that  we  ignore  the  first  two  outputs  of  the  encoder  (we  only  want  to  feed  the
codings  to  the  decoder).  Lastly,  we  must  add  the  latent  loss  and  the  reconstruction
loss:

latent_loss = -0.5 * K.sum(
    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),
    axis=-1)
variational_ae.add_loss(K.mean(latent_loss) / 784.)
variational_ae.compile(loss="binary_crossentropy", optimizer="rmsprop")

We first apply Equation 17-4 to compute the latent loss for each instance in the batch
(we sum over the last axis). Then we compute the mean loss over all the instances in
the batch, and we divide the result by 784 to ensure it has the appropriate scale com‐
pared to the reconstruction loss. Indeed, the variational autoencoder’s reconstruction
loss  is  supposed  to  be  the  sum  of  the  pixel  reconstruction  errors,  but  when  Keras
computes the "binary_crossentropy" loss, it computes the mean over all 784 pixels,

Variational Autoencoders 

| 

589

rather than the sum. So, the reconstruction loss is 784 times smaller than we need it
to be. We could define a custom loss to compute the sum rather than the mean, but it
is simpler to divide the latent loss by 784 (the final loss will be 784 times smaller than
it should be, but this just means that we should use a larger learning rate).

Note that we use the RMSprop optimizer, which works well in this case. And finally we
can train the autoencoder!

history = variational_ae.fit(X_train, X_train, epochs=50, batch_size=128,
                             validation_data=[X_valid, X_valid])

Generating Fashion MNIST Images
Now  let’s  use  this  variational  autoencoder  to  generate  images  that  look  like  fashion
items. All we need to do is sample random codings from a Gaussian distribution and
decode them:

codings = tf.random.normal(shape=[12, codings_size])
images = variational_decoder(codings).numpy()

Figure 17-13 shows the 12 generated images.

Figure 17-13. Fashion MNIST images generated by the variational autoencoder

The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not
great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn!
Give it a bit more fine-tuning and training time, and those images should look better.

Variational autoencoders make it possible to perform semantic interpolation: instead
of interpolating two images at the pixel level (which would look as if the two images
were  overlaid),  we  can  interpolate  at  the  codings  level.  We  first  run  both  images
through  the  encoder,  then  we  interpolate  the  two  codings  we  get,  and  finally  we
decode the interpolated codings to get the final image. It will look like a regular Fash‐
ion MNIST image, but it will be an intermediate between the original images. In the
following code example, we take the 12 codings we just generated, we organize them

590 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

in a 3 × 4 grid, and we use TensorFlow’s tf.image.resize() function to resize this
grid to 5 × 7. By default, the resize() function will perform bilinear interpolation, so
every  other  row  and  column  will  contain  interpolated  codings.  We  then  use  the
decoder to produce all the images:

codings_grid = tf.reshape(codings, [1, 3, 4, codings_size])
larger_grid = tf.image.resize(codings_grid, size=[5, 7])
interpolated_codings = tf.reshape(larger_grid, [-1, codings_size])
images = variational_decoder(interpolated_codings).numpy()

Figure 17-14 shows the resulting images. The original images are framed, and the rest
are the result of semantic interpolation between the nearby images. Notice, for exam‐
ple, how the shoe in the fourth row and fifth column is a nice interpolation between
the two shoes located above and below it.

Figure 17-14. Semantic interpolation

For several years, variational autoencoders were quite popular, but GANs eventually
took the lead, in particular because they are capable of generating much more realistic
and crisp images. So let’s turn our attention to GANs.

Variational Autoencoders 

| 

591

Generative Adversarial Networks
Generative adversarial networks were proposed in a 2014 paper10 by Ian Goodfellow
et  al.,  and  although  the  idea  got  researchers  excited  almost  instantly,  it  took  a  few
years to overcome some of the difficulties of training GANs. Like many great ideas, it
seems simple in hindsight: make neural networks compete against each other in the
hope that this competition will push them to excel. As shown in Figure 17-15, a GAN
is composed of two neural networks:

Generator

Takes a random distribution as input (typically Gaussian) and outputs some data
—typically, an image. You can think of the random inputs as the latent represen‐
tations (i.e., codings) of the image to be generated. So, as you can see, the genera‐
tor offers the same functionality as a decoder in a variational autoencoder, and it
can be used in the same way to generate new images (just feed it some Gaussian
noise, and it outputs a brand-new image). However, it is trained very differently,
as we will soon see.

Discriminator

Takes either a fake image from the generator or a real image from the training set
as input, and must guess whether the input image is fake or real.

Figure 17-15. A generative adversarial network

10 Ian Goodfellow et al., “Generative Adversarial Nets,” Proceedings of the 27th International Conference on Neu‐

ral Information Processing Systems 2 (2014): 2672–2680.

592 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

During  training,  the  generator  and  the  discriminator  have  opposite  goals:  the  dis‐
criminator tries to tell fake images from real images, while the generator tries to pro‐
duce  images  that  look  real  enough  to  trick  the  discriminator.  Because  the  GAN  is
composed of two networks with different objectives, it cannot be trained like a regu‐
lar neural network. Each training iteration is divided into two phases:

• In the first phase, we train the discriminator. A batch of real images is sampled
from the training set and is completed with an equal number of fake images pro‐
duced  by  the  generator.  The  labels  are  set  to  0  for  fake  images  and  1  for  real
images, and the discriminator is trained on this labeled batch for one step, using
the  binary  cross-entropy  loss.  Importantly,  backpropagation  only  optimizes  the
weights of the discriminator during this phase.

• In  the  second  phase,  we  train  the  generator.  We  first  use  it  to  produce  another
batch of fake images, and once again the discriminator is used to tell whether the
images are fake or real. This time we do not add real images in the batch, and all
the  labels  are  set  to  1  (real):  in  other  words,  we  want  the  generator  to  produce
images  that  the  discriminator  will  (wrongly)  believe  to  be  real!  Crucially,  the
weights of the discriminator are frozen during this step, so backpropagation only
affects the weights of the generator.

The generator never actually sees any real images, yet it gradually
learns  to  produce  convincing  fake  images!  All  it  gets  is  the  gradi‐
ents flowing back through the discriminator. Fortunately, the better
the discriminator gets, the more information about the real images
is  contained  in  these  secondhand  gradients,  so  the  generator  can
make significant progres
 with these toolkits it is possible to build web-scale language models
using advanced smoothing algorithms like the Kneser-Ney algorithm we will see in
Section 3.8, Brants et al. (2007) show that with very large language models a much
simpler algorithm may be sufﬁcient. The algorithm is called stupid backoff. Stupid
backoff gives up the idea of trying to make the language model a true probability dis-
tribution. There is no discounting of the higher-order probabilities. If a higher-order
n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a
ﬁxed (context-independent) weight. This algorithm does not produce a probability

Bloom ﬁlters

stupid backoff

3.8

• ADVANCED: KNESER-NEY SMOOTHING

51

distribution, so we’ll follow Brants et al. (2007) in referring to it as S:

count(wi
count(wi
λ S(wi|
wi

N+1 : i)
−
1)
N+1 : i
−
1)




S(wi|

wi

N+1 : i

1) =

−

−

−
N+2 : i
−
The backoff terminates in the unigram, which has score S(w) = count(w)
(2007) ﬁnd that a value of 0.4 worked well for λ .

otherwise



−

N

if count(wi

N+1 : i) > 0

−

(3.30)

. Brants et al.

3.8 Advanced: Kneser-Ney Smoothing

Kneser-Ney

A popular advanced n-gram smoothing method is the interpolated Kneser-Ney al-
gorithm (Kneser and Ney 1995, Chen and Goodman 1998).

3.8.1 Absolute Discounting

Kneser-Ney has its roots in a method called absolute discounting. Recall that dis-
counting of the counts for frequent n-grams is necessary to save some probability
mass for the smoothing algorithm to distribute to the unseen n-grams.

To see this, we can use a clever idea from Church and Gale (1991). Consider
an n-gram that has count 4. We need to discount this count by some amount. But
how much should we discount it? Church and Gale’s clever idea was to look at a
held-out corpus and just see what the count is for all those bigrams that had count
4 in the training set. They computed a bigram grammar from 22 million words of
AP newswire and then checked the counts of each of these bigrams in another 22
million words. On average, a bigram that occurred 4 times in the ﬁrst 22 million
words occurred 3.23 times in the next 22 million words. Fig. 3.9 from Church and
Gale (1991) shows these counts for bigrams with c from 0 to 9.

Bigram count in
training set
0
1
2
3
4
5
6
7
8
9

Bigram count in
heldout set
0.0000270
0.448
1.25
2.24
3.23
4.21
5.23
6.21
7.21
8.26

Figure 3.9 For all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the
counts of these bigrams in a held-out corpus also of 22 million words.

absolute
discounting

Notice in Fig. 3.9 that except for the held-out counts for 0 and 1, all the other
bigram counts in the held-out set could be estimated pretty well by just subtracting
0.75 from the count in the training set! Absolute discounting formalizes this intu-
ition by subtracting a ﬁxed (absolute) discount d from each count. The intuition is
that since we have good estimates already for the very high counts, a small discount

52 CHAPTER 3

• N-GRAM LANGUAGE MODELS

d won’t affect them much. It will mainly modify the smaller counts, for which we
don’t necessarily trust the estimate anyway, and Fig. 3.9 suggests that in practice this
discount is actually a good one for bigrams with counts 2 through 9. The equation
for interpolated absolute discounting applied to bigrams:

PAbsoluteDiscounting(wi|

wi

−

1) =

C(wi

1wi)
−
v C(wi

d
−
1 v)

−

+ λ (wi

1)P(wi)

−

(3.31)

The ﬁrst term is the discounted bigram, with 0
1, and the second term is the
(cid:80)
unigram with an interpolation weight λ . By inspection of Fig. 3.9, it looks like just
setting all the d values to .75 would work very well, or perhaps keeping a separate
second discount value of 0.5 for the bigrams with counts of 1. There are principled
methods for setting d; for example, Ney et al. (1994) set d as a function of n1 and
n2, the number of unigrams that have a count of 1 and a count of 2, respectively:

≤

≤

d

d =

n1
n1 + 2n2

(3.32)

3.8.2 Kneser-Ney Discounting

Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discounting
with a more sophisticated way to handle the lower-order unigram distribution. Con-
sider the job of predicting the next word in this sentence, assuming we are interpo-
lating a bigram and a unigram model.

I can’t see without my reading

.

The word glasses seems much more likely to follow here than, say, the word
Kong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is
more common, since Hong Kong is a very frequent word. A standard unigram model
will assign Kong a higher probability than glasses. We would like to capture the
intuition that although Kong is frequent, it is mainly only frequent in the phrase Hong
Kong, that is, after the word Hong. The word glasses has a much wider distribution.
In other words, instead of P(w), which answers the question “How likely is
w?”, we’d like to create a unigram model that we might call PCONTINUATION, which
answers the question “How likely is w to appear as a novel continuation?”. How can
we estimate this probability of seeing the word w as a novel continuation, in a new
unseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION
on the number of different contexts word w has appeared in, that is, the number of
bigram types it completes. Every bigram type was a novel continuation the ﬁrst time
it was seen. We hypothesize that words that have appeared in more contexts in the
past are more likely to appear in some new context as well. The number of times a
word w appears as a novel continuation can be expressed as:

PCONTINUATION(w) ∝

v : C(vw) > 0

}|

|{

(3.33)

To turn this count into a probability, we normalize by the total number of word
bigram types. In summary:

PCONTINUATION(w) =

|{

v : C(vw) > 0
(u(cid:48), w(cid:48)) : C(u(cid:48)w(cid:48)) > 0

}|

|{

}|

(3.34)

An equivalent formulation based on a different metaphor is to use the number of
word types seen to precede w (Eq. 3.33 repeated):

PCONTINUATION(w) ∝

v : C(vw) > 0

}|

|{

(3.35)

3.8

• ADVANCED: KNESER-NEY SMOOTHING

53

normalized by the number of words preceding all words, as follows:

PCONTINUATION(w) = |{

v : C(vw) > 0

}|
v : C(vw(cid:48)) > 0

w(cid:48) |{

}|

(3.36)

(cid:80)
A frequent word (Kong) occurring in only one context (Hong) will have a low con-
tinuation probability.

The ﬁnal equation for Interpolated Kneser-Ney smoothing for bigrams is then:

Interpolated
Kneser-Ney

PKN(wi|

wi

1) =

−

max(C(wi

−
C(wi

1wi)
1)

−

d, 0)

−

+ λ (wi

1)PCONTINUATION(wi)
−

(3.37)

The λ is a normalizing constant that is used to distribute the probability mass we’ve
discounted:

λ (wi

1) =

−

d
v C(wi

1v) |{

−

w : C(wi

1w) > 0

−

}|

(3.38)

The ﬁrst term,

d
v C(wi

(cid:80)
, is the normalized discount (the discount d, 0

d

1v)

≤
1, was introduced in the absolute discounting section above). The second term,
1 or, equiva-
|{
lently, the number of word types that we discounted; in other words, the number of
times we applied the normalized discount.

, is the number of word types that can follow wi

(cid:80)
1w) > 0

w : C(wi

}|

≤

−

−

−

The general recursive formulation is as follows:

PKN(wi|

wi

n+1:i

−

1) =

−

max(cKN(w i
−
v cKN(wi

n+1: i)

d, 0)

−
1 v)

−

n+1:i

−

+ λ (wi

n+1:i

−

1)PKN(wi|

wi

−

−

n+2:i

1)

−

(3.39)

(cid:80)

where the deﬁnition of the count cKN depends on whether we are counting the
highest-order n-gram being interpolated (for example trigram if we are interpolating
trigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram
if we are interpolating trigram, bigram, and unigram):

) =
cKN(
·

count(
)
for the highest order
·
)
continuationcount(
·

for lower orders

(cid:26)

(3.40)

The continuation count of a string
that string

.
·

is the number of unique single word contexts for

·

At the termination of the recursion, unigrams are interpolated with the uniform

distribution, where the parameter (cid:15) is the empty string:

PKN(w) =

max(cKN(w)

d, 0)

−
cKN(w(cid:48))

w(cid:48)

+ λ ((cid:15))

1
V

(3.41)

modiﬁed
Kneser-Ney

If we want to include an unknown word <UNK>, it’s just included as a regular vo-
cabulary entry with count zero, and hence its probability will be a lambda-weighted
uniform distribution λ ((cid:15))
V .

The best performing version of Kneser-Ney smoothing is called modiﬁed Kneser-

(cid:80)

Ney smoothing, and is due to Chen and Goodman (1998). Rather than use a single
ﬁxed discount d, modiﬁed Kneser-Ney uses three different discounts d1, d2, and
d3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and
Goodman (1998, p. 19) or Heaﬁeld et al. (2013) for the details.

54 CHAPTER 3

• N-GRAM LANGUAGE MODELS

3.9 Advanced: Perplexity’s Relation to Entropy

We introduced perplexity in Section 3.3 as a way to evaluate n-gram models on
a test set. A better n-gram model is one that assigns a higher probability to the
test data, and perplexity is a normalized version of the probability of the test set.
The perplexity measure actually arises from the information-theoretic concept of
cross-entropy, which explains otherwise mysterious properties of perplexity (why
the inverse probability, for example?) and its relationship to entropy. Entropy is a
measure of information. Given a random variable X ranging over whatever we are
predicting (words, letters, parts of speech, the set of which we’ll call χ) and with a
particular probability function, call it p(x), the entropy of the random variable X is:

Entropy

H(X) =

−

χ
x
(cid:88)
∈

p(x) log2 p(x)

(3.42)

The log can, in principle, be computed in any base. If we use log base 2, the

resulting value of entropy will be measured in bits.

One intuitive way to think about entropy is as a lower bound on the number of
bits it would take to encode a certain decision or piece of information in the optimal
coding scheme.

Consider an example from the standard information theory textbook Cover and
Thomas (1991). Imagine that we want to place a bet on a horse race but it is too
far to go all the way to Yonkers Racetrack, so we’d like to send a short message to
the bookie to tell him which of the eight horses to bet on. One way to encode this
message is just to use the binary representation of the horse’s number as the code;
thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded
as 000. If we spend the whole day betting and each horse is coded with 3 bits, on
average we would be sending 3 bits per race.

Can we do better? Suppose that the spread is the actual distribution of the bets

placed and that we represent it as the prior probability of each horse as follows:

Horse 5 1
Horse 1 1
64
2
Horse 6 1
Horse 2 1
64
4
Horse 7 1
Horse 3 1
64
8
16 Horse 8 1
Horse 4 1
64

The entropy of the random variable X that ranges over horses gives us a lower

bound on the number of bits and is

H(X) =

i=8

p(i) log2 p(i)

−

(cid:88)i=1
=
1
2 log2
−
= 2 bits

1
2 −

1
4 log2

1
4 −

1
8 log2

1
8 −

1
16 log2

1
16 −

4( 1

64 log2

1
64 )

(3.43)

A code that averages 2 bits per race can be built with short encodings for more
probable horses, and longer encodings for less probable horses. For example, we
could encode the most likely horse with the code 0, and the remaining horses as 10,
then 110, 1110, 111100, 111101, 111110, and 111111.

3.9

• ADVANCED: PERPLEXITY’S RELATION TO ENTROPY

55

What if the horses are equally likely? We saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3.
Is the entropy the same? In this case each horse would have a
probability of 1

8 . The entropy of the choice of horses is then

H(X) =

i=8

−

(cid:88)i=1

1
8

log2

1
8

=

−

log2

1
8

= 3 bits

(3.44)

Until now we have been computing the entropy of a single variable. But most
of what we will use entropy for involves sequences. For a grammar, for example,
w1, w2, . . . , wn}
we will be computing the entropy of some sequence of words W =
.
One way to do this is to have a variable that ranges over sequences of words. For
example we can compute the entropy of a random variable that ranges over all ﬁnite
sequences of words of length n in some language L as follows:

{

H(w1, w2, . . . , wn) =

p(w1 : n) log p(w1 : n)

(3.45)

−

L
(cid:88)w1 : n∈
We could deﬁne the entropy rate (we could also think of this as the per-word

entropy) as the entropy of this sequence divided by the number of words:

1
n

H(w1 : n) =

1
n

−

L
(cid:88)w1 : n∈

p(w1 : n) log p(w1 : n)

(3.46)

But to measure the true entropy of a language, we need to consider sequences of
inﬁnite length. If we think of a language as a stochastic process L that produces a
sequence of words, and allow W to represent the sequence of words w1, . . . , wn, then
L’s entropy rate H(L) is deﬁned as

H(L) = lim
∞
n
→

1
n

H(w1, w2, . . . , wn)

=

lim
∞
n
→

−

p(w1, . . . , wn) log p(w1, . . . , wn)

(3.47)

1
n

L
(cid:88)W
∈

The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and
Thomas 1991) states that if the language is regular in certain ways (to be exact, if it
is both stationary and ergodic),

H(L) = lim
→

n

∞ −

1
n

log p(w1w2 . . . wn)

(3.48)

That is, we can take a single sequence that is long enough instead of summing
over all possible sequences. The intuition of the Shannon-McMillan-Breiman the-
orem is that a long-enough sequence of words will contain in it many other shorter
sequences and that each of these shorter sequences will reoccur in the longer se-
quence according to their probabilities.

A stochastic process is said to be stationary if the probabilities it assigns to a
sequence are invariant with respect to shifts in the time index. In other words, the
probability distribution for words at time t is the same as the probability distribution
at time t + 1. Markov models, and hence n-grams, are stationary. For example, in
a bigram, Pi is dependent only on Pi
1. So if we shift our time index by x, Pi+x is
still dependent on Pi+x
1. But natural language is not stationary, since as we show

−

−

entropy rate

Stationary

56 CHAPTER 3

• N-GRAM LANGUAGE MODELS

cross-entropy

in Appendix D, the probability of upcoming words can be dependent on events that
were arbitrarily distant and time dependent. Thus, our statistical models only give
an approximation to the correct distributions and entropies of natural language.

To summarize, by making some incorrect but convenient simplifying assump-
tions, we can compute the entropy of some stochastic process by taking a very long
sample of the output and computing its average log probability.

Now we are ready to introduce cross-entropy. The cross-entropy is useful when
we don’t know the actual probability distribution p that generated some data.
It
allows us to use some m, which is a model of p (i.e., an approximation to p). The
cross-entropy of m on p is deﬁned by

H(p, m) = lim
n
→

∞ −

1
n

L
(cid:88)W
∈

p(w1, . . . , wn) log m(w1, . . . , wn)

(3.49)

That is, we draw sequences according to the probability distribution p, but sum

the log of their probabilities according to m.

Again, following the Shannon-McMillan-Breiman theorem, for a stationary er-

godic process:

H(p, m) = lim
n
→

∞ −

1
n

log m(w1w2 . . . wn)

(3.50)

This means that, as for entropy, we can estimate the cross-entropy of a model
m on some distribution p by t
ion(X_age_bh)
bounds_age = preds.conf_int(alpha =0.05)

320

7. Moving Beyond Linearity

partial_age = preds.predicted_mean
center = partial_age.mean ()
partial_age -= center
bounds_age -= center
fig , ax = subplots(figsize =(8 ,8))
ax.plot(age_grid , partial_age , 'b', linewidth =3)
ax.plot(age_grid , bounds_age [:,0], 'r--', linewidth =3)
ax.plot(age_grid , bounds_age [:,1], 'r--', linewidth =3)
ax.set_xlabel('Age')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of age on wage ', fontsize =20);

Let’s explain in some detail what we did above. The idea is to create a new
prediction matrix, where all but the columns belonging to age are constant
(and set to their training-data means). The four columns for age are filled
in with the natural spline basis evaluated at the 100 values in age_grid.
1. We made a grid of length 100 in age, and created a matrix X_age_bh
with 100 rows and the same number of columns as X_bh.
2. We replaced every row of this matrix with the column means of the
original.
3. We then replace just the first four columns representing age with the
natural spline basis computed at the values in age_grid.
The remaining steps should by now be familiar.
We also look at the effect of year on wage; the process is the same.
In [30]: year_grid = np.linspace (2003 , 2009, 100)
year_grid = np.linspace(Wage['year '].min(),
Wage['year '].max(),
100)
X_year_bh = X_bh.copy () [:100]
X_year_bh [:] = X_bh [:]. mean (0)[None ,:]
X_year_bh [: ,4:9] = ns_year.transform(year_grid)
preds = gam_bh.get_prediction(X_year_bh)
bounds_year = preds.conf_int(alpha =0.05)
partial_year = preds.predicted_mean
center = partial_year.mean ()
partial_year -= center
bounds_year -= center
fig , ax = subplots(figsize =(8 ,8))
ax.plot(year_grid , partial_year , 'b', linewidth =3)
ax.plot(year_grid , bounds_year [:,0], 'r--', linewidth =3)
ax.plot(year_grid , bounds_year [:,1], 'r--', linewidth =3)
ax.set_xlabel('Year ')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of year on wage ', fontsize =20);

We now fit the model (7.16) using smoothing splines rather than natural splines. All of the terms in (7.16) are fit simultaneously, taking each
other into account to explain the response. The pygam package only works
with matrices, so we must convert the categorical series education to its
array representation, which can be found with the cat.codes attribute of
education. As year only has 7 unique values, we use only seven basis functions for it.

7.8 Lab: Non-Linear Modeling

321

In [31]: gam_full = LinearGAM(s_gam (0) +
s_gam(1, n_splines =7) +
f_gam(2, lam =0))
Xgam = np.column_stack ([age ,
Wage['year '],
Wage['education '].cat.codes ])
gam_full = gam_full.fit(Xgam , y)

The two s_gam() terms result in smoothing spline fits, and use a default
value for λ (lam=0.6), which is somewhat arbitrary. For the categorical term
education, specified using a f_gam() term, we specify lam=0 to avoid any
shrinkage. We produce the partial dependence plot in age to see the effect
of these choices.
The values for the plot are generated by the pygam package. We provide
a plot_gam() function for partial-dependence plots in ISLP.pygam, which
plot_gam()
makes this job easier than in our last example with natural splines.
In [32]: fig , ax = subplots(figsize =(8 ,8))
plot_gam(gam_full , 0, ax=ax)
ax.set_xlabel('Age')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of age on wage - default lam =0.6 ',
fontsize =20);

We see that the function is somewhat wiggly. It is more natural to specify
the df than a value for lam. We refit a GAM using four degrees of freedom
each for age and year. Recall that the addition of one below takes into
account the intercept of the smoothing spline.
In [33]: age_term = gam_full.terms [0]
age_term.lam = approx_lam(Xgam , age_term , df =4+1)
year_term = gam_full.terms [1]
year_term.lam = approx_lam(Xgam , year_term , df =4+1)
gam_full = gam_full.fit(Xgam , y)

Note that updating age_term.lam above updates it in gam_full.terms[0] as
well! Likewise for year_term.lam.
Repeating the plot for age, we see that it is much smoother. We also
produce the plot for year.
In [34]: fig , ax = subplots(figsize =(8 ,8))
plot_gam(gam_full ,
1,
ax=ax)
ax.set_xlabel('Year ')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of year on wage ', fontsize =20)

Finally we plot education, which is categorical. The partial dependence
plot is different, and more suitable for the set of fitted constants for each
level of this variable.
In [35]: fig , ax = subplots(figsize =(8, 8))
ax = plot_gam(gam_full , 2)
ax.set_xlabel('Education ')
ax.set_ylabel('Effect on wage ')

322

7. Moving Beyond Linearity

ax.set_title('Partial dependence of wage on education ',
fontsize =20);
ax.set_xticklabels(Wage['education '].cat.categories , fontsize =8);

ANOVA Tests for Additive Models
In all of our models, the function of year looks rather linear. We can perform
a series of ANOVA tests in order to determine which of these three models
is best: a GAM that excludes year (M1 ), a GAM that uses a linear function
of year (M2 ), or a GAM that uses a spline function of year (M3 ).
In [36]: gam_0 = LinearGAM(age_term + f_gam(2, lam =0))
gam_0.fit(Xgam , y)
gam_linear = LinearGAM(age_term +
l_gam(1, lam =0) +
f_gam(2, lam =0))
gam_linear.fit(Xgam , y)
Out[36]: LinearGAM(callbacks =[ Deviance (), Diffs ()], fit_intercept=True ,
max_iter =100, scale=None , terms=s(0) + l(1) + f(2) + intercept ,
tol =0.0001 , verbose=False)

Notice our use of age_term in the expressions above. We do this because
earlier we set the value for lam in this term to achieve four degrees of
freedom.
To directly assess the effect of year we run an ANOVA on the three
models fit above.
In [37]: anova_gam(gam_0 , gam_linear , gam_full)
Out[37]:
0
1
2

deviance
3714362.366
3696745.823
3693142.930

df deviance_diff
2991.004
NaN
2990.005
17616.543
2987.007
3602.894

df_diff
NaN
0.999
2.998

F
NaN
14.265
0.972

pvalue
NaN
0.002
0.436

We find that there is compelling evidence that a GAM with a linear function
in year is better than a GAM that does not include year at all (p-value=
0.002). However, there is no evidence that a non-linear function of year
is needed (p-value=0.435). In other words, based on the results of this
ANOVA, M2 is preferred.
We can repeat the same process for age as well. We see there is very clear
evidence that a non-linear term is required for age.
In [38]: gam_0 = LinearGAM(year_term +
f_gam(2, lam =0))
gam_linear = LinearGAM(l_gam(0, lam =0) +
year_term +
f_gam(2, lam =0))
gam_0.fit(Xgam , y)
gam_linear.fit(Xgam , y)
anova_gam(gam_0 , gam_linear , gam_full)

7.8 Lab: Non-Linear Modeling
Out[38]:

0
1
2

deviance
df deviance_diff
3975443.045 2991.001
NaN
3850246.908 2990.001
125196.137
3693142.930 2987.007
157103.978

df_diff
NaN
1.000
2.993

F
NaN
101.270
42.448

323

pvalue
NaN
0.000
0.000

There is a (verbose) summary() method for the GAM fit. (We do not
reproduce it here.)
In [39]: gam_full.summary ()

We can make predictions from gam objects, just like from lm objects, using
the predict() method for the class gam. Here we make predictions on the
training set.
In [40]: Yhat = gam_full.predict(Xgam)

In order to fit a logistic regression GAM, we use LogisticGAM() from

pygam.

In [41]: gam_logit = LogisticGAM(age_term +
l_gam(1, lam =0) +
f_gam(2, lam =0))
gam_logit.fit(Xgam , high_earn)
Out[41]: LogisticGAM(callbacks =[ Deviance (), Diffs(), Accuracy ()],
fit_intercept=True , max_iter =100,
terms=s(0) + l(1) + f(2) + intercept , tol =0.0001 , verbose=False)
In [42]: fig , ax = subplots(figsize =(8, 8))
ax = plot_gam(gam_logit , 2)
ax.set_xlabel('Education ')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of wage on education ',
fontsize =20);
ax.set_xticklabels(Wage['education '].cat.categories , fontsize =8);

The model seems to be very flat, with especially high error bars for the
first category. Let’s look at the data a bit more closely.
In [43]: pd.crosstab(Wage['high_earn '], Wage['education '])

We see that there are no high earners in the first category of education,
meaning that the model will have a hard time fitting. We will fit a logistic
regression GAM excluding all observations falling into this category. This
provides more sensible results.
To do so, we could subset the model matrix, though this will not remove
the column from Xgam. While we can deduce which column corresponds to
this feature, for reproducibility’s sake we reform the model matrix on this
smaller subset.
In [44]: only_hs = Wage['education '] == '1. < HS Grad '
Wage_ = Wage.loc[∼only_hs]
Xgam_ = np.column_stack ([ Wage_['age'],
Wage_['year '],
Wage_['education '].cat.codes -1])
high_earn_ = Wage_['high_earn ']

LogisticGAM()

324

7. Moving Beyond Linearity

In the second-to-last line above, we subtract one from the codes of the
category, due to a bug in pygam. It just relabels the education values and
hence has no effect on the fit.
We now fit the model.
In [45]: gam_logit_ = LogisticGAM(age_term +
year_term +
f_gam(2, lam =0))
gam_logit_.fit(Xgam_ , high_earn_)
Out[45]: LogisticGAM(callbacks =[ Deviance (), Diffs(), Accuracy ()],
fit_intercept=True , max_iter =100,
terms=s(0) + s(1) + f(2) + intercept , tol =0.0001 , verbose=False)

Let’s look at the effect of education, year and age on high earner status
now that we’ve removed those observations.
In [46]: fig , ax = subplots(figsize =(8, 8))
ax = plot_gam(gam_logit_ , 2)
ax.set_xlabel('Education ')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of high earner status on education
', fontsize =20);
ax.set_xticklabels(Wage['education '].cat.categories [1:],
fontsize =8);
In [47]: fig , ax = subplots(figsize =(8, 8))
ax = plot_gam(gam_logit_ , 1)
ax.set_xlabel('Year ')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of high earner status on year ',
fontsize =20);
In [48]: fig , ax = subplots(figsize =(8, 8))
ax = plot_gam(gam_logit_ , 0)
ax.set_xlabel('Age')
ax.set_ylabel('Effect on wage ')
ax.set_title('Partial dependence of high earner status on age',
fontsize =20);

7.8.4 Local Regression
We illustrate the use of local regression using the lowess() function from
lowess()
sm.nonparametric. Some implementations of GAMs allow terms to be local
regression operators; this is not the case in pygam.
Here we fit local linear regression models using spans of 0.2 and 0.5;
that is, each neighborhood consists of 20% or 50% of the observations. As
expected, using a span of 0.5 is smoother than 0.2.
In [49]: lowess = sm.nonparametric.lowess
fig , ax = subplots(figsize =(8 ,8))
ax.scatter(age , y, facecolor='gray ', alpha =0.5)
for span in [0.2, 0.5]:
fitted = lowess(y,

7.9 Exercises

325

age ,
frac=span ,
xvals=age_grid)
ax.plot(age_grid ,
fitted ,
label='{:.1f}'.format(span),
linewidth =4)
ax.set_xlabel('Age', fontsize =20)
ax.set_ylabel('Wage ', fontsize =20);
ax.legend(title='span ', fontsize =15);

7.9

Exercises

Conceptual
1. It was mentioned in this chapter that a cubic regression spline with
one knot at ξ can be obtained using a basis of the form x, x2 , x3 ,
(x − ξ)3+ , where (x − ξ)3+ = (x − ξ)3 if x > ξ and equals 0 otherwise.
We will now show that a function of the form
f (x) = β0 + β1 x + β2 x2 + β3 x3 + β4 (x − ξ)3+

is indeed a cubic regression spline, regardless of the values of β0 , β1 , β2 ,
β3 , β 4 .
(a) Find a cubic polynomial
f1 (x) = a1 + b1 x + c1 x2 + d1 x3
such that f (x) = f1 (x) for all x ≤ ξ. Express a1 , b1 , c1 , d1 in
terms of β0 , β1 , β2 , β3 , β4 .
(b) Find a cubic polynomial
f2 (x) = a2 + b2 x + c2 x2 + d2 x3
such that f (x) = f2 (x) for all x > ξ. Express a2 , b2 , c2 , d2 in
terms of β0 , β1 , β2 , β3 , β4 . We have now established that f (x) is
a piecewise polynomial.
(c) Show that f1 (ξ) = f2 (ξ). That is, f (x) is continuous at ξ.
(d) Show that f1$ (ξ) = f2$ (ξ). That is, f $ (x) is continuous at ξ.
(e) Show that f1$$ (ξ) = f2$$ (ξ). That is, f $$ (x) is continuous at ξ.
Therefore, f (x) is indeed a cubic spline.
Hint: Parts (d) and (e) of this problem require knowledge of singlevariable calculus. As a reminder, given a cubic polynomial
f1 (x) = a1 + b1 x + c1 x2 + d1 x3 ,
the first derivative takes the form
f1$ (x) = b1 + 2c1 x + 3d1 x2

326

7. Moving Beyond Linearity

and the second derivative takes the form
f1$$ (x) = 2c1 + 6d1 x.
2. Suppose that a curve ĝ is computed to smoothly fit a set of n points
using the following formula:
> n
?
L 7
82
0
2
(m)
ĝ = arg min
(yi − g(xi )) + λ
g (x) dx ,
g

i=1

where g (m) represents the mth derivative of g (and g (0) = g). Provide
example sketches of ĝ in each of the following scenarios.
(a) λ = ∞, m = 0.

(b) λ = ∞, m = 1.
(c) λ = ∞, m = 2.

(d) λ = ∞, m = 3.
(e) λ = 0, m = 3.

3. Suppose we fit a curve with basis functions b1 (X) = X, b2 (X) =
(X − 1)2 I(X ≥ 1). (Note that I(X ≥ 1) equals 1 for X ≥ 1 and 0
otherwise.) We fit the linear regression model
Y = β0 + β1 b1 (X) + β2 b2 (X) + ",
and obtain coefficient estimates β̂0 = 1, β̂1 = 1, β̂2 = −2. Sketch the
estimated curve between X = −2 and X = 2. Note the intercepts,
slopes, and other relevant information.
4. Suppose we fit a curve with basis functions b1 (X) = I(0 ≤ X ≤ 2) −
(X − 1)I(1 ≤ X ≤ 2), b2 (X) = (X − 3)I(3 ≤ X ≤ 4) + I(4 < X ≤ 5).
We fit the linear regression model
Y = β0 + β1 b1 (X) + β2 b2 (X) + ",
and obtain coefficient estimates β̂0 = 1, β̂1 = 1, β̂2 = 3. Sketch the
estimated curve between X = −2 and X = 6. Note the intercepts,
slopes, and other relevant information.
5. Consider two curves, ĝ1 and ĝ2 , defined by
> n
?
L 7
82
0
ĝ1 = arg min
(yi − g(xi ))2 + λ
g (3) (x) dx ,
g

ĝ2 = arg min
g

i=1

> n
0
i=1

2

(yi − g(xi )) + λ

L 7

where g (m) represents the mth derivative of g.

g

(4)

(x)

82

?

dx ,

(a) As λ → ∞, will ĝ1 or ĝ2 have the smaller training RSS?

(b) As λ → ∞, will ĝ1 or ĝ2 have the smaller test RSS?

(c) For λ = 0, will ĝ1 or ĝ2 have the smaller training and test RSS?

7.9 Exercises

327

Applied
6. In this exercise, you will further analyze the Wage data set considered
throughout this chapter.
(a) Perform polynomial regression to predict wage using age. Use
cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to
the results of hypothesis testing using ANOVA? Make a plot of
the resulting polynomial fit to the data.
(b) Fit a step function to predict wage using age, and perform crossvalidation to choose the optimal number of cuts. Make a plot of
the fit obtained.
7. The Wage data set contains a number of other features not explored
in this chapter, such as marital status (maritl), job class (jobclass),
and others. Explore the relationships between some of these other
predictors and wage, and use non-linear fitting techniques in order to
fit flexible models to the data. Create plots of the results obtained,
and write a summary of your findings.
8. Fit some of the non-linear models investigated in this chapter to the
Auto data set. Is there evidence for non-linear relationships in this
data set? Create some informative plots to justify your answer.
9. This question uses the variables dis (the weighted mean of distances
to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat
dis as the predictor and nox as the response.
(a) Use the poly() function from the ISLP.models module to fit a
cubic polynomial regre
to the floats –1.5, 0.0, and +1.5, respectively. Note that
0.0  always  maps  to  0  when  using  symmetrical  quantization  (also  note  that  the  byte
values +68 to +127 will not be used, since they map to floats greater than +0.8).

Figure 19-8. From 32-bit floats to 8-bit integers, using symmetrical quantization

To perform this post-training quantization, simply add OPTIMIZE_FOR_SIZE to the list
of converter optimizations before calling the convert() method:

converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]

This technique dramatically reduces the model’s size, so it’s much faster to download
and  store.  However,  at  runtime  the  quantized  weights  get  converted  back  to  floats
before they are used (these recovered floats are not perfectly identical to the original

686 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

floats, but not too far off, so the accuracy loss is usually acceptable). To avoid recom‐
puting them all the time, the recovered floats are cached, so there is no reduction of
RAM usage. And there is no reduction either in compute speed.

The most effective way to reduce latency and power consumption is to also quantize
the activations so that the computations can be done entirely with integers, without
the need for any floating-point operations. Even when using the same bit-width (e.g.,
32-bit  integers  instead  of  32-bit  floats),  integer  computations  use  less  CPU  cycles,
consume less energy, and produce less heat. And if you also reduce the bit-width (e.g.,
down to 8-bit integers), you can get huge speedups. Moreover, some neural network
accelerator devices (such as the Edge TPU) can only process integers, so full quanti‐
zation of both weights and activations is compulsory. This can be done post-training;
it requires a calibration step to find the maximum absolute value of the activations, so
you  need  to  provide  a  representative  sample  of  training  data  to  TFLite  (it  does  not
need  to  be  huge),  and  it  will  process  the  data  through  the  model  and  measure  the
activation statistics required for quantization (this step is typically fast).

The main problem with quantization is that it loses a bit of accuracy: it is equivalent
to adding noise to the weights and activations. If the accuracy drop is too severe, then
you may need to use quantization-aware training. This means adding fake quantiza‐
tion operations to the model so it can learn to ignore the quantization noise during
training;  the  final  weights  will  then  be  more  robust  to  quantization.  Moreover,  the
calibration step can be taken care of automatically during training, which simplifies
the whole process.

I have explained the core concepts of TFLite, but going all the way to coding a mobile
app  or  an  embedded  program  would  require  a  whole  other  book.  Fortunately,  one
exists: if you want to learn more about building TensorFlow applications for mobile
and embedded devices, check out the O’Reilly book TinyML: Machine Learning with
TensorFlow on Arduino and Ultra-Low Power Micro-Controllers, by Pete Warden (who
leads the TFLite team) and Daniel Situnayake.

Deploying a Model to a Mobile or Embedded Device 

| 

687

TensorFlow in the Browser
What  if  you  want  to  use  your  model  in  a  website,  running  directly  in  the  user’s
browser? This can be useful in many scenarios, such as:

• When your web application is often used in situations where the user’s connec‐
tivity  is  intermittent  or  slow  (e.g.,  a  website  for  hikers),  so  running  the  model
directly on the client side is the only way to make your website reliable.

• When you need the model’s responses to be as fast as possible (e.g., for an online
game). Removing the need to query the server to make predictions will definitely
reduce the latency and make the website much more responsive.

• When your web service makes predictions based on some private user data, and
you  want  to  protect  the  user’s  privacy  by  making  the  predictions  on  the  client
side so that the private data never has to leave the user’s machine.9

For  all  these  scenarios,  you  can  export  your  model  to  a  special  format  that  can  be
loaded by the TensorFlow.js JavaScript library. This library can then use your model
to make predictions directly in the user’s browser. The TensorFlow.js project includes
a  tensorflowjs_converter  tool  that  can  convert  a  TensorFlow  SavedModel  or  a
Keras model file to the TensorFlow.js Layers format: this is a directory containing a set
of sharded weight files in binary format and a model.json file that describes the mod‐
el’s architecture and links to the weight files. This format is optimized to be downloa‐
ded efficiently on the web. Users can then download the model and run predictions in
the browser using the TensorFlow.js library. Here is a code snippet to give you an idea
of what the JavaScript API looks like:

import * as tf from '@tensorflow/tfjs';
const model = await tf.loadLayersModel('https://example.com/tfjs/model.json');
const image = tf.fromPixels(webcamElement);
const prediction = model.predict(image);

Once  again,  doing  justice  to  this  topic  would  require  a  whole  book.  If  you  want  to
learn more about TensorFlow.js, check out the O’Reilly book Practical Deep Learning
for Cloud, Mobile, and Edge, by Anirudh Koul, Siddha Ganju, and Meher Kasam.

Next, we will see how to use GPUs to speed up computations!

9 If you’re interested in this topic, check out federated learning.

688 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

Using GPUs to Speed Up Computations
In Chapter 11 we discussed several techniques that can considerably speed up train‐
ing:  better  weight  initialization,  Batch  Normalization,  sophisticated  optimizers,  and
so on. But even with all of these techniques, training a large neural network on a sin‐
gle machine with a single CPU can take days or even weeks.

In this section we will look at how to speed up your models by using GPUs. We will
also  see  how  to  split  the  computations  across  multiple  devices,  including  the  CPU
and multiple GPU devices (see Figure 19-9). For now we will run everything on a sin‐
gle machine, but later in this chapter we will discuss how to distribute computations
across multiple servers.

Figure 19-9. Executing a TensorFlow graph across multiple devices in parallel

Thanks  to  GPUs,  instead  of  waiting  for  days  or  weeks  for  a  training  algorithm  to
complete, you may end up waiting for just a few minutes or hours. Not only does this
save  an  enormous  amount  of  time,  but  it  also  means  that  you  can  experiment  with
various models much more easily and frequently retrain your models on fresh data.

You  can  often  get  a  major  performance  boost  simply  by  adding
GPU cards to a single machine. In fact, in many cases this will suf‐
fice;  you  won’t  need  to  use  multiple  machines  at  all.  For  example,
you  can  typically  train  a  neural  network  just  as  fast  using  four
GPUs on a single machine rather than eight GPUs across multiple
machines, due to the extra delay imposed by network communica‐
tions in a distributed setup. Similarly, using a single powerful GPU
is often preferable to using multiple slower GPUs.

Using GPUs to Speed Up Computations 

| 

689

The first step is to get your hands on a GPU. There are two options for this: you can
either  purchase  your  own  GPU(s),  or  you  can  use  GPU-equipped  virtual  machines
on the cloud. Let’s start with the first option.

Getting Your Own GPU
If you choose to purchase a GPU card, then take some time to make the right choice.
Tim Dettmers wrote an excellent blog post to help you choose, and he updates it reg‐
ularly:  I  encourage  you  to  read  it  carefully.  At  the  time  of  this  writing,  TensorFlow
only supports Nvidia cards with CUDA Compute Capability 3.5+ (as well as Google’s
TPUs,  of  course),  but  it  may  extend  its  support  to  other  manufacturers.  Moreover,
although TPUs are currently only available on GCP, it is highly likely that TPU-like
cards will be available for sale in the near future, and TensorFlow may support them.
In  short,  make  sure  to  check  TensorFlow’s  documentation  to  see  what  devices  are
supported at this point.

If  you  go  for  an  Nvidia  GPU  card,  you  will  need  to  install  the  appropriate  Nvidia
drivers  and  several  Nvidia  libraries.10  These  include  the  Compute  Unified  Device
Architecture  library  (CUDA),  which  allows  developers  to  use  CUDA-enabled  GPUs
for  all  sorts  of  computations  (not  just  graphics  acceleration),  and  the  CUDA  Deep
Neural Network library (cuDNN), a GPU-accelerated library of primitives for DNNs.
cuDNN  provides  optimized  implementations  of  common  DNN  computations  such
as activation layers, normalization, forward and backward convolutions, and pooling
(see  Chapter  14).  It  is  part  of  Nvidia’s  Deep  Learning  SDK  (note  that  you’ll  need  to
create an Nvidia developer account in order to download it). TensorFlow uses CUDA
and  cuDNN  to  control  the  GPU  cards  and  accelerate  computations  (see
Figure 19-10).

Figure 19-10. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs

10 Please check the docs for detailed and up-to-date installation instructions, as they change quite often.

690 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

Once  you  have  installed  the  GPU  card(s)  and  all  the  required  drivers  and  libraries,
you can use the  nvidia-smi command to check that CUDA is properly installed. It
lists the available GPU cards, as well as processes running on each card:

$ nvidia-smi
Sun Jun  2 10:05:22 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   61C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

At the time of this writing, you’ll also need to install the GPU version of TensorFlow
(i.e., the tensorflow-gpu library); however, there is ongoing work to have a unified
installation  procedure  for  both  CPU-only  and  GPU  machines,  so  please  check  the
installation documentation to see which library you should install. In any case, since
installing every required library correctly is a bit long and tricky (and all hell breaks
loose if you do not install the correct library versions), TensorFlow provides a Docker
image with everything you need inside. However, in order for the Docker container
to have access to the GPU, you will still need to install the Nvidia drivers on the host
machine.

To check that TensorFlow actually sees the GPUs, run the following tests:

>>> import tensorflow as tf
>>> tf.test.is_gpu_available()
True
>>> tf.test.gpu_device_name()
'/device:GPU:0'
>>> tf.config.experimental.list_physical_devices(device_type='GPU')
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

The is_gpu_available() function checks whether at least one GPU is available. The
gpu_device_name()  function  gives  the  first  GPU’s  name:  by  default,  operations  will

Using GPUs to Speed Up Computations 

| 

691

run on this GPU. The list_physical_devices() function returns the list of all avail‐
able GPU devices (just one in this example).11

Now,  what  if  you  don’t  want  to  invest  time  and  money  in  getting  your  own  GPU
card? Just use a GPU VM on the cloud!

Using a GPU-Equipped Virtual Machine
All major cloud platforms now offer GPU VMs, some preconfigured with all the driv‐
ers and libraries you need (including TensorFlow). Google Cloud Platform enforces
various  GPU  quotas,  both  worldwide  and  per  region:  you  cannot  just  create  thou‐
sands of GPU VMs without prior authorization from Google.12 By default, the world‐
wide GPU quota is zero, so you cannot use any GPU VMs. Therefore, the very first
thing  you  need  to  do  is  to  request  a  higher  worldwide  quota.  In  the  GCP  console,
open  the  navigation  menu  and  go  to  IAM  &  admin  →  Quotas.  Click  Metric,  click
None to uncheck all locations, then search for “GPU” and select “GPUs (all regions)”
to  see  the  corresponding  quota.  If  this  quota’s  value  is  zero  (or  just  insufficient  for
your  needs),  then  check  the  box  next  to  it  (it  should  be  the  only  selected  one)  and
click “Edit quotas.” Fill in the requested information, then click “Submit request.” It
may take a few hours (or up to a few days) for your quota request to be processed and
(generally) accepted. By default, there is also a quota of one GPU per region and per
GPU type. You can request to increase these quotas too: click Metric, select None to
uncheck  all  metrics,  search  for  “GPU,”  and  select  the  type  of  GPU  you  want  (e.g.,
NVIDIA  P4  GPUs).  Then  click  the  Location  drop-down  menu,  click  None  to
uncheck  all  metrics,  and  click  the  location  you  want;  check  the  boxes  next  to  the
quota(s) you want to change, and click “Edit quotas” to file a request.

Once your GPU quota requests are approved, you can in no time create a VM equip‐
ped with one or more GPUs by using Google Cloud AI Platform’s Deep Learning VM
Images: go to https://homl.info/dlvm, click View Console, then click “Launch on Com‐
pute Engine” and fill in the VM configuration form. Note that some locations do not
have all types of GPUs, and some have no GPUs at all (change the location to see the
types of GPUs available, if any). Make sure to select TensorFlow 2.0 as the framework,
and  check  “Install  NVIDIA  GPU  driver  automatically  on  first  startup.”  It  is  also  a
good  idea  to  check  “Enable  access  to  JupyterLab  via  URL  instead  of  SSH”:  this  will
make it very easy to start a Jupyter notebook running on this GPU VM, powered by

11 Many code examples in this chapter use experimental APIs. They are very likely to be moved to the core API
in future versions. So if an experimental function fails, try simply removing the word experimental, and
hopefully it will work. If not, then perhaps the API has changed a bit; please check the Jupyter notebook, as I
will ensure it contains the correct code.

12 Presumably, these quotas are meant to stop bad guys who might be tempted to use GCP with stolen credit

cards to mine cryptocurrencies.

692 

| 

Chapter 19: Training and Deplo
to compute gradients: using manual differentia‐
tion  and  using  finite  difference  approximation.  Unfortunately,  both  were  fatally
flawed to train a large-scale neural network. So let’s turn to autodiff, starting with for‐
ward mode.

Forward-Mode Autodiff
Figure  D-1  shows  how  forward-mode  autodiff  works  on  an  even  simpler  function,
g(x, y) = 5 + xy. The graph for that function is represented on the left. After forward-
mode autodiff, we get the graph on the right, which represents the partial derivative
∂g/∂x = 0 + (0 × x + y × 1) = y (we could similarly obtain the partial derivative with
regard to y).

Figure D-1. Forward-mode autodiff

Autodiff 

| 

767

The algorithm will go through the computation graph from the inputs to the outputs
(hence  the  name  “forward  mode”).  It  starts  by  getting  the  partial  derivatives  of  the
leaf nodes. The constant node (5) returns the constant 0, since the derivative of a con‐
stant is always 0. The variable x returns the constant 1 since ∂x/∂x = 1, and the vari‐
able  y  returns  the  constant  0  since  ∂y/∂x  =  0  (if  we  were  looking  for  the  partial
derivative with regard to y, it would be the reverse).

Now we have all we need to move up the graph to the multiplication node in function
g.  Calculus  tells  us  that  the  derivative  of  the  product  of  two  functions  u  and  v  is
∂(u × v)/∂x = ∂v/∂x × u + v × ∂u/∂x. We can therefore construct a large part of the
graph on the right, representing 0 × x + y × 1.

Finally, we can go up to the addition node in function g. As mentioned, the derivative
of  a  sum  of  functions  is  the  sum  of  these  functions’  derivatives.  So  we  just  need  to
create an addition node and connect it to the parts of the graph we have already com‐
puted. We get the correct partial derivative: ∂g/∂x = 0 + (0 × x + y × 1).

However, this equation can be simplified (a lot). A few pruning steps can be applied
to the computation graph to get rid of all unnecessary operations, and we get a much
smaller graph with just one node: ∂g/∂x = y. In this case simplification is fairly easy,
but  for  a  more  complex  function  forward-mode  autodiff  can  produce  a  huge  graph
that may be tough to simplify and lead to suboptimal performance.

Note that we started with a computation graph, and forward-mode autodiff produced
another computation graph. This is called symbolic differentiation, and it has two nice
features:  first,  once  the  computation  graph  of  the  derivative  has  been  produced,  we
can use it as many times as we want to compute the derivatives of the given function
for  any  value  of  x  and  y;  second,  we  can  run  forward-mode  autodiff  again  on  the
resulting graph to get second-order derivatives if we ever need to (i.e., derivatives of
derivatives). We could even compute third-order derivatives, and so on.

But  it  is  also  possible  to  run  forward-mode  autodiff  without  constructing  a  graph
(i.e., numerically, not symbolically), just by computing intermediate results on the fly.
One way to do this is to use dual numbers, which are weird but fascinating numbers
of the form a + bε, where a and b are real numbers and ε is an infinitesimal number
such that ε2 = 0 (but ε ≠ 0). You can think of the dual number 42 + 24ε as something
akin to 42.0000⋯000024 with an infinite number of 0s (but of course this is simpli‐
fied just to give you some idea of what dual numbers are). A dual number is repre‐
sented in memory as a pair of floats. For example, 42 + 24ε is represented by the pair
(42.0, 24.0).

768 

|  Appendix D: Autodiff

Dual numbers can be added, multiplied, and so on, as shown in Equation D-3.

Equation D-3. A few operations with dual numbers

λ a + bε = λa + λbε
a + bε + c + dε = a + c + b + d ε
a + bε × c + dε = ac + ad + bc ε + bd ε2 = ac + ad + bc ε

Most importantly, it can be shown that h(a + bε) = h(a) + b × h′(a)ε, so computing
h(a  +  ε)  gives  you  both  h(a)  and  the  derivative  h′(a)  in  just  one  shot.  Figure  D-2
shows that the partial derivative of f(x, y) with regard to x at x = 3 and y = 4 (which
we will write ∂f/∂x (3, 4)) can be computed using dual numbers. All we need to do is
compute f(3 + ε, 4); this will output a dual number whose first component is equal to
f(3, 4) and whose second component is equal to ∂f/∂x (3, 4).

Figure D-2. Forward-mode autodiff using dual numbers

To compute ∂f/∂x (3, 4) we would have to go through the graph again, but this time
with x = 3 and y = 4 + ε.

So forward-mode autodiff is much more accurate than finite difference approxima‐
tion, but it suffers from the same major flaw, at least when there are many inputs and
few  outputs  (as  is  the  case  when  dealing  with  neural  networks):  if  there  were  1,000
parameters, it would require 1,000 passes through the graph to compute all the partial

Autodiff 

| 

769

derivatives. This is where reverse-mode autodiff shines: it can compute all of them in
just two passes through the graph. Let’s see how.

Reverse-Mode Autodiff
Reverse-mode  autodiff  is  the  solution  implemented  by  TensorFlow.  It  first  goes
through  the  graph  in  the  forward  direction  (i.e.,  from  the  inputs  to  the  output)  to
compute the value of each node. Then it does a second pass, this time in the reverse
direction (i.e., from the output to the inputs), to compute all the partial derivatives.
The name “reverse mode” comes from this second pass through the graph, where gra‐
dients  flow  in  the  reverse  direction.  Figure  D-3  represents  the  second  pass.  During
the first pass, all the node values were computed, starting from x = 3 and y = 4. You
can see those values at the bottom right of each node (e.g., x × x = 9). The nodes are
labeled n1 to n7 for clarity. The output node is n7: f(3, 4) = n7 = 42.

Figure D-3. Reverse-mode autodiff

770 

|  Appendix D: Autodiff

The idea is to gradually go down the graph, computing the partial derivative of f(x, y)
with  regard  to  each  consecutive  node,  until  we  reach  the  variable  nodes.  For  this,
reverse-mode autodiff relies heavily on the chain rule, shown in Equation D-4.

Equation D-4. Chain rule

∂ f
∂x

=

∂ f
∂ni

×

∂ni
∂x

Since n7 is the output node, f = n7 so ∂f/∂n7 = 1.

Let’s  continue  down  the  graph  to  n5:  how  much  does  f  vary  when  n5  varies?  The
answer is ∂f/∂n5 = ∂f/∂n7 × ∂n7/∂n5. We already know that ∂f/∂n7 = 1, so all we need is
∂n7/∂n5. Since n7 simply performs the sum n5 + n6, we find that ∂n7/∂n5 = 1, so ∂f/∂n5
= 1 × 1 = 1.

Now we can proceed to node n4: how much does f vary when n4 varies? The answer is
∂f/∂n4 = ∂f/∂n5 × ∂n5/∂n4. Since n5 = n4 × n2, we find that ∂n5/∂n4 = n2, so ∂f/∂n4 = 1 ×
n2 = 4.

The process continues until we reach the bottom of the graph. At that point we will
have calculated all the partial derivatives of f(x, y) at the point x = 3 and y = 4. In this
example, we find ∂f/∂x = 24 and ∂f/∂y = 10. Sounds about right!

Reverse-mode  autodiff  is  a  very  powerful  and  accurate  technique,  especially  when
there are many inputs and few outputs, since it requires only one forward pass plus
one reverse pass per output to compute all the partial derivatives for all outputs with
regard to all the inputs. When training neural networks, we generally want to mini‐
mize the loss, so there is a single output (the loss), and hence only two passes through
the graph are needed to compute the gradients. Reverse-mode autodiff can also han‐
dle functions that are not entirely differentiable, as long as you ask it to compute the
partial derivatives at points that are differentiable.

In Figure D-3, the numerical results are computed on the fly, at each node. However,
that’s not exactly what TensorFlow does: instead, it creates a new computation graph.
In other words, it implements symbolic reverse-mode autodiff. This way, the compu‐
tation graph to compute the gradients of the loss with regard to all the parameters in
the neural network only needs to be generated once, and then it can be executed over
and  over  again,  whenever  the  optimizer  needs  to  compute  the  gradients.  Moreover,
this makes it possible to compute higher-order derivatives if needed.

Autodiff 

| 

771

If you ever want to implement a new type of low-level TensorFlow
operation in C++, and you want to make it compatible with auto‐
diff, then you will need to provide a function that returns the par‐
tial  derivatives  of  the  function’s  outputs  with  regard  to  its  inputs.
For example, suppose you implement a function that computes the
square of its input: f(x) = x2. In that case you would need to provide
the corresponding derivative function: f′(x) = 2x.

772 

|  Appendix D: Autodiff

APPENDIX E
Other Popular ANN Architectures

In  this  appendix  I  will  give  a  quick  overview  of  a  few  historically  important  neural
network architectures that are much less used today than deep Multilayer Perceptrons
(Chapter 10), convolutional neural networks (Chapter 14), recurrent neural networks
(Chapter 15), or autoencoders (Chapter 17). They are often mentioned in the litera‐
ture, and some are still used in a range of applications, so it is worth knowing about
them. Additionally, we will discuss deep belief nets, which were the state of the art in
Deep Learning until the early 2010s. They are still the subject of very active research,
so they may well come back with a vengeance in the future.

Hopfield Networks
Hopfield networks were first introduced by W. A. Little in 1974, then popularized by J.
Hopfield in 1982. They are associative memory networks: you first teach them some
patterns,  and  then  when  they  see  a  new  pattern  they  (hopefully)  output  the  closest
learned  pattern.  This  made  them  useful  for  character  recognition,  in  particular,
before  they  were  outperformed  by  other  approaches:  you  first  train  the  network  by
showing it examples of character images (each binary pixel maps to one neuron), and
then  when  you  show  it  a  new  character  image,  after  a  few  iterations  it  outputs  the
closest learned character.

Hopfield networks are fully connected graphs (see Figure E-1); that is, every neuron
is  connected  to  every  other  neuron.  Note  that  in  the  diagram  the  images  are  6  ×  6
pixels, so the neural network on the left should contain 36 neurons (and 630 connec‐
tions), but for visual clarity a much smaller network is represented.

773

Figure E-1. Hopfield network

The  training  algorithm  works  by  using  Hebb’s  rule  (see  “The  Perceptron”  on  page
284): for each training image, the weight between two neurons is increased if the cor‐
responding  pixels  are  both  on  or  both  off,  but  decreased  if  one  pixel  is  on  and  the
other is off.

To show a new image to the network, you just activate the neurons that correspond to
active pixels. The network then computes the output of every neuron, and this gives
you  a  new  image.  You  can  then  take  this  new  image  and  repeat  the  whole  process.
After  a  while,  the  network  reaches  a  stable  state.  Generally,  this  corresponds  to  the
training image that most resembles the input image.

A  so-called  energy  function  is  associated  with  Hopfield  nets.  At  each  iteration,  the
energy decreases, so the network is guaranteed to eventually stabilize to a low-energy
state.  The  training  algorithm  tweaks  the  weights  in  a  way  that  decreases  the  energy
level of the training patterns, so the network is likely to stabilize in one of these low-
energy configurations. Unfortunately, some patterns that were not in the training set
also end up with low energy, so the network sometimes stabilizes in a configuration
that was not learned. These are called spurious patterns.

Another major flaw with Hopfield nets is that they don’t scale very well—their mem‐
ory capacity is roughly equal to 14% of the number of neurons. For example, to clas‐
sify 28 × 28–pixel images, you would need a Hopfield net with 784 fully connected
neurons and 306,936 weights. Such a network would only be able to learn about 110
different characters (14% of 784). That’s a lot of parameters for such a small memory.

774 

|  Appendix E: Other Popular ANN Architectures

Boltzmann Machines
Boltzmann machines were invented in 1985 by Geoffrey Hinton and Terrence Sejnow‐
ski. Just like Hopfield nets, they are fully connected ANNs, but they are based on sto‐
chastic neurons: instead of using a deterministic step function to decide what value to
output, these neurons output 1 with some probability, and 0 otherwise. The probabil‐
ity  function  that  these  ANNs  use  is  based  on  the  Boltzmann  distribution  (used  in
statistical  mechanics),  hence  their  name.  Equation  E-1  gives  the  probability  that  a
particular neuron will output 1.

Equation E-1. Probability that the ith neuron will output 1

next step = 1 = σ

p si

N wi, js j + bi
∑ j = 1
T

• sj is the jth neuron’s state (0 or 1).
• wi,j is the connection weight between the ith and jth neurons. Note that wi,i = 0.
• bi is the ith neuron’s bias term. We can implement this term by adding a bias neu‐

ron to the network.

• N is the number of neurons in the network.

• T is a number called the network’s temperature; the higher the temperature, the

more random the output is (i.e., the more the probability approaches 50%).

• σ is the logistic function.

Neurons in Boltzmann machines are separated into two groups: visible units and hid‐
den units (see Figure E-2). All neurons work in the same stochastic way, but the visi‐
ble units are the ones that receive the inputs and from which outputs are read.

Because of its stochastic nature, a Boltzmann machine will never stabilize into a fixed
configuration; instead, it will keep switching between many configurations. If it is left
running for a sufficiently long time, the probability of observing a particular configu‐
ration  will  only  be  a  function  of  the  connection  weights  and  bias  terms,  not  of  the
original configuration (similarly, after you shuffle a deck of cards for long enough, the
configuration  of  the  deck  does  not  depend  on  the  initial  state).  When  the  network
reaches  this  state  where  the  original  configuration  is  “forgotten,”  it  is  said  to  be  in
thermal equilibrium (although its configuration keeps changing all the time). By set‐
ting the network parameters appropriately, letting the network reach thermal equili‐
brium,  and  then  observing  its  state,  we  can  simulate  a  wide  range  of  probability
distributions. This is called a generative model.

Other Popular ANN Architectures 

| 

775

Figure E-2. Boltzmann machine

Training a Boltzmann machine means finding the parameters that will make the net‐
work approximate the training set’s probability distribution. For example, if there are
three visible neurons and the training set contains 75% (0, 1, 1) triplets, 10% (0, 0, 1)
triplets, and 15% (1, 1, 1) triplets, then after training a Boltzmann machine, you could
use  it  to  generate  random  binary  triplets  with  about  the  same  probability  distribu‐
tion. For example, about 75% of the time it would output the (0, 1, 1) triplet.

Such a generative model can be used in a variety of ways. For example, if it is trained
on  images,  and  you  provide  an  incomplete  or  noisy  image  to  the  network,  it  will
automa
oice, and use it to
generate images. Alternatively, you can try to find an unlabeled dataset that you
are interested in and see if you can generate new samples.

11. Train a DCGAN to tackle the image dataset of your choice, and use it to generate
images.  Add  experience  replay  and  see  if  this  helps.  Turn  it  into  a  conditional
GAN where you can control the generated class.

Solutions to these exercises are available in Appendix A.

Exercises 

| 

607

CHAPTER 18
Reinforcement Learning

Reinforcement Learning (RL) is one of the most exciting fields of Machine Learning
today, and also one of the oldest. It has been around since the 1950s, producing many
interesting  applications  over  the  years,1  particularly  in  games  (e.g.,  TD-Gammon,  a
Backgammon-playing  program)  and  in  machine  control,  but  seldom  making  the
headline news. But a revolution took place in 2013, when researchers from a British
startup  called  DeepMind  demonstrated  a  system  that  could  learn  to  play  just  about
any  Atari  game  from  scratch,2  eventually  outperforming  humans3  in  most  of  them,
using only raw pixels as inputs and without any prior knowledge of the rules of the
games.4  This  was  the  first  of  a  series  of  amazing  feats,  culminating  in  March  2016
with the victory of their system AlphaGo against Lee Sedol, a legendary professional
player of the game of Go, and in May 2017 against Ke Jie, the world champion. No
program  had  ever  come  close  to  beating  a  master  of  this  game,  let  alone  the  world
champion. Today the whole field of RL is boiling with new ideas, with a wide range of
applications. DeepMind was bought by Google for over $500 million in 2014.

So  how  did  DeepMind  achieve  all  this?  With  hindsight  it  seems  rather  simple:  they
applied  the  power  of  Deep  Learning  to  the  field  of  Reinforcement  Learning,  and  it
worked  beyond  their  wildest  dreams.  In  this  chapter  we  will  first  explain  what

1 For more details, be sure to check out Richard Sutton and Andrew Barto’s book on RL, Reinforcement Learn‐

ing: An Introduction (MIT Press).

2 Volodymyr Mnih et al., “Playing Atari with Deep Reinforcement Learning,” arXiv preprint arXiv:1312.5602

(2013).

3 Volodymyr Mnih et al., “Human-Level Control Through Deep Reinforcement Learning,” Nature 518 (2015):

529–533.

4 Check out the videos of DeepMind’s system learning to play Space Invaders, Breakout, and other video games

at https://homl.info/dqn3.

609

Reinforcement Learning is and what it’s good at, then present two of the most impor‐
tant  techniques  in  Deep  Reinforcement  Learning:  policy  gradients  and  deep  Q-
networks  (DQNs),  including  a  discussion  of  Markov  decision  processes  (MDPs).  We
will use these techniques to train models to balance a pole on a moving cart; then I’ll
introduce  the  TF-Agents  library,  which  uses  state-of-the-art  algorithms  that  greatly
simplify building powerful RL systems, and we will use the library to train an agent to
play Breakout, the famous Atari game. I’ll close the chapter by taking a look at some
of the latest advances in the field.

Learning to Optimize Rewards
In  Reinforcement  Learning,  a  software  agent  makes  observations  and  takes  actions
within an environment, and in return it receives rewards. Its objective is to learn to act
in a way that will maximize its expected rewards over time. If you don’t mind a bit of
anthropomorphism,  you  can  think  of  positive  rewards  as  pleasure,  and  negative
rewards as pain (the term “reward” is a bit misleading in this case). In short, the agent
acts  in  the  environment  and  learns  by  trial  and  error  to  maximize  its  pleasure  and
minimize its pain.

This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few
examples (see Figure 18-1):

a. The agent can be the program controlling a robot. In this case, the environment
is  the  real  world,  the  agent  observes  the  environment  through  a  set  of  sensors
such as cameras and touch sensors, and its actions consist of sending signals to
activate  motors.  It  may  be  programmed  to  get  positive  rewards  whenever  it
approaches the target destination, and negative rewards whenever it wastes time
or goes in the wrong direction.

b. The agent can be the program controlling Ms. Pac-Man. In this case, the environ‐
ment is a simulation of the Atari game, the actions are the nine possible joystick
positions (upper left, down, center, and so on), the observations are screenshots,
and the rewards are just the game points.

c. Similarly, the agent can be the program playing a board game such as Go.

d. The agent does not have to control a physically (or virtually) moving thing. For
example,  it  can  be  a  smart  thermostat,  getting  positive  rewards  whenever  it  is
close  to  the  target  temperature  and  saves  energy,  and  negative  rewards  when
humans  need  to  tweak  the  temperature,  so  the  agent  must  learn  to  anticipate
human needs.

e. The agent can observe stock market prices and decide how much to buy or sell

every second. Rewards are obviously the monetary gains and losses.

610 

| 

Chapter 18: Reinforcement Learning

Figure 18-1. Reinforcement Learning examples: (a) robotics, (b) Ms. Pac-Man, (c) Go
player, (d) thermostat, (e) automatic trader5

Note  that  there  may  not  be  any  positive  rewards  at  all;  for  example,  the  agent  may
move around in a maze, getting a negative reward at every time step, so it had better
find the exit as quickly as possible! There are many other examples of tasks to which
Reinforcement  Learning  is  well  suited,  such  as  self-driving  cars,  recommender  sys‐
tems, placing ads on a web page, or controlling where an image classification system
should focus its attention.

5 Image (a) is from NASA (public domain). (b) is a screenshot from the Ms. Pac-Man game, copyright Atari

(fair use in this chapter). Images (c) and (d) are reproduced from Wikipedia. (c) was created by user Stever‐
tigo and released under Creative Commons BY-SA 2.0. (d) is in the public domain. (e) was reproduced from
Pixabay, released under Creative Commons CC0.

Learning to Optimize Rewards 

| 

611

Policy Search
The algorithm a software agent uses to determine its actions is called its policy. The
policy  could  be  a  neural  network  taking  observations  as  inputs  and  outputting  the
action to take (see Figure 18-2).

Figure 18-2. Reinforcement Learning using a neural network policy

The policy can be any algorithm you can think of, and it does not have to be deter‐
ministic. In fact, in some cases it does not even have to observe the environment! For
example,  consider  a  robotic  vacuum  cleaner  whose  reward  is  the  amount  of  dust  it
picks up in 30 minutes. Its policy could be to move forward with some probability p
every  second,  or  randomly  rotate  left  or  right  with  probability  1  –  p.  The  rotation
angle  would  be  a  random  angle  between  –r  and  +r.  Since  this  policy  involves  some
randomness, it is called a stochastic policy. The robot will have an erratic trajectory,
which guarantees that it will eventually get to any place it can reach and pick up all
the dust. The question is, how much dust will it pick up in 30 minutes?

How  would  you  train  such  a  robot?  There  are  just  two  policy  parameters  you  can
tweak: the probability p and the angle range r. One possible learning algorithm could
be  to  try  out  many  different  values  for  these  parameters,  and  pick  the  combination
that performs best (see Figure 18-3). This is an example of policy search, in this case
using a brute force approach. When the policy space is too large (which is generally
the case), finding a good set of parameters this way is like searching for a needle in a
gigantic haystack.

Another way to explore the policy space is to use genetic algorithms. For example, you
could randomly create a first generation of 100 policies and try them out, then “kill”
the  80  worst  policies6  and  make  the  20  survivors  produce  4  offspring  each.  An

6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene

pool.”

612 

| 

Chapter 18: Reinforcement Learning

offspring is a copy of its parent7 plus some random variation. The surviving policies
plus  their  offspring  together  constitute  the  second  generation.  You  can  continue  to
iterate through generations this way until you find a good policy.8

Figure 18-3. Four points in policy space (left) and the agent’s corresponding behavior
(right)

Yet another approach is to use optimization techniques, by evaluating the gradients of
the rewards with regard to the policy parameters, then tweaking these parameters by
following  the  gradients  toward  higher  rewards.9  We  will  discuss  this  approach,  is
called  policy  gradients  (PG),  in  more  detail  later  in  this  chapter.  Going  back  to  the
vacuum  cleaner  robot,  you  could  slightly  increase  p  and  evaluate  whether  doing  so
increases  the  amount  of  dust  picked  up  by  the  robot  in  30  minutes;  if  it  does,  then
increase p some more, or else reduce p. We will implement a popular PG algorithm
using TensorFlow, but before we do, we need to create an environment for the agent
to live in—so it’s time to introduce OpenAI Gym.

Introduction to OpenAI Gym
One  of  the  challenges  of  Reinforcement  Learning  is  that  in  order  to  train  an  agent,
you first need to have a working environment. If you want to program an agent that

7 If there is a single parent, this is called asexual reproduction. With two (or more) parents, it is called sexual

reproduction. An offspring’s genome (in this case a set of policy parameters) is randomly composed of parts of
its parents’ genomes.

8 One interesting example of a genetic algorithm used for Reinforcement Learning is the NeuroEvolution of

Augmenting Topologies (NEAT) algorithm.

9 This is called Gradient Ascent. It’s just like Gradient Descent but in the opposite direction: maximizing instead

of minimizing.

Introduction to OpenAI Gym 

| 

613

will learn to play an Atari game, you will need an Atari game simulator. If you want to
program  a  walking  robot,  then  the  environment  is  the  real  world,  and  you  can
directly train your robot in that environment, but this has its limits: if the robot falls
off a cliff, you can’t just click Undo. You can’t speed up time either; adding more com‐
puting power won’t make the robot move any faster. And it’s generally too expensive
to train 1,000 robots in parallel. In short, training is hard and slow in the real world,
so  you  generally  need  a  simulated  environment  at  least  for  bootstrap  training.  For
example, you may use a library like PyBullet or MuJoCo for 3D physics simulation.

OpenAI  Gym10  is  a  toolkit  that  provides  a  wide  variety  of  simulated  environments
(Atari games, board games, 2D and 3D physical simulations, and so on), so you can
train agents, compare them, or develop new RL algorithms.

Before installing the toolkit, if you created an isolated environment using virtualenv,
you first need to activate it:

$ cd $ML_PATH                # Your ML working directory (e.g., $HOME/ml)
$ source my_env/bin/activate # on Linux or MacOS
$ .\my_env\Scripts\activate  # on Windows

Next, install OpenAI Gym (if you are not using a virtual environment, you will need
to add the --user option, or have administrator rights):

$ python3 -m pip install -U gym

Depending  on  your  system,  you  may  also  need  to  install  the  Mesa  OpenGL  Utility
(GLU) library (e.g., on Ubuntu 18.04 you need to run apt install libglu1-mesa).
This library will be needed to render the first environment. Next, open up a Python
shell or a Jupyter notebook and create an environment with make():

>>> import gym
>>> env = gym.make("CartPole-v1")
>>> obs = env.reset()
>>> obs
array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])

Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart
can  be  accelerated  left  or  right  in  order  to  balance  a  pole  placed  on  top  of  it  (see
Figure  18-4).  You  can  get  the  list  of  all  available  environments  by  running
gym.envs.registry.all().  After  the  environment  is  created,  you  must  initialize  it
using  the  reset()  method.  This  returns  the  first  observation.  Observations  depend
on the type of environment. For the CartPole environment, each observation is a 1D
NumPy  array  containing  four  floats:  these  floats  represent  the  cart’s  horizontal

10 OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to pro‐

mote and develop friendly AIs that will benefit humanity (rather than exterminate it).

614 

| 

Chapter 18: Reinforcement Learning

position (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 =
vertical), and its angular velocity (positive means clockwise).

Now let’s display this environment by calling its render() method (see Figure 18-4).
On Windows, this requires first installing an X Server, such as VcXsrv or Xming:

>>> env.render()
True

Figure 18-4. The CartPole environment

If you are using a headless server (i.e., without a screen), such as a
virtual machine on the cloud, rendering will fail. The only way to
avoid  this  is  to  use  a  fake  X  server  such  as  Xvfb  or  Xdummy.  For
example,  you  can  install  Xvfb  (apt  install  xvfb  on  Ubuntu  or
Debian) and start Python using the following command: xvfb-run
-s "-screen 0 1400x900x24" python3. Alternatively, install Xvfb
and  the  pyvirtualdisplay  library  (which  wraps  Xvfb)  and  run
pyvirtualdisplay.Display(visible=0, 
size=(1400,
900)).start() at the beginning of your program.

If  you  want  render()  to  return  the  rendered  image  as  a  NumPy  array,  you  can  set
mode="rgb_array" (oddly, this environment will render the environment to screen as
well):

>>> img = env.render(mode="rgb_array")
>>> img.shape  # height, width, channels (3 = Red, Green, Blue)
(800, 1200, 3)

Let’s ask the environment what actions are possible:

>>> env.action_space
Discrete(2)

Discrete(2)  means  that  the  possible  actions  are  integers  0  and  1,  which  represent
accelerating  left  (0)  or  right  (1).  Other  environments  may  have  additional  discrete

Introduction to OpenAI Gym 

| 

615

actions, or other kinds of actions (e.g., continuous). Since the pole is leaning toward
the right (obs[2] > 0), let’s accelerate the cart toward the right:

>>> action = 1  # accelerate right
>>> obs, reward, done, info = env.step(action)
>>> obs
array([-0.01261699,  0.19292789,  0.04204097, -0.28092127])
>>> reward
1.0
>>> done
False
>>> info
{}

The step() method executes the given action and returns four values:

obs

This is the new observation. The cart is now moving toward the right (obs[1] >
0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is
now negative (obs[3] < 0), so it will likely be tilted toward the left after the next
step.

reward

In this environment, you get a reward of 1.0 at every step, no matter what you do,
so the goal is to keep the episode running as long as possible.

done

This value will be True when the episode is 
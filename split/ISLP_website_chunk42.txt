s that we obtain
will not be accurate estimates of the test error.
We now compute the cross-validated predicted values using 5-fold crossvalidation.
In [14]: K = 5
kfold = skm.KFold(K,
random_state =0,
shuffle=True)
Yhat_cv = skm.cross_val_predict (full_path ,
Hitters ,
Y,
cv=kfold)
Yhat_cv.shape

Out[14]: (263, 20)

The prediction matrix Yhat_cv is the same shape as Yhat_in; the difference
is that the predictions in each row, corresponding to a particular sample
index, were made from models fit on a training fold that did not include
that row.
At each model along the path, we compute the MSE in each of the crossvalidation folds. These we will average to get the mean MSE, and can also
use the individual values to compute a crude estimate of the standard error
of the mean.9 Hence we must know the test indices for each cross-validation
split. This can be found by using the split() method of kfold. Because we
fixed the random state above, whenever we split any array with the same
number of rows as Y we recover the same training and test indices, though
we simply ignore the training indices below.
In [15]: cv_mse = []
for train_idx , test_idx in kfold.split(Y):
errors = (Yhat_cv[test_idx] - Y[test_idx ,None ])**2
cv_mse.append(errors.mean (0)) # column means
cv_mse = np.array(cv_mse).T
cv_mse.shape
Out[15]: (20, 5)
9 The estimate is crude because the five error estimates are based on overlapping
training sets, and hence are not independent.

skm.KFold()
skm.cross_
val_predict()

272

6. Linear Model Selection and Regularization

We now add the cross-validation error estimates to our MSE plot. We
include the mean error across the five folds, and the estimate of the standard
error of the mean.
In [16]: ax.errorbar(np.arange(n_steps),
cv_mse.mean (1),
cv_mse.std (1) / np.sqrt(K),
label='Cross -validated ',
c='r') # color red
ax.set_ylim ([50000 ,250000])
ax.legend ()
mse_fig

To repeat the above using the validation set approach, we simply change
our cv argument to a validation set: one random split of the data into a
test and training. We choose a test size of 20%, similar to the size of each
test set in 5-fold cross-validation.
skm.Shuffle
In [17]: validation = skm.ShuffleSplit(n_splits =1,
test_size =0.2,
random_state =0)
for train_idx , test_idx in validation.split(Y):
full_path.fit(Hitters.iloc[train_idx],
Y[train_idx ])
Yhat_val = full_path.predict(Hitters.iloc[test_idx ])
errors = (Yhat_val - Y[test_idx ,None ])**2
validation_mse = errors.mean (0)

As for the in-sample MSE case, the validation set approach does not provide
standard errors.
In [18]: ax.plot(np.arange(n_steps),
validation_mse ,
'b--', # color blue , broken line
label='Validation ')
ax.set_xticks(np.arange(n_steps)[::2])
ax.set_ylim ([50000 ,250000])
ax.legend ()
mse_fig

Best Subset Selection
Forward stepwise is a greedy selection procedure; at each step it augments
the current set by including one additional variable. We now apply best
subset selection to the Hitters data, which for every subset size, searches
for the best set of predictors.
We will use a package called l0bnb to perform best subset selection.
Instead of constraining the subset to be a given size, this package produces a
path of solutions using the subset size as a penalty rather than a constraint.
Although the distinction is subtle, the difference comes when we crossvalidate.
In [19]: D = design.fit_transform(Hitters)
D = D.drop('intercept ', axis =1)
X = np.asarray(D)

Split()

6.5 Lab: Linear Models and Regularization Methods

273

Here we excluded the first column corresponding to the intercept, as l0bnb
will fit the intercept separately. We can find a path using the fit_path()
function.
In [20]: path = fit_path(X,
Y,
max_nonzeros=X.shape [1])

The function fit_path() returns a list whose values include the fitted
coefficients as B, an intercept as B0, as well as a few other attributes related
to the particular path algorithm used. Such details are beyond the scope
of this book.
In [21]: path [3]
Out[21]: {'B': array ([0.
, 3.254844 , 0.
0.
, 0.
, 0.
0.
, 0.677753 , 0.
0.
, 0.
, 0.
'B0': -38.98216739555494 ,
'lambda_0 ': 0.011416248027450194 ,
'M': 0.5829861733382011 ,
'Time_exceeded ': False}

, 0.
, 0.
, 0.

, 0.

, 0.
, 0.
]),

, 0.

,
,

,

In the example above, we see that at the fourth step in the path, we have
two nonzero coefficients in 'B', corresponding to the value 0.114 for the
penalty parameter lambda_0. We could make predictions using this sequence
of fits on a validation set as a function of lambda_0, or with more work using
cross-validation.

6.5.2

Ridge Regression and the Lasso

We will use the sklearn.linear_model package (for which we use skl as
shorthand below) to fit ridge and lasso regularized linear models on the
Hitters data. We start with the model matrix X (without an intercept)
that we computed in the previous section on best subset regression.
Ridge Regression
We will use the function skl.ElasticNet() to fit both ridge and the lasso. To skl.Elastic
fit a path of ridge regressions models, we use skl.ElasticNet.path(), which Net()
can fit both ridge and lasso, as well as a hybrid mixture; ridge regression skl.Elastic
corresponds to l1_ratio=0. It is good practice to standardize the columns Net.path()
of X in these applications, if the variables are measured in different units.
Since skl.ElasticNet() does no normalization, we have to take care of that
ourselves. Since we standardize first, in order to find coefficient estimates
on the original scale, we must unstandardize the coefficient estimates. The
parameter λ in (6.5) and (6.7) is called alphas in sklearn. In order to be
consistent with the rest of this chapter, we use lambdas rather than alphas
in what follows. 10
10 At the time of publication, ridge fits like the one in code chunk [22] issue unwarranted
convergence warning messages; we expect these to disappear as this package matures.

274

6. Linear Model Selection and Regularization

In [22]: Xs = X - X.mean (0)[None ,:]
X_scale = X.std (0)
Xs = Xs / X_scale[None ,:]
lambdas = 10** np.linspace (8, -2, 100) / Y.std()
soln_array = skl.ElasticNet.path(Xs ,
Y,
l1_ratio =0.,
alphas=lambdas)[1]
soln_array.shape
Out[22]: (19, 100)

Here we extract the array of coefficients corresponding to the solutions
along the regularization path. By default the skl.ElasticNet.path method
fits a path along an automatically selected range of λ values, except for
the case when l1_ratio=0, which results in ridge regression (as is the case
here).11 So here we have chosen to implement the function over a grid of
values ranging from λ = 108 to λ = 10−2 scaled by the standard deviation
of y, essentially covering the full range of scenarios from the null model
containing only the intercept, to the least squares fit.
Associated with each value of λ is a vector of ridge regression coefficients,
that can be accessed by a column of soln_array. In this case, soln_array is
a 19 × 100 matrix, with 19 rows (one for each predictor) and 100 columns
(one for each value of λ).
We transpose this matrix and turn it into a data frame to facilitate
viewing and plotting.
In [23]: soln_path = pd.DataFrame(soln_array.T,
columns=D.columns ,
index=-np.log(lambdas))
soln_path.index.name = 'negative log(lambda)'
soln_path
Out[23]:

AtBat
negative
log(lambda)
-12.310855
0.000800
-12.078271
0.001010
-11.845686
0.001274
-11.613102
0.001608
-11.380518
0.002029
...
...
100 rows × 19 columns

Hits

HmRun

Runs

...

0.000889
0.001122
0.001416
0.001787
0.002255
...

0.000695
0.000878
0.001107
0.001397
0.001763
...

0.000851
0.001074
0.001355
0.001710
0.002158
...

...
...
...
...
...
...

We plot the paths to get a sense of how the coefficients vary with λ. To
control the location of the legend we first set legend to False in the plot
method, adding it afterward with the legend() method of ax.
In [24]: path_fig , ax = subplots(figsize =(8 ,8))
soln_path.plot(ax=ax , legend=False)
ax.set_xlabel('$-\log(\ lambda)$', fontsize =20)
11 The reason is rather technical; for all models except ridge, we can find the smallest
value of λ for which all coefficients are zero. For ridge this value is ∞.

6.5 Lab: Linear Models and Regularization Methods

275

ax.set_ylabel('Standardized coefficients ', fontsize =20)
ax.legend(loc='upper left ');

(We have used latex formatting in the horizontal label, in order to format
the Greek λ appropriately.) We expect the coefficient estimates to be much
smaller, in terms of %2 norm, when a large value of λ is used, as compared
to when a small value of λ is used. (Recall that the %2 norm is the square
root of the sum of squared coefficient values.) We display the coefficients
at the 40th step, where λ is 25.535.
In [25]: beta_hat = soln_path.loc[soln_path.index [39]]
lambdas [39], beta_hat
Out[25]: (25.535 ,
AtBat
Hits
HmRun
Runs
RBI
Walks
Years
...

5.433750
6.223582
4.585498
5.880855
6.195921
6.277975
5.299767
...

Let’s compute the %2 norm of the standardized coefficients.
In [26]: np.linalg.norm(beta_hat)
Out[26]: 24.17

In contrast, here is the %2 norm when λ is 2.44e-01. Note the much larger
%2 norm of the coefficients associated with this smaller value of λ.
In [27]: beta_hat = soln_path.loc[soln_path.index [59]]
lambdas [59], np.linalg.norm(beta_hat)
Out[27]: (0.2437 , 160.4237)

Above we normalized X upfront, and fit the ridge model using Xs. The
Pipeline() object in sklearn provides a clear way to separate feature normalization from the fitting of the ridge model itself.
In [28]: ridge = skl.ElasticNet(alpha=lambdas [59], l1_ratio =0)
scaler = StandardScaler(with_mean=True , with_std=True)
pipe = Pipeline(steps =[('scaler ', scaler), ('ridge ', ridge)])
pipe.fit(X, Y)

We show that it gives the same %2 norm as in our previous fit on the
standardized data.
In [29]: np.linalg.norm(ridge.coef_)
Out[29]: 160.4237

Notice that the operation pipe.fit(X, Y) above has changed the ridge
object, and in particular has added attributes such as coef_ that were not
there before.

276

6. Linear Model Selection and Regularization

Estimating Test Error of Ridge Regression
Choosing an a priori value of λ for ridge regression is difficult if not impossible. We will want to use the validation method or cross-validation to
select the tuning parameter. The reader may not be surprised that the
Pipeline() approach can be used in skm.cross_validate() with either a
validation method (i.e. validation) or k-fold cross-validation.
We fix the random state of the splitter so that the results obtained will
be reproducible.
In [30]: validation = skm.ShuffleSplit(n_splits =1,
test_size =0.5,
random_state =0)
ridge.alpha = 0.01
results = skm.cross_validate(ridge ,
X,
Y,
scoring='neg_mean_squared_error ',
cv=validation)
-results['test_score ']
Out[30]: array ([134214.0])

The test MSE is 1.342e+05. Note that if we had instead simply fit a model
with just an intercept, we would have predicted each test observation using
the mean of the training observations. We can get the same result by fitting
a ridge regression model with a very large value of λ. Note that 1e10 means
1010 .
In [31]: ridge.alpha = 1e10
results = skm.cross_validate(ridge ,
X,
Y,
scoring='neg_mean_squared_error ',
cv=validation)
-results['test_score ']
Out[31]: array ([231788.32])

Obviously choosing λ = 0.01 is arbitrary, so we will use cross-validation or
the validation-set approach to choose the tuning parameter λ. The object
GridSearchCV() allows exhaustive grid search to choose such a parameter.
Grid
We first use the validation set method to choose λ.
SearchCV()
In [32]: param_grid = {'ridge__alpha ': lambdas}
grid = skm.GridSearchCV(pipe ,
param_grid ,
cv=validation ,
scoring='neg_mean_squared_error ')
grid.fit(X, Y)
grid.best_params_['ridge__alpha ']
grid.best_estimator_
Out[32]: Pipeline(steps =[('scaler ', StandardScaler ()),
('ridge ', ElasticNet(alpha =0.005899 , l1_ratio =0))])

6.5 Lab: Linear Models and Regularization Methods

277

Alternatively, we can use 5-fold cross-validation.
In [33]: grid = skm.GridSearchCV(pipe ,
param_grid ,
cv=kfold ,
scoring='neg_mean_squared_error ')
grid.fit(X, Y)
grid.best_params_['ridge__alpha ']
grid.best_estimator_

Recall we set up the kfold object for 5-fold cross-validation on page 271.
We now plot the cross-validated MSE as a function of − log(λ), which has
shrinkage decreasing from left to right.
In [34]: ridge_fig , ax = subplots(figsize =(8 ,8))
ax.errorbar(-np.log(lambdas),
-grid.cv_results_['mean_test_score '],
yerr=grid.cv_results_['std_test_score '] / np.sqrt(K))
ax.set_ylim ([50000 ,250000])
ax.set_xlabel('$-\log(\ lambda)$', fontsize =20)
ax.set_ylabel('Cross -validated MSE', fontsize =20);

One can cross-validate different metrics to choose a parameter. The default metric for skl.ElasticNet() is test R2 . Let’s compare R2 to MSE for
cross-validation here.
In [35]: grid_r2 = skm.GridSearchCV(pipe ,
param_grid ,
cv=kfold)
grid_r2.fit(X, Y)

Finally, let’s plot the results for cross-validated R2 .
In [36]: r2_fig , ax = subplots(figsize =(8 ,8))
ax.errorbar(-np.log(lambdas),
grid_r2.cv_results_['mean_test_score '],
yerr=grid_r2.cv_results_['std_test_score '] / np.sqrt(K)
)
ax.set_xlabel('$-\log(\ lambda)$', fontsize =20)
ax.set_ylabel('Cross -validated $R^2$', fontsize =20);

Fast Cross-Validation for Solution Paths
The ridge, lasso, and elastic net can be efficiently fit along a sequence of
λ values, creating what is known as a solution path or regularization path.
Hence there is specialized code to fit such paths, and to choose a suitable
value of λ using cross-validation. Even with identical splits the results will
not agree exactly with our grid above because the standardization of each
feature in grid is carried out on each fold, while in pipeCV below it is carried
out only once. Nevertheless, the results are similar as the normalization is
relatively stable across folds.
In [37]: ridgeCV = skl.ElasticNetCV(alphas=lambdas ,
l1_ratio =0,
cv=kfold)
pipeCV = Pipeline(steps =[('scaler ', scaler),

278

6. Linear Model Selection and Regularization

pipeCV.fit(X, Y)

('ridge ', ridgeCV)])

Let’s produce a plot again of the cross-validation error to see that it is
similar to using skm.GridSearchCV.
In [38]: tuned_ridge = pipeCV.named_steps['ridge ']
ridgeCV_fig , ax = subplots(figsize =(8 ,8))
ax.errorbar(-np.log(lambdas),
tuned_ridge.mse_path_.mean (1),
yerr=tuned_ridge.mse_path_.std (1) / np.sqrt(K))
ax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')
ax.set_ylim ([50000 ,250000])
ax.set_xlabel('$-\log(\ lambda)$', fontsize =20)
ax.set_ylabel('Cross -validated MSE', fontsize =20);

We see that the value of λ that results in the smallest cross-validation
error is 1.19e-02, available as the value tuned_ridge.alpha_. What is the
test MSE associated with this value of λ?
In [39]: np.min(tuned_ridge.mse_path_.mean (1))
Out[39]: 115526.71

This represents a further improvement over the test MSE that we got using
λ = 4. Finally, tuned_ridge.coef_ has the coefficients fit on the entire data
set at this value of λ.
In [40]: tuned_ridge.coef_
Out[40]: array ([ -222.80877051 ,
3.64888723 ,
122.00714801 ,
-150.21959435 ,
40.07350744 ,

238.77246614 ,
108.90953869 ,
57.1859509 ,
30.36634231 ,
-25.02151514 ,

3.21103754 ,
-2.93050845 ,
-50.81896152 , -105.15731984 ,
210.35170348 , 118.05683748 ,
-61.62459095 ,
77.73832472 ,
-13.68429544])

As expected, none of the coefficients are zero—ridge regression does not
perform variable selection!
Evaluating Test Error of Cross-Valid
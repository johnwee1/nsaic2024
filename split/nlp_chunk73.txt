apter 8, measuring the percentage of relations labeled NSUBJ by
the system that were correct (precision), and the percentage of the NSUBJ relations
present in the development set that were in fact discovered by the system (recall).
We can employ a confusion matrix to keep track of how often each dependency type
was confused for another.

18.5 Summary

This chapter has introduced the concept of dependency grammars and dependency
parsing. Here’s a summary of the main points that we covered:

• In dependency-based approaches to syntax, the structure of a sentence is de-
scribed in terms of a set of binary relations that hold between the words in a
sentence. Larger notions of constituency are not directly encoded in depen-
dency analyses.

• The relations in a dependency structure capture the head-dependent relation-

ship among the words in a sentence.

• Dependency-based analysis provides information directly useful in further
language processing tasks including information extraction, semantic parsing
and question answering.

• Transition-based parsing systems employ a greedy stack-based algorithm to

create dependency structures.

• Graph-based methods for creating dependency structures are based on the use

of maximum spanning tree methods from graph theory.

• Both transition-based and graph-based approaches are developed using super-

vised machine learning techniques.

• Treebanks provide the data needed to train these systems. Dependency tree-
banks can be created directly by human annotators or via automatic transfor-
mation from phrase-structure treebanks.

• Evaluation of dependency parsers is based on labeled and unlabeled accuracy

scores as measured against withheld development and test corpora.

BIBLIOGRAPHICAL AND HISTORICAL NOTES

413

Bibliographical and Historical Notes

The dependency-based approach to grammar is much older than the relatively recent
phrase-structure or constituency grammars, which date only to the 20th century. De-
pendency grammar dates back to the Indian grammarian P¯an. ini sometime between
the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions.
Contemporary theories of dependency grammar all draw heavily on the 20th cen-
tury work of Tesni`ere (1959).

Automatic parsing using dependency grammars was ﬁrst introduced into compu-
tational linguistics by early work on machine translation at the RAND Corporation
led by David Hays. This work on dependency parsing closely paralleled work on
constituent parsing and made explicit use of grammars to guide the parsing process.
After this early period, computational work on dependency parsing remained inter-
mittent over the following decades. Notable implementations of dependency parsers
for English during this period include Link Grammar (Sleator and Temperley, 1993),
Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).

Dependency parsing saw a major resurgence in the late 1990’s with the appear-
ance of large dependency-based treebanks and the associated advent of data driven
approaches described in this chapter. Eisner (1996) developed an efﬁcient dynamic
programming approach to dependency parsing based on bilexical grammars derived
from the Penn Treebank. Covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. Yamada and Mat-
sumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce
paradigm and the use of supervised machine learning in the form of support vector
machines to dependency parsing.

Transition-based parsing is based on the shift-reduce parsing algorithm orig-
inally developed for analyzing programming languages (Aho and Ullman, 1972).
Shift-reduce parsing also makes use of a context-free grammar. Input tokens are
successively shifted onto the stack and the top two elements of the stack are matched
against the right-hand side of the rules in the grammar; when a match is found the
matched elements are replaced on the stack (reduced) by the non-terminal from the
left-hand side of the rule being matched. In transition-based dependency parsing
we skip the grammar, and alter the reduce operation to add a dependency relation
between a word and its head.

Nivre (2003) deﬁned the modern, deterministic, transition-based approach to
dependency parsing. Subsequent work by Nivre and his colleagues formalized and
analyzed the performance of numerous transition systems, training methods, and
methods for dealing with non-projective language (Nivre and Scholz 2004, Nivre
2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-
proach was pioneered by Chen and Manning (2014) and extended by Kiperwasser
and Goldberg (2016); Kulmizev et al. (2019).

The graph-based maximum spanning tree approach to dependency parsing was
introduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classiﬁer
was introduced by (Kiperwasser and Goldberg, 2016).

The long-running Prague Dependency Treebank project (Hajiˇc, 1998) is the most
signiﬁcant effort to directly annotate a corpus with multiple layers of morphological,
syntactic and semantic information. PDT 3.0 contains over 1.5 M tokens (Bejˇcek
et al., 2013).

Universal Dependencies (UD) (de Marneffe et al., 2021) is an open community

414 CHAPTER 18

• DEPENDENCY PARSING

project to create a framework for dependency treebank annotation, with nearly 200
treebanks in over 100 languages. The UD annotation scheme evolved out of several
distinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marn-
effe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-of-speech
tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets
(Zeman, 2008).

The Conference on Natural Language Learning (CoNLL) has conducted an in-
ﬂuential series of shared tasks related to dependency parsing over the years (Buch-
holz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajiˇc et al. 2009).
More recent evaluations have focused on parser robustness with respect to morpho-
logically rich languages (Seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (Petrov and McDonald, 2012).
Choi et al. (2015) presents a performance analysis of 10 dependency parsers across
a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool.

Exercises

CHAPTER

19 Information

Relations, Events, and Time

Extraction:

Time will explain.
Jane Austen, Persuasion

Imagine that you are an analyst with an investment ﬁrm that tracks airline stocks.
You’re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. His-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? You will need to know at least the name of the airline, the nature of
the proposed fare hike, the dates of the announcement, and possibly the response of
other airlines. Fortunately, these can be all found in news articles like this one:

Citing high fuel prices, United Airlines said Friday it has increased fares
by $6 per round trip on ﬂights to some cities also served by lower-
cost carriers. American Airlines, a unit of AMR Corp., immediately
matched the move, spokesman Tim Wagner said. United, a unit of UAL
Corp., said the increase took effect Thursday and applies to most routes
where it competes against discount carriers, such as Chicago to Dallas
and Denver to San Francisco.

This chapter presents techniques for extracting limited kinds of semantic con-
tent from text. This process of information extraction (IE) turns the unstructured
information embedded in texts into structured data, for example for populating a
relational database to enable further processing.

We begin with the task of relation extraction: ﬁnding and classifying semantic
relations among entities mentioned in a text, like child-of (X is the child-of Y), or
part-whole or geospatial relations. Relation extraction has close links to populat-
ing a relational database, and knowledge graphs, datasets of structured relational
knowledge, are a useful way for search engines to present information to users.

Next, we discuss event extraction, the task of ﬁnding events in which these en-
tities participate, like, in our sample text, the fare increases by United and American
and the reporting events said and cite. Events are also situated in time, occurring at
a particular date or time, and events can be related temporally, happening before or
after or simultaneously with each other. We’ll need to recognize temporal expres-
sions like Friday, Thursday or two days from now and times such as 3:30 P.M., and
normalize them onto speciﬁc calendar dates or times. We’ll need to link Friday to
the time of United’s announcement, Thursday to the previous day’s fare increase,
and we’ll need to produce a timeline in which United’s announcement follows the
fare increase and American’s announcement follows both of those events.

The related task of template ﬁlling is to ﬁnd recurring stereotypical events or
situations in documents and ﬁll in the template slots. These slot-ﬁllers may consist
of text segments extracted directly from the text, or concepts like times, amounts, or
ontology entities that have been inferred through additional processing. Our airline

information
extraction

relation
extraction

knowledge
graphs

event
extraction

template ﬁlling

416 CHAPTER 19

•

INFORMATION EXTRACTION: RELATIONS, EVENTS, AND TIME

Figure 19.1 The 17 relations used in the ACE relation extraction task.

text presents such a stereotypical situation since airlines often raise fares and then
wait to see if competitors follow along. Here we can identify United as a lead air-
line that initially raised its fares, $6 as the amount, Thursday as the increase date,
and American as an airline that followed along, leading to a ﬁlled template like the
following:

FARE-RAISE ATTEMPT:

LEAD AIRLINE:
AMOUNT:
EFFECTIVE DATE:
FOLLOWER:

UNITED AIRLINES
$6
2006-10-26
AMERICAN AIRLINES















19.1 Relation Extraction

Let’s assume that we have detected the named entities in our sample text (perhaps
using the techniques of Chapter 8), and would like to discern the relationships that
exist among the detected entities:

Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
has increased fares by [MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ORG American Airlines], a
unit of [ORG AMR Corp.], immediately matched the move, spokesman
[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
said the increase took effect [TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [LOC Chicago]
to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].

The text tells us, for example, that Tim Wagner is a spokesman for American
Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR.
These binary relations are instances of more generic relations such as part-of or
employs that are fairly frequent in news-style texts. Figure 19.1 lists the 17 relations
used in the ACE relation extraction evaluations and Fig. 19.2 shows some sample
relations. We might also extract more domain-speciﬁc relation such as the notion of
an airline route. For example from this text we can conclude that United has routes
to Chicago, Dallas, Denver, and San Francisco.

ARTIFACTGENERALAFFILIATIONORGAFFILIATIONPART-WHOLEPERSON-SOCIALPHYSICALLocatedNearBusinessFamilyLasting PersonalCitizen-Resident-Ethnicity-ReligionOrg-Location-OriginFounderEmploymentMembershipOwnershipStudent-AlumInvestorUser-Owner-Inventor-ManufacturerGeographicalSubsidiarySports-Affiliation19.1

• RELATION EXTRACTION

417

Relations
Physical-Located
Part-Whole-Subsidiary
Person-Social-Family
Org-AFF-Founder
Figure 19.2 Semantic relations with examples and the named entity types they involve.

Examples
He was in Tennessee
XYZ, the parent company of ABC
Yoko’s husband John
Steve Jobs, co-founder of Apple...

Types
PER-GPE
ORG-ORG
PER-PER
PER-ORG

Sets of relations have been deﬁned for many other domains as well. For example
UMLS, the Uniﬁed Medical Language System from the US National Library of
Medicine has a network that deﬁnes 134 broad subject categories, entity types, and
54 relations between the entities, such as the following:

Relation
Entity
disrupts
Injury
location-of Biologic Function
Bodily Location
Anatomical Structure
part-of
Pharmacologic Substance causes
Pharmacologic Substance treats

Organism
Pathological Function
Pathologic Function

Entity
Physiological Function

Given a medical sentence like this one:

(19.1) Doppler echocardiography can be used to diagnose left anterior descending

artery stenosis in patients with type 2 diabetes

We could thus extract the UMLS relation:

Echocardiography, Doppler Diagnoses Acquired stenosis

infoboxes

RDF

RDF triple

Wikipedia also offers a large supply of relations, drawn from infoboxes, struc-
tured tables associated with certain Wikipedia articles. For example, the Wikipedia
infobox for Stanford includes structured facts like state = "California" or
president = "Marc Tessier-Lavigne". These facts can be turned into rela-
tions like president-of or located-in. or into relations in a metalanguage called RDF
(Resource Description Framework). An RDF triple is a tuple of entity-relation-
entity, called a subject-predicate-object expression. Here’s a sample RDF triple:

subject
Golden Gate Park location

predicate object

San Francisco

Freebase

For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-
rived from Wikipedia containing over 2 billion RDF triples. Another dataset from
Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrandeˇci´c
and Kr¨otzsch, 2014), has relations between people and their nationality, or locations,
and other locations they are contained in.

is-a

hypernym

WordNet or other ontologies offer useful ontological relations that express hier-
archical relations between words or concepts. For example WordNet has the is-a or
hypernym relation between classes,

Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...

WordNet also has Instance-of relation between individuals and classes, so that for
example San Francisco is in the Instance-of relation with city. Extracting these
relations is an important step in extending or building ontologies.

Finally, there are large datasets that contain sentences hand-labeled with their
relations, designed for training and testing relation extractors. The TACRED dataset
(Zhang et al., 2017) contains 106,264 examples of relation triples about particular
people or organizations, labeled in sentences from news and web text drawn from the

418 CHAPTER 19

•

INFORMATION EXTRACTION: RELATIONS, EVENTS, AND TIME

annual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains
41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse),
plus a no relation tag; examples are shown in Fig. 19.3. About 80% of all examples
are annotated as no relation; having sufﬁcient negative data is important for training
supervised classiﬁers.

Example
Carey will succeed Cathleen P. Black, who held the position for 15
years and will take on a new role as chairwoman of Hearst Maga-

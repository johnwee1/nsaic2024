er than this.
Usually, most variables inï¬‚uence each other only indirectly.
For example, consider modeling the ï¬?nishing times of a team in a relay race.
Suppose the team consists of three runners: Alice, Bob and Carol. At the start of
the race, Alice carries a baton and begins running around a track. After completing
her lap around the track, she hands the baton to Bob. Bob then runs his own
lap and hands the baton to Carol, who runs the ï¬?nal lap. We can model each of
their ï¬?nishing times as a continuous random variable. Aliceâ€™s ï¬?nishing time does
not depend on anyone elseâ€™s, since she goes ï¬?rst. Bobâ€™s ï¬?nishing time depends
on Aliceâ€™s, because Bob does not have the opportunity to start his lap until Alice
has completed hers. If Alice ï¬?nishes faster, Bob will ï¬?nish faster, all else being
562

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

equal. Finally, Carolâ€™s ï¬?nishing time depends on both her teammates. If Alice is
slow, Bob will probably ï¬?nish late too. As a consequence, Carol will have quite a
late starting time and thus is likely to have a late ï¬?nishing time as well. However,
Carolâ€™s ï¬?nishing time depends only indirectly on Aliceâ€™s ï¬?nishing time via Bobâ€™s.
If we already know Bobâ€™s ï¬?nishing time, we will not be able to estimate Carolâ€™s
ï¬?nishing time better by ï¬?nding out what Aliceâ€™s ï¬?nishing time was. This means
we can model the relay race using only two interactions: Aliceâ€™s eï¬€ect on Bob and
Bobâ€™s eï¬€ect on Carol. We can omit the third, indirect interaction between Alice
and Carol from our model.
Structured probabilistic models provide a formal framework for modeling only
direct interactions between random variables. This allows the models to have
signiï¬?cantly fewer parameters and therefore be estimated reliably from less data.
These smaller models also have dramatically reduced computational cost in terms
of storing the model, performing inference in the model, and drawing samples from
the model.

16.2

Using Graphs to Describe Model Structure

Structured probabilistic models use graphs (in the graph theory sense of â€œnodesâ€? or
â€œverticesâ€? connected by edges) to represent interactions between random variables.
Each node represents a random variable. Each edge represents a direct interaction.
These direct interactions imply other, indirect interactions, but only the direct
interactions need to be explicitly modeled.
There is more than one way to describe the interactions in a probability
distribution using a graph. In the following sections we describe some of the most
popular and useful approaches. Graphical models can be largely divided into
two categories: models based on directed acyclic graphs, and models based on
undirected graphs.

16.2.1

Directed Models

One kind of structured probabilistic model is the directed graphical model,
otherwise known as the belief network or Bayesian network2 (Pearl, 1985).
Directed graphical models are called â€œdirectedâ€? because their edges are directed,
2

Judea Pearl suggested using the term â€œBayesian networkâ€? when one wishes to â€œemphasize
the judgmentalâ€? nature of the values computed by the network, i.e. to highlight that they usually
represent degrees of belief rather than frequencies of events.
563

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Alice

Bob

Carol

t0

t1

t2

Figure 16.2: A directed graphical model depicting the relay race example. Aliceâ€™s ï¬?nishing
time t0 inï¬‚uences Bobâ€™s ï¬?nishing time t1 , because Bob does not get to start running until
Alice ï¬?nishes. Likewise, Carol only gets to start running after Bob ï¬?nishes, so Bobâ€™s
ï¬?nishing time t1 directly inï¬‚uences Carolâ€™s ï¬?nishing time t2 .

that is, they point from one vertex to another. This direction is represented in
the drawing with an arrow. The direction of the arrow indicates which variableâ€™s
probability distribution is deï¬?ned in terms of the otherâ€™s. Drawing an arrow from
a to b means that we deï¬?ne the probability distribution over b via a conditional
distribution, with a as one of the variables on the right side of the conditioning
bar. In other words, the distribution over b depends on the value of a.
Continuing with the relay race example from section 16.1, suppose we name
Aliceâ€™s ï¬?nishing time t 0, Bobâ€™s ï¬?nishing time t1, and Carolâ€™s ï¬?nishing time t2.
As we saw earlier, our estimate of t 1 depends on t0 . Our estimate of t2 depends
directly on t1 but only indirectly on t0 . We can draw this relationship in a directed
graphical model, illustrated in ï¬?gure 16.2.
Formally, a directed graphical model deï¬?ned on variables x is deï¬?ned by a
directed acyclic graph G whose vertices are the random variables in the model,
and a set of local conditional probability distributions p(xi | P aG (xi)) where
P aG(xi ) gives the parents of xi in G . The probability distribution over x is given
by
p(x) = Î i p(xi | P aG (xi )).
(16.1)
In our relay race example, this means that, using the graph drawn in ï¬?gure 16.2,
p(t0 , t1, t 2) = p(t0)p(t 1 | t0)p(t2 | t1 ).

(16.2)

This is our ï¬?rst time seeing a structured probabilistic model in action. We
can examine the cost of using it, in order to observe how structured modeling has
many advantages relative to unstructured modeling.
Suppose we represented time by discretizing time ranging from minute 0 to
minute 10 into 6 second chunks. This would make t0 , t1 and t2 each be a discrete
variable with 100 possible values. If we attempted to represent p (t0, t 1 , t2) with a
table, it would need to store 999,999 values (100 values of t0 Ã— 100 values of t 1 Ã—
100 values of t2, minus 1, since the probability of one of the conï¬?gurations is made
564

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

redundant by the constraint that the sum of the probabilities be 1). If instead, we
only make a table for each of the conditional probability distributions, then the
distribution over t 0 requires 99 values, the table deï¬?ning t1 given t0 requires 9900
values, and so does the table deï¬?ning t2 given t1. This comes to a total of 19,899
values. This means that using the directed graphical model reduced our number of
parameters by a factor of more than 50!
In general, to model n discrete variables each having k values, the cost of the
single table approach scales like O(k n), as we have observed before. Now suppose
we build a directed graphical model over these variables. If m is the maximum
number of variables appearing (on either side of the conditioning bar) in a single
conditional probability distribution, then the cost of the tables for the directed
model scales like O(km). As long as we can design a model such that m << n, we
get very dramatic savings.
In other words, so long as each variable has few parents in the graph, the
distribution can be represented with very few parameters. Some restrictions on
the graph structure, such as requiring it to be a tree, can also guarantee that
operations like computing marginal or conditional distributions over subsets of
variables are eï¬ƒcient.
It is important to realize what kinds of information can and cannot be encoded in
the graph. The graph encodes only simplifying assumptions about which variables
are conditionally independent from each other. It is also possible to make other
kinds of simplifying assumptions. For example, suppose we assume Bob always
runs the same regardless of how Alice performed. (In reality, Aliceâ€™s performance
probably inï¬‚uences Bobâ€™s performanceâ€”depending on Bobâ€™s personality, if Alice
runs especially fast in a given race, this might encourage Bob to push hard and
match her exceptional performance, or it might make him overconï¬?dent and lazy).
Then the only eï¬€ect Alice has on Bobâ€™s ï¬?nishing time is that we must add Aliceâ€™s
ï¬?nishing time to the total amount of time we think Bob needs to run. This
observation allows us to deï¬?ne a model with O(k) parameters instead of O(k 2).
However, note that t0 and t1 are still directly dependent with this assumption,
because t1 represents the absolute time at which Bob ï¬?nishes, not the total time
he himself spends running. This means our graph must still contain an arrow from
t0 to t1. The assumption that Bobâ€™s personal running time is independent from
all other factors cannot be encoded in a graph over t0, t1 , and t2 . Instead, we
encode this information in the deï¬?nition of the conditional distribution itself. The
conditional distribution is no longer a k Ã— k âˆ’ 1 element table indexed by t0 and t1
but is now a slightly more complicated formula using only k âˆ’ 1 parameters. The
directed graphical model syntax does not place any constraint on how we deï¬?ne
565

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

our conditional distributions. It only deï¬?nes which variables they are allowed to
take in as arguments.

16.2.2

Undirected Models

Directed graphical models give us one language for describing structured probabilistic models. Another popular language is that of undirected models, otherwise
known as Markov random ï¬?elds (MRFs) or Markov networks (Kindermann, 1980). As their name implies, undirected models use graphs whose edges
are undirected.
Directed models are most naturally applicable to situations where there is
a clear reason to draw each arrow in one particular direction. Often these are
situations where we understand the causality and the causality only ï¬‚ows in one
direction. One such situation is the relay race example. Earlier runners aï¬€ect the
ï¬?nishing times of later runners; later runners do not aï¬€ect the ï¬?nishing times of
earlier runners.
Not all situations we might want to model have such a clear direction to their
interactions. When the interactions seem to have no intrinsic direction, or to
operate in both directions, it may be more appropriate to use an undirected model.
As an example of such a situation, suppose we want to model a distribution
over three binary variables: whether or not you are sick, whether or not your
coworker is sick, and whether or not your roommate is sick. As in the relay race
example, we can make simplifying assumptions about the kinds of interactions that
take place. Assuming that your coworker and your roommate do not know each
other, it is very unlikely that one of them will give the other an infection such as a
cold directly. This event can be seen as so rare that it is acceptable not to model
it. However, it is reasonably likely that either of them could give you a cold, and
that you could pass it on to the other. We can model the indirect transmission of
a cold from your coworker to your roommate by modeling the transmission of the
cold from your coworker to you and the transmission of the cold from you to your
roommate.
In this case, it is just as easy for you to cause your roommate to get sick as
it is for your roommate to make you sick, so there is not a clean, uni-directional
narrative on which to base the model. This motivates using an undirected model.
As with directed models, if two nodes in an undirected model are connected by an
edge, then the random variables corresponding to those nodes interact with each
other directly. Unlike directed models, the edge in an undirected model has no
arrow, and is not associated with a conditional probability distribution.
566

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

hr

hy

hc

Figure 16.3: An undirected graph representing how your roommateâ€™s health h r , your
health hy , and your work colleagueâ€™s health h c aï¬€ect each other. You and your roommate
might infect each other with a cold, and you and your work colleague might do the same,
but assuming that your roommate and your colleague do not know each other, they can
only infect each other indirectly via you.

We denote the random variable representing your health as h y, the random
variable representing your roommateâ€™s health as hr , and the random variable
representing your colleagueâ€™s health as h c. See ï¬?gure 16.3 for a drawing of the
graph representing this scenario.
Formally, an undirected graphical model is a structured probabilistic model
deï¬?ned on an undirected graph G . For each clique C in the graph,3 a factor Ï†(C)
(also called a clique potential) measures the aï¬ƒnity of the variables in that clique
for being in each of their possible joint states. The factors are constrained to be
non-negative. Together they deï¬?ne an unnormalized probability distribution

pÌƒ(x) = Î CâˆˆG Ï†(C ).

(16.3)

The unnormalized probability distribution is eï¬ƒcient to work with so long as
all the cliques are small. It encodes the idea that states with higher aï¬ƒnity are
more likely. However, unlike in a Bayesian network, there is little structure to the
deï¬?nition of the cliques, so there is nothing to guarantee that multiplying them
together will yield a valid probability distribution. See ï¬?gure 16.4 for an example
of reading factorization information from an undirected graph.
Our example of the cold spreading between you, your roommate, and your
colleague contains two cliques. One clique contains h y and hc . The factor for this
clique can be deï¬?ned by a table, and might have values resembling these:

hc = 0
hc = 1

hy = 0
2
1

3

hy = 1
1
10

A clique of the graph is a subset of nodes that are all connected to each other by an edge of
the graph.
567

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A state of 1 indicates good health, while a state of 0 indicates poor health
(having been infected with a cold). Both of you are usually healthy, so the
corresponding state has the highest aï¬ƒnity. The state where only one of you is
sick has the lowest aï¬ƒnity, because this is a rare state. The state where both of
you are sick (because one of you has infected the other) is a higher aï¬ƒnity state,
though still not as common as the state where both are healthy.
To complete the model, we would need to also deï¬?ne a similar factor for the
clique containing hy and hr .

16.2.3

The Partition Function

While the unnormalized probability distribution is guaranteed to be non-negative
everywhere, it is not guaranteed to sum or integrate to 1. To obtain a valid
probability distribution, we must use the corresponding normalized probability
distribution:4
1
p(x) = pÌƒ(x)
(16.4)
Z
where Z is the value that results in the probability distribution summing or
integrating to 1:
î?š
Z=

pÌƒ(x)dx.

(16.5)

You can think of Z as a constant when the Ï† functions are held constant. Note
that if the Ï† functions have parameters, then Z is a function of those parameters.
It is common in the literature to write Z with its arguments omitted to save space.
The normalizing constant Z is known as the partition function, a term borrowed
from statistical physics.
Since Z is an integral or sum over all possible joint assignments of the state x
it is often intractable to compute. In order to be able to obtain the normalized
probability distribution of an undirected model, the model structure and the
deï¬?nitions of the Ï† functions must be conducive to computing Z eï¬ƒciently. In
the context of deep learning, Z is usually intractable. Due to the intractability
of computing Z exactly, we must resort to approximations. Such approximate
algorithms are the topic of chapter 18.
One important consideration to keep in mind when designing undirected models
is that it is 
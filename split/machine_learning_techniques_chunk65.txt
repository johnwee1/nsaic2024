is sentence into a single vector representation, and then the
decoder would decode this vector into a sentence in another language. This two-step
model, called an Encoder–Decoder, works much better than trying to translate on the
fly with a single sequence-to-sequence RNN (like the one represented at the top left):
the last words of a sentence can affect the first words of the translation, so you need
to wait until you have seen the whole sentence before translating it. We will see how
to implement an Encoder–Decoder in Chapter 16 (as we will see, it is a bit more com‐
plex than in Figure 15-4 suggests).

Recurrent Neurons and Layers 

| 

501

Figure 15-4. Seq-to-seq (top left), seq-to-vector (top right), vector-to-seq (bottom left),
and Encoder–Decoder (bottom right) networks

Sounds promising, but how do you train a recurrent neural network?

Training RNNs
To  train  an  RNN,  the  trick  is  to  unroll  it  through  time  (like  we  just  did)  and  then
simply use regular backpropagation (see Figure 15-5). This strategy is called backpro‐
pagation through time (BPTT).

Just like in regular backpropagation, there is a first forward pass through the unrolled
network (represented by the dashed arrows). Then the output sequence is evaluated
using a cost function C(Y(0), Y(1), …Y(T)) (where T is the max time step). Note that this
cost function may ignore some outputs, as shown in Figure 15-5 (for example, in a
sequence-to-vector  RNN,  all  outputs  are  ignored  except  for  the  very  last  one).  The
gradients  of  that  cost  function  are  then  propagated  backward  through  the  unrolled
network (represented by the solid arrows). Finally the model parameters are updated
using  the  gradients  computed  during  BPTT.  Note  that  the  gradients  flow  backward
through all the outputs used by the cost function, not just through the final output
(for example, in Figure 15-5 the cost function is computed using the last three out‐
puts of the network, Y(2), Y(3), and Y(4), so gradients flow through these three outputs,

502 

| 

Chapter 15: Processing Sequences Using RNNs and CNNs

but not through Y(0) and Y(1)). Moreover, since the same parameters W and b are used
at each time step, backpropagation will do the right thing and sum over all time steps.

Figure 15-5. Backpropagation through time

Fortunately, tf.keras takes care of all of this complexity for you—so let’s start coding!

Forecasting a Time Series
Suppose you are studying the number of active users per hour on your website, or the
daily  temperature  in  your  city,  or  your  company’s  financial  health,  measured  quar‐
terly using multiple metrics. In all these cases, the data will be a sequence of one or
more values per time step. This is called a time series. In the first two examples there
is a single value per time step, so these are univariate time series, while in the financial
example  there  are  multiple  values  per  time  step  (e.g.,  the  company’s  revenue,  debt,
and so on), so it is a multivariate time series. A typical task is to predict future values,
which is called forecasting. Another common task is to fill in the blanks: to predict (or
rather “postdict”) missing values from the past. This is called imputation. For exam‐
ple, Figure 15-6 shows 3 univariate time series, each of them 50 time steps long, and
the goal here is to forecast the value at the next time step (represented by the X) for
each of them.

Forecasting a Time Series 

| 

503

Figure 15-6. Time series forecasting

For simplicity, we are using a time series generated by the generate_time_series()
function, shown here:

def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise
    return series[..., np.newaxis].astype(np.float32)

This  function  creates  as  many  time  series  as  requested  (via  the  batch_size  argu‐
ment), each of length n_steps, and there is just one value per time step in each series
(i.e.,  all  series  are  univariate).  The  function  returns  a  NumPy  array  of  shape  [batch
size, time steps, 1], where each series is the sum of two sine waves of fixed amplitudes
but random frequencies and phases, plus a bit of noise.

When dealing with time series (and other types of sequences such
as  sentences),  the  input  features  are  generally  represented  as  3D
arrays  of  shape  [batch  size,  time  steps,  dimensionality],  where
dimensionality  is  1  for  univariate  time  series  and  more  for  multi‐
variate time series.

Now let’s create a training set, a validation set, and a test set using this function:

n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]

X_train contains 7,000 time series (i.e., its shape is [7000, 50, 1]), while X_valid con‐
tains  2,000  (from  the  7,000th  time  series  to  the  8,999th)  and  X_test  contains  1,000
(from the 9,000th to the 9,999th). Since we want to forecast a single value for each ser‐
ies, the targets are column vectors (e.g., y_train has a shape of [7000, 1]).

504 

| 

Chapter 15: Processing Sequences Using RNNs and CNNs

Baseline Metrics
Before we start using RNNs, it is often a good idea to have a few baseline metrics, or
else  we  may  end  up  thinking  our  model  works  great  when  in  fact  it  is  doing  worse
than basic models. For example, the simplest approach is to predict the last value in
each series. This is called naive forecasting, and it is sometimes surprisingly difficult to
outperform. In this case, it gives us a mean squared error of about 0.020:

>>> y_pred = X_valid[:, -1]
>>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
0.020211367

Another simple approach is to use a fully connected network. Since it expects a flat
list of features for each input, we need to add a Flatten layer. Let’s just use a simple
Linear Regression model so that each prediction will be a linear combination of the
values in the time series:

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)
])

If we compile this model using the MSE loss and the default Adam optimizer, then fit
it  on  the  training  set  for  20  epochs  and  evaluate  it  on  the  validation  set,  we  get  an
MSE of about 0.004. That’s much better than the naive approach!

Implementing a Simple RNN
Let’s see if we can beat that with a simple RNN:

model = keras.models.Sequential([
  keras.layers.SimpleRNN(1, input_shape=[None, 1])
])

That’s really the simplest RNN you can build. It just contains a single layer, with a sin‐
gle  neuron,  as  we  saw  in  Figure  15-1.  We  do  not  need  to  specify  the  length  of  the
input sequences (unlike in the previous model), since a recurrent neural network can
process  any  number  of  time  steps  (this  is  why  we  set  the  first  input  dimension  to
None).  By  default,  the  SimpleRNN  layer  uses  the  hyperbolic  tangent  activation  func‐
tion. It works exactly as we saw earlier: the initial state h(init) is set to 0, and it is passed
to a single recurrent neuron, along with the value of the first time step, x(0). The neu‐
ron computes a weighted sum of these values and applies the hyperbolic tangent acti‐
vation function to the result, and this gives the first output, y0. In a simple RNN, this
output is also the new state h0. This new state is passed to the same recurrent neuron
along  with  the  next  input  value,  x(1),  and  the  process  is  repeated  until  the  last  time
step. Then the layer just outputs the last value, y49. All of this is performed simultane‐
ously for every time series.

Forecasting a Time Series 

| 

505

By default, recurrent layers in Keras only return the final output. To
make  them  return  one  output  per  time  step,  you  must  set
return_sequences=True, as we will see.

If  you  compile,  fit,  and  evaluate  this  model  (just  like  earlier,  we  train  for  20  epochs
using  Adam),  you  will  find  that  its  MSE  reaches  only  0.014,  so  it  is  better  than  the
naive approach but it does not beat a simple linear model. Note that for each neuron,
a linear model has one parameter per input and per time step, plus a bias term (in the
simple  linear  model  we  used,  that’s  a  total  of  51  parameters).  In  contrast,  for  each
recurrent neuron in a simple RNN, there is just one parameter per input and per hid‐
den state dimension (in a simple RNN, that’s just the number of recurrent neurons in
the layer), plus a bias term. In this simple RNN, that’s a total of just three parameters.

Trend and Seasonality
There are many other models to forecast time series, such as weighted moving average
models or autoregressive integrated moving average (ARIMA) models. Some of them
require you to first remove the trend and seasonality. For example, if you are studying
the number of active users on your website, and it is growing by 10% every month,
you would have to remove this trend from the time series. Once the model is trained
and starts making predictions, you would have to add the trend back to get the final
predictions. Similarly, if you are trying to predict the amount of sunscreen lotion sold
every  month,  you  will  probably  observe  strong  seasonality:  since  it  sells  well  every
summer, a similar pattern will be repeated every year. You would have to remove this
seasonality from the time series, for example by computing the difference between the
value at each time step and the value one year earlier (this technique is called differ‐
encing). Again, after the model is trained and makes predictions, you would have to
add the seasonal pattern back to get the final predictions.

When using RNNs, it is generally not necessary to do all this, but it may improve per‐
formance  in  some  cases,  since  the  model  will  not  have  to  learn  the  trend  or  the
seasonality.

Apparently our simple RNN was too simple to get good performance. So let’s try to
add more recurrent layers!

Deep RNNs
It  is  quite  common  to  stack  multiple  layers  of  cells,  as  shown  in  Figure  15-7.  This
gives you a deep RNN.

506 

| 

Chapter 15: Processing Sequences Using RNNs and CNNs

Figure 15-7. Deep RNN (left) unrolled through time (right)

Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In
this  example,  we  use  three  SimpleRNN  layers  (but  we  could  add  any  other  type  of
recurrent layer, such as an LSTM layer or a GRU layer, which we will discuss shortly):

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20, return_sequences=True),
    keras.layers.SimpleRNN(1)
])

Make  sure  to  set  return_sequences=True  for  all  recurrent  layers
(except the last one, if you only care about the last output). If you
don’t,  they  will  output  a  2D  array  (containing  only  the  output  of
the last time step) instead of a 3D array (containing outputs for all
time steps), and the next recurrent layer will complain that you are
not feeding it sequences in the expected 3D format.

If you compile, fit, and evaluate this model, you will find that it reaches an MSE of
0.003. We finally managed to beat the linear model!

Note that the last layer is not ideal: it must have a single unit because we want to fore‐
cast a univariate time series, and this means we must have a single output value per
time step. However, having a single unit means that the hidden state is just a single
number.  That’s  really  not  much,  and  it’s  probably  not  that  useful;  presumably,  the
RNN will mostly use the hidden states of the other recurrent layers to carry over all
the information it needs from time step to time step, and it will not use the final lay‐
er’s hidden state very much. Moreover, since a SimpleRNN layer uses the tanh activa‐
tion function by default, the predicted values must lie within the range –1 to 1. But
what if you want to use another activation function? For both these reasons, it might
be  preferable  to  replace  the  output  layer  with  a  Dense  layer:  it  would  run  slightly

Forecasting a Time Series 

| 

507

faster, the accuracy would be roughly the same, and it would allow us to choose any
output  activation  function  we  want.  If  you  make  this  change,  also  make  sure  to
remove return_sequences=True from the second (now last) recurrent layer:

model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Dense(1)
])

If you train this model, you will see that it converges faster and performs just as well.
Plus, you could change the output activation function if you wanted.

Forecasting Several Time Steps Ahead
So  far  we  have  only  predicted  the  value  at  the  next  time  step,  but  we  could  just  as
easily have predicted the value several steps ahead by changing the targets appropri‐
ately (e.g., to predict 10 steps ahead, just change the targets to be the value 10 steps
ahead instead of 1 step ahead). But what if we want to predict the next 10 values?

The first option is to use the model we already trained, make it predict the next value,
then add that value to the inputs (acting as if this predicted value had actually occur‐
red), and use the model again to predict the following value, and so on, as in the fol‐
lowing code:

series = generate_time_series(1, n_steps + 10)
X_new, Y_new = series[:, :n_steps], series[:, n_steps:]
X = X_new
for step_ahead in range(10):
    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]
    X = np.concatenate([X, y_pred_one], axis=1)

Y_pred = X[:, n_steps:]

As  you  might  expect,  the  prediction  for  the  next  step  will  usually  be  more  accurate
than  the  predictions  for  later  time  steps,  since  the  errors  might  accumulate  (as  you
can see in Figure 15-8). If you evaluate this approach on the validation set, you will
find an MSE of about 0.029. This is much higher than the previous models, but it’s
also  a  much  harder  task,  so  the  comparison  doesn’t  mean  much.  It’s  much  more
meaningful to compare this performance with naive predictions (just forecasting that
the time series will remain constant for 10 time steps) or with a simple linear model.
The naive approach is terrible (it gives an MSE of about 0.223), but the linear model
gives  an  MSE  of  about  0.0188:  it’s  much  better  than  using  our  RNN  to  forecast  the
future one step at a time, and also much faster to train and run. Still, if you only want
to forecast a few time steps ahead, on more complex tasks, this approach may work
well.

508 

| 

Chapter 15: Processing Sequences Using RNNs and CNNs

Figure 15-8. Forecasting 10 steps ahead, 1 step at a time

The second option is to train an RNN to predict all 10 next values at once. We can
still use a sequence-to-vector model, but it will output 10 values instead of 1. How‐

ervations and three
values of p, the number of features. Of the p features, 20 were associated with
the response. The boxplots show the test MSEs that result using three different
values of the tuning parameter λ in (6.7). For ease of interpretation, rather than
reporting λ, the degrees of freedom are reported; for the lasso this turns out
to be simply the number of estimated non-zero coefficients. When p = 20, the
lowest test MSE was obtained with the smallest amount of regularization. When
p = 50, the lowest test MSE was achieved when there is a substantial amount
of regularization. When p = 2,000 the lasso performed poorly regardless of the
amount of regularization, due to the fact that only 20 of the 2,000 features truly
are associated with the outcome.

In Section 6.1.3, we saw a number of approaches for adjusting the training
set RSS or R2 in order to account for the number of variables used to fit
a least squares model. Unfortunately, the Cp , AIC, and BIC approaches
are not appropriate in the high-dimensional setting, because estimating σ̂ 2
is problematic. (For instance, the formula for σ̂ 2 from Chapter 3 yields an
estimate σ̂ 2 = 0 in this setting.) Similarly, problems arise in the application
of adjusted R2 in the high-dimensional setting, since one can easily obtain
a model with an adjusted R2 value of 1. Clearly, alternative approaches
that are better-suited to the high-dimensional setting are required.

6.4.3

Regression in High Dimensions

It turns out that many of the methods seen in this chapter for fitting
less flexible least squares models, such as forward stepwise selection, ridge
regression, the lasso, and principal components regression, are particularly
useful for performing regression in the high-dimensional setting. Essentially,
these approaches avoid overfitting by using a less flexible fitting approach
than least squares.
Figure 6.24 illustrates the performance of the lasso in a simple simulated
example. There are p = 20, 50, or 2,000 features, of which 20 are truly
associated with the outcome. The lasso was performed on n = 100 training
observations, and the mean squared error was evaluated on an independent
test set. As the number of features increases, the test set error increases.
When p = 20, the lowest validation set error was achieved when λ in

266

6. Linear Model Selection and Regularization

(6.7) was small; however, when p was larger then the lowest validation
set error was achieved using a larger value of λ. In each boxplot, rather
than reporting the values of λ used, the degrees of freedom of the resulting
lasso solution is displayed; this is simply the number of non-zero coefficient
estimates in the lasso solution, and is a measure of the flexibility of the
lasso fit. Figure 6.24 highlights three important points: (1) regularization
or shrinkage plays a key role in high-dimensional problems, (2) appropriate
tuning parameter selection is crucial for good predictive performance, and
(3) the test error tends to increase as the dimensionality of the problem
(i.e. the number of features or predictors) increases, unless the additional
features are truly associated with the response.
The third point above is in fact a key principle in the analysis of highdimensional data, which is known as the curse of dimensionality. One might
curse of dithink that as the number of features used to fit a model increases, the mensionality
quality of the fitted model will increase as well. However, comparing the
left-hand and right-hand panels in Figure 6.24, we see that this is not
necessarily the case: in this example, the test set MSE almost doubles as
p increases from 20 to 2,000. In general, adding additional signal features
that are truly associated with the response will improve the fitted model,
in the sense of leading to a reduction in test set error. However, adding
noise features that are not truly associated with the response will lead
to a deterioration in the fitted model, and consequently an increased test
set error. This is because noise features increase the dimensionality of the
problem, exacerbating the risk of overfitting (since noise features may be
assigned nonzero coefficients due to chance associations with the response
on the training set) without any potential upside in terms of improved test
set error. Thus, we see that new technologies that allow for the collection
of measurements for thousands or millions of features are a double-edged
sword: they can lead to improved predictive models if these features are in
fact relevant to the problem at hand, but will lead to worse results if the
features are not relevant. Even if they are relevant, the variance incurred
in fitting their coefficients may outweigh the reduction in bias that they
bring.

6.4.4

Interpreting Results in High Dimensions

When we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting, we must be quite cautious in the way
that we report the results obtained. In Chapter 3, we learned about multicollinearity, the concept that the variables in a regression might be correlated with each other. In the high-dimensional setting, the multicollinearity
problem is extreme: any variable in the model can be written as a linear
combination of all of the other variables in the model. Essentially, this
means that we can never know exactly which variables (if any) truly are
predictive of the outcome, and we can never identify the best coefficients
for use in the regression. At most, we can hope to assign large regression
coefficients to variables that are correlated with the variables that truly are
predictive of the outcome.

6.5 Lab: Linear Models and Regularization Methods

267

For instance, suppose that we are trying to predict blood pressure on the
basis of half a million SNPs, and that forward stepwise selection indicates
that 17 of those SNPs lead to a good predictive model on the training data.
It would be incorrect to conclude that these 17 SNPs predict blood pressure
more effectively than the other SNPs not included in the model. There are
likely to be many sets of 17 SNPs that would predict blood pressure just
as well as the selected model. If we were to obtain an independent data set
and perform forward stepwise selection on that data set, we would likely
obtain a model containing a different, and perhaps even non-overlapping,
set of SNPs. This does not detract from the value of the model obtained—
for instance, the model might turn out to be very effective in predicting
blood pressure on an independent set of patients, and might be clinically
useful for physicians. But we must be careful not to overstate the results
obtained, and to make it clear that what we have identified is simply one
of many possible models for predicting blood pressure, and that it must be
further validated on independent data sets.
It is also important to be particularly careful in reporting errors and measures of model fit in the high-dimensional setting. We have seen that when
p > n, it is easy to obtain a useless model that has zero residuals. Therefore, one should never use sum of squared errors, p-values, R2 statistics, or
other traditional measures of model fit on the training data as evidence of
a good model fit in the high-dimensional setting. For instance, as we saw
in Figure 6.23, one can easily obtain a model with R2 = 1 when p > n.
Reporting this fact might mislead others into thinking that a statistically
valid and useful model has been obtained, whereas in fact this provides
absolutely no evidence of a compelling model. It is important to instead
report results on an independent test set, or cross-validation errors. For
instance, the MSE or R2 on an independent test set is a valid measure of
model fit, but the MSE on the training set certainly is not.

6.5

Lab: Linear Models and Regularization
Methods

In this lab we implement many of the techniques discussed in this chapter.
We import some of our libraries at this top level.
In [1]: import numpy as np
import pandas as pd
from matplotlib.pyplot import subplots
from statsmodels.api import OLS
import sklearn.model_selection as skm
import sklearn.linear_model as skl
from sklearn.preprocessing import StandardScaler
from ISLP import load_data
from ISLP.models import ModelSpec as MS
from functools import partial

We again collect the new imports needed for this lab.
In [2]: from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA

268

6. Linear Model Selection and Regularization

from sklearn. cross_decomposition import PLSRegression
from ISLP.models import \
(Stepwise ,
sklearn_selected ,
sklearn_selection_path )
!pip install l0bnb
from l0bnb import fit_path

We have installed the package l0bnb on the fly. Note the escaped !pip
install — this is run as a separate system command.

6.5.1

Subset Selection Methods

Here we implement methods that reduce the number of parameters in a
model by restricting the model to a subset of the input variables.
Forward Selection
We will apply the forward-selection approach to the Hitters data. We wish
to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.
First of all, we note that the Salary variable is missing for some of the
players. The np.isnan() function can be used to identify the missing obnp.isnan()
servations. It returns an array of the same shape as the input vector, with
a True for any elements that are missing, and a False for non-missing elements. The sum() method can then be used to count all of the missing
sum()
elements.
In [3]: Hitters = load_data('Hitters ')
np.isnan(Hitters['Salary ']).sum()
Out[3]: 59

We see that Salary is missing for 59 players. The dropna() method of data
frames removes all of the rows that have missing values in any variable (by
default — see Hitters.dropna?).
In [4]: Hitters = Hitters.dropna ();
Hitters.shape
Out[4]: (263, 20)

We first choose the best model using forward selection based on Cp (6.2).
This score is not built in as a metric to sklearn. We therefore define a
function to compute it ourselves, and use it as a scorer. By default, sklearn
tries to maximize a score, hence our scoring function computes the negative
Cp statistic.
In [5]: def nCp(sigma2 , estimator , X, Y):
"Negative Cp statistic"
n, p = X.shape
Yhat = estimator.predict(X)
RSS = np.sum((Y - Yhat)**2)
return -(RSS + 2 * p * sigma2) / n

6.5 Lab: Linear Models and Regularization Methods

269

We need to estimate the residual variance σ 2 , which is the first argument
in our scoring function above. We will fit the biggest model, using all the
variables, and estimate σ 2 based on its MSE.
In [6]: design = MS(Hitters.columns.drop('Salary ')).fit(Hitters)
Y = np.array(Hitters['Salary '])
X = design.transform(Hitters)
sigma2 = OLS(Y,X).fit().scale

The function sklearn_selected() expects a scorer with just three arguments — the last three in the definition of nCp() above. We use the function
partial() first seen in Section 5.3.3 to freeze the first argument with our
estimate of σ 2 .
In [7]: neg_Cp = partial(nCp , sigma2)

We can now use neg_Cp() as a scorer for model selection.
Along with a score we need to specify the search strategy. This is done
through the object Stepwise() in the ISLP.models package. The method
Stepwise.first_peak() runs forward stepwise until any further additions
to the model do not result in an improvement in the evaluation score.
Similarly, the method Stepwise.fixed_steps() runs a fixed number of steps
of stepwise search.
In [8]: strategy = Stepwise.first_peak(design ,
direction='forward ',
max_terms=len(design.terms))

We now fit a linear regression model with Salary as outcome using forward selection. To do so, we use the function sklearn_selected() from the sklearn_
ISLP.models package. This takes a model from statsmodels along with a selected()
search strategy and selects a model with its fit method. Without specifying a scoring argument, the score defaults to MSE, and so all 19 variables
will be selected (output not shown).
In [9]: hitters_MSE = sklearn_selected (OLS ,
strategy)
hitters_MSE.fit(Hitters , Y)
hitters_MSE.selected_state_

Using neg_Cp results in a smaller model, as expected, with just 10 variables selected.
In [10]: hitters_Cp = sklearn_selected (OLS ,
strategy ,
scoring=neg_Cp)
hitters_Cp.fit(Hitters , Y)
hitters_Cp. selected_state_
Out[10]: ('Assists ',
'AtBat ',
'CAtBat ',
'CRBI ',
'CRuns ',
'CWalks ',
'Division ',

270

6. Linear Model Selection and Regularization

'Hits ',
'PutOuts ',
'Walks ')

Choosing Among Models Using the Validation Set Approach and
Cross-Validation
As an alternative to using Cp , we might try cross-validation to select a
model in forward selection. For this, we need a method that stores the
full path of models found in forward selection, and allows predictions for
each of these. This can be done with the sklearn_selection_path() estima- sklearn_
tor from ISLP.models. The function cross_val_predict() from ISLP.models selection_
computes the cross-validated predictions for each of the models along the path()
path, which we can use to evaluate the cross-validated MSE along the path. cross_val_
Here we define a strategy that fits the full forward selection path. While predict()
there are various parameter choices for sklearn_selection_path(), we use
the defaults here, which selects the model at each step based on the biggest
reduction in RSS.
In [11]: strategy = Stepwise.fixed_steps(design ,
len(design.terms),
direction='forward ')
full_path = sklearn_selection_path(OLS , strategy)

We now fit the full forward-selection path on the Hitters data and compute the fitted values.
In [12]: full_path.fit(Hitters , Y)
Yhat_in = full_path.predict(Hitters)
Yhat_in.shape
Out[12]: (263, 20)

This gives us an array of fitted values — 20 steps in all, including the
fitted mean for the null model — which we can use to evaluate in-sample
MSE. As expected, the in-sample MSE improves each step we take, indicating we must use either the validation or cross-validation approach to select
the number of steps. We fix the y-axis to range from 50,000 to 250,000
to compare to the cross-validation and validation set MSE below, as well
as other methods such as ridge regression, lasso and principal components
regression.
In [13]: mse_fig , ax = subplots(figsize =(8 ,8))
insample_mse = (( Yhat_in - Y[:,None ]) **2).mean (0)
n_steps = insample_mse.shape [0]
ax.plot(np.arange(n_steps),
insample_mse ,
'k', # color black
label='In -sample ')
ax.set_ylabel('MSE',
fontsize =20)
ax.set_xlabel('# steps of forward stepwise ',
fontsize =20)
ax.set_xticks(np.arange(n_steps)[::2])
ax.legend ()

6.5 Lab: Linear Models and Regularization Methods

271

ax.set_ylim ([50000 ,250000]);

Notice the expression None in Y[:,None] above. This adds an axis (dimension) to the one-dimensional array Y, which allows it to be recycled when
subtracted from the two-dimensional Yhat_in.
We are now ready to use cross-validation to estimate test error along
the model path. We must use only the training observations to perform
all aspects of model-fitting — including variable selection. Therefore, the
determination of which model of a given size is best must be made using
only the training observations in each training fold. This point is subtle but
important. If the full data set is used to select the best subset at each step,
then the validation set errors and cross-validation error
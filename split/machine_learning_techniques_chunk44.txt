
come  back,  overshoot  again,  and  oscillate  like  this  many  times
before  stabilizing  at  the  minimum.  This  is  one  of  the  reasons  it’s
good to have a bit of friction in the system: it gets rid of these oscil‐
lations and thus speeds up convergence.

Implementing  momentum  optimization  in  Keras  is  a  no-brainer:  just  use  the  SGD
optimizer and set its momentum hyperparameter, then lie back and profit!

352 

| 

Chapter 11: Training Deep Neural Networks

optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)

The one drawback of momentum optimization is that it adds yet another hyperpara‐
meter  to  tune.  However,  the  momentum  value  of  0.9  usually  works  well  in  practice
and almost always goes faster than regular Gradient Descent.

Nesterov Accelerated Gradient
One small variant to momentum optimization, proposed by Yurii Nesterov in 1983,14
is  almost  always  faster  than  vanilla  momentum  optimization.  The  Nesterov  Acceler‐
ated Gradient (NAG) method, also known as Nesterov momentum optimization, meas‐
ures the gradient of the cost function not at the local position θ but slightly ahead in
the direction of the momentum, at θ + βm (see Equation 11-5).

Equation 11-5. Nesterov Accelerated Gradient algorithm
1 . m βm − η∇θJ θ + βm
θ + m
2 .

θ

This small tweak works because in general the momentum vector will be pointing in
the right direction (i.e., toward the optimum), so it will be slightly more accurate to
use  the  gradient  measured  a  bit  farther  in  that  direction  rather  than  the  gradient  at
the original position, as you can see in Figure 11-6 (where ∇1 represents the gradient
of the cost function measured at the starting point θ, and ∇2 represents the gradient
at the point located at θ + βm).

As you can see, the Nesterov update ends up slightly closer to the optimum. After a
while, these small improvements add up and NAG ends up being significantly faster
than  regular  momentum  optimization.  Moreover,  note  that  when  the  momentum
pushes  the  weights  across  a  valley,  ∇1  continues  to  push  farther  across  the  valley,
while ∇2 pushes back toward the bottom of the valley. This helps reduce oscillations
and thus NAG converges faster.

NAG  is  generally  faster  than  regular  momentum  optimization.  To  use  it,  simply  set
nesterov=True when creating the SGD optimizer:

optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)

14 Yurii Nesterov, “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence

O(1/k2),” Doklady AN USSR 269 (1983): 543–547.

Faster Optimizers 

| 

353

Figure 11-6. Regular versus Nesterov momentum optimization: the former applies the
gradients computed before the momentum step, while the latter applies the gradients
computed after

AdaGrad
Consider the elongated bowl problem again: Gradient Descent starts by quickly going
down  the  steepest  slope,  which  does  not  point  straight  toward  the  global  optimum,
then it very slowly goes down to the bottom of the valley. It would be nice if the algo‐
rithm  could  correct  its  direction  earlier  to  point  a  bit  more  toward  the  global  opti‐
mum. The AdaGrad algorithm15 achieves this correction by scaling down the gradient
vector along the steepest dimensions (see Equation 11-6).

Equation 11-6. AdaGrad algorithm

1 .

2 .

s

θ

s + ∇θJ θ ⊗ ∇θJ θ
θ − η ∇θJ θ ⊘ s + ε

The first step accumulates the square of the gradients into the vector s (recall that the
⊗ symbol represents the element-wise multiplication). This vectorized form is equiv‐
alent to computing si ← si + (∂ J(θ) / ∂ θi)2 for each element si of the vector s; in other
words,  each  si  accumulates  the  squares  of  the  partial  derivative  of  the  cost  function
with regard to parameter θi. If the cost function is steep along the ith dimension, then
si will get larger and larger at each iteration.

The second step is almost identical to Gradient Descent, but with one big difference:
the gradient vector is scaled down by a factor of  s + ε (the ⊘ symbol represents the

15 John Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” Journal

of Machine Learning Research 12 (2011): 2121–2159.

354 

| 

Chapter 11: Training Deep Neural Networks

element-wise division, and ε is a smoothing term to avoid division by zero, typically
set  to  10–10).  This  vectorized  form  is  equivalent  to  simultaneously  computing
θi

θi − η ∂J θ / ∂θi/ si + ε for all parameters θi.

In short, this algorithm decays the learning rate, but it does so faster for steep dimen‐
sions than for dimensions with gentler slopes. This is called an adaptive learning rate.
It  helps  point  the  resulting  updates  more  directly  toward  the  global  optimum  (see
Figure 11-7). One additional benefit is that it requires much less tuning of the learn‐
ing rate hyperparameter η.

Figure 11-7. AdaGrad versus Gradient Descent: the former can correct its direction ear‐
lier to point to the optimum

AdaGrad frequently performs well for simple quadratic problems, but it often stops
too early when training neural networks. The learning rate gets scaled down so much
that the algorithm ends up stopping entirely before reaching the global optimum. So
even though Keras has an Adagrad optimizer, you should not use it to train deep neu‐
ral networks (it may be efficient for simpler tasks such as Linear Regression, though).
Still,  understanding  AdaGrad  is  helpful  to  grasp  the  other  adaptive  learning  rate
optimizers.

RMSProp
As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never con‐
verging to the global optimum. The RMSProp algorithm16 fixes this by accumulating
only  the  gradients  from  the  most  recent  iterations  (as  opposed  to  all  the  gradients

16 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hin‐
ton in his Coursera class on neural networks (slides: https://homl.info/57; video: https://homl.info/58). Amus‐
ingly, since the authors did not write a paper to describe the algorithm, researchers often cite “slide 29 in
lecture 6” in their papers.

Faster Optimizers 

| 

355

since the beginning of training). It does so by using exponential decay in the first step
(see Equation 11-7).

Equation 11-7. RMSProp algorithm

1 .

2 .

s

θ

βs + 1 − β ∇θJ θ ⊗ ∇θJ θ
θ − η ∇θJ θ ⊘ s + ε

The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but
this default value often works well, so you may not need to tune it at all.

As you might expect, Keras has an RMSprop optimizer:

optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)

Note that the rho argument corresponds to β in Equation 11-7. Except on very simple
problems, this optimizer almost always performs much better than AdaGrad. In fact,
it was the preferred optimization algorithm of many researchers until Adam optimi‐
zation came around.

Adam and Nadam Optimization
Adam,17 which stands for adaptive moment estimation, combines the ideas of momen‐
tum optimization and RMSProp: just like momentum optimization, it keeps track of
an exponentially decaying average of past gradients; and just like RMSProp, it keeps
track  of  an  exponentially  decaying  average  of  past  squared  gradients  (see  Equation
11-8).18

Equation 11-8. Adam algorithm

1 . m β1m − 1 − β1
2 .

s

∇θJ θ

∇θJ θ ⊗ ∇θJ θ

β2s + 1 − β2
m
t
1 − β1
s
1 − β2
θ + η m ⊘ s + ε

t

3 . m

4 .

5 .

s

θ

17 Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” arXiv preprint arXiv:

1412.6980 (2014).

18 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the

first moment while the variance is often called the second moment, hence the name of the algorithm.

356 

| 

Chapter 11: Training Deep Neural Networks

In this equation, t represents the iteration number (starting at 1).

If  you  just  look  at  steps  1,  2,  and  5,  you  will  notice  Adam’s  close  similarity  to  both
momentum optimization and RMSProp. The only difference is that step 1 computes
an  exponentially  decaying  average  rather  than  an  exponentially  decaying  sum,  but
these are actually equivalent except for a constant factor (the decaying average is just
1 – β1 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since
m and s are initialized at 0, they will be biased toward 0 at the beginning of training,
so these two steps will help boost m and s at the beginning of training.

The momentum decay hyperparameter β1 is typically initialized to 0.9, while the scal‐
ing  decay  hyperparameter  β2  is  often  initialized  to  0.999.  As  earlier,  the  smoothing
term ε is usually initialized to a tiny number such as 10–7. These are the default values
for  the  Adam  class  (to  be  precise,  epsilon  defaults  to  None,  which  tells  Keras  to  use
keras.backend.epsilon(),  which  defaults  to  10–7;  you  can  change  it  using
keras.backend.set_epsilon()).  Here  is  how  to  create  an  Adam  optimizer  using
Keras:

optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)

Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it
requires  less  tuning  of  the  learning  rate  hyperparameter  η.  You  can  often  use  the
default value η = 0.001, making Adam even easier to use than Gradient Descent.

If you are starting to feel overwhelmed by all these different techni‐
ques and are wondering how to choose the right ones for your task,
don’t  worry:  some  practical  guidelines  are  provided  at  the  end  of
this chapter.

Finally, two variants of Adam are worth mentioning:

AdaMax

Notice that in step 2 of Equation 11-8, Adam accumulates the squares of the gra‐
dients  in  s  (with  a  greater  weight  for  more  recent  gradients).  In  step  5,  if  we
ignore  ε  and  steps  3  and  4  (which  are  technical  details  anyway),  Adam  scales
down the parameter updates by the square root of s. In short, Adam scales down
the parameter updates by the ℓ2 norm of the time-decayed gradients (recall that
the ℓ2 norm is the square root of the sum of squares). AdaMax, introduced in the
same paper as Adam, replaces the ℓ2 norm with the ℓ∞ norm (a fancy way of say‐
ing  the  max).  Specifically,  it  replaces  step  2  in  Equation  11-8  with  s  ←  max
(β2s,∇θJ(θ)), it drops step 4, and in step 5 it scales down the gradient updates by a
factor of s, which is just the max of the time-decayed gradients. In practice, this
can make AdaMax more stable than Adam, but it really depends on the dataset,

Faster Optimizers 

| 

357

and in general Adam performs better. So, this is just one more optimizer you can
try if you experience problems with Adam on some task.

Nadam

Nadam  optimization  is  Adam  optimization  plus  the  Nesterov  trick,  so  it  will
often  converge  slightly  faster  than  Adam.  In  his  report  introducing  this  techni‐
que,19 the researcher Timothy Dozat compares many different optimizers on vari‐
ous  tasks  and  finds  that  Nadam  generally  outperforms  Adam  but  is  sometimes
outperformed by RMSProp.

Adaptive  optimization  methods  (including  RMSProp,  Adam,  and
Nadam optimization) are often great, converging fast to a good sol‐
ution.  However,  a  2017  paper20  by  Ashia  C.  Wilson  et  al.  showed
that they can lead to solutions that generalize poorly on some data‐
sets.  So  when  you  are  disappointed  by  your  model’s  performance,
try using plain Nesterov Accelerated Gradient instead: your dataset
may just be allergic to adaptive gradients. Also check out the latest
research, because it’s moving fast.

All  the  optimization  techniques  discussed  so  far  only  rely  on  the  first-order  partial
derivatives (Jacobians). The optimization literature also contains amazing algorithms
based  on  the  second-order  partial  derivatives  (the  Hessians,  which  are  the  partial
derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply
to  deep  neural  networks  because  there  are  n2  Hessians  per  output  (where  n  is  the
number of parameters), as opposed to just n Jacobians per output. Since DNNs typi‐
cally have tens of thousands of parameters, the second-order optimization algorithms
often  don’t  even  fit  in  memory,  and  even  when  they  do,  computing  the  Hessians  is
just too slow.

19 Timothy Dozat, “Incorporating Nesterov Momentum into Adam” (2016).

20 Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning,” Advances in

Neural Information Processing Systems 30 (2017): 4148–4158.

358 

| 

Chapter 11: Training Deep Neural Networks

Training Sparse Models
All the optimization algorithms just presented produce dense models, meaning that
most parameters will be nonzero. If you need a blazingly fast model at runtime, or if
you  need  it  to  take  up  less  memory,  you  may  prefer  to  end  up  with  a  sparse  model
instead.

One  easy  way  to  achieve  this  is  to  train  the  model  as  usual,  then  get  rid  of  the  tiny
weights  (set  them  to  zero).  Note  that  this  will  typically  not  lead  to  a  very  sparse
model, and it may degrade the model’s performance.

A better option is to apply strong ℓ1 regularization during training (we will see how
later in this chapter), as it pushes the optimizer to zero out as many weights as it can
(as discussed in “Lasso Regression” on page 137 in Chapter 4).

If  these  techniques  remain  insufficient,  check  out  the  TensorFlow  Model  Optimiza‐
tion Toolkit (TF-MOT), which provides a pruning API capable of iteratively remov‐
ing connections during training based on their magnitude.

Table 11-2 compares all the optimizers we’ve discussed so far (* is bad, ** is average,
and *** is good).

Table 11-2. Optimizer comparison

Class

SGD

Convergence speed Convergence quality
*

***

SGD(momentum=...)

**

SGD(momentum=..., nesterov=True) **

Adagrad

RMSprop

Adam

Nadam

AdaMax

***

***

***

***

***

***

***

* (stops too early)

** or ***

** or ***

** or ***

** or ***

Learning Rate Scheduling
Finding a good learning rate is very important. If you set it much too high, training
may  diverge  (as  we  discussed  in  “Gradient  Descent”  on  page  118).  If  you  set  it  too
low,  training  will  eventually  converge  to  the  optimum,  but  it  will  take  a  very  long
time. If you set it slightly too high, it will make progress very quickly at first, but it
will  end  up  dancing  around  the  optimum,  never  really  settling  down.  If  you  have  a
limited computing budget, you may have to interrupt training before it has converged
properly, yielding a suboptimal solution (see Figure 11-8).

Faster Optimizers 

| 

359

Figure 11-8. Learning curves for various learning rates η

As  we  discussed  in  Chapter  10,  you  can  find  a  good  learning  rate  by  training  the
model for a few hundred iterations, exponentially increasing the learning rate from a
very  small  value  to  a  very  large  value,  and  then  looking  at  the  learning  curve  and
picking a learning rate slightly lower than the one at which the learning curve starts
shooting back up. You can then reinitialize your model and train it with that learning
rate.

But you can do better than a constant learning rate: if you start with a large
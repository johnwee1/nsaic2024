h, as Weizenbaum points out, one can “assume the pose of knowing
almost nothing of the real world”. If a patient says “I went for a long boat ride” and
the psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what

conversation

dialogue

dialogue system

frame

chatbot

316 CHAPTER 15

• CHATBOTS & DIALOGUE SYSTEMS

Men are all alike.
IN WHAT WAY
They’re always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.
I AM SORRY TO HEAR YOU ARE DEPRESSED
Figure 15.1 Sample ELIZA dialogue from Weizenbaum (1966).

a boat is, but rather assume she had some conversational goal.1

Weizenbaum made use of this property of Rogerian psychiatric conversations,
along with clever regular expressions, to allow ELIZA to interact in ways that seemed
deceptively human-like, as in the sample conversational fragment in Fig. 15.1.

As we foreshadowed in Chapter 2, ELIZA worked by simple rules roughly like:

(.*) YOU (.*) ME

->

WHAT MAKES YOU THINK I \2 YOU

to transform a user sentence like “You hate me” into a system response like
WHAT MAKES YOU THINK I HATE YOU

Among Weizenbaum’s clever tricks are the linking of each ELIZA pattern/rule

to a keyword. Consider the following user sentence:

I know everybody laughed at me

Because it has the word “I”, this sentence could match the following rule whose

keyword is I:

I (.*) -> You say you \1

producing:

YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU

Weizenbaum points out, however, that a more powerful response would rely on
the keyword “everybody”, since someone using universals like everybody or always
is probably thinking about a speciﬁc person or situation. So the ELIZA algorithm
prefers to respond using patterns associated more speciﬁc keywords like everybody:

WHO IN PARTICULAR ARE YOU THINKING OF?

If no keyword matches, the algorithm chooses a non-committal response like

“PLEASE GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”.

ELIZA illustrates a number of important issues with chatbots. First, people
became deeply emotionally involved and conducted very personal conversations,
even to the extent of asking Weizenbaum to leave the room while they were typ-
ing. Reeves and Nass (1996) show that people tend to assign human characteristics
to computers and interact with them in ways that are typical of human-human in-
teractions. They interpret an utterance in the way they would if it had spoken by a
human, (even though they are aware they are talking to a computer). This means that
chatbots can have signiﬁcant inﬂuences on people’s cognitive and emotional state.

A second related issue is privacy. When Weizenbaum suggested that he might
want to store the ELIZA conversations, people immediately pointed out that this
would violate people’s privacy. Modern chatbots in the home are likely to overhear

1 This is due to the Gricean principle of relevance that we’ll discuss in the next section..

15.1

• PROPERTIES OF HUMAN CONVERSATION

317

private information, even if they aren’t used for counseling as ELIZA was. Indeed,
if a chatbot is human-like, users are more likely to disclose private information, and
yet less likely to worry about the harm of this disclosure (Ischen et al., 2019).

Both of these issues (emotional engagement and privacy) mean we need to think
carefully about how we deploy chatbots and the people who are interacting with
them. Dialogue research that uses human participants often requires getting permis-
sion from the Institutional Review Board (IRB) of your institution.

In the next section we introduce some basic properties of human conversation.
We then turn in the rest of the chapter to the two basic paradigms for conversational
interaction: frame-based dialogue systems and chatbots.

15.1 Properties of Human Conversation

Conversation between humans is an intricate and complex joint activity. Before
we attempt to design a dialogue system to converse with humans, it is crucial to
understand something about how humans converse with each other. Consider some
of the phenomena that occur in the conversation between a human travel agent and
a human client excerpted in Fig. 15.2.

. . . I need to travel in May.

C1:
A2: And, what day in May did you want to travel?
C3: OK uh I need to be there for a meeting that’s from the 12th to the 15th.
A4: And you’re ﬂying into what city?
C5: Seattle.
A6: And what time would you like to leave Pittsburgh?
C7: Uh hmm I don’t think there’s many options for non-stop.
A8: Right. There’s three non-stops today.
C9: What are they?
A10: The ﬁrst one departs PGH at 10:00am arrives Seattle at 12:05 their time.
The second ﬂight departs PGH at 5:55pm, arrives Seattle at 8pm. And the
last ﬂight departs PGH at 8:15pm arrives Seattle at 10:28pm.

C11: OK I’ll take the 5ish ﬂight on the night before on the 11th.
A12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air

ﬂight 115.

C13: OK.
A14: And you said returning on May 15th?
C15: Uh, yeah, at the end of the day.
A16: OK. There’s #two non-stops . . . #
C17:
A18: It’s a Friday.
C19: Uh hmm. I would consider staying there an extra day til Sunday.
A20: OK. . . OK. On Sunday I have . . .
Figure 15.2 Part of a phone conversation between a human travel agent (A) and human
client (C). The passages framed by # in A16 and C17 indicate overlaps in speech.

#, what day of the week is the 15th?

#Act. . . actually

Turns

turn

A dialogue is a sequence of turns (C1, A2, C3, and so on), each a single contribution
from one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,

318 CHAPTER 15

• CHATBOTS & DIALOGUE SYSTEMS

then me, and so on). There are 20 turns in Fig. 15.2. A turn can consist of a sentence
(like C1), although it might be as short as a single word (C13) or as long as multiple
sentences (A10).

Turn structure has important implications for spoken dialogue. A human has
to know when to stop talking; the client interrupts (in A16 and C17), so a system
that was performing this role must know to stop talking (and that the user might be
making a correction). A system also has to know when to start talking. For example,
most of the time in conversation, speakers start their turns almost immediately after
the other speaker ﬁnishes, without a long pause, because people are can usually
predict when the other person is about to ﬁnish talking. Spoken dialogue systems
must also detect whether a user is done speaking, so they can process the utterance
and respond. This task—called endpointing or endpoint detection— can be quite
challenging because of noise and because people often pause in the middle of turns.

endpointing

Speech Acts

A key insight into conversation—due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962)—is that each utterance in a
dialogue is a kind of action being performed by the speaker. These actions are com-
monly called speech acts or dialogue acts: here’s one taxonomy consisting of 4
major classes (Bach and Harnish, 1979):

committing the speaker to something’s being the case (answering, claiming,
conﬁrming, denying, disagreeing, stating)
attempts by the speaker to get the addressee to do something (advising, ask-
ing, forbidding, inviting, ordering, requesting)
committing the speaker to some future course of action (promising, planning,
vowing, betting, opposing)

speech acts

Constatives:

Directives:

Commissives:

Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some so-
cial action (apologizing, greeting, thanking, accepting an acknowledgment)

A user asking a person or a dialogue system to do something (‘Turn up the mu-
sic’) is issuing a DIRECTIVE. Asking a question that requires an answer is also
a way of issuing a DIRECTIVE: in a sense when the system says (A2) “what day
in May did you want to travel?” it’s as if the system is (very politely) command-
ing the user to answer. By contrast, a user stating a constraint (like C1 ‘I need to
travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing
an ACKNOWLEDGMENT. The speech act expresses an important component of the
intention of the speaker (or writer) in saying what they said.

Grounding

common
ground
grounding

A dialogue is not just a series of independent speech acts, but rather a collective act
performed by the speaker and the hearer. Like all collective acts, it’s important for
the participants to establish what they both agree on, called the common ground
(Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Ground-
ing means acknowledging that the hearer has understood the speaker (Clark, 1996).
(People need grounding for non-linguistic actions as well; the reason an elevator but-
ton lights up when it’s pressed is to acknowledge that the elevator has indeed been
called, essentially grounding your action of pushing the button (Norman, 1988).)

Humans constantly ground each other’s utterances. We can ground by explicitly
saying “OK”, as the agent does in A8 or A10. Or we can ground by repeating what
the other person says; in utterance A2 the agent repeats “in May”, demonstrating her

15.1

• PROPERTIES OF HUMAN CONVERSATION

319

understanding to the client. Or notice that when the client answers a question, the
agent begins the next question with “And”. The “And” implies that the new question
is ‘in addition’ to the old question, again indicating to the client that the agent has
successfully understood the answer to the last question.

Subdialogues and Dialogue Structure

conversational
analysis

adjacency pair

side sequence

subdialogue

Conversations have structure. Consider, for example, the local structure between
speech acts discussed in the ﬁeld of conversational analysis (Sacks et al., 1974).
QUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed by
ACCEPTANCE (or REJECTION). COMPLIMENTS (“Nice jacket!”) often give rise to
DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs are
composed of a ﬁrst pair part and a second pair part (Schegloff, 1968), and these
expectations can help systems decide what actions to take.

However, dialogue acts aren’t always followed immediately by their second pair
part. The two parts can be separated by a side sequence (Jefferson 1972) or sub-
dialogue. For example utterances C17 to A20 constitute a correction subdialogue
(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):

C17: #Act. . . actually#, what day of the week is the 15th?
A18: It’s a Friday.
C19: Uh hmm. I would consider staying there an extra day til Sunday.
A20: OK. . . OK. On Sunday I have . . .

The question in C17 interrupts the prior discourse, in which the agent was looking
for a May 15 return ﬂight. The agent must answer the question and also realize that
‘’I would consider staying...til Sunday” means that the client would probably like to
change their plan, and now go back to ﬁnding return ﬂights, but for the 17th.

Another side sequence is the clariﬁcation question, which can form a subdia-
logue between a REQUEST and a RESPONSE. This is especially common in dialogue
systems where speech recognition errors causes the system to have to ask for clari-
ﬁcations or repetitions like the following:

User:

What do you have going to UNKNOWN WORD on the 5th?

System:
User:
System:

Let’s see, going where on the 5th?
Going to Hong Kong.
OK, here are some ﬂights...

presequence

In addition to side-sequences, questions often have presequences, like the fol-
lowing example where a user starts with a question about the system’s capabilities
(“Can you make train reservations”) before making a request.

Can you make train reservations?

User:
System: Yes I can.
User:

Great, I’d like to reserve a seat on the 4pm train to New York.

Initiative

initiative

Sometimes a conversation is completely controlled by one participant. For exam-
ple a reporter interviewing a chef might ask questions, and the chef responds. We
say that the reporter in this case has the conversational initiative (Carbonell, 1970;
Nickerson, 1976). In normal human-human dialogue, however, it’s more common
for initiative to shift back and forth between the participants, as they sometimes
answer questions, sometimes ask them, sometimes take the conversations in new di-
rections, sometimes not. You may ask me a question, and then I respond asking you

320 CHAPTER 15

• CHATBOTS & DIALOGUE SYSTEMS

to clarify something you said, which leads the conversation in all sorts of ways. We
call such interactions mixed initiative (Carbonell, 1970).

Full mixed initiative, while the norm for human-human conversations, can be
difﬁcult for dialogue systems. The most primitive dialogue systems tend to use
system-initiative, where the system asks a question and the user can’t do anything
until they answer it, or user-initiative like simple search engines, where the user
speciﬁes a query and the system passively responds. Even modern large language
model-based dialogue systems, which come much closer to using full mixed initia-
tive, often don’t have completely natural initiative switching. Getting this right is an
important goal for modern systems.

Inference and Implicature

Inference is also important in dialogue understanding. Consider the client’s response
C2, repeated here:

A2: And, what day in May did you want to travel?
C3: OK uh I need to be there for a meeting that’s from the 12th to the 15th.

Notice that the client does not in fact answer the agent’s question. The client
merely mentions a meeting at a certain time. What is it that licenses the agent to
infer that the client is mentioning this meeting so as to inform the agent of the travel
dates?

The speaker seems to expect the hearer to draw certain inferences; in other
words, the speaker is communicating more information than seems to be present
in the uttered words. This kind of example was pointed out by Grice (1975, 1978)
as part of his theory of conversational implicature. Implicature means a particu-
lar class of licensed inferences. Grice proposed that what enables hearers to draw
these inferences is that conversation is guided by a set of maxims, general heuristics
that play a guiding role in the interpretation of conversational utterances. One such
maxim is the maxim of relevance which says that speakers attempt to be relevant,
they don’t just utter random speech acts. When the client mentions a meeting on the
12th, the agent reasons ‘There must be some relevance for mentioning this meeting.
What could it be?’. The agent knows that one precondition for having a meeting
(at least before Web conferencing) is being at the place where the meeting is held,
and therefore that maybe the meeting is a reason for the travel, and if so, then since
people like to arrive the day before a meeting, the agent should infer that the ﬂight
should be on the 11th.

These subtle characteristics of human conversations (turns, speech acts, ground-
ing, dialogue structure, initiative, and implicature) are among the reasons it is dif-
ﬁcult to build dialogue systems that can carry on natural conversations with humans.
Many of these challenges are active areas of dialogue systems research.

implicature

relevance

15.2 Frame-Based Dialogue Systems

A task-based dialogue system has the goal of helping a user solve a speciﬁc task
like making an travel rese
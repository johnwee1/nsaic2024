l be a good default for all hidden layers.
For the output layer, it really depends on your task.

24 Dominic Masters and Carlo Luschi, “Revisiting Small Batch Training for Deep Neural Networks,” arXiv pre‐

print arXiv:1804.07612 (2018).

25 Elad Hoffer et al., “Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training

of Neural Networks,” Proceedings of the 31st International Conference on Neural Information Processing Systems
(2017): 1729–1739.

26 Priya Goyal et al., “Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour,” arXiv preprint arXiv:

1706.02677 (2017).

326 

| 

Chapter 10: Introduction to Artificial Neural Networks with Keras

Number of iterations

In  most  cases,  the  number  of  training  iterations  does  not  actually  need  to  be
tweaked: just use early stopping instead.

The optimal learning rate depends on the other hyperparameters—
especially  the  batch  size—so  if  you  modify  any  hyperparameter,
make sure to update the learning rate as well.

For more best practices regarding tuning neural network hyperparameters, check out
the excellent 2018 paper27 by Leslie Smith.

This concludes our introduction to artificial neural networks and their implementa‐
tion  with  Keras.  In  the  next  few  chapters,  we  will  discuss  techniques  to  train  very
deep nets. We will also explore how to customize models using TensorFlow’s lower-
level API and how to load and preprocess data efficiently using the Data API. And we
will dive into other popular neural network architectures: convolutional neural net‐
works  for  image  processing,  recurrent  neural  networks  for  sequential  data,  autoen‐
coders for representation learning, and generative adversarial networks to model and
generate data.28

Exercises

1. The  TensorFlow  Playground  is  a  handy  neural  network  simulator  built  by  the
TensorFlow team. In this exercise, you will train several binary classifiers in just a
few  clicks,  and  tweak  the  model’s  architecture  and  its  hyperparameters  to  gain
some  intuition  on  how  neural  networks  work  and  what  their  hyperparameters
do. Take some time to explore the following:

a. The patterns learned by a neural net. Try training the default neural network
by clicking the Run button (top left). Notice how it quickly finds a good solu‐
tion  for  the  classification  task.  The  neurons  in  the  first  hidden  layer  have
learned  simple  patterns,  while  the  neurons  in  the  second  hidden  layer  have
learned  to  combine  the  simple  patterns  of  the  first  hidden  layer  into  more
complex patterns. In general, the more layers there are, the more complex the
patterns can be.

b. Activation functions. Try replacing the tanh activation function with a ReLU
activation function, and train the network again. Notice that it finds a solution

27 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch

Size, Momentum, and Weight Decay,” arXiv preprint arXiv:1803.09820 (2018).

28 A few extra ANN architectures are presented in Appendix E.

Exercises 

| 

327

even faster, but this time the boundaries are linear. This is due to the shape of
the ReLU function.

c. The  risk  of  local  minima.  Modify  the  network  architecture  to  have  just  one
hidden layer with three neurons. Train it multiple times (to reset the network
weights, click the Reset button next to the Play button). Notice that the train‐
ing time varies a lot, and sometimes it even gets stuck in a local minimum.

d. What  happens  when  neural  nets  are  too  small.  Remove  one  neuron  to  keep
just  two.  Notice  that  the  neural  network  is  now  incapable  of  finding  a  good
solution,  even  if  you  try  multiple  times.  The  model  has  too  few  parameters
and systematically underfits the training set.

e. What happens when neural nets are large enough. Set the number of neurons
to eight, and train the network several times. Notice that it is now consistently
fast and never gets stuck. This highlights an important finding in neural net‐
work  theory:  large  neural  networks  almost  never  get  stuck  in  local  minima,
and  even  when  they  do  these  local  optima  are  almost  as  good  as  the  global
optimum. However, they can still get stuck on long plateaus for a long time.

f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the
bottom-right dataset under “DATA”), and change the network architecture to
have  four  hidden  layers  with  eight  neurons  each.  Notice  that  training  takes
much  longer  and  often  gets  stuck  on  plateaus  for  long  periods  of  time.  Also
notice  that  the  neurons  in  the  highest  layers  (on  the  right)  tend  to  evolve
faster than the neurons in the lowest layers (on the left). This problem, called
the “vanishing gradients” problem, can be alleviated with better weight initial‐
ization and other techniques, better optimizers (such as AdaGrad or Adam),
or Batch Normalization (discussed in Chapter 11).

g. Go further. Take an hour or so to play around with other parameters and get a
feel  for  what  they  do,  to  build  an  intuitive  understanding  about  neural
networks.

2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)
that  computes  A  ⊕  B  (where  ⊕  represents  the  XOR  operation).  Hint:  A  ⊕  B  =
(A ∧ ¬ B ∨ (¬ A ∧ B).

3. Why is it generally preferable to use a Logistic Regression classifier rather than a
classical Perceptron (i.e., a single layer of threshold logic units trained using the
Perceptron  training  algorithm)?  How  can  you  tweak  a  Perceptron  to  make  it
equivalent to a Logistic Regression classifier?

4. Why  was  the  logistic  activation  function  a  key  ingredient  in  training  the  first

MLPs?

5. Name three popular activation functions. Can you draw them?

328 

| 

Chapter 10: Introduction to Artificial Neural Networks with Keras

6. Suppose  you  have  an  MLP  composed  of  one  input  layer  with  10  passthrough
neurons, followed by one hidden layer with 50 artificial neurons, and finally one
output layer with 3 artificial neurons. All artificial neurons use the ReLU activa‐
tion function.

• What is the shape of the input matrix X?

• What are the shapes of the hidden layer’s weight vector Wh and its bias vector

bh?

• What are the shapes of the output layer’s weight vector Wo and its bias vector

bo?

• What is the shape of the network’s output matrix Y?

• Write the equation that computes the network’s output matrix Y as a function

of X, Wh, bh, Wo, and bo.

7. How many neurons do you need in the output layer if you want to classify email
into spam or ham? What activation function should you use in the output layer?
If instead you want to tackle MNIST, how many neurons do you need in the out‐
put layer, and which activation function should you use? What about for getting
your network to predict housing prices, as in Chapter 2?

8. What is backpropagation and how does it work? What is the difference between

backpropagation and reverse-mode autodiff?

9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP
overfits the training data, how could you tweak these hyperparameters to try to
solve the problem?

10. Train  a  deep  MLP  on  the  MNIST  dataset  (you  can  load  it  using  keras.data
sets.mnist.load_data(). See if you can get over 98% precision. Try searching
for the optimal learning rate by using the approach presented in this chapter (i.e.,
by  growing  the  learning  rate  exponentially,  plotting  the  loss,  and  finding  the
point  where  the  loss  shoots  up).  Try  adding  all  the  bells  and  whistles—save
checkpoints, use early stopping, and plot learning curves using TensorBoard.

Solutions to these exercises are available in Appendix A.

Exercises 

| 

329

CHAPTER 11
Training Deep Neural Networks

In  Chapter  10  we  introduced  artificial  neural  networks  and  trained  our  first  deep
neural networks. But they were shallow nets, with just a few hidden layers. What if
you need to tackle a complex problem, such as detecting hundreds of types of objects
in high-resolution images? You may need to train a much deeper DNN, perhaps with
10 layers or many more, each containing hundreds of neurons, linked by hundreds of
thousands  of  connections.  Training  a  deep  DNN  isn’t  a  walk  in  the  park.  Here  are
some of the problems you could run into:

• You  may  be  faced  with  the  tricky  vanishing  gradients  problem  or  the  related
exploding  gradients  problem.  This  is  when  the  gradients  grow  smaller  and
smaller,  or  larger  and  larger,  when  flowing  backward  through  the  DNN  during
training. Both of these problems make lower layers very hard to train.

• You might not have enough training data for such a large network, or it might be

too costly to label.

• Training may be extremely slow.

• A model with millions of parameters would severely risk overfitting the training
set, especially if there are not enough training instances or if they are too noisy.

In this chapter we will go through each of these problems and present techniques to
solve  them.  We  will  start  by  exploring  the  vanishing  and  exploding  gradients  prob‐
lems and some of their most popular solutions. Next, we will look at transfer learning
and  unsupervised  pretraining,  which  can  help  you  tackle  complex  tasks  even  when
you have little labeled data. Then we will discuss various optimizers that can speed up
training large models tremendously. Finally, we will go through a few popular regula‐
rization techniques for large neural networks.

With these tools, you will be able to train very deep nets. Welcome to Deep Learning!

331

The Vanishing/Exploding Gradients Problems
As we discussed in Chapter 10, the backpropagation algorithm works by going from
the output layer to the input layer, propagating the error gradient along the way. Once
the  algorithm  has  computed  the  gradient  of  the  cost  function  with  regard  to  each
parameter  in  the  network,  it  uses  these  gradients  to  update  each  parameter  with  a
Gradient Descent step.

Unfortunately,  gradients  often  get  smaller  and  smaller  as  the  algorithm  progresses
down to the lower layers. As a result, the Gradient Descent update leaves the lower
layers’  connection  weights  virtually  unchanged,  and  training  never  converges  to  a
good solution. We call this the vanishing gradients problem. In some cases, the oppo‐
site  can  happen:  the  gradients  can  grow  bigger  and  bigger  until  layers  get  insanely
large weight updates and the algorithm diverges. This is the exploding gradients prob‐
lem,  which  surfaces  in  recurrent  neural  networks  (see  Chapter  15).  More  generally,
deep  neural  networks  suffer  from  unstable  gradients;  different  layers  may  learn  at
widely different speeds.

This unfortunate behavior was empirically observed long ago, and it was one of the
reasons  deep  neural  networks  were  mostly  abandoned  in  the  early  2000s.  It  wasn’t
clear  what  caused  the  gradients  to  be  so  unstable  when  training  a  DNN,  but  some
light  was  shed  in  a  2010  paper  by  Xavier  Glorot  and  Yoshua  Bengio.1  The  authors
found a few suspects, including the combination of the popular logistic sigmoid acti‐
vation function and the weight initialization technique that was most popular at the
time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In
short,  they  showed  that  with  this  activation  function  and  this  initialization  scheme,
the  variance  of  the  outputs  of  each  layer  is  much  greater  than  the  variance  of  its
inputs. Going forward in the network, the variance keeps increasing after each layer
until  the  activation  function  saturates  at  the  top  layers.  This  saturation  is  actually
made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyper‐
bolic  tangent  function  has  a  mean  of  0  and  behaves  slightly  better  than  the  logistic
function in deep networks).

Looking  at  the  logistic  activation  function  (see  Figure  11-1),  you  can  see  that  when
inputs  become  large  (negative  or  positive),  the  function  saturates  at  0  or  1,  with  a
derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually
no  gradient  to  propagate  back  through  the  network;  and  what  little  gradient  exists
keeps getting diluted as backpropagation progresses down through the top layers, so
there is really nothing left for the lower layers.

1 Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural Net‐

works,” Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 249–256.

332 

| 

Chapter 11: Training Deep Neural Networks

Figure 11-1. Logistic activation function saturation

Glorot and He Initialization
In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable
gradients problem. They point out that we need the signal to flow properly in both
directions:  in  the  forward  direction  when  making  predictions,  and  in  the  reverse
direction when backpropagating gradients. We don’t want the signal to die out, nor
do  we  want  it  to  explode  and  saturate.  For  the  signal  to  flow  properly,  the  authors
argue that we need the variance of the outputs of each layer to be equal to the var‐
iance of its inputs,2 and we need the gradients to have equal variance before and after
flowing through a layer in the reverse direction (please check out the paper if you are
interested  in  the  mathematical  details).  It  is  actually  not  possible  to  guarantee  both
unless the layer has an equal number of inputs and neurons (these numbers are called
the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compro‐
mise  that  has  proven  to  work  very  well  in  practice:  the  connection  weights  of  each
layer must be initialized randomly as described in Equation 11-1, where fanavg = (fanin
+ fanout)/2. This initialization strategy is called Xavier initialization or Glorot initiali‐
zation, after the paper’s first author.

2 Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but
if you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐
ing. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come
out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude
as it came in.

The Vanishing/Exploding Gradients Problems 

| 

333

Equation 11-1. Glorot initialization (when using the logistic activation function)

Normal distribution with mean 0 and variance σ2 =

1
fan

avg

Or a uniform distribution between −r and  + r, with r =

3
fan

avg

If you replace fanavg with fanin in Equation 11-1, you get an initialization strategy that
Yann LeCun proposed in the 1990s. He called it LeCun initialization. Genevieve Orr
and Klaus-Robert Müller even recommended it in their 1998 book Neural Networks:
Tricks of the Trade (Springer). LeCun initialization is equivalent to Glorot initializa‐
tion when fanin = 
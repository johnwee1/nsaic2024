ge of OOV words that appear in the test set is called the
OOV rate. One way to create an open vocabulary system is to model potential
unknown words in the test set by adding a pseudo-word called <UNK>. Again, most
modern language models are closed vocabulary and don’t use an <UNK> token. But
when necessary, we can train <UNK> probabilities by turning the problem back into
a closed vocabulary one by choosing a ﬁxed vocabulary in advance:

1. Choose a vocabulary (word list) that is ﬁxed in advance.
2. Convert in the training set any word that is not in this set (any OOV word) to

the unknown word token <UNK> in a text normalization step.

3. Estimate the probabilities for <UNK> from its counts just like any other regular

word in the training set.

The exact choice of <UNK> has an effect on perplexity. A language model can
achieve low perplexity by choosing a small vocabulary and assigning the unknown
word a high probability. Thus perplexities can only be compared across language
models with <UNK> if they have the exact same vocabularies (Buck et al., 2014).

closed
vocabulary

OOV

open
vocabulary

3.6 Smoothing

What do we do with words that are in our vocabulary (they are not unknown words)
but appear in a test set in an unseen context (for example they appear after a word
they never appeared after in training)? To keep a language model from assigning
zero probability to these unseen events, we’ll have to shave off a bit of probability
mass from some more frequent events and give it to the events we’ve never seen.
This modiﬁcation is called smoothing or discounting. In this section and the fol-
lowing ones we’ll introduce a variety of ways to do smoothing: Laplace (add-one)

smoothing

discounting

46 CHAPTER 3

• N-GRAM LANGUAGE MODELS

smoothing, add-k smoothing, and stupid backoff. At the end of the chapter we
also summarize a more complex method, Kneser-Ney smoothing.

3.6.1 Laplace Smoothing

The simplest way to do smoothing is to add one to all the n-gram counts, before
we normalize them into probabilities. All the counts that used to be zero will now
have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called
Laplace smoothing. Laplace smoothing does not perform well enough to be used
in modern n-gram models, but it usefully introduces many of the concepts that we
see in other smoothing algorithms, gives a useful baseline, and is also a practical
smoothing algorithm for other tasks like text classiﬁcation (Chapter 4).

Let’s start with the application of Laplace smoothing to unigram probabilities.
Recall that the unsmoothed maximum likelihood estimate of the unigram probability
of the word wi is its count ci normalized by the total number of word tokens N:

Laplace
smoothing

P(wi) =

ci
N

add-one

Laplace smoothing merely adds one to each count (hence its alternate name add-
one smoothing). Since there are V words in the vocabulary and each one was incre-
mented, we also need to adjust the denominator to take into account the extra V
observations. (What happens to our P values if we don’t increase the denominator?)

PLaplace(wi) =

ci + 1
N +V

(3.21)

Instead of changing both the numerator and denominator, it is convenient to
describe how a smoothing algorithm affects the numerator, by deﬁning an adjusted
count c∗. This adjusted count is easier to compare directly with the MLE counts and
can be turned into a probability like an MLE count by normalizing by N. To deﬁne
this count, since we are only changing the numerator in addition to adding 1 we’ll
also need to multiply by a normalization factor N

N+V :

c∗i = (ci + 1)

N
N +V

(3.22)

discounting

discount

We can now turn c∗i into a probability P∗i by normalizing by N.

A related way to view smoothing is as discounting (lowering) some non-zero
counts in order to get the probability mass that will be assigned to the zero counts.
Thus, instead of referring to the discounted counts c∗, we might describe a smooth-
ing algorithm in terms of a relative discount dc, the ratio of the discounted counts to
the original counts:

dc =

c∗
c

Now that we have the intuition for the unigram case, let’s smooth our Berkeley
Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the
bigrams in Fig. 3.1.

Figure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.
Recall that normal bigram probabilities are computed by normalizing each row of
counts by the unigram count:

3.6

• SMOOTHING

47

i
want
to
eat
chinese
food
lunch
spend

i
6
3
3
1
2
16
3
2

want
828
1
1
1
1
1
1
1

to
1
609
5
3
1
16
1
2

eat
10
2
687
1
1
1
1
1

chinese
1
7
3
17
1
2
1
1

food
1
7
1
3
83
5
2
1

lunch
1
6
7
43
2
1
1
1

spend
3
2
212
1
1
1
1
1

Figure 3.6 Add-one smoothed bigram counts for eight of the words (out of V = 1446) in
the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.

P(wn|

wn

−

1) =

C(wn
−
C(wn

1wn)
1)

−

(3.23)

For add-one smoothed bigram counts, we need to augment the unigram count by

the number of total word types in the vocabulary V :

PLaplace(wn|

C(wn
−
w (C(wn
−
Thus, each of the unigram counts given in the previous section will need to be
augmented by V = 1446. The result is the smoothed bigram probabilities in Fig. 3.7.

1wn) + 1
1) +V

C(wn
−
C(wn

1wn) + 1

1w) + 1)

1) =

(3.24)

wn

(cid:80)

=

−

−

i
0.0015
0.0013
0.00078
0.00046
0.0012
0.0063
0.0017
0.0012

spend
eat
0.00075
0.0025
i
0.00084
0.00084
want
0.055
0.18
to
0.00046
0.00046
eat
0.00062
0.00062
chinese
0.00039
0.00039
food
0.00056
0.00056
lunch
spend
0.00058
0.00058
Figure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP
corpus of 9332 sentences. Previously-zero probabilities are in gray.

want
0.21
0.00042
0.00026
0.00046
0.00062
0.00039
0.00056
0.00058

chinese
0.00025
0.0029
0.00078
0.0078
0.00062
0.00079
0.00056
0.00058

food
0.00025
0.0029
0.00026
0.0014
0.052
0.002
0.0011
0.00058

lunch
0.00025
0.0025
0.0018
0.02
0.0012
0.00039
0.00056
0.00058

to
0.00025
0.26
0.0013
0.0014
0.00062
0.0063
0.00056
0.0012

It is often convenient to reconstruct the count matrix so we can see how much a
smoothing algorithm has changed the original counts. These adjusted counts can be
computed by Eq. 3.25. Figure 3.8 shows the reconstructed counts.

−

−

−

1)

C(wn

c∗(wn

[C(wn

1wn) =

×
1) +V

1wn) + 1]
C(wn
Note that add-one smoothing has made a very big change to the counts. Com-
paring Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to) changed
from 608 to 238! We can see this in probability space as well: P(to
want) decreases
|
from .66 in the unsmoothed case to .26 in the smoothed case. Looking at the dis-
count d (the ratio between new and old counts) shows us how strikingly the counts
for each preﬁx word have been reduced; the discount for the bigram want to is .39,
while the discount for Chinese food is .10, a factor of 10!

(3.25)

−

The sharp change in counts and probabilities occurs because too much probabil-

ity mass is moved to all the zeros.

48 CHAPTER 3

• N-GRAM LANGUAGE MODELS

i
3.8
1.2
1.9
0.34
0.2
6.9
0.57
0.32

i
want
to
eat
chinese
food
lunch
spend
Figure 3.8 Add-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus
of 9332 sentences. Previously-zero counts are in gray.

chinese
0.64
2.7
1.9
5.8
0.098
0.86
0.19
0.16

spend
1.9
0.78
133
0.34
0.098
0.43
0.19
0.16

lunch
0.64
2.3
4.4
15
0.2
0.43
0.19
0.16

eat
6.4
0.78
430
0.34
0.098
0.43
0.19
0.16

want
527
0.39
0.63
0.34
0.098
0.43
0.19
0.16

to
0.64
238
3.1
1
0.098
6.9
0.19
0.32

food
0.64
2.7
0.63
1
8.2
2.2
0.38
0.16

3.6.2 Add-k smoothing

One alternative to add-one smoothing is to move a bit less of the probability mass
from the seen to the unseen events. Instead of adding 1 to each count, we add a frac-
tional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.

add-k

P∗Add-k(wn|

wn

−

1) =

C(wn
−
C(wn

1wn) + k
1) + kV

−

(3.26)

Add-k smoothing requires that we have a method for choosing k; this can be
done, for example, by optimizing on a devset. Although add-k is useful for some
tasks (including text classiﬁcation), it turns out that it still doesn’t work well for
language modeling, generating counts with poor variances and often inappropriate
discounts (Gale and Church, 1994).

3.6.3 Backoff and Interpolation

The discounting we have been discussing so far can help solve the problem of zero
frequency n-grams. But there is an additional source of knowledge we can draw on.
If we are trying to compute P(wn|
1) but we have no examples of a particular
1wn, we can instead estimate its probability by using the bigram
trigram wn
2wn
1),
1). Similarly, if we don’t have counts to compute P(wn|
probability P(wn|
wn
we can look to the unigram P(wn).

2wn

wn

wn

−

−

−

−

−

−

backoff

interpolation

In other words, sometimes using less context is a good thing, helping to general-
ize more for contexts that the model hasn’t learned much about. There are two ways
to use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is
sufﬁcient, otherwise we use the bigram, otherwise the unigram. In other words, we
only “back off” to a lower-order n-gram if we have zero evidence for a higher-order
n-gram. By contrast, in interpolation, we always mix the probability estimates
from all the n-gram estimators, weighting and combining the trigram, bigram, and
unigram counts.

In simple linear interpolation, we combine different order n-grams by linearly
1) by
interpolating them. Thus, we estimate the trigram probability P(wn|
mixing together the unigram, bigram, and trigram probabilities, each weighted by a
λ :

2wn

wn

−

−

ˆP(wn|

wn

−

2wn

−

1) = λ1P(wn)
+λ2P(wn|
+λ3P(wn|

wn
wn

−

−

1)
2wn

1)

−

(3.27)

3.6

• SMOOTHING

49

The λ s must sum to 1, making Eq. 3.27 equivalent to a weighted average.
In a
slightly more sophisticated version of linear interpolation, each λ weight is com-
puted by conditioning on the context. This way, if we have particularly accurate
counts for a particular bigram, we assume that the counts of the trigrams based on
this bigram will be more trustworthy, so we can make the λ s for those trigrams
higher and thus give that trigram more weight in the interpolation. Equation 3.28
shows the equation for interpolation with context-conditioned weights:

ˆP(wn|

wn

−

−

2wn

1) = λ1(wn

2:n

−
+λ2(wn
−
+ λ3(wn

−
2:n

1)P(wn)
1)P(wn|
wn
−
1)P(wn|
wn

−

−
2:n

−

−

1)
2wn

1)

−

(3.28)

held-out

discount

Katz backoff

How are these λ values set? Both the simple interpolation and conditional interpo-
lation λ s are learned from a held-out corpus. A held-out corpus is an additional
training corpus, so-called because we hold it out from the training data, that we use
to set hyperparameters like these λ values. We do so by choosing the λ values that
maximize the likelihood of the held-out corpus. That is, we ﬁx the n-gram probabil-
ities and then search for the λ values that—when plugged into Eq. 3.27—give us the
highest probability of the held-out set. There are various ways to ﬁnd this optimal
set of λ s. One way is to use the EM algorithm, an iterative learning algorithm that
converges on locally optimal λ s (Jelinek and Mercer, 1980).

In a backoff n-gram model, if the n-gram we need has zero counts, we approx-
imate it by backing off to the (n-1)-gram. We continue backing off until we reach a
history that has some counts.

In order for a backoff model to give a correct probability distribution, we have
to discount the higher-order n-grams to save some probability mass for the lower
order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t
discounted and we just used the undiscounted MLE probability, then as soon as we
replaced an n-gram which has zero probability with a lower-order n-gram, we would
be adding probability mass, and the total probability assigned to all possible strings
by the language model would be greater than 1! In addition to this explicit discount
factor, we’ll need a function α to distribute this probability mass to the lower order
n-grams.

This kind of backoff with discounting is also called Katz backoff. In Katz back-
off we rely on a discounted probability P∗ if we’ve seen this n-gram before (i.e., if
we have non-zero counts). Otherwise, we recursively back off to the Katz probabil-
ity for the shorter-history (n-1)-gram. The probability for a backoff n-gram PBO is
thus computed as follows:

PBO(wn|

wn

−

N+1:n

−

1) =




P∗(wn|
α(wn

−

wn

−

N+1:n

−

1),
1)PBO(wn|

N+1:n

−

wn

−

N+2:n

if C(wn

N+1:n) > 0

−

1),

−

otherwise.

(3.29)

Good-Turing



Katz backoff is often combined with a smoothing method called Good-Turing.
The combined Good-Turing backoff algorithm involves quite detailed computation
for estimating the Good-Turing smoothing and the P∗ and α values.

50 CHAPTER 3

• N-GRAM LANGUAGE MODELS

3.7 Huge Language Models and Stupid Backoff

By using text from the web or other enormous collections, it is possible to build
extremely large language models. The Web 1 Trillion 5-gram corpus released by
Google includes various large sets of n-grams, including 1-grams through 5-grams
from all the ﬁve-word sequences that appear at least 40 times from 1,024,908,267,229
words of text from publicly accessible Web pages in English (Franz and Brants,
2006). Google has also released Google Books Ngrams corpora with n-grams drawn
from their book collections, including another 800 billion tokens of n-grams from
Chinese, English, French, German, Hebrew, Italian, Russian, and Spanish (Lin et al.,
2012a). Smaller but more carefully curated n-gram corpora for English include
the million most frequent n-grams drawn from the COCA (Corpus of Contempo-
rary American English) 1 billion word corpus of American English (Davies, 2020).
COCA is a balanced corpus, meaning that it has roughly equal numbers of words
from different genres: web, newspapers, spoken conversation transcripts, ﬁction,
and so on, drawn from the period 1990-2019, and has the context of each n-gram as
well as labels for genre and provenance.

Some example 4-grams from the Google Web corpus:

4-gram
serve as the incoming
serve as the incubator
serve as the independent
serve as the index
serve as the indication
serve as the indicator
serve as the indicators

Count
92
99
794
223
72
120
45

Efﬁciency considerations are important when building language models that use
such large sets of n-grams. Rather than store each word as a string, it is generally
represented in memory as a 64-bit hash number, with the words themselves stored
on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte
ﬂoats), and n-grams are stored in reverse tries.

An n-gram language model can also be shrunk by pruning, for example only
storing n-grams with counts greater than some threshold (such as the count threshold
of 40 used for the Google n-gram release) or using entropy to prune less-important
n-grams (Stolcke, 1998). Another option is to build approximate language models
using techniques like Bloom ﬁlters (Talbot and Osborne 2007, Church et al. 2007).
Finally, efﬁcient language model toolkits like KenLM (Heaﬁeld 2011, Heaﬁeld et al.
2013) use sorted arrays, efﬁciently combine probabilities and backoffs in a single
value, and use merge sorts to efﬁciently build the probability tables in a minimal
number of passes through a large corpus.

Although
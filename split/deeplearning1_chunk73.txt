ient because it is applied to a linear feature extractor, and can
thus be trained in closed form. Like some variants of ICA, SFA is not quite a
generative model per se, in the sense that it deï¬?nes a linear map between input
space and feature space but does not deï¬?ne a prior over feature space and thus
does not impose a distribution p(x) on input space.
The SFA algorithm (Wiskott and Sejnowski, 2002) consists of deï¬?ning f (x; Î¸)
to be a linear transformation, and solving the optimization problem
min Et (f (x(t+1) )i âˆ’ f (x (t)) i)2

(13.8)

Etf (x (t))i = 0

(13.9)

Et [f (x(t))2i ] = 1.

(13.10)

Î¸

subject to the constraints
and

494

CHAPTER 13. LINEAR FACTOR MODELS

The constraint that the learned feature have zero mean is necessary to make the
problem have a unique solution; otherwise we could add a constant to all feature
values and obtain a diï¬€erent solution with equal value of the slowness objective.
The constraint that the features have unit variance is necessary to prevent the
pathological solution where all features collapse to 0. Like PCA, the SFA features
are ordered, with the ï¬?rst feature being the slowest. To learn multiple features, we
must also add the constraint
âˆ€i < j, Et [f (x(t)) if (x (t) )j ] = 0.

(13.11)

This speciï¬?es that the learned features must be linearly decorrelated from each
other. Without this constraint, all of the learned features would simply capture the
one slowest signal. One could imagine using other mechanisms, such as minimizing
reconstruction error, to force the features to diversify, but this decorrelation
mechanism admits a simple solution due to the linearity of SFA features. The SFA
problem may be solved in closed form by a linear algebra package.
SFA is typically used to learn nonlinear features by applying a nonlinear basis
expansion to x before running SFA. For example, it is common to replace x by the
quadratic basis expansion, a vector containing elements x ixj for all i and j. Linear
SFA modules may then be composed to learn deep nonlinear slow feature extractors
by repeatedly learning a linear SFA feature extractor, applying a nonlinear basis
expansion to its output, and then learning another linear SFA feature extractor on
top of that expansion.
When trained on small spatial patches of videos of natural scenes, SFA with
quadratic basis expansions learns features that share many characteristics with
those of complex cells in V1 cortex (Berkes and Wiskott, 2005). When trained
on videos of random motion within 3-D computer rendered environments, deep
SFA learns features that share many characteristics with the features represented
by neurons in rat brains that are used for navigation (Franzius et al., 2007). SFA
thus seems to be a reasonably biologically plausible model.
A major advantage of SFA is that it is possibly to theoretically predict which
features SFA will learn, even in the deep, nonlinear setting. To make such theoretical
predictions, one must know about the dynamics of the environment in terms of
conï¬?guration space (e.g., in the case of random motion in the 3-D rendered
environment, the theoretical analysis proceeds from knowledge of the probability
distribution over position and velocity of the camera). Given the knowledge of how
the underlying factors actually change, it is possible to analytically solve for the
optimal functions expressing these factors. In practice, experiments with deep SFA
applied to simulated data seem to recover the theoretically predicted functions.
495

CHAPTER 13. LINEAR FACTOR MODELS

This is in comparison to other learning algorithms where the cost function depends
highly on speciï¬?c pixel values, making it much more diï¬ƒcult to determine what
features the model will learn.
Deep SFA has also been used to learn features for object recognition and pose
estimation (Franzius et al., 2008). So far, the slowness principle has not become
the basis for any state of the art applications. It is unclear what factor has limited
its performance. We speculate that perhaps the slowness prior is too strong, and
that, rather than imposing a prior that features should be approximately constant,
it would be better to impose a prior that features should be easy to predict from
one time step to the next. The position of an object is a useful feature regardless of
whether the objectâ€™s velocity is high or low, but the slowness principle encourages
the model to ignore the position of objects that have high velocity.

13.4

Sparse Coding

Sparse coding (Olshausen and Field, 1996) is a linear factor model that has
been heavily studied as an unsupervised feature learning and feature extraction
mechanism. Strictly speaking, the term â€œsparse codingâ€? refers to the process of
inferring the value of h in this model, while â€œsparse modelingâ€? refers to the process
of designing and learning the model, but the term â€œsparse codingâ€? is often used to
refer to both.
Like most other linear factor models, it uses a linear decoder plus noise to
obtain reconstructions of x, as speciï¬?ed in equation 13.2. More speciï¬?cally, sparse
coding models typically assume that the linear factors have Gaussian noise with
isotropic precision Î² :
1
p(x | h) = N (x; W h + b, I).
Î²

(13.12)

The distribution p(h) is chosen to be one with sharp peaks near 0 (Olshausen
and Field, 1996). Common choices include factorized Laplace, Cauchy or factorized
Student-t distributions. For example, the Laplace prior parametrized in terms of
the sparsity penalty coeï¬ƒcient Î» is given by
2
Î» 1
p(hi) = Laplace(hi ; 0, ) = eâˆ’ 2 Î»|hi |
Î»
4

(13.13)

and the Student-t prior by
p(h i) âˆ?

1
h2

Î½+1

(1 + Î½i ) 2
496

.

(13.14)

CHAPTER 13. LINEAR FACTOR MODELS

Training sparse coding with maximum likelihood is intractable. Instead, the
training alternates between encoding the data and training the decoder to better
reconstruct the data given the encoding. This approach will be justiï¬?ed further as
a principled approximation to maximum likelihood later, in section 19.3.
For models such as PCA, we have seen the use of a parametric encoder function
that predicts h and consists only of multiplication by a weight matrix. The encoder
that we use with sparse coding is not a parametric encoder. Instead, the encoder
is an optimization algorithm, that solves an optimization problem in which we seek
the single most likely code value:
hâˆ— = f (x) = arg max p(h | x).

(13.15)

h

When combined with equation 13.13 and equation 13.12, this yields the following
optimization problem:
arg max p(h | x)

(13.16)

= arg max log p(h | x)

(13.17)

= arg min Î»||h||1 + Î²||x âˆ’ W h||22,

(13.18)

h
h

h

where we have dropped terms not depending on h and divided by positive scaling
factors to simplify the equation.
Due to the imposition of an L1 norm on h, this procedure will yield a sparse
hâˆ— (See section 7.1.2).
To train the model rather than just perform inference, we alternate between
minimization with respect to h and minimization with respect to W . In this
presentation, we treat Î² as a hyperparameter. Typically it is set to 1 because its
role in this optimization problem is shared with Î» and there is no need for both
hyperparameters. In principle, we could also treat Î² as a parameter of the model
and learn it. Our presentation here has discarded some terms that do not depend
on h but do depend on Î² . To learn Î², these terms must be included, or Î² will
collapse to 0.
Not all approaches to sparse coding explicitly build a p (h) and a p(x | h).
Often we are just interested in learning a dictionary of features with activation
values that will often be zero when extracted using this inference procedure.
If we sample h from a Laplace prior, it is in fact a zero probability event for
an element of h to actually be zero. The generative model itself is not especially
sparse, only the feature extractor is. Goodfellow et al. (2013d) describe approximate
497

CHAPTER 13. LINEAR FACTOR MODELS

inference in a diï¬€erent model family, the spike and slab sparse coding model, for
which samples from the prior usually contain true zeros.
The sparse coding approach combined with the use of the non-parametric
encoder can in principle minimize the combination of reconstruction error and
log-prior better than any speciï¬?c parametric encoder. Another advantage is that
there is no generalization error to the encoder. A parametric encoder must learn
how to map x to h in a way that generalizes. For unusual x that do not resemble
the training data, a learned, parametric encoder may fail to ï¬?nd an h that results
in accurate reconstruction or a sparse code. For the vast majority of formulations
of sparse coding models, where the inference problem is convex, the optimization
procedure will always ï¬?nd the optimal code (unless degenerate cases such as
replicated weight vectors occur). Obviously, the sparsity and reconstruction costs
can still rise on unfamiliar points, but this is due to generalization error in the
decoder weights, rather than generalization error in the encoder. The lack of
generalization error in sparse codingâ€™s optimization-based encoding process may
result in better generalization when sparse coding is used as a feature extractor for
a classiï¬?er than when a parametric function is used to predict the code. Coates
and Ng (2011) demonstrated that sparse coding features generalize better for
object recognition tasks than the features of a related model based on a parametric
encoder, the linear-sigmoid autoencoder. Inspired by their work, Goodfellow et al.
(2013d) showed that a variant of sparse coding generalizes better than other feature
extractors in the regime where extremely few labels are available (twenty or fewer
labels per class).
The primary disadvantage of the non-parametric encoder is that it requires
greater time to compute h given x because the non-parametric approach requires
running an iterative algorithm. The parametric autoencoder approach, developed
in chapter 14, uses only a ï¬?xed number of layers, often only one. Another
disadvantage is that it is not straight-forward to back-propagate through the
non-parametric encoder, which makes it diï¬ƒcult to pretrain a sparse coding model
with an unsupervised criterion and then ï¬?ne-tune it using a supervised criterion.
Modiï¬?ed versions of sparse coding that permit approximate derivatives do exist
but are not widely used (Bagnell and Bradley, 2009).
Sparse coding, like other linear factor models, often produces poor samples, as
shown in ï¬?gure 13.2. This happens even when the model is able to reconstruct
the data well and provide useful features for a classiï¬?er. The reason is that each
individual feature may be learned well, but the factorial prior on the hidden code
results in the model including random subsets of all of the features in each generated
sample. This motivates the development of deeper models that can impose a non498

CHAPTER 13. LINEAR FACTOR MODELS

Figure 13.2: Example samples and weights from a spike and slab sparse coding model
trained on the MNIST dataset. (Left)The samples from the model do not resemble the
training examples. At ï¬?rst glance, one might assume the model is poorly ï¬?t. (Right)The
weight vectors of the model have learned to represent penstrokes and sometimes complete
digits. The model has thus learned useful features. The problem is that the factorial prior
over features results in random subsets of features being combined. Few such subsets
are appropriate to form a recognizable MNIST digit. This motivates the development of
generative models that have more powerful distributions over their latent codes. Figure
reproduced with permission from Goodfellow et al. (2013d).

factorial distribution on the deepest code layer, as well as the development of more
sophisticated shallow models.

13.5

Manifold Interpretation of PCA

Linear factor models including PCA and factor analysis can be interpreted as
learning a manifold (Hinton et al., 1997). We can view probabilistic PCA as
deï¬?ning a thin pancake-shaped region of high probabilityâ€”a Gaussian distribution
that is very narrow along some axes, just as a pancake is very ï¬‚at along its vertical
axis, but is elongated along other axes, just as a pancake is wide along its horizontal
axes. This is illustrated in ï¬?gure 13.3. PCA can be interpreted as aligning this
pancake with a linear manifold in a higher-dimensional space. This interpretation
applies not just to traditional PCA but also to any linear autoencoder that learns
matrices W and V with the goal of making the reconstruction of x lie as close to
x as possible,
Let the encoder be
h = f (x ) = W î€¾ ( x âˆ’ Âµ ).
499

(13.19)

CHAPTER 13. LINEAR FACTOR MODELS

The encoder computes a low-dimensional representation of h. With the autoencoder
view, we have a decoder computing the reconstruction
xÌ‚ = g(h) = b + V h.

(13.20)

Figure 13.3: Flat Gaussian capturing probability concentration near a low-dimensional
manifold. The ï¬?gure shows the upper half of the â€œpancakeâ€? above the â€œmanifold planeâ€?
which goes through its middle. The variance in the direction orthogonal to the manifold is
very small (arrow pointing out of plane) and can be considered like â€œnoise,â€? while the other
variances are large (arrows in the plane) and correspond to â€œsignal,â€? and a coordinate
system for the reduced-dimension data.

The choices of linear encoder and decoder that minimize reconstruction error
E[||x âˆ’ xÌ‚|| 2]

(13.21)

correspond to V = W , Âµ = b = E[x] and the columns of W form an orthonormal
basis which spans the same subspace as the principal eigenvectors of the covariance
matrix
(13.22)
C = E[(x âˆ’ Âµ)(x âˆ’ Âµ)î€¾ ].
In the case of PCA, the columns of W are these eigenvectors, ordered by the
magnitude of the corresponding eigenvalues (which are all real and non-negative).
One can also show that eigenvalue Î»i of C corresponds to the variance of x
in the direction of eigenvector v(i) . If x âˆˆ RD and h âˆˆ Rd with d < D, then the
500

CHAPTER 13. LINEAR FACTOR MODELS

optimal reconstruction error (choosing Âµ, b, V and W as above) is
2

min E[||x âˆ’ xÌ‚|| ] =

D
î?˜

Î»i .

(13.23)

i=d+1

Hence, if the covariance has rank d , the eigenvalues Î» d+1 to Î»D are 0 and reconstruction error is 0.
Furthermore, one can also show that the above solution can be obtained by
maximizing the variances of the elements of h, under orthogonal W , instead of
minimizing reconstruction error.
Linear factor models are some of the simplest generative models and some of the
simplest models that learn a representation of data. Much as linear classiï¬?ers and
linear regression models may be extended to deep feedforward networks, these linear
factor models may be extended to autoencoder networks and deep probabilistic
models that perform the same tasks but with a much more powerful and ï¬‚exible
model family.

501

Chapter 14

Autoencoders
An autoencoder is a neural network that is trained to attempt to copy its input
to its output. Internally, it has a hidden layer h that describes a code used to
represent the input. The network may be viewed as consisting of two parts: an
encoder function h = f (x) and a decoder that produces a reconstruction r = g(h).
This architecture is presented in ï¬?gure 14.1. If an autoencoder succeeds in simply
learning to set g(f (x)) = x everywhere, then it is not especially useful. Instead,
autoencoders are designed to be unable to le
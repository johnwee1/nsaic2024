he data.

To compute the BIC and AIC, call the bic() and aic() methods:

>>> gm.bic(X)
8189.74345832983
>>> gm.aic(X)
8102.518178214792

Figure 9-21 shows the BIC for different numbers of clusters k. As you can see, both
the BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note
that we could also search for the best value for the covariance_type hyperparameter.
For example, if it is "spherical" rather than "full", then the model has significantly
fewer parameters to learn, but it does not fit the data as well.

Gaussian Mixtures 

| 

269

Figure 9-21. AIC and BIC for different numbers of clusters k

Bayesian Gaussian Mixture Models
Rather than manually searching for the optimal number of clusters, you can use the
BayesianGaussianMixture class, which is capable of giving weights equal (or close)
to zero to unnecessary clusters. Set the number of clusters n_components to a value
that you have good reason to believe is greater than the optimal number of clusters
(this  assumes  some  minimal  knowledge  about  the  problem  at  hand),  and  the  algo‐
rithm will eliminate the unnecessary clusters automatically. For example, let’s set the
number of clusters to 10 and see what happens:

>>> from sklearn.mixture import BayesianGaussianMixture
>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10)
>>> bgm.fit(X)
>>> np.round(bgm.weights_, 2)
array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])

Perfect: the algorithm automatically detected that only three clusters are needed, and
the resulting clusters are almost identical to the ones in Figure 9-17.

In this model, the cluster parameters (including the weights, means, and covariance
matrices)  are  not  treated  as  fixed  model  parameters  anymore,  but  as  latent  random
variables, like the cluster assignments (see Figure 9-22). So z now includes both the
cluster parameters and the cluster assignments.

The Beta distribution is commonly used to model random variables whose values lie
within a fixed range. In this case, the range is from 0 to 1. The Stick-Breaking Process
(SBP) is best explained through an example: suppose Φ=[0.3, 0.6, 0.5,…], then 30% of
the instances will be assigned to cluster 0, then 60% of the remaining instances will be
assigned to cluster 1, then 50% of the remaining instances will be assigned to cluster
2, and so on. This process is a good model for datasets where new instances are more
likely to join large clusters than small clusters (e.g., people are more likely to move to
larger cities). If the concentration α is high, then Φ values will likely be close to 0, and
the SBP generate many clusters. Conversely, if the concentration is low, then Φ values

270 

| 

Chapter 9: Unsupervised Learning Techniques

will likely be close to 1, and there will be few clusters. Finally, the Wishart distribution
is used to sample covariance matrices: the parameters d and V control the distribu‐
tion of cluster shapes.

Figure 9-22. Bayesian Gaussian mixture model

Prior knowledge about the latent variables z can be encoded in a probability distribu‐
tion p(z) called the prior. For example, we may have a prior belief that the clusters are
likely to be few (low concentration), or conversely, that they are likely to be plentiful
(high concentration). This prior belief about the number of clusters can be adjusted
using the weight_concentration_prior hyperparameter. Setting it to 0.01 or 10,000
gives  very  different  clusterings  (see  Figure  9-23).  The  more  data  we  have,  however,
the  less  the  priors  matter.  In  fact,  to  plot  diagrams  with  such  large  differences,  you
must use very strong priors and little data.

Figure 9-23. Using different concentration priors on the same data results in different
numbers of clusters

Gaussian Mixtures 

| 

271

Bayes’ theorem (Equation 9-2) tells us how to update the probability distribution over
the latent variables after we observe some data X. It computes the posterior distribu‐
tion p(z|X), which is the conditional probability of z given X.

Equation 9-2. Bayes’ theorem

p z X = posterior =

likelihood × prior
evidence

=

p X z p z
p X

Unfortunately, in a Gaussian mixture model (and many other problems), the denomi‐
nator  p(x)  is  intractable,  as  it  requires  integrating  over  all  the  possible  values  of  z
(Equation 9-3), which would require considering all possible combinations of cluster
parameters and cluster assignments.

Equation 9-3. The evidence p(X) is often intractable
p X = ∫ p X z p z dz

This intractability is one of the central problems in Bayesian statistics, and there are
several  approaches  to  solving  it.  One  of  them  is  variational  inference,  which  picks  a
family  of  distributions  q(z;  λ)  with  its  own  variational  parameters  λ  (lambda),  then
optimizes  these  parameters  to  make  q(z)  a  good  approximation  of  p(z|X).  This  is
achieved  by  finding  the  value  of  λ  that  minimizes  the  KL  divergence  from  q(z)  to
p(z|X), noted DKL(q‖p). The KL divergence equation is shown in Equation 9-4, and it
can be rewritten as the log of the evidence (log p(X)) minus the evidence lower bound
(ELBO). Since the log of the evidence does not depend on q, it is a constant term, so
minimizing the KL divergence just requires maximizing the ELBO.

Equation 9-4. KL divergence from q(z) to p(z|X)

DKL q ∥ p = q log

q z
p z X
= q log q z − log p z X
p z, X
p X

= q log q z − log

= q log q z − log p z, X + log p X
= q log q z − q log p z, X + q log p X
= q log p X − q log p z, X − q log q z
= log p X − ELBO

where ELBO = q log p z, X − q log q z

272 

| 

Chapter 9: Unsupervised Learning Techniques

In practice, there are different techniques to maximize the ELBO. In mean field varia‐
tional inference, it is necessary to pick the family of distributions q(z; λ) and the prior
p(z) very carefully to ensure that the equation for the ELBO simplifies to a form that
can be computed. Unfortunately, there is no general way to do this. Picking the right
family  of  distributions  and  the  right  prior  depends  on  the  task  and  requires  some
mathematical skills. For example, the distributions and lower-bound equations used
in  Scikit-Learn’s  BayesianGaussianMixture  class  are  presented  in  the  documenta‐
tion.  From  these  equations  it  is  possible  to  derive  update  equations  for  the  cluster
parameters  and  assignment  variables:  these  are  then  used  very  much  like  in  the
Expectation-Maximization  algorithm.  In  fact,  the  computational  complexity  of  the
BayesianGaussianMixture class is similar to that of the GaussianMixture class (but
generally significantly slower). A simpler approach to maximizing the ELBO is called
black box stochastic variational inference (BBSVI): at each iteration, a few samples are
drawn from q, and they are used to estimate the gradients of the ELBO with regard to
the  variational  parameters  λ,  which  are  then  used  in  a  gradient  ascent  step.  This
approach  makes  it  possible  to  use  Bayesian  inference  with  any  kind  of  model  (pro‐
vided it is differentiable), even deep neural networks; using Bayesian inference with
deep neural networks is called Bayesian Deep Learning.

If  you  want  to  dive  deeper  into  Bayesian  statistics,  check  out  the
book Bayesian Data Analysis by Andrew Gelman et al. (Chapman
& Hall).

Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try
to fit a dataset with different shapes, you may have bad surprises. For example, let’s
see what happens if we use a Bayesian Gaussian mixture model to cluster the moons
dataset (see Figure 9-24).

Figure 9-24. Fitting a Gaussian mixture to nonellipsoidal clusters

Gaussian Mixtures 

| 

273

Oops!  The  algorithm  desperately  searched  for  ellipsoids,  so  it  found  eight  different
clusters  instead  of  two.  The  density  estimation  is  not  too  bad,  so  this  model  could
perhaps be used for anomaly detection, but it failed to identify the two moons. Let’s
now  look  at  a  few  clustering  algorithms  capable  of  dealing  with  arbitrarily  shaped
clusters.

Other Algorithms for Anomaly and Novelty Detection
Scikit-Learn implements other algorithms dedicated to anomaly detection or novelty
detection:

PCA (and other dimensionality reduction techniques with an inverse_transform()
method)

If  you  compare  the  reconstruction  error  of  a  normal  instance  with  the  recon‐
struction error of an anomaly, the latter will usually be much larger. This is a sim‐
ple  and  often  quite  efficient  anomaly  detection  approach  (see  this  chapter’s
exercises for an application of this approach).

Fast-MCD (minimum covariance determinant)

Implemented by the EllipticEnvelope class, this algorithm is useful for outlier
detection, in particular to clean up a dataset. It assumes that the normal instances
(inliers) are generated from a single Gaussian distribution (not a mixture). It also
assumes  that  the  dataset  is  contaminated  with  outliers  that  were  not  generated
from this Gaussian distribution. When the algorithm estimates the parameters of
the Gaussian distribution (i.e., the shape of the elliptic envelope around the inli‐
ers), it is careful to ignore the instances that are most likely outliers. This techni‐
que  gives  a  better  estimation  of  the  elliptic  envelope  and  thus  makes  the
algorithm better at identifying the outliers.

Isolation Forest

This is an efficient algorithm for outlier detection, especially in high-dimensional
datasets. The algorithm builds a Random Forest in which each Decision Tree is
grown randomly: at each node, it picks a feature randomly, then it picks a ran‐
dom  threshold  value  (between  the  min  and  max  values)  to  split  the  dataset  in
two.  The  dataset  gradually  gets  chopped  into  pieces  this  way,  until  all  instances
end  up  isolated  from  the  other  instances.  Anomalies  are  usually  far  from  other
instances, so on average (across all the Decision Trees) they tend to get isolated in
fewer steps than normal instances.

Local Outlier Factor (LOF)

This  algorithm  is  also  good  for  outlier  detection.  It  compares  the  density  of
instances around a given instance to the density around its neighbors. An anom‐
aly is often more isolated than its k nearest neighbors.

274 

| 

Chapter 9: Unsupervised Learning Techniques

One-class SVM

This  algorithm  is  better  suited  for  novelty  detection.  Recall  that  a  kernelized
SVM classifier separates two classes by first (implicitly) mapping all the instances
to a high-dimensional space, then separating the two classes using a linear SVM
classifier within this high-dimensional space (see Chapter 5). Since we just have
one class of instances, the one-class SVM algorithm instead tries to separate the
instances  in  high-dimensional  space  from  the  origin.  In  the  original  space,  this
will correspond to finding a small region that encompasses all the instances. If a
new  instance  does  not  fall  within  this  region,  it  is  an  anomaly.  There  are  a  few
hyperparameters  to  tweak:  the  usual  ones  for  a  kernelized  SVM,  plus  a  margin
hyperparameter that corresponds to the probability of a new instance being mis‐
takenly  considered  as  novel  when  it  is  in  fact  normal.  It  works  great,  especially
with  high-dimensional  datasets,  but  like  all  SVMs  it  does  not  scale  to  large
datasets.

Exercises

1. How would you define clustering? Can you name a few clustering algorithms?

2. What are some of the main applications of clustering algorithms?

3. Describe  two  techniques  to  select  the  right  number  of  clusters  when  using

K-Means.

4. What is label propagation? Why would you implement it, and how?

5. Can  you  name  two  clustering  algorithms  that  can  scale  to  large  datasets?  And

two that look for regions of high density?

6. Can you think of a use case where active learning would be useful? How would

you implement it?

7. What is the difference between anomaly detection and novelty detection?

8. What is a Gaussian mixture? What tasks can you use it for?

9. Can you name two techniques to find the right number of clusters when using a

Gaussian mixture model?

10. The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of
faces.  Each  image  is  flattened  to  a  1D  vector  of  size  4,096.  40  different  people
were  photographed  (10  times  each),  and  the  usual  task  is  to  train  a  model  that
can predict which person is represented in each picture. Load the dataset using
the  sklearn.datasets.fetch_olivetti_faces()  function,  then  split  it  into  a
training set, a validation set, and a test set (note that the dataset is already scaled
between 0 and 1). Since the dataset is quite small, you probably want to use strati‐
fied sampling to ensure that there are the same number of images per person in
each  set.  Next,  cluster  the  images  using  K-Means,  and  ensure  that  you  have  a

Exercises 

| 

275

good number of clusters (using one of the techniques discussed in this chapter).
Visualize the clusters: do you see similar faces in each cluster?

11. Continuing with the Olivetti faces dataset, train a classifier to predict which per‐
son is represented in each picture, and evaluate it on the validation set. Next, use
K-Means as a dimensionality reduction tool, and train a classifier on the reduced
set. Search for the number of clusters that allows the classifier to get the best per‐
formance:  what  performance  can  you  reach?  What  if  you  append  the  features
from the reduced set to the original features (again, searching for the best num‐
ber of clusters)?

12. Train  a  Gaussian  mixture  model  on  the  Olivetti  faces  dataset.  To  speed  up  the
algorithm,  you  should  probably  reduce  the  dataset’s  dimensionality  (e.g.,  use
PCA, preserving 99% of the variance). Use the model to generate some new faces
(using the sample() method), and visualize them (if you used PCA, you will need
to  use  its  inverse_transform()  method).  Try  to  modify  some  images  (e.g.,
rotate, flip, darken) and see if the model can detect the anomalies (i.e., compare
the output of the  score_samples() method for normal images and for anoma‐
lies).

13. Some dimensionality reduction techniques can also be used for anomaly detec‐
tion. For example, take the Olivetti faces dataset and reduce it with PCA, preserv‐
ing 99% of the variance. Then compute the reconstruction error for each image.
Next,  take  some  of  the  modified  images  you  built  in  the  previous  exercise,  and
look  at  their  reconstruction  error:  notice  how  much  larger  the  reconstruction
error is. If you plot a reconstructed image, you will see why: it tries to reconstruct
a normal face.

Solutions to these exercises are available in Appendix A.

276 

| 

Chapter 9: Unsupervised Learning Techniques

PART II
Neural Networks and Deep Learning

CHAPTER 10
Introduction to Artificial Neural Networks
with Keras

Birds  inspired  us  to  fly,  burdock  plants  inspired  Velcro,  and  nature  has  inspired
countless more inventions. It seems only logical, then, to look at the brain’s architec‐
ture  for  inspiration  on  how  to  build  an  intelligent  machine.  This  is  the  logic  that
sparked  artificial  neural  networks  (ANN
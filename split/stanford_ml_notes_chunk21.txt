treat the intermediate computation in the Transformer as a blackbox. We
refer the students to the transformer paper or more advanced courses for more
details. As shown in Figure 14.1, given a document (x1, Â· Â· Â· , xT ), we ï¬rst
translate the sequence of discrete variables into a sequence of corresponding

4To see this, you can verify that the function âˆ’ log p

p+q is decreasing in p, and increasing

in q when p, q > 0.

5In the practical implementations, typically all the data are concatenated into a single
sequence in some order, and each example typically corresponds a sub-sequence of consec-
utive words which may corresponds to a subset of a document or may span across multiple
documents.

6Technically, words may be decomposed into tokens which could be words or sub-words
(combinations of letters), but this note omits this technicality. In fact most commons words
are a single token themselves.

182

word embeddings (ex1, Â· Â· Â· , exT ). We also introduce a ï¬xed special token
x0 = âŠ¥ in the vocabulary with corresponding embedding ex0 to mark the
beginning of a document. Then, the word embeddings are passed into a
Transformer model, which takes in a sequence of vectors (ex0, ex1, Â· Â· Â· , exT )
and outputs a sequence of vectors (u1, u2, Â· Â· Â· , uT +1), where ut âˆˆ RV will be
interpreted as the logits for the probability distribution of the next word.
Here we use the autoregressive version of the Transformers which by design
ensures ut only depends on x1, Â· Â· Â· , xtâˆ’1 (note that this property does not
hold in masked language models [Devlin et al., 2019] where the losses are
also diï¬€erent.) We view the whole mapping from xâ€™s to uâ€™s a blackbox in
this subsection and call it a Transformer, denoted it by fÎ¸, where Î¸ include
both the parameters in the Transformer and the input embeddings. We write
ut = fÎ¸(x0, x1, . . . , xtâˆ’1) where fÎ¸ denotes the mapping from the input to the
outputs.

Figure 14.1: The inputs and outputs of a Transformer model.

The conditional probability p(xt|x1, Â· Â· Â· , xtâˆ’1) is the softtmax of the logits:

ï£®

ï£¯
ï£¯
ï£¯
ï£°

p(xt = 1|x1 Â· Â· Â· , xtâˆ’1)
p(xt = 2|x1 Â· Â· Â· , xtâˆ’1)
...
p(xt = V |x1 Â· Â· Â· , xtâˆ’1)

ï£¹

ï£º
ï£º
ï£º
ï£»

= softmax(ut) âˆˆ RV

(14.6)

= softmax(fÎ¸(x0, . . . , xtâˆ’1))

(14.7)

We train the Transformer parameter Î¸ by minimizing the negative log-
likelihood of seeing the data under the probabilistic model deï¬ned by Î¸,

ð‘¥!ð‘¥"ð‘¥#ð‘’$!ð‘’$"ð‘’$#â€¦Transformer ð‘“%(ð‘¥)ð‘¥&ð‘’$$ð‘¢"ð‘¢â€™ð‘¢#(!ð‘¢!â€¦183

which is the cross-entropy loss on the logitis.

loss(Î¸) =

=

=

1
T

1
T

1
T

T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1

âˆ’ log(pÎ¸(xt|x1, . . . , xtâˆ’1))

(14.8)

(cid:96)ce(fÎ¸(x0, x1, Â· Â· Â· , xtâˆ’1), xt)

âˆ’ log(softmax(fÎ¸(x0, x1, Â· Â· Â· , xtâˆ’1))xt) .

Autoregressive text decoding / generation. Given a autoregressive
Transformer, we can simply sample text from it sequentially. Given a pre-
ï¬x x1, . . . xt, we generate text completion xt+1, . . . xT sequentially using the
conditional distribution.

xt+1 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt))
xt+2 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt+1))

. . .

xT âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xT âˆ’1)) .

(14.9)
(14.10)
(14.11)
(14.12)

Note that each generated token is used as the input to the model when gen-
erating the following tokens. In practice, people often introduce a parameter
Ï„ > 0 named temperature to further adjust the entropy/sharpness of the
generated distribution,

xt+1 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt)/Ï„ )
xt+2 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt+1)/Ï„ )

. . .

xT âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xT âˆ’1)/Ï„ ) .

(14.13)
(14.14)
(14.15)
(14.16)

When Ï„ = 1, the text is sampled from the original conditional probability
deï¬ned by the model. With a decreasing Ï„ , the generated text gradually
becomes more â€œdeterministicâ€. Ï„ â†’ 0 reduces to greedy decoding, where we
generate the most probable next token from the conditional probability.

14.3.1 Zero-shot learning and in-context learning

For language models, there are many ways to adapt a pretrained model to
downstream tasks. In this notes, we discuss three of them: ï¬netuning, zero-
shot learning, and in-context learning.

184

Finetuning is not very common for the autoregressive language models that
we introduced in Section 14.3 but much more common for other variants
such as masked language models which has similar input-output interfaces
but are pretrained diï¬€erently [Devlin et al., 2019]. The ï¬netuning method is
the same as introduced generally in Section 14.1â€”the only question is how
we deï¬ne the prediction task with an additional linear head. One option
is to treat cT +1 = Ï†Î¸(x1, Â· Â· Â· , xT ) as the representation and use w(cid:62)cT +1 =
w(cid:62)Ï†Î¸(x1, Â· Â· Â· , xT ) to predict task label. As described in Section 14.1, we
initialize Î¸ to the pretrained model Ë†Î¸ and then optimize both w and Î¸.

Zero-shot adaptation or zero-shot learning is the setting where there is no
input-output pairs from the downstream tasks. For language problems tasks,
typically the task is formatted as a question or a cloze test form via natural
language. For example, we can format an example as a question:

xtask = (xtask,1, Â· Â· Â· , xtask,T ) = â€œIs the speed of light a universal constant?â€

Then, we compute the most likely next word predicted by the lan-
p(xT +1 |
guage model given this question, that is, computing argmaxxT +1
xtask,1, Â· Â· Â· , xtask,T ). In this case, if the most likely next word xT +1 is â€œNoâ€,
then we solve the task. (The speed of light is only a constant in vacuum).
We note that there are many ways to decode the answer from the language
models, e.g., instead of computing the argmax, we may use the language
model to generate a few words word. It is an active research question to ï¬nd
the best way to utilize the language models.

In-context learning is mostly used for few-shot settings where we have a
few labeled examples (x(1)
task), Â· Â· Â· , (x(ntask)
). Given a test example
xtest, we construct a document (x1, Â· Â· Â· , xT ), which is more commonly called
a â€œpromptâ€ in this context, by concatenating the labeled examples and the
text example in some format. For example, we may construct the prompt as
follows

task, y(1)

, y(ntask)
task

task

x1, Â· Â· Â· , xT = â€œQ: 2 âˆ¼ 3 = ?

A: 5

Q: 6 âˆ¼ 7 = ?

A: 13

Â· Â· Â·

Q: 15 âˆ¼ 2 = ?â€

x(1)
task
y(1)
task
x(2)
task
y(2)
task

xtest

185

Then, we let the pretrained model generate the most likely xT +1, xT +2, Â· Â· Â· .
In this case, if the model can â€œlearnâ€ that the symbol âˆ¼ means addition from
the few examples, we will obtain the following which suggests the answer is
17.

xT +1, xT +2, Â· Â· Â· = â€œA: 17â€.

The area of foundation models is very new and quickly growing. The notes
here only attempt to introduce these models on a conceptual level with a
signiï¬cant amount of simpliï¬cation. We refer the readers to other materials,
e.g., Bommasani et al. [2021], for more details.

Part V

Reinforcement Learning and
Control

186

Chapter 15

Reinforcement learning

We now begin our study of reinforcement learning and adaptive control.

In supervised learning, we saw algorithms that tried to make their outputs
mimic the labels y given in the training set. In that setting, the labels gave
an unambiguous â€œright answerâ€ for each of the inputs x.
In contrast, for
many sequential decision making and control problems, it is very diï¬ƒcult to
provide this type of explicit supervision to a learning algorithm. For example,
if we have just built a four-legged robot and are trying to program it to walk,
then initially we have no idea what the â€œcorrectâ€ actions to take are to make
it walk, and so do not know how to provide explicit supervision for a learning
algorithm to try to mimic.

In the reinforcement learning framework, we will instead provide our al-
gorithms only a reward function, which indicates to the learning agent when
it is doing well, and when it is doing poorly. In the four-legged walking ex-
ample, the reward function might give the robot positive rewards for moving
forwards, and negative rewards for either moving backwards or falling over.
It will then be the learning algorithmâ€™s job to ï¬gure out how to choose actions
over time so as to obtain large rewards.

Reinforcement learning has been successful in applications as diverse as
autonomous helicopter ï¬‚ight, robot legged locomotion, cell-phone network
routing, marketing strategy selection, factory control, and eï¬ƒcient web-page
indexing. Our study of reinforcement learning will begin with a deï¬nition of
the Markov decision processes (MDP), which provides the formalism in
which RL problems are usually posed.

187

188

15.1 Markov decision processes

A Markov decision process is a tuple (S, A, {Psa}, Î³, R), where:

â€¢ S is a set of states. (For example, in autonomous helicopter ï¬‚ight, S
might be the set of all possible positions and orientations of the heli-
copter.)

â€¢ A is a set of actions. (For example, the set of all possible directions in

which you can push the helicopterâ€™s control sticks.)

â€¢ Psa are the state transition probabilities. For each state s âˆˆ S and
action a âˆˆ A, Psa is a distribution over the state space. Weâ€™ll say more
about this later, but brieï¬‚y, Psa gives the distribution over what states
we will transition to if we take action a in state s.

â€¢ Î³ âˆˆ [0, 1) is called the discount factor.

â€¢ R : S Ã— A (cid:55)â†’ R is the reward function. (Rewards are sometimes also
written as a function of a state S only, in which case we would have
R : S (cid:55)â†’ R).

The dynamics of an MDP proceeds as follows: We start in some state s0,
and get to choose some action a0 âˆˆ A to take in the MDP. As a result of our
choice, the state of the MDP randomly transitions to some successor state
s1, drawn according to s1 âˆ¼ Ps0a0. Then, we get to pick another action a1.
As a result of this action, the state transitions again, now to some s2 âˆ¼ Ps1a1.
We then pick a2, and so on. . . . Pictorially, we can represent this process as
follows:

s0

a0âˆ’â†’ s1

a1âˆ’â†’ s2

a2âˆ’â†’ s3

a3âˆ’â†’ . . .

Upon visiting the sequence of states s0, s1, . . . with actions a0, a1, . . ., our

total payoï¬€ is given by

R(s0, a0) + Î³R(s1, a1) + Î³2R(s2, a2) + Â· Â· Â· .

Or, when we are writing rewards as a function of the states only, this becomes

R(s0) + Î³R(s1) + Î³2R(s2) + Â· Â· Â· .

For most of our development, we will use the simpler state-rewards R(s),
though the generalization to state-action rewards R(s, a) oï¬€ers no special
diï¬ƒculties.

189

Our goal in reinforcement learning is to choose actions over time so as to

maximize the expected value of the total payoï¬€:

E (cid:2)R(s0) + Î³R(s1) + Î³2R(s2) + Â· Â· Â· (cid:3)

Note that the reward at timestep t is discounted by a factor of Î³t. Thus, to
make this expectation large, we would like to accrue positive rewards as soon
as possible (and postpone negative rewards as long as possible). In economic
applications where R(Â·) is the amount of money made, Î³ also has a natural
interpretation in terms of the interest rate (where a dollar today is worth
more than a dollar tomorrow).

A policy is any function Ï€ : S (cid:55)â†’ A mapping from the states to the
actions. We say that we are executing some policy Ï€ if, whenever we are
in state s, we take action a = Ï€(s). We also deï¬ne the value function for
a policy Ï€ according to

V Ï€(s) = E (cid:2)R(s0) + Î³R(s1) + Î³2R(s2) + Â· Â· Â· (cid:12)

(cid:12) s0 = s, Ï€].

V Ï€(s) is simply the expected sum of discounted rewards upon starting in
state s, and taking actions according to Ï€.1

Given a ï¬xed policy Ï€, its value function V Ï€ satisï¬es the Bellman equa-

tions:

V Ï€(s) = R(s) + Î³

(cid:88)

PsÏ€(s)(s(cid:48))V Ï€(s(cid:48)).

s(cid:48)âˆˆS
This says that the expected sum of discounted rewards V Ï€(s) for starting
in s consists of two terms: First, the immediate reward R(s) that we get
right away simply for starting in state s, and second, the expected sum of
future discounted rewards. Examining the second term in more detail, we
see that the summation term above can be rewritten Es(cid:48)âˆ¼PsÏ€(s)[V Ï€(s(cid:48))]. This
is the expected sum of discounted rewards for starting in state s(cid:48), where s(cid:48)
is distributed according PsÏ€(s), which is the distribution over where we will
end up after taking the ï¬rst action Ï€(s) in the MDP from state s. Thus, the
second term above gives the expected sum of discounted rewards obtained
after the ï¬rst step in the MDP.

Bellmanâ€™s equations can be used to eï¬ƒciently solve for V Ï€. Speciï¬cally,
in a ï¬nite-state MDP (|S| < âˆž), we can write down one such equation for
V Ï€(s) for every state s. This gives us a set of |S| linear equations in |S|
variables (the unknown V Ï€(s)â€™s, one for each state), which can be eï¬ƒciently
solved for the V Ï€(s)â€™s.

1This notation in which we condition on Ï€ isnâ€™t technically correct because Ï€ isnâ€™t a

random variable, but this is quite standard in the literature.

We also deï¬ne the optimal value function according to

V âˆ—(s) = max

Ï€

V Ï€(s).

190

(15.1)

In other words, this is the best possible expected sum of discounted rewards
that can be attained using any policy. There is also a version of Bellmanâ€™s
equations for the optimal value function:

V âˆ—(s) = R(s) + max
aâˆˆA

Î³

(cid:88)

s(cid:48)âˆˆS

Psa(s(cid:48))V âˆ—(s(cid:48)).

(15.2)

The ï¬rst term above is the immediate reward as before. The second term
is the maximum over all actions a of the expected future sum of discounted
rewards weâ€™ll get upon after action a. You should make sure you understand
this equation and see why it makes sense.

We also deï¬ne a policy Ï€âˆ— : S (cid:55)â†’ A as follows:

Ï€âˆ—(s) = arg max
aâˆˆA

(cid:88)

s(cid:48)âˆˆS

Psa(s(cid:48))V âˆ—(s(cid:48)).

(15.3)

Note that Ï€âˆ—(s) gives the action a that attains the maximum in the â€œmaxâ€
in Equation (15.2).

It is a fact that for every state s and every policy Ï€, we have

V âˆ—(s) = V Ï€âˆ—(s) â‰¥ V Ï€(s).

The ï¬rst equality says that the V Ï€âˆ—, the value function for Ï€âˆ—, is equal to the
optimal value function V âˆ— for every state s. Further, the inequality above
says that Ï€âˆ—â€™s value is at least a large as the value of any other other policy.
In other words, Ï€âˆ— as deï¬ned in Equation (15.3) is the optimal policy.

Note that Ï€âˆ— has the interesting property that it is the optimal policy
for all states s. Speciï¬cally, it is not the case that if we were starting in
some state s then thereâ€™d be some optimal policy for that state, and if we
were starting in some other state s(cid:48) then thereâ€™d be some other policy thatâ€™s
optimal policy for s(cid:48). The same policy Ï€âˆ— attains the maximum in Equa-
tion (15.1) for all states s. This means that we can use the same policy Ï€âˆ—
no matter what the initial state of our MDP is.

15.2 Value iteration and policy iteration

We now describe two eï¬ƒcient algorithms for solving ï¬nite-state MDPs. For
now, we will consider only MDPs with ï¬nite state and action spaces (|S| <

âˆž, |A| < âˆž). In this section, we will also assume that we know the state
transition probabilities {Psa} and the reward function R.
The ï¬rst algorithm, value iteration, is as follows:

191

Algorithm 4 Value Iteration

1: For each state s, initialize V (s) := 0.
2: for until convergence do
3:
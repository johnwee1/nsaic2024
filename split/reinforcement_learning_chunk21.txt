state. Because a state’s value is the expected return,
this average can become a good approximation to the value. In control meth-
ods we are particularly interested in approximating action-value functions,
because these can be used to improve the policy without requiring a model of
the environment’s transition dynamics. Monte Carlo methods intermix policy
evaluation and policy improvement steps on an episode-by-episode basis, and
can be incrementally implemented on an episode-by-episode basis.

Maintaining suﬃcient exploration is an issue in Monte Carlo control meth-
ods. It is not enough just to select the actions currently estimated to be best,
because then no returns will be obtained for alternative actions, and it may
never be learned that they are actually better. One approach is to ignore this
problem by assuming that episodes begin with state–action pairs randomly
selected to cover all possibilities. Such exploring starts can sometimes be ar-
ranged in applications with simulated episodes, but are unlikely in learning
In on-policy methods, the agent commits to always
from real experience.
exploring and tries to ﬁnd the best policy that still explores.
In oﬀ-policy
methods, the agent also explores, but learns a deterministic optimal policy

5.9. SUMMARY

139

that may be unrelated to the policy followed.

Oﬀ-policy Monte Carlo prediction refers to learning the value function of
a target policy from data generated by a diﬀerent behavior policy. Such learn-
ing methods are all based on some form of importance sampling, that is, on
weighting returns by the ratio of the probabilities of taking the observed ac-
tions under the two policies. Ordinary importance sampling uses a simple
average of the weighted returns, whereas weighted importance sampling uses
a weighted average. Ordinary importance sampling produces unbiased esti-
mates, but has larger, possibly inﬁnite, variance, whereas weighted importance
sampling always has ﬁnite variance and are preferred in practice. Despite their
conceptual simplicity, oﬀ-policy Monte Carlo methods for both prediction and
control remain unsettled and a subject of ongoing research.

The Monte Carlo methods treated in this chapter diﬀer from the DP meth-
ods treated in the previous chapter in two major ways. First, they operate on
sample experience, and thus can be used for direct learning without a model.
Second, they do not bootstrap. That is, they do not update their value es-
timates on the basis of other value estimates. These two diﬀerences are not
tightly linked, and can be separated. In the next chapter we consider methods
that learn from experience, like Monte Carlo methods, but also bootstrap, like
DP methods.

Bibliographical and Historical Remarks

The term “Monte Carlo” dates from the 1940s, when physicists at Los Alamos
devised games of chance that they could study to help understand complex
physical phenomena relating to the atom bomb. Coverage of Monte Carlo
methods in this sense can be found in several textbooks (e.g., Kalos and Whit-
lock, 1986; Rubinstein, 1981).

An early use of Monte Carlo methods to estimate action values in a re-
In pole
inforcement learning context was by Michie and Chambers (1968).
balancing (Example 3.4), they used averages of episode durations to assess
the worth (expected balancing “life”) of each possible action in each state,
and then used these assessments to control action selections. Their method is
similar in spirit to Monte Carlo ES with every-visit MC estimates. Narendra
and Wheeler (1986) studied a Monte Carlo method for ergodic ﬁnite Markov
chains that used the return accumulated from one visit to a state to the next
as a reward for adjusting a learning automaton’s action probabilities.

Barto and Duﬀ (1994) discussed policy evaluation in the context of classi-
cal Monte Carlo algorithms for solving systems of linear equations. They used

140

CHAPTER 5. MONTE CARLO METHODS

the analysis of Curtiss (1954) to point out the computational advantages of
Monte Carlo policy evaluation for large problems. Singh and Sutton (1996)
distinguished between every-visit and ﬁrst-visit MC methods and proved re-
sults relating these methods to reinforcement learning algorithms.

The blackjack example is based on an example used by Widrow, Gupta,
and Maitra (1973). The soap bubble example is a classical Dirichlet problem
whose Monte Carlo solution was ﬁrst proposed by Kakutani (1945; see Hersh
and Griego, 1969; Doyle and Snell, 1984). The racetrack exercise is adapted
from Barto, Bradtke, and Singh (1995), and from Gardner (1973).

Monte Carlo ES was introduced in the 1998 edition of this book. That
may have been the ﬁrst explicit connection between Monte Carlo estimation
and control methods based on policy iteration.

Eﬃcient oﬀ-policy learning has become recognized as an important chal-
lenge that arises in several ﬁelds. For example, it is closely related to the idea
of “interventions” and “counterfactuals” in probabalistic graphical (Bayesian)
models (e.g., Pearl, 1995; Balke and Pearl, 1994). Oﬀ-policy methods using
importance sampling have a long history and yet still are not well understood.
Weighted importance sampling, which is also sometimes called normalized im-
portance sampling (e.g., Koller and Friedman, 2009), is discussed by, for ex-
ample, Rubinstein (1981), Hesterberg (1988), Shelton (2001), and Liu (2001).
Combining oﬀ-policy learning with temporal-diﬀerence learning and approxi-
mation methods introduces subtle issues that we consider in later chapters.

The target policy in oﬀ-policy learning is sometimes referred to in the
literature as the “estimation” policy, as it was in the ﬁrst edition of this book.

Our treatment of the idea of importance sampling based on truncated re-
turns is based on the analysis and “forward view” of Sutton, Mahmood, Pre-
cup, and van Hasselt (2014). A related idea is that of per-decision importance
sampling (Precup, Sutton and Singh, 2000).

Exercises

Exercise 5.1 Consider the diagrams on the right in Figure 5.2. Why does
the estimated value function jump up for the last two rows in the rear? Why
does it drop oﬀ for the whole last row on the left? Why are the frontmost
values higher in the upper diagrams than in the lower?

Exercise 5.2 What is the backup diagram for Monte Carlo estimation of qπ?

Exercise 5.3 What is the Monte Carlo estimate analogous to (5.5) for action

5.9. SUMMARY

141

Figure 5.11: A couple of right turns for the racetrack task.

values, given returns generated using µ?

Exercise 5.4 What is the equation analogous to (5.5) for action values Q(s, a)
instead of state values V (s)?

Exercise 5.5 In learning curves such as those shown in Figure 5.7 error gener-
ally decreases with training, as indeed happened for the ordinary importance-
sampling method. But for the weighted importance-sampling method error
ﬁrst increased and then decreased. Why do you think this happened?

Exercise 5.6 The results with Example 5.5 and shown in Figure 5.8 used
a ﬁrst-visit MC method. Suppose that instead an every-visit MC method
was used on the same problem. Would the variance of the estimator still be
inﬁnite? Why or why not?

Exercise 5.7 Modify the algorithm for ﬁrst-visit MC policy evaluation (Fig-
ure 5.1) to use the incremental implementation for sample averages described
in Section 2.4.

Exercise 5.8 Derive the weighted-average update rule (5.7) from (5.6). Fol-
low the pattern of the derivation of the unweighted rule (2.3).

Exercise 5.9: Racetrack (programming) Consider driving a race car
around a turn like those shown in Figure 5.11. You want to go as fast as
possible, but not so fast as to run oﬀ the track. In our simpliﬁed racetrack,
the car is at one of a discrete set of grid positions, the cells in the diagram. The
velocity is also discrete, a number of grid cells moved horizontally and vertically
per time step. The actions are increments to the velocity components. Each
1, or 0 in one step, for a total of nine actions.
may be changed by +1,

−

Starting lineFinishlineStarting lineFinishline142

CHAPTER 5. MONTE CARLO METHODS

−

1 for each step that stays on the track, and

Both velocity components are restricted to be nonnegative and less than 5,
and they cannot both be zero. Each episode begins in one of the randomly
selected start states and ends when the car crosses the ﬁnish line. The rewards
are
5 if the agent tries to drive
oﬀ the track. Actually leaving the track is not allowed, but the position is
always advanced by at least one cell along either the horizontal or vertical
axes. With these restrictions and considering only right turns, such as shown
in the ﬁgure, all episodes are guaranteed to terminate, yet the optimal policy
is unlikely to be excluded. To make the task more challenging, we assume that
on half of the time steps the position is displaced forward or to the right by
one additional cell beyond that speciﬁed by the velocity. Apply a Monte Carlo
control method to this task to compute the optimal policy from each starting
state. Exhibit several trajectories following the optimal policy.

−

∗Exercise 5.10 Modify the algorithm for oﬀ-policy Monte Carlo control (Fig-
ure 5.10) to use the idea of the truncated weighted-average estimator (5.9).
Note that you will ﬁrst need to convert this equation to action values.

Chapter 6

Temporal-Diﬀerence Learning

If one had to identify one idea as central and novel to reinforcement learning,
it would undoubtedly be temporal-diﬀerence (TD) learning. TD learning is
a combination of Monte Carlo ideas and dynamic programming (DP) ideas.
Like Monte Carlo methods, TD methods can learn directly from raw experience
without a model of the environment’s dynamics. Like DP, TD methods update
estimates based in part on other learned estimates, without waiting for a ﬁnal
outcome (they bootstrap). The relationship between TD, DP, and Monte
Carlo methods is a recurring theme in the theory of reinforcement learning.
This chapter is the beginning of our exploration of it. Before we are done,
we will see that these ideas and methods blend into each other and can be
combined in many ways. In particular, in Chapter 7 we introduce the TD(λ)
algorithm, which seamlessly integrates TD and Monte Carlo methods.

As usual, we start by focusing on the policy evaluation or prediction prob-
lem, that of estimating the value function vπ for a given policy π. For the
control problem (ﬁnding an optimal policy), DP, TD, and Monte Carlo meth-
ods all use some variation of generalized policy iteration (GPI). The diﬀerences
in the methods are primarily diﬀerences in their approaches to the prediction
problem.

6.1 TD Prediction

Both TD and Monte Carlo methods use experience to solve the prediction
problem. Given some experience following a policy π, both methods update
their estimate v of vπ for the nonterminal states St occurring in that experience.
Roughly speaking, Monte Carlo methods wait until the return following the
visit is known, then use that return as a target for V (St). A simple every-visit

143

144

CHAPTER 6. TEMPORAL-DIFFERENCE LEARNING

Monte Carlo method suitable for nonstationary environments is

,

(cid:104)

←

V (St)

Gt −

V (St) + α

V (St)
(cid:105)
where Gt is the actual return following time t, and α is a constant step-
size parameter (c.f., Equation 2.4). Let us call this method constant-α MC.
Whereas Monte Carlo methods must wait until the end of the episode to
determine the increment to V (St) (only then is Gt known), TD methods need
wait only until the next time step. At time t+1 they immediately form a target
and make a useful update using the observed reward Rt+1 and the estimate
V (St+1). The simplest TD method, known as TD(0), is

(6.1)

V (St)

←

V (St) + α

Rt+1 + γV (St+1)

V (St)

.

(6.2)

(cid:104)

−

(cid:105)

In eﬀect, the target for the Monte Carlo update is Gt, whereas the target for
the TD update is Rt+1 + γV (St+1).

Because the TD method bases its update in part on an existing estimate,
we say that it is a bootstrapping method, like DP. We know from Chapter 3
that

vπ(s) = Eπ[Gt |
= Eπ
(cid:34)

∞

St = s]

γkRt+k+1

St = s

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
γkRt+k+2
(cid:12)

(cid:88)k=0
Rt+1 + γ

= Eπ
(cid:34)

∞

(cid:88)k=0

= Eπ[Rt+1 + γvπ(St+1)

St = s

(cid:12)
(cid:12)
(cid:12)
St = s] .
(cid:12)
(cid:12)

|

(6.3)

(6.4)

(cid:35)

Roughly speaking, Monte Carlo methods use an estimate of (6.3) as a target,
whereas DP methods use an estimate of (6.4) as a target. The Monte Carlo
target is an estimate because the expected value in (6.3) is not known; a
sample return is used in place of the real expected return. The DP target
is an estimate not because of the expected values, which are assumed to be
completely provided by a model of the environment, but because vπ(St+1) is
not known and the current estimate, V (St+1), is used instead. The TD target
is an estimate for both reasons: it samples the expected values in (6.4) and it
uses the current estimate V instead of the true vπ. Thus, TD methods combine
the sampling of Monte Carlo with the bootstrapping of DP. As we shall see,
with care and imagination this can take us a long way toward obtaining the
advantages of both Monte Carlo and DP methods.

Figure 6.1 speciﬁes TD(0) completely in procedural form, and Figure 6.2
shows its backup diagram. The value estimate for the state node at the top of

6.1. TD PREDICTION

145

Input: the policy π to be evaluated
Initialize V (s) arbitrarily (e.g., V (s) = 0,
Repeat (for each episode):

S+)

s
∀

∈

Initialize S
Repeat (for each step of episode):

←

action given by π for S

A
Take action A; observe reward, R, and next state, S(cid:48)
V (S)
S

R + γV (S(cid:48))

V (S) + α

V (S)

−

←
S(cid:48)
until S is terminal

←

(cid:2)

(cid:3)

Figure 6.1: Tabular TD(0) for estimating vπ.

Figure 6.2: The backup diagram for TD(0).

the backup diagram is updated on the basis of the one sample transition from
it to the immediately following state. We refer to TD and Monte Carlo updates
as sample backups because they involve looking ahead to a sample successor
state (or state–action pair), using the value of the successor and the reward
along the way to compute a backed-up value, and then changing the value
of the original state (or state–action pair) accordingly. Sample backups diﬀer
from the full backups of DP methods in that they are based on a single sample
successor rather than on a complete distribution of all possible successors.

Example 6.1: Driving Home Each day as you drive home from work, you
try to predict how long it will take to get home. When you leave your oﬃce,
you note the time, the day of week, and anything else that might be relevant.
Say on this Friday you are leaving at exactly 6 o’clock, and you estimate that
it will take 30 minutes to get home. As you reach your car it is 6:05, and
you notice it is starting to rain. Traﬃc is often slower in the rain, so you
reestimate that it will take 35 minutes from then, or a total of 40 minutes.
Fifteen minutes later you have completed the highway portion of your journey
in good time. As you exit onto a secondary road you cut your estimate of total
travel time to 35 minutes. Unfortunately, at this point you get stuck behind
a slow truck, and the road is too narrow to pass. You end up having to follow
the truck until you turn onto the side street where you live at 6:40. Three
minutes later you are home. The sequence of states, t
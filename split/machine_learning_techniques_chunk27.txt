modeling the manifold.

8 Sam T. Roweis and Lawrence K. Saul, “Nonlinear Dimensionality Reduction by Locally Linear Embedding,”

Science 290, no. 5500 (2000): 2323–2326.

230 

| 

Chapter 8: Dimensionality Reduction

Figure 8-12. Unrolled Swiss roll using LLE

Here’s  how  LLE  works:  for  each  training  instance  x(i),  the  algorithm  identifies  its  k
closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a lin‐
ear  function  of  these  neighbors.  More  specifically,  it  finds  the  weights  wi,j  such  that
m wi, jx j  is as small as possible, assuming wi,j
the squared distance between x(i) and ∑ j = 1
= 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is the
constrained optimization problem described in Equation 8-4, where W is the weight
matrix  containing  all  the  weights  wi,j.  The  second  constraint  simply  normalizes  the
weights for each training instance x(i).

Equation 8-4. LLE step one: linearly modeling local relationships

W = argmin

W

m
∑
i = 1

2

wi, jx j

m
x i − ∑
j = 1
if x j

subject to

wi, j = 0
m
∑
j = 1

wi, j = 1 for i = 1, 2, ⋯, m

is not one of the k c.n. of x i

After this step, the weight matrix W (containing the weights wi, j) encodes the local
linear  relationships  between  the  training  instances.  The  second  step  is  to  map  the
training  instances  into  a  d-dimensional  space  (where  d  <  n)  while  preserving  these
local relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional

LLE 

| 

231

m wi, jz j  to be as small
space, then we want the squared distance between z(i) and ∑ j = 1
as possible. This idea leads to the unconstrained optimization problem described in
Equation 8-5. It looks very similar to the first step, but instead of keeping the instan‐
ces  fixed  and  finding  the  optimal  weights,  we  are  doing  the  reverse:  keeping  the
weights  fixed  and  finding  the  optimal  position  of  the  instances’  images  in  the  low-
dimensional space. Note that Z is the matrix containing all z(i).

Equation 8-5. LLE step two: reducing dimensionality while preserving relationships

Z = argmin

Z

m
∑
i = 1

m
z i − ∑
j = 1

wi, jz j

2

Scikit-Learn’s  LLE  implementation  has  the  following  computational  complexity:
O(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the
weights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐
nately, the m2 in the last term makes this algorithm scale poorly to very large datasets.

Other Dimensionality Reduction Techniques
There  are  many  other  dimensionality  reduction  techniques,  several  of  which  are
available in Scikit-Learn. Here are some of the most popular ones:

Random Projections

As its name suggests, projects the data to a lower-dimensional space using a ran‐
dom linear projection. This may sound crazy, but it turns out that such a random
projection is actually very likely to preserve distances well, as was demonstrated
mathematically  by  William  B.  Johnson  and  Joram  Lindenstrauss  in  a  famous
lemma.  The  quality  of  the  dimensionality  reduction  depends  on  the  number  of
instances and the target dimensionality, but surprisingly not on the initial dimen‐
sionality.  Check  out  the  documentation  for  the  sklearn.random_projection
package for more details.

Multidimensional Scaling (MDS)

Reduces  dimensionality  while  trying  to  preserve  the  distances  between  the
instances.

232 

| 

Chapter 8: Dimensionality Reduction

Isomap

Creates  a  graph  by  connecting  each  instance  to  its  nearest  neighbors,  then
reduces  dimensionality  while  trying  to  preserve  the  geodesic  distances9  between
the instances.

t-Distributed Stochastic Neighbor Embedding (t-SNE)

Reduces dimensionality while trying to keep similar instances close and dissimi‐
lar  instances  apart.  It  is  mostly  used  for  visualization,  in  particular  to  visualize
clusters  of  instances  in  high-dimensional  space  (e.g.,  to  visualize  the  MNIST
images in 2D).

Linear Discriminant Analysis (LDA)

Is a classification algorithm, but during training it learns the most discriminative
axes between the classes, and these axes can then be used to define a hyperplane
onto which to project the data. The benefit of this approach is that the projection
will  keep  classes  as  far  apart  as  possible,  so  LDA  is  a  good  technique  to  reduce
dimensionality before running another classification algorithm such as an SVM
classifier.

Figure 8-13 shows the results of a few of these techniques.

Figure 8-13. Using various techniques to reduce the Swill roll to 2D

Exercises

1. What are the main motivations for reducing a dataset’s dimensionality? What are

the main drawbacks?

2. What is the curse of dimensionality?

9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between

these nodes.

Exercises 

| 

233

3. Once  a  dataset’s  dimensionality  has  been  reduced,  is  it  possible  to  reverse  the

operation? If so, how? If not, why?

4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?

5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained
variance ratio to 95%. How many dimensions will the resulting dataset have?

6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,

or Kernel PCA?

7. How can you evaluate the performance of a dimensionality reduction algorithm

on your dataset?

8. Does  it  make  any  sense  to  chain  two  different  dimensionality  reduction  algo‐

rithms?

9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set
and  a  test  set  (take  the  first  60,000  instances  for  training,  and  the  remaining
10,000 for testing). Train a Random Forest classifier on the dataset and time how
long it takes, then evaluate the resulting model on the test set. Next, use PCA to
reduce  the  dataset’s  dimensionality,  with  an  explained  variance  ratio  of  95%.
Train a new Random Forest classifier on the reduced dataset and see how long it
takes. Was training much faster? Next, evaluate the classifier on the test set. How
does it compare to the previous classifier?

10. Use  t-SNE  to  reduce  the  MNIST  dataset  down  to  two  dimensions  and  plot  the
result using Matplotlib. You can use a scatterplot using 10 different colors to rep‐
resent  each  image’s  target  class.  Alternatively,  you  can  replace  each  dot  in  the
scatterplot with the corresponding instance’s class (a digit from 0 to 9), or even
plot  scaled-down  versions  of  the  digit  images  themselves  (if  you  plot  all  digits,
the visualization will be too cluttered, so you should either draw a random sam‐
ple  or  plot  an  instance  only  if  no  other  instance  has  already  been  plotted  at  a
close distance). You should get a nice visualization with well-separated clusters of
digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or
MDS and compare the resulting visualizations.

Solutions to these exercises are available in Appendix A.

234 

| 

Chapter 8: Dimensionality Reduction

CHAPTER 9
Unsupervised Learning Techniques

Although  most  of  the  applications  of  Machine  Learning  today  are  based  on  super‐
vised learning (and as a result, this is where most of the investments go to), the vast
majority  of  the  available  data  is  unlabeled:  we  have  the  input  features  X,  but  we  do
not have the labels y. The computer scientist Yann LeCun famously said that “if intel‐
ligence  was  a  cake,  unsupervised  learning  would  be  the  cake,  supervised  learning
would be the icing on the cake, and reinforcement learning would be the cherry on
the cake.” In other words, there is a huge potential in unsupervised learning that we
have only barely started to sink our teeth into.

Say you want to create a system that will take a few pictures of each item on a manu‐
facturing production line and detect which items are defective. You can fairly easily
create  a  system  that  will  take  pictures  automatically,  and  this  might  give  you  thou‐
sands of pictures every day. You can then build a reasonably large dataset in just a few
weeks. But wait, there are no labels! If you want to train a regular binary classifier that
will  predict  whether  an  item  is  defective  or  not,  you  will  need  to  label  every  single
picture  as  “defective”  or  “normal.”  This  will  generally  require  human  experts  to  sit
down  and  manually  go  through  all  the  pictures.  This  is  a  long,  costly,  and  tedious
task, so it will usually only be done on a small subset of the available pictures. As a
result, the labeled dataset will be quite small, and the classifier’s performance will be
disappointing. Moreover, every time the company makes any change to its products,
the whole process will need to be started over from scratch. Wouldn’t it be great if the
algorithm  could  just  exploit  the  unlabeled  data  without  needing  humans  to  label
every picture? Enter unsupervised learning.

In Chapter 8 we looked at the most common unsupervised learning task: dimension‐
ality reduction. In this chapter we will look at a few more unsupervised learning tasks
and algorithms:

235

Clustering

The goal is to group similar instances together into clusters. Clustering is a great
tool  for  data  analysis,  customer  segmentation,  recommender  systems,  search
engines,  image  segmentation,  semi-supervised  learning,  dimensionality  reduc‐
tion, and more.

Anomaly detection

The  objective  is  to  learn  what  “normal”  data  looks  like,  and  then  use  that  to
detect abnormal instances, such as defective items on a production line or a new
trend in a time series.

Density estimation

This is the task of estimating the probability density function (PDF) of the random
process  that  generated  the  dataset.  Density  estimation  is  commonly  used  for
anomaly detection: instances located in very low-density regions are likely to be
anomalies. It is also useful for data analysis and visualization.

Ready  for  some  cake?  We  will  start  with  clustering,  using  K-Means  and  DBSCAN,
and then we will discuss Gaussian mixture models and see how they can be used for
density estimation, clustering, and anomaly detection.

Clustering
As you enjoy a hike in the mountains, you stumble upon a plant you have never seen
before. You look around and you notice a few more. They are not identical, yet they
are sufficiently similar for you to know that they most likely belong to the same spe‐
cies (or at least the same genus). You may need a botanist to tell you what species that
is, but you certainly don’t need an expert to identify groups of similar-looking objects.
This  is  called  clustering:  it  is  the  task  of  identifying  similar  instances  and  assigning
them to clusters, or groups of similar instances.

Just like in classification, each instance gets assigned to a group. However, unlike clas‐
sification,  clustering  is  an  unsupervised  task.  Consider  Figure  9-1:  on  the  left  is  the
iris dataset (introduced in Chapter 4), where each instance’s species (i.e., its class) is
represented  with  a  different  marker.  It  is  a  labeled  dataset,  for  which  classification
algorithms such as Logistic Regression, SVMs, or Random Forest classifiers are well
suited. On the right is the same dataset, but without the labels, so you cannot use a
classification algorithm anymore. This is where clustering algorithms step in: many of
them can easily detect the lower-left cluster. It is also quite easy to see with our own
eyes, but it is not so obvious that the upper-right cluster is composed of two distinct
sub-clusters.  That  said,  the  dataset  has  two  additional  features  (sepal  length  and
width), not represented here, and clustering algorithms can make good use of all fea‐
tures, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mix‐
ture model, only 5 instances out of 150 are assigned to the wrong cluster).

236 

| 

Chapter 9: Unsupervised Learning Techniques

Figure 9-1. Classification (left) versus clustering (right)

Clustering is used in a wide variety of applications, including these:

For customer segmentation

You  can  cluster  your  customers  based  on  their  purchases  and  their  activity  on
your website. This is useful to understand who your customers are and what they
need, so you can adapt your products and marketing campaigns to each segment.
For  example,  customer  segmentation  can  be  useful  in  recommender  systems  to
suggest content that other users in the same cluster enjoyed.

For data analysis

When you analyze a new dataset, it can be helpful to run a clustering algorithm,
and then analyze each cluster separately.

As a dimensionality reduction technique

Once a dataset has been clustered, it is usually possible to measure each instance’s
affinity with each cluster (affinity is any measure of how well an instance fits into
a cluster). Each instance’s feature vector x can then be replaced with the vector of
its cluster affinities. If there are k clusters, then this vector is k-dimensional. This
vector is typically much lower-dimensional than the original feature vector, but it
can preserve enough information for further processing.

For anomaly detection (also called outlier detection)

Any instance that has a low affinity to all the clusters is likely to be an anomaly.
For  example,  if  you  have  clustered  the  users  of  your  website  based  on  their
behavior, you can detect users with unusual behavior, such as an unusual number
of  requests  per  second.  Anomaly  detection  is  particularly  useful  in  detecting
defects in manufacturing, or for fraud detection.

For semi-supervised learning

If  you  only  have  a  few  labels,  you  could  perform  clustering  and  propagate  the
labels to all the instances in the same cluster. This technique can greatly increase

Clustering 

| 

237

the  number  of  labels  available  for  a  subsequent  supervised  learning  algorithm,
and thus improve its performance.

For search engines

Some  search  engines  let  you  search  for  images  that  are  similar  to  a  reference
image. To build such a system, you would first apply a clustering algorithm to all
the  images  in  your  database;  similar  images  would  end  up  in  the  same  cluster.
Then  when  a  user  provides  a  reference  image,  all  you  need  to  do  is  use  the
trained  clustering  model  to  find  this  image’s  cluster,  and  you  can  then  simply
return all the images from this cluster.

To segment an image

By  clustering  pixels  according  to  their  color,  then  replacing  each  pixel’s  color
with the mean color of its cluster, it is possible to considerably reduce the num‐
ber of different colors in the image. Image segmentation is used in many object
detection and tracking systems, as it makes it easier to detect the contour of each
object.

There is no universal definition of what a cluster is: it really depends on the context,
and  different  algorithms  will  capture  different  kinds  of  clusters.  Some  algorithms
look for instances centered around a particular point, called a centroid. Others look
for  continuous  regions  of  densely  pac
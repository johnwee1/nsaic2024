tent variable z are high-dimensional discrete variables, one popular as-
sumption is the mean ï¬eld assumption, which assumes that Qi(z) gives a
distribution with independent coordinates, or in other words, Qi can be de-
composed into Qi(z) = Q1
i (zk). There are tremendous applications
of mean ï¬eld assumptions to learning generative models with discrete latent
variables, and we refer to Blei et al. [2017] for a survey of these models and

i (z1) Â· Â· Â· Qk

162

their impact to a wide range of applications including computational biology,
computational neuroscience, social sciences. We will not get into the details
about the discrete latent variable cases, and our main focus is to deal with
continuous latent variables, which requires not only mean ï¬eld assumptions,
but additional techniques.

When z âˆˆ Rk is a continuous latent variable, there are several decisions to
make towards successfully optimizing (11.20). First we need to give a succinct
representation of the distribution Qi because it is over an inï¬nite number of
points. A natural choice is to assume Qi is a Gaussian distribution with some
mean and variance. We would also like to have more succinct representation
of the means of Qi of all the examples. Note that Qi(z(i)) is supposed to
approximate p(z(i)|x(i); Î¸). It would make sense let all the means of the Qiâ€™s
be some function of x(i). Concretely, let q(Â·; Ï†), v(Â·; Ï†) be two functions that
map from dimension d to k, which are parameterized by Ï† and Ïˆ, we assume
that

Qi = N (q(x(i); Ï†), diag(v(x(i); Ïˆ))2)

(11.21)

Here diag(w) means the k Ã— k matrix with the entries of w âˆˆ Rk on the
diagonal. In other words, the distribution Qi is assumed to be a Gaussian
distribution with independent coordinates, and the mean and standard de-
viations are governed by q and v. Often in variational auto-encoder, q and v
are chosen to be neural networks.8 In recent deep learning literature, often
q, v are called encoder (in the sense of encoding the data into latent code),
whereas g(z; Î¸) if often referred to as the decoder.

We remark that Qi of such form in many cases are very far from a good ap-
proximation of the true posterior distribution. However, some approximation
is necessary for feasible optimization. In fact, the form of Qi needs to satisfy
other requirements (which happened to be satisï¬ed by the form (11.21))

Before optimizing the ELBO, letâ€™s ï¬rst verify whether we can eï¬ƒciently
evaluate the value of the ELBO for ï¬xed Q of the form (11.21) and Î¸. We
rewrite the ELBO as a function of Ï†, Ïˆ, Î¸ by

ELBO(Ï†, Ïˆ, Î¸) =

n
(cid:88)

i=1

(cid:20)

Ez(i)âˆ¼Qi

log

p(x(i), z(i); Î¸)
Qi(z(i))

(cid:21)

,

(11.22)

where Qi = N (q(x(i); Ï†), diag(v(x(i); Ïˆ))2)

Note that to evaluate Qi(z(i)) inside the expectation, we should be able to
compute the density of Qi. To estimate the expectation Ez(i)âˆ¼Qi, we

8q and v can also share parameters. We sweep this level of details under the rug in this

note.

163

should be able to sample from distribution Qi so that we can build an
empirical estimator with samples. It happens that for Gaussian distribution
Qi = N (q(x(i); Ï†), diag(v(x(i); Ïˆ))2), we are able to be both eï¬ƒciently.

Now letâ€™s optimize the ELBO. It turns out that we can run gradient ascent
over Ï†, Ïˆ, Î¸ instead of alternating maximization. There is no strong need to
compute the maximum over each variable at a much greater cost. (For Gaus-
sian mixture model in Section 11.4, computing the maximum is analytically
feasible and relatively cheap, and therefore we did alternating maximization.)
Mathematically, let Î· be the learning rate, the gradient ascent step is

Î¸ := Î¸ + Î·âˆ‡Î¸ELBO(Ï†, Ïˆ, Î¸)
Ï† := Ï† + Î·âˆ‡Ï†ELBO(Ï†, Ïˆ, Î¸)
Ïˆ := Ïˆ + Î·âˆ‡ÏˆELBO(Ï†, Ïˆ, Î¸)

Computing the gradient over Î¸ is simple because

âˆ‡Î¸ELBO(Ï†, Ïˆ, Î¸) = âˆ‡Î¸

= âˆ‡Î¸

(cid:20)

Ez(i)âˆ¼Qi

log

(cid:21)

p(x(i), z(i); Î¸)
Qi(z(i))

Ez(i)âˆ¼Qi

(cid:2)log p(x(i), z(i); Î¸)(cid:3)

n
(cid:88)

i=1
n
(cid:88)

i=1

=

n
(cid:88)

i=1

Ez(i)âˆ¼Qi

(cid:2)âˆ‡Î¸ log p(x(i), z(i); Î¸)(cid:3) ,

(11.23)

But computing the gradient over Ï† and Ïˆ is tricky because the sam-
pling distribution Qi depends on Ï† and Ïˆ.
(Abstractly speaking, the is-
sue we face can be simpliï¬ed as the problem of computing the gradi-
ent Ezâˆ¼QÏ†[f (Ï†)] with respect to variable Ï†. We know that in general,
âˆ‡Ezâˆ¼QÏ†[f (Ï†)] (cid:54)= Ezâˆ¼QÏ†[âˆ‡f (Ï†)] because the dependency of QÏ† on Ï† has to be
taken into account as well. )

The idea that comes to rescue is the so-called re-parameterization
trick: we rewrite z(i) âˆ¼ Qi = N (q(x(i); Ï†), diag(v(x(i); Ïˆ))2) in an equivalent
way:

z(i) = q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i) where Î¾(i) âˆ¼ N (0, IkÃ—k)

(11.24)

Here x (cid:12) y denotes the entry-wise product of two vectors of the same
dimension. Here we used the fact that x âˆ¼ N (Âµ, Ïƒ2) is equivalent to that
x = Âµ+Î¾Ïƒ with Î¾ âˆ¼ N (0, 1). We mostly just used this fact in every dimension
simultaneously for the random variable z(i) âˆ¼ Qi.

164

(11.25)

With this re-parameterization, we have that

(cid:20)

Ez(i)âˆ¼Qi

log

(cid:21)

p(x(i), z(i); Î¸)
Qi(z(i))

(cid:20)

= EÎ¾(i)âˆ¼N (0,1)

log

p(x(i), q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i); Î¸)
Qi(q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i))

(cid:21)

It follows that

(cid:20)

âˆ‡Ï†Ez(i)âˆ¼Qi

log

(cid:21)

p(x(i), z(i); Î¸)
Qi(z(i))

= âˆ‡Ï†EÎ¾(i)âˆ¼N (0,1)

log

(cid:20)

(cid:20)

= EÎ¾(i)âˆ¼N (0,1)

âˆ‡Ï† log

p(x(i), q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i); Î¸)
Qi(q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i))
p(x(i), q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i); Î¸)
Qi(q(x(i); Ï†) + v(x(i); Ïˆ) (cid:12) Î¾(i))

(cid:21)

(cid:21)

We can now sample multiple copies of Î¾(i)â€™s to estimate the the expecta-
tion in the RHS of the equation above.9 We can estimate the gradient with
respect to Ïˆ similarly, and with these, we can implement the gradient ascent
algorithm to optimize the ELBO over Ï†, Ïˆ, Î¸.

There are not many high-dimensional distributions with analytically com-
putable density function are known to be re-parameterizable. We refer to
Kingma and Welling [2013] for a few other choices that can replace Gaussian
distribution.

9Empirically people sometimes just use one sample to estimate it for maximum com-

putational eï¬ƒciency.

Chapter 12

Principal components analysis

In this set of notes, we will develop a method, Principal Components Analysis
(PCA), that tries to identify the subspace in which the data approximately
lies. PCA is computationally eï¬ƒcient:
it will require only an eigenvector
calculation (easily done with the eig function in Matlab).

Suppose we are given a dataset {x(i); i = 1, . . . , n} of attributes of n dif-
ferent types of automobiles, such as their maximum speed, turn radius, and
so on. Let x(i) âˆˆ Rd for each i (d (cid:28) n). But unknown to us, two diï¬€erent
attributesâ€”some xi and xjâ€”respectively give a carâ€™s maximum speed mea-
sured in miles per hour, and the maximum speed measured in kilometers per
hour. These two attributes are therefore almost linearly dependent, up to
only small diï¬€erences introduced by rounding oï¬€ to the nearest mph or kph.
Thus, the data really lies approximately on an n âˆ’ 1 dimensional subspace.
How can we automatically detect, and perhaps remove, this redundancy?

For a less contrived example, consider a dataset resulting from a survey of
pilots for radio-controlled helicopters, where x(i)
is a measure of the piloting
1
skill of pilot i, and x(i)
captures how much he/she enjoys ï¬‚ying. Because
2
RC helicopters are very diï¬ƒcult to ï¬‚y, only the most committed students,
ones that truly enjoy ï¬‚ying, become good pilots. So, the two attributes
Indeed, we might posit that that the
x1 and x2 are strongly correlated.
data actually likes along some diagonal axis (the u1 direction) capturing the
intrinsic piloting â€œkarmaâ€ of a person, with only a small amount of noise
lying oï¬€ this axis. (See ï¬gure.) How can we automatically compute this u1
direction?

165

166

We will shortly develop the PCA algorithm. But prior to running PCA
per se, typically we ï¬rst preprocess the data by normalizing each feature
to have mean 0 and variance 1. We do this by subtracting the mean and
dividing by the empirical standard deviation:

x(i)
j â†

x(i)
j âˆ’ Âµj
Ïƒj

i=1 x(i)
where Âµj = 1
n
feature j, respectively.

(cid:80)n

j and Ïƒ2

j = 1
n

(cid:80)n

i=1(x(i)

j âˆ’ Âµj)2 are the mean variance of

Subtracting Âµj zeros out the mean and may be omitted for data known
to have zero mean (for instance, time series corresponding to speech or other
acoustic signals). Dividing by the standard deviation Ïƒj rescales each coor-
dinate to have unit variance, which ensures that diï¬€erent attributes are all
treated on the same â€œscale.â€ For instance, if x1 was carsâ€™ maximum speed in
mph (taking values in the high tens or low hundreds) and x2 were the num-
ber of seats (taking values around 2-4), then this renormalization rescales
the diï¬€erent attributes to make them more comparable. This rescaling may
be omitted if we had a priori knowledge that the diï¬€erent attributes are all
on the same scale. One example of this is if each data point represented a
grayscale image, and each x(i)
took a value in {0, 1, . . . , 255} corresponding
j
to the intensity value of pixel j in image i.

Now, having normalized our data, how do we compute the â€œmajor axis
of variationâ€ uâ€”that is, the direction on which the data approximately lies?
One way is to pose this problem as ï¬nding the unit vector u so that when

x1x2(enjoyment)(skill)1uu2167

the data is projected onto the direction corresponding to u, the variance of
the projected data is maximized. Intuitively, the data starts oï¬€ with some
amount of variance/information in it. We would like to choose a direction u
so that if we were to approximate the data as lying in the direction/subspace
corresponding to u, as much as possible of this variance is still retained.

Consider the following dataset, on which we have already carried out the

normalization steps:

Now, suppose we pick u to correspond the the direction shown in the
ï¬gure below. The circles denote the projections of the original data onto this
line.

168

We see that the projected data still has a fairly large variance, and the
points tend to be far from zero. In contrast, suppose had instead picked the
following direction:

Here, the projections have a signiï¬cantly smaller variance, and are much

closer to the origin.

We would like to automatically select the direction u corresponding to
the ï¬rst of the two ï¬gures shown above. To formalize this, note that given a

(cid:0)(cid:0)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0
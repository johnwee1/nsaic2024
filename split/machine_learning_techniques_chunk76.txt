  second  phase  of  training.  Finally,  we  build  a  big  sandwich
using all these autoencoders, as shown in Figure 17-7 (i.e., we first stack the hidden
layers of each autoencoder, then the output layers in reverse order). This gives us the
final stacked autoencoder (see the “Training One Autoencoder at a Time” section in
the notebook for an implementation). We could easily train more autoencoders this
way, building a very deep stacked autoencoder.

578 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

As we discussed earlier, one of the triggers of the current tsunami of interest in Deep
Learning  was  the  discovery  in  2006  by  Geoffrey  Hinton  et  al.  that  deep  neural  net‐
works  can  be  pretrained  in  an  unsupervised  fashion,  using  this  greedy  layerwise
approach. They used restricted Boltzmann machines (RBMs; see Appendix E) for this
purpose, but in 2007 Yoshua Bengio et al. showed3 that autoencoders worked just as
well. For several years this was the only efficient way to train deep nets, until many of
the techniques introduced in Chapter 11 made it possible to just train a deep net in
one shot.

Autoencoders  are  not  limited  to  dense  networks:  you  can  also  build  convolutional
autoencoders, or even recurrent autoencoders. Let’s look at these now.

Convolutional Autoencoders
If  you  are  dealing  with  images,  then  the  autoencoders  we  have  seen  so  far  will  not
work well (unless the images are very small): as we saw in Chapter 14, convolutional
neural networks are far better suited than dense networks to work with images. So if
you  want  to  build  an  autoencoder  for  images  (e.g.,  for  unsupervised  pretraining  or
dimensionality  reduction),  you  will  need  to  build  a  convolutional  autoencoder.4  The
encoder  is  a  regular  CNN  composed  of  convolutional  layers  and  pooling  layers.  It
typically reduces the spatial dimensionality of the inputs (i.e., height and width) while
increasing  the  depth  (i.e.,  the  number  of  feature  maps).  The  decoder  must  do  the
reverse (upscale the image and reduce its depth back to the original dimensions), and
for this you can use transpose convolutional layers (alternatively, you could combine
upsampling layers with convolutional layers). Here is a simple convolutional autoen‐
coder for Fashion MNIST:

conv_encoder = keras.models.Sequential([
    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),
    keras.layers.Conv2D(16, kernel_size=3, padding="same", activation="selu"),
    keras.layers.MaxPool2D(pool_size=2),
    keras.layers.Conv2D(32, kernel_size=3, padding="same", activation="selu"),
    keras.layers.MaxPool2D(pool_size=2),
    keras.layers.Conv2D(64, kernel_size=3, padding="same", activation="selu"),
    keras.layers.MaxPool2D(pool_size=2)
])
conv_decoder = keras.models.Sequential([
    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding="valid",
                                 activation="selu",
                                 input_shape=[3, 3, 64]),

3 Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks,” Proceedings of the 19th International

Conference on Neural Information Processing Systems (2006): 153–160.

4 Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,” Proceed‐

ings of the 21st International Conference on Artificial Neural Networks 1 (2011): 52–59.

Convolutional Autoencoders 

| 

579

    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding="same",
                                 activation="selu"),
    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding="same",
                                 activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])

Recurrent Autoencoders
If you want to build an autoencoder for sequences, such as time series or text (e.g., for
unsupervised learning or dimensionality reduction), then recurrent neural networks
(see  Chapter  15)  may  be  better  suited  than  dense  networks.  Building  a  recurrent
autoencoder  is  straightforward:  the  encoder  is  typically  a  sequence-to-vector  RNN
which  compresses  the  input  sequence  down  to  a  single  vector.  The  decoder  is  a
vector-to-sequence RNN that does the reverse:

recurrent_encoder = keras.models.Sequential([
    keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 28]),
    keras.layers.LSTM(30)
])
recurrent_decoder = keras.models.Sequential([
    keras.layers.RepeatVector(28, input_shape=[30]),
    keras.layers.LSTM(100, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(28, activation="sigmoid"))
])
recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])

This recurrent autoencoder can process sequences of any length, with 28 dimensions
per  time  step.  Conveniently,  this  means  it  can  process  Fashion  MNIST  images  by
treating each image as a sequence of rows: at each time step, the RNN will process a
single  row  of  28  pixels.  Obviously,  you  could  use  a  recurrent  autoencoder  for  any
kind  of  sequence.  Note  that  we  use  a  RepeatVector  layer  as  the  first  layer  of  the
decoder, to ensure that its input vector gets fed to the decoder at each time step.

OK,  let’s  step  back  for  a  second.  So  far  we  have  seen  various  kinds  of  autoencoders
(basic,  stacked,  convolutional,  and  recurrent),  and  we  have  looked  at  how  to  train
them (either in one shot or layer by layer). We also looked at a couple applications:
data visualization and unsupervised pretraining.

Up  to  now,  in  order  to  force  the  autoencoder  to  learn  interesting  features,  we  have
limited  the  size  of  the  coding  layer,  making  it  undercomplete.  There  are  actually
many other kinds of constraints that can be used, including ones that allow the cod‐
ing layer to be just as large as the inputs, or even larger, resulting in an overcomplete
autoencoder. Let’s look at some of those approaches now.

580 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

Denoising Autoencoders
Another  way  to  force  the  autoencoder  to  learn  useful  features  is  to  add  noise  to  its
inputs, training it to recover the original, noise-free inputs. This idea has been around
since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008
paper,5 Pascal Vincent et al. showed that autoencoders could also be used for feature
extraction. In a 2010 paper,6 Vincent et al. introduced stacked denoising autoencoders.

The  noise  can  be  pure  Gaussian  noise  added  to  the  inputs,  or  it  can  be  randomly
switched-off  inputs,  just  like  in  dropout  (introduced  in  Chapter  11).  Figure  17-8
shows both options.

Figure 17-8. Denoising autoencoders, with Gaussian noise (left) or dropout (right)

The  implementation  is  straightforward:  it  is  a  regular  stacked  autoencoder  with  an
additional  Dropout  layer  applied  to  the  encoder’s  inputs  (or  you  could  use  a  Gaus
sianNoise layer instead). Recall that the Dropout layer is only active during training
(and so is the GaussianNoise layer):

5 Pascal Vincent et al., “Extracting and Composing Robust Features with Denoising Autoencoders,” Proceedings

of the 25th International Conference on Machine Learning (2008): 1096–1103.

6 Pascal Vincent et al., “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network

with a Local Denoising Criterion,” Journal of Machine Learning Research 11 (2010): 3371–3408.

Denoising Autoencoders 

| 

581

dropout_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(30, activation="selu")
])
dropout_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[30]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])

Figure  17-9  shows  a  few  noisy  images  (with  half  the  pixels  turned  off),  and  the
images  reconstructed  by  the  dropout-based  denoising  autoencoder.  Notice  how  the
autoencoder guesses details that are actually not in the input, such as the top of the
white  shirt  (bottom  row,  fourth  image).  As  you  can  see,  not  only  can  denoising
autoencoders  be  used  for  data  visualization  or  unsupervised  pretraining,  like  the
other autoencoders we’ve discussed so far, but they can also be used quite simply and
efficiently to remove noise from images.

Figure 17-9. Noisy images (top) and their reconstructions (bottom)

Sparse Autoencoders
Another kind of constraint that often leads to good feature extraction is sparsity: by
adding an appropriate term to the cost function, the autoencoder is pushed to reduce
the number of active neurons in the coding layer. For example, it may be pushed to
have on average only 5% significantly active neurons in the coding layer. This forces
the autoencoder to represent each input as a combination of a small number of acti‐
vations. As a result, each neuron in the coding layer typically ends up representing a
useful feature (if you could speak only a few words per month, you would probably
try to make them worth listening to).

A  simple  approach  is  to  use  the  sigmoid  activation  function  in  the  coding  layer  (to
constrain the codings to values between 0 and 1), use a large coding layer (e.g., with

582 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

300  units),  and  add  some  ℓ1  regularization  to  the  coding  layer’s  activations  (the
decoder is just a regular decoder):

sparse_l1_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(300, activation="sigmoid"),
    keras.layers.ActivityRegularization(l1=1e-3)
])
sparse_l1_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[300]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])

This ActivityRegularization layer just returns its inputs, but as a side effect it adds
a training loss equal to the sum of absolute values of its inputs (this layer only has an
effect during training). Equivalently, you could remove the ActivityRegularization
layer and set activity_regularizer=keras.regularizers.l1(1e-3) in the previous
layer. This penalty will encourage the neural network to produce codings close to 0,
but since it will also be penalized if it does not reconstruct the inputs correctly, it will
have  to  output  at  least  a  few  nonzero  values.  Using  the  ℓ1  norm  rather  than  the  ℓ2
norm  will  push  the  neural  network  to  preserve  the  most  important  codings  while
eliminating the ones that are not needed for the input image (rather than just reduc‐
ing all codings).

Another approach, which often yields better results, is to measure the actual sparsity
of the coding layer at each training iteration, and penalize the model when the meas‐
ured sparsity differs from a target sparsity. We do so by computing the average activa‐
tion of each neuron in the coding layer, over the whole training batch. The batch size
must not be too small, or else the mean will not be accurate.

Once we have the mean activation per neuron, we want to penalize the neurons that
are too active, or not active enough, by adding a sparsity loss to the cost function. For
example, if we measure that a neuron has an average activation of 0.3, but the target
sparsity  is  0.1,  it  must  be  penalized  to  activate  less.  One  approach  could  be  simply
adding  the  squared  error  (0.3  –  0.1)2  to  the  cost  function,  but  in  practice  a  better
approach is to use the Kullback–Leibler (KL) divergence (briefly discussed in Chap‐
ter 4), which has much stronger gradients than the mean squared error, as you can
see in Figure 17-10.

Sparse Autoencoders 

| 

583

Figure 17-10. Sparsity loss

Given  two  discrete  probability  distributions  P  and  Q,  the  KL  divergence  between
these distributions, noted DKL(P ∥ Q), can be computed using Equation 17-1.

Equation 17-1. Kullback–Leibler divergence

DKL P ∥ Q = ∑
i

P i

log

P i
Q i

In our case, we want to measure the divergence between the target probability p that a
neuron  in  the  coding  layer  will  activate  and  the  actual  probability  q  (i.e.,  the  mean
activation over the training batch). So the KL divergence simplifies to Equation 17-2.

Equation 17-2. KL divergence between the target sparsity p and the actual sparsity q

DKL p ∥ q = p log

p
q

+ 1 − p log

1 − p
1 − q

Once we have computed the sparsity loss for each neuron in the coding layer, we sum
up these losses and add the result to the cost function. In order to control the relative
importance of the sparsity loss and the reconstruction loss, we can multiply the spar‐
sity loss by a sparsity weight hyperparameter. If this weight is too high, the model will
stick  closely  to  the  target  sparsity,  but  it  may  not  reconstruct  the  inputs  properly,
making  the  model  useless.  Conversely,  if  it  is  too  low,  the  model  will  mostly  ignore
the sparsity objective and will not learn any interesting features.

584 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

We now have all we need to implement a sparse autoencoder based on the KL diver‐
gence. First, let’s create a custom regularizer to apply KL divergence regularization:

K = keras.backend
kl_divergence = keras.losses.kullback_leibler_divergence

class KLDivergenceRegularizer(keras.regularizers.Regularizer):
    def __init__(self, weight, target=0.1):
        self.weight = weight
        self.target = target
    def __call__(self, inputs):
        mean_activities = K.mean(inputs, axis=0)
        return self.weight * (
            kl_divergence(self.target, mean_activities) +
            kl_divergence(1. - self.target, 1. - mean_activities))

Now  we  can  build  the  sparse  autoencoder,  using  the  KLDivergenceRegularizer  for
the coding layer’s activations:

kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)
sparse_kl_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(300, activation="sigmoid", activity_regularizer=kld_reg)
])
sparse_kl_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[300]),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])
sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])

After training this sparse autoencoder on Fashion MNIST, the activations of the neu‐
rons in the coding layer are mostly close to 0 (about 70% of all activations are lower
than 0.1), and all neurons have a mean activation around 0.1 (about 90% of all neu‐
rons have a mean activation between 0.1 and 0.2), as shown in Figure 17-11.

Figure 17-11. Distributi
est but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note

The algorithm is based on series of rewrite rules run in series: the output of each
pass is fed as input to the next pass. Here are some sample rules (more details can
be found at https://tartarus.org/martin/PorterStemmer/):

ATIONAL

ING

SSES

→

→

→

ATE (e.g., relational
(cid:15)

→

relate)

if the stem contains a vowel (e.g., motoring

SS (e.g., grasses

grass)

→

motor)

→

Simple stemmers can be useful in cases where we need to collapse across differ-
ent variants of the same lemma. Nonetheless, they do tend to commit errors of both
over- and under-generalizing, as shown in the table below (Krovetz, 1993):

Errors of Commission
organization organ
doing
numerical
policy

doe
numerous
police

Errors of Omission
European Europe
analysis
analyzes
noise
noisy
sparse
sparsity

2.7 Sentence Segmentation

2.7

• SENTENCE SEGMENTATION

25

sentence
segmentation

Sentence segmentation is another important step in text processing. The most use-
ful cues for segmenting a text into sentences are punctuation, like periods, question
marks, and exclamation points. Question marks and exclamation points are rela-
tively unambiguous markers of sentence boundaries. Periods, on the other hand, are
more ambiguous. The period character “.” is ambiguous between a sentence bound-
ary marker and a marker of abbreviations like Mr. or Inc. The previous sentence that
you just read showed an even more complex case of this ambiguity, in which the ﬁnal
period of Inc. marked both an abbreviation and the sentence boundary marker. For
this reason, sentence tokenization and word tokenization may be addressed jointly.
In general, sentence tokenization methods work by ﬁrst deciding (based on rules
or machine learning) whether a period is part of the word or is a sentence-boundary
marker. An abbreviation dictionary can help determine whether the period is part
of a commonly used abbreviation; the dictionaries can be hand-built or machine-
learned (Kiss and Strunk, 2006), as can the ﬁnal sentence splitter. In the Stanford
CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based,
a deterministic consequence of tokenization; a sentence ends when a sentence-ending
punctuation (., !, or ?) is not already grouped with other characters into a token (such
as for an abbreviation or number), optionally followed by additional ﬁnal quotes or
brackets.

2.8 Minimum Edit Distance

Much of natural language processing is concerned with measuring how similar two
strings are. For example in spelling correction, the user typed some erroneous
string—let’s say graffe–and we want to know what the user meant. The user prob-
ably intended a word that is similar to graffe. Among candidate similar words,
the word giraffe, which differs by only one letter from graffe, seems intuitively
to be more similar than, say grail or graf, which differ in more letters. Another
example comes from coreference, the task of deciding whether two strings such as
the following refer to the same entity:

Stanford President Marc Tessier-Lavigne
Stanford University President Marc Tessier-Lavigne

Again, the fact that these two strings are very similar (differing by only one word)
seems like useful evidence for deciding that they might be coreferent.

Edit distance gives us a way to quantify both of these intuitions about string sim-
ilarity. More formally, the minimum edit distance between two strings is deﬁned
as the minimum number of editing operations (operations like insertion, deletion,
substitution) needed to transform one string into another.

The gap between intention and execution, for example, is 5 (delete an i, substi-
tute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see
this by looking at the most important visualization for string distances, an alignment
between the two strings, shown in Fig. 2.14. Given two sequences, an alignment is
a correspondence between substrings of the two sequences. Thus, we say I aligns
with the empty string, N with E, and so on. Beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the

minimum edit
distance

alignment

26 CHAPTER 2

• REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE

top string into the bottom string: d for deletion, s for substitution, i for insertion.

I N T E * N T I O N

| | | | | | | | | |
* E X E C U T I O N
i s
d s s

Figure 2.14 Representing the minimum edit distance between two strings as an alignment.
The ﬁnal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.

We can also assign a particular cost or weight to each of these operations. The
Levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume
that the substitution of a letter for itself, for example, t for t, has zero cost. The Lev-
enshtein distance between intention and execution is 5. Levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (This is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). Using this version, the Levenshtein distance between
intention and execution is 8.

2.8.1 The Minimum Edit Distance Algorithm

How do we ﬁnd the minimum edit distance? We can think of this as a search task, in
which we are searching for the shortest path—a sequence of edits—from one string
to another.

dynamic
programming

Figure 2.15 Finding the edit distance viewed as a search problem

The space of all possible edits is enormous, so we can’t search naively. However,
lots of distinct edit paths will end up in the same state (string), so rather than recom-
puting all those paths, we could just remember the shortest path to a state each time
we saw it. We can do this by using dynamic programming. Dynamic program-
ming is the name for a class of algorithms, ﬁrst introduced by Bellman (1957), that
apply a table-driven method to solve problems by combining solutions to subprob-
lems. Some of the most commonly used algorithms in natural language processing
make use of dynamic programming, such as the Viterbi algorithm (Chapter 8) and
the CKY algorithm for parsing (Chapter 17).

The intuition of a dynamic programming problem is that a large problem can
be solved by properly combining the solutions to various subproblems. Consider
the shortest path of transformed words that represents the minimum edit distance
between the strings intention and execution shown in Fig. 2.16.

Imagine some string (perhaps it is exention) that is in this optimal path (whatever
it is). The intuition of dynamic programming is that if exention is in the optimal

n t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o n2.8

• MINIMUM EDIT DISTANCE

27

minimum edit
distance
algorithm

Figure 2.16 Path from intention to execution.

operation list, then the optimal sequence must also include the optimal path from
intention to exention. Why? If there were a shorter path from intention to exention,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn’t be optimal, thus leading to a contradiction.

The minimum edit distance algorithm was named by Wagner and Fischer
(1974) but independently discovered by many people (see the Historical Notes sec-
tion of Chapter 8).

Let’s ﬁrst deﬁne the minimum edit distance between two strings. Given two
strings, the source string X of length n, and target string Y of length m, we’ll deﬁne
D[i, j] as the edit distance between X[1..i] and Y [1.. j], i.e., the ﬁrst i characters of X
and the ﬁrst j characters of Y . The edit distance between X and Y is thus D[n, m].

We’ll use dynamic programming to compute D[n, m] bottom up, combining so-
lutions to subproblems. In the base case, with a source substring of length i but an
empty target string, going from i characters to 0 requires i deletes. With a target
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. Having computed D[i, j] for small i, j we then compute larger
D[i, j] based on previously computed smaller values. The value of D[i, j] is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:

D[i, j] = min

D[i
−
D[i, j
D[i

−




−
1, j

1, j] + del-cost(source[i])
1] + ins-cost(target[ j])

1] + sub-cost(source[i], target[ j])

−

(2.8)

If we assume the version of Levenshtein distance in which the insertions and dele-
) = 1), and substitutions have a
) = del-cost(
tions each have a cost of 1 (ins-cost(
·
·
cost of 2 (except substitution of identical letters have zero cost), the computation for
D[i, j] becomes:



D[i, j] = min 


D[i
−
D[i, j

D[i

−

1, j] + 1
1] + 1

−
1, j

2;
0;

if source[i]
= target[ j]
if source[i] = target[ j]

1] +

−

(cid:26)

(2.9)

The algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying
the algorithm to the distance between intention and execution with the version of
Levenshtein in Eq. 2.9.



Alignment Knowing the minimum edit distance is useful for algorithms like ﬁnd-
ing potential spelling error corrections. But the edit distance algorithm is important
in another way; with a small change, it can also provide the minimum cost align-
ment between two strings. Aligning two strings is useful throughout speech and

n t e n t i o ni n t e n t i o ne t e n t i o ne x e n t i o ne x e n u t i o ne x e c u t i o ndelete isubstitute n by esubstitute t by xinsert usubstitute n by c(cid:54)
28 CHAPTER 2

• REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE

function MIN-EDIT-DISTANCE(source, target) returns min-distance

LENGTH(source)
LENGTH(target)

n
m
Create a distance matrix D[n+1,m+1]

←
←

# Initialization: the zeroth row and column is the distance from the empty string
D[0,0] = 0
for each row i from 1 to n do

←

D[i,0]

D[i-1,0] + del-cost(source[i])

for each column j from 1 to m do

D[0,j]

D[0, j-1] + ins-cost(target[j])

←
# Recurrence relation:
for each row i from 1 to n do

D[i, j]

for each column j from 1 to m do
MIN( D[i
−
D[i
−
D[i, j

←

1, j] + del-cost(source[i]),
1, j

1] + sub-cost(source[i], target[j]),

−
1] + ins-cost(target[j]))

−

# Termination
return D[n,m]

Figure 2.17 The minimum edit distance algorithm, an example of the class of dynamic
x, ins-cost(x) = 1)
programming algorithms. The various costs can either be ﬁxed (e.g.,
or can be speciﬁc to the letter (to model the fact that some letters are more likely to be in-
serted than others). We assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x, x) = 0).

∀

Src
\

#
0
1
2
3
4
5
6
7
8
9

e
1
2
3
4
3
4
5
6
7
8

Tar
#
i
n
t
e
n
t
i
o
n

c
4
5
6
7
6
7
8
9
10
11
Figure 2.18 Computation of minimum edit distance between intention and execution with
the algorithm of Fig. 2.17, using Levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.

e
3
4
5
6
5
6
7
8
9
10

u
5
6
7
8
7
8
9
10
11
12

n
9
8
7
8
9
10
11
10
9
8

i
7
6
7
8
9
10
9
8
9
10

t
6
7
8
7
8
9
8
9
10
11

o
8
7
8
9
10
11
10
9
8
9

x
2
3
4
5
4
5
6
7
8
9

language processing. In speech recognition, minimum edit distance alignment is
used to compute the word error rate (Chapter 16). Alignment plays a role in ma-
chine translation, in which sentences in a parallel corpus (a corpus with a text in two
languages) need to be matched to each other.

To extend the edit distance algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the edit distance matrix. Figure 2.19
shows this path with boldfaced cells. Each boldfaced cell represents an alignment
of a pair of letters in the two strings. If two boldfaced cells occur in the same row,
there will be an insertion in going from the source to the target; two boldfaced cells

2.9

• SUMMARY

29

in the same column indicate a deletion.

Figure 2.19 also shows the intuition of how to compute this alignment path. The
computation proceeds in two steps. In the ﬁrst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. The backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. In the second step, we perform a backtrace. In a backtrace, we start
from the last cell (at the ﬁnal row and column), and follow the pointers back through
the dynamic programming matrix. Each complete path between the ﬁnal cell and the
initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the
minimum edit distance algorithm to store the pointers and compute the backtrace to
output an alignment.

backtrace

↑

↑

(cid:45)

←

←

←

←

←

←

←

←

←

←

←

(cid:45)

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

c
4
5
6
7
← 6
7
8
9
10
11

#
i
n ↑
t
↑
e
↑
n ↑
t
↑
i
↑
o ↑
n ↑

e
3
4
5
6
(cid:45)← 5
6
(cid:45)←↑
7
8
9
10

#
0
←
↑ 1 (cid:45)←↑
2
3
4
5
6
7
8
9

e
1
2
(cid:45)←↑
(cid:45)←↑ 3 (cid:45)←↑
4
(cid:45)←↑
3
4
5
6
7
8

x
2
3
4
(cid:45)←↑
(cid:45)←↑ 5 (cid:45)←↑
4
5
6
7
8
9

n
9
8
7
8
9
↑
10
11
10
9
(cid:45) 8
Figure 2.19 When entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. After the table is full we compute an alignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
following the arrows back. The sequence of bold cells represents one possible minimum cost
alignment between the two strings. Diagram design after Gusﬁeld (1997).

u
5
6
7
8
7
←↑
(cid:45)←↑ 8 (cid:45)←↑
9
(cid:45)←↑
10
11
12

o
8
7
8
9
10
11
10
9
←
←
(cid:45) 8 ←
9
↑

i
7
6
7
8
9
10
9
←
(cid:45) 8
9
↑
10

t
6
7
8
7
8
9
(cid:45) 8
9
↑
10
11

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

(cid:45)←↑

←↑

←↑

(cid:45)↑

←

(cid:45)

(cid:45)

←

↑

↑

↑

↑

↑

↑

↑

↑

While we worked our example with simple Levenshtein distance, the algorithm
in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. The Viterbi algorithm is a probabilistic extension of
minimum edit distance. Instead of computing the “minimum edit distance” between
two strings, Viterbi computes the “maximum probability alignment” of one string
with another. We’ll discuss this more in Chapter 8.

2.9 Summary

This chapter introduced a fundamental tool in language processing, the regular ex-
pression, and showed how to perform basic text normalization tasks including
word segmentation and normalization, sentence segmentation, and stemming.
We also introduced the important minimum edit distan
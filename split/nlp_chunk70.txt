1.2 Projectivity

projective

The notion of projectivity imposes an additional constraint that is derived from the
order of the words in the input. An arc from a head to a dependent is said to be
projective if there is a path from the head to every word that lies between the head
and the dependent in the sentence. A dependency tree is then said to be projective if
all the arcs that make it up are projective. All the dependency trees we’ve seen thus
far have been projective. There are, however, many valid constructions which lead
to non-projective trees, particularly in languages with relatively ﬂexible word order.

Consider the following example.

root

obl

nsubj

obj

det

acl:relcl

det

nsubj

adv

cop

(18.3)

JetBlue canceled our

ﬂight this morning which was already late

In this example, the arc from ﬂight to its modiﬁer late is non-projective since there
is no path from ﬂight to the intervening words this and morning. As we can see from
this diagram, projectivity (and non-projectivity) can be detected in the way we’ve
been drawing our trees. A dependency tree is projective if it can be drawn with
no crossing edges. Here there is no way to link ﬂight to its dependent late without
crossing the arc that links morning to its head.

18.1

• DEPENDENCY RELATIONS

395

Our concern with projectivity arises from two related issues. First, the most
widely used English dependency treebanks were automatically derived from phrase-
structure treebanks through the use of head-ﬁnding rules. The trees generated in such
a fashion will always be projective, and hence will be incorrect when non-projective
examples like this one are encountered.

Second, there are computational limitations to the most widely used families of
parsing algorithms. The transition-based approaches discussed in Section 18.2 can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. This limitation is one of the motivations for
the more ﬂexible graph-based parsing approach described in Section 18.3.

18.1.3 Dependency Treebanks

Treebanks play a critical role in the development and evaluation of dependency
parsers. They are used for training parsers, they act as the gold labels for evaluating
parsers, and they also provide useful information for corpus linguistics studies.

Dependency treebanks are created by having human annotators directly generate
dependency structures for a given corpus, or by hand-correcting the output of an
automatic parser. A few early treebanks were also based on using a deterministic
process to translate existing constituent-based treebanks into dependency trees.

The largest open community project for building dependency trees is the Univer-
sal Dependencies project at https://universaldependencies.org/ introduced
above, which currently has almost 200 dependency treebanks in more than 100 lan-
guages (de Marneffe et al., 2021). Here are a few UD examples showing dependency
trees for sentences in Spanish, Basque, and Mandarin Chinese:

punct

obl:tmod

obl

case

det

case

det

VERB
Subiremos

a
we-will-board on

ADP DET NOUN ADP DET NUM PUNCT
a
at

cinco
ﬁve

tren
train

las
the

el
the

.
.

[Spanish] Subiremos al tren a las cinco. “We will be boarding the train at ﬁve.”(18.4)

nsubj

punct

obj

aux

NOUN
Ekaitzak

NOUN

itsasontzia hondoratu

VERB AUX PUNCT
du
has

sunk

.
.

storm (Erg.) ship (Abs.)

[Basque] Ekaitzak itsasontzia hondoratu du. “The storm has sunk the ship.”(18.5)

396 CHAPTER 18

• DEPENDENCY PARSING

adv

nsubj

obj:tmod

obj

advmod

compound:vv

ADV PRON NOUN
昨天

ADV
才
yesterday only-then receive

VERB
收

但 我
I
but

VERB NOUN

到
arrive

信
letter

.

[Chinese] 但我昨天才收到信 “But I didn’t receive the letter until yesterday”(18.6)

18.2 Transition-Based Dependency Parsing

transition-based

Our ﬁrst approach to dependency parsing is called transition-based parsing. This
architecture draws on shift-reduce parsing, a paradigm originally developed for
In transition-based
analyzing programming languages (Aho and Ullman, 1972).
parsing we’ll have a stack on which we build the parse, a buffer of tokens to be
parsed, and a parser which takes actions on the parse via a predictor called an oracle,
as illustrated in Fig. 18.4.

Figure 18.4 Basic transition-based parser. The parser examines the top two elements of the
stack and selects an action by consulting an oracle that examines the current conﬁguration.

The parser walks through the sentence left-to-right, successively shifting items
from the buffer onto the stack. At each time point we examine the top two elements
on the stack, and the oracle makes a decision about what transition to apply to build
the parse. The possible transitions correspond to the intuitive actions one might take
in creating a dependency tree by examining the words in a single pass over the input
from left to right (Covington, 2001):

• Assign the current word as the head of some previously seen word,
• Assign some previously seen word as the head of the current word,
• Postpone dealing with the current word, storing it for later processing.

We’ll formalize this intuition with the following three transition operators that

will operate on the top two elements of the stack:

• LEFTARC: Assert a head-dependent relation between the word at the top of

the stack and the second word; remove the second word from the stack.

• RIGHTARC: Assert a head-dependent relation between the second word on

the stack and the word at the top; remove the top word from the stack;

wnw1w2s2...s1snParserInput buﬀerStackOracleLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w218.2

• TRANSITION-BASED DEPENDENCY PARSING

397

• SHIFT: Remove the word from the front of the input buffer and push it onto

the stack.

We’ll sometimes call operations like LEFTARC and RIGHTARC reduce operations,
based on a metaphor from shift-reduce parsing, in which reducing means combin-
ing elements on the stack. There are some preconditions for using operators. The
LEFTARC operator cannot be applied when ROOT is the second element of the stack
(since by deﬁnition the ROOT node cannot have any incoming arcs). And both the
LEFTARC and RIGHTARC operators require two elements to be on the stack to be
applied.

This particular set of operators implements what is known as the arc standard
approach to transition-based parsing (Covington 2001, Nivre 2003). In arc standard
parsing the transition operators only assert relations between elements at the top of
the stack, and once an element has been assigned its head it is removed from the
stack and is not available for further processing. As we’ll see, there are alterna-
tive transition systems which demonstrate different parsing behaviors, but the arc
standard approach is quite effective and is simple to implement.

The speciﬁcation of a transition-based parser is quite simple, based on repre-
senting the current state of the parse as a conﬁguration: the stack, an input buffer
of words or tokens, and a set of relations representing a dependency tree. Parsing
means making a sequence of transitions through the space of possible conﬁgura-
tions. We start with an initial conﬁguration in which the stack contains the ROOT
node, the buffer has the tokens in the sentence, and an empty set of relations repre-
sents the parse. In the ﬁnal goal state, the stack and the word list should be empty,
and the set of relations will represent the ﬁnal parse. Fig. 18.5 gives the algorithm.

function DEPENDENCYPARSE(words) returns dependency tree

arc standard

conﬁguration

[root], [words], []

← {

state
while state not ﬁnal
ORACLE(state)

; initial conﬁguration

}

; choose a transition operator to apply

APPLY(t, state) ; apply it, creating a new state

t
←
state
return state

←

Figure 18.5 A generic transition-based dependency parser

At each step, the parser consults an oracle (we’ll come back to this shortly) that
provides the correct transition operator to use given the current conﬁguration. It then
applies that operator to the current conﬁguration, producing a new conﬁguration.
The process ends when all the words in the sentence have been consumed and the
ROOT node is the only element remaining on the stack.

The efﬁciency of transition-based parsers should be apparent from the algorithm.
The complexity is linear in the length of the sentence since it is based on a single
left to right pass through the words in the sentence. (Each word must ﬁrst be shifted
onto the stack and then later reduced.)

Note that unlike the dynamic programming and search-based approaches dis-
cussed in Chapter 17, this approach is a straightforward greedy algorithm—the or-
acle provides a single choice at each step and the parser proceeds with that choice,
no other options are explored, no backtracking is employed, and a single parse is
returned in the end.

Figure 18.6 illustrates the operation of the parser with the sequence of transitions

398 CHAPTER 18

• DEPENDENCY PARSING

leading to a parse for the following example.

root

obj

det

iobj

compound

Book me the morning

ﬂight

(18.7)

Let’s consider the state of the conﬁguration at Step 2, after the word me has been

pushed onto the stack.

Stack

Word List

Relations

[root, book, me] [the, morning, ﬂight]

The correct operator to apply here is RIGHTARC which assigns book as the head of
me and pops me from the stack resulting in the following conﬁguration.

Stack

Word List

Relations

[root, book] [the, morning, ﬂight] (book

me)

→

After several subsequent applications of the SHIFT and LEFTARC operators, the con-
ﬁguration in Step 6 looks like the following:

Stack
[root, book, the, morning, ﬂight]

Word List
[]

Relations

(book

me)

→

Here, all the remaining words have been passed onto the stack and all that is left
to do is to apply the appropriate reduce operators. In the current conﬁguration, we
employ the LEFTARC operator resulting in the following state.

Stack
[root, book, the, ﬂight]

Word List
[]

Relations

(book
(morning

me)
ﬂight)

→
←

At this point, the parse for this sentence consists of the following structure.

iobj

compound

Book me the morning ﬂight

(18.8)

There are several important things to note when examining sequences such as
the one in Figure 18.6. First, the sequence given is not the only one that might lead
to a reasonable parse. In general, there may be more than one path that leads to the
same result, and due to ambiguity, there may be other transition sequences that lead
to different equally valid parses.

Second, we are assuming that the oracle always provides the correct operator
at each point in the parse—an assumption that is unlikely to be true in practice.
As a result, given the greedy nature of this algorithm, incorrect choices will lead to
incorrect parses since the parser has no opportunity to go back and pursue alternative
choices. Section 18.2.4 will introduce several techniques that allow transition-based
approaches to explore the search space more fully.

18.2

• TRANSITION-BASED DEPENDENCY PARSING

399

Stack Word List
[root]
[root, book]
[root, book, me]
[root, book]
[root, book, the]
[root, book, the, morning]
[root, book, the, morning, ﬂight]
[root, book, the, ﬂight]
[root, book, ﬂight]
[root, book]
[root]

Step
0
1
2
3
4
5
6
7
8
9
10
Figure 18.6 Trace of a transition-based parse.

[book, me, the, morning, ﬂight]
[me, the, morning, ﬂight]
[the, morning, ﬂight]
[the, morning, ﬂight]
[morning, ﬂight]
[ﬂight]
[]
[]
[]
[]
[]

Action
SHIFT

SHIFT
RIGHTARC
SHIFT

SHIFT

SHIFT
LEFTARC
LEFTARC
RIGHTARC
RIGHTARC
Done

Relation Added

(book

me)

→

ﬂight)

(morning
(the
(book
(root

←
→
→

←
ﬂight)
ﬂight)
book)

Finally, for simplicity, we have illustrated this example without the labels on
the dependency relations. To produce labeled trees, we can parameterize the LEFT-
ARC and RIGHTARC operators with dependency labels, as in LEFTARC(NSUBJ) or
RIGHTARC(OBJ). This is equivalent to expanding the set of transition operators from
our original set of three to a set that includes LEFTARC and RIGHTARC operators for
each relation in the set of dependency relations being used, plus an additional one
for the SHIFT operator. This, of course, makes the job of the oracle more difﬁcult
since it now has a much larger set of operators from which to choose.

18.2.1 Creating an Oracle

The oracle for greedily selecting the appropriate transition is trained by supervised
machine learning. As with all supervised machine learning methods, we will need
training data: conﬁgurations annotated with the correct transition to take. We can
draw these from dependency trees. And we need to extract features of the con-
ﬁguration. We’ll introduce neural classiﬁers that represent the conﬁguration via
embeddings, as well as classic systems that use hand-designed features.

Generating Training Data

The oracle from the algorithm in Fig. 18.5 takes as input a conﬁguration and returns a
transition operator. Therefore, to train a classiﬁer, we will need conﬁgurations paired
with transition operators (i.e., LEFTARC, RIGHTARC, or SHIFT). Unfortunately,
treebanks pair entire sentences with their corresponding trees, not conﬁgurations
with transitions.

To generate the required training data, we employ the oracle-based parsing algo-
rithm in a clever way. We supply our oracle with the training sentences to be parsed
along with their corresponding reference parses from the treebank. To produce train-
ing instances, we then simulate the operation of the parser by running the algorithm
and relying on a new training oracle to give us correct transition operators for each
successive conﬁguration.

To see how this works, let’s ﬁrst review the operation of our parser. It begins with
a default initial conﬁguration where the stack contains the ROOT, the input list is just
the list of words, and the set of relations is empty. The LEFTARC and RIGHTARC
operators each add relations between the words at the top of the stack to the set of
relations being accumulated for a given sentence. Since we have a gold-standard
reference parse for each training sentence, we know which dependency relations are
valid for a given sentence. Therefore, we can use the reference parse to guide the

training oracle

400 CHAPTER 18

• DEPENDENCY PARSING

Predicted Action
SHIFT

Stack
[root]
[root, book]
[root, book, the]
[root, book, the, ﬂight]
[root, book, ﬂight]
[root, book, ﬂight, through]
[root, book, ﬂight, through, houston]
[root, book, ﬂight, houston ]
[root, book, ﬂight]
[root, book]
[root]

Step
0
1
2
3
4
5
6
7
8
9
10
Figure 18.7 Generating training items consisting of conﬁguration/predicted action pairs by simulating a parse
with a given reference parse.

Word List
[book, the, ﬂight, through, houston]
[the, ﬂight, through, houston]
[ﬂight, through, houston]
[through, houston]
[through, houston]
[houston]
[]
[]
[]
[]
[]

SHIFT
LEFTARC
RIGHTARC
RIGHTARC
RIGHTARC
Done

SHIFT
LEFTARC
SHIFT

SHIFT

selection of operators as the parser steps through a sequence of conﬁgurations.

To be more precise, given a reference parse and a conﬁguration, the training

oracle proceeds as follows:

• Choose LEFTARC if it produces a correct head-dependent relation given the

reference parse and the current conﬁguration,

• Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent re-
lation given the reference parse and (2) all of the dependents of the word at
the top of the stack have already been assigned,

• Otherwise
possible to specify the factors in such a way that Z does not exist.
This happens if some of the variables in the model are continuous and the integral
4

A distribution deï¬?ned by normalizing a product of clique potentials is also called a Gibbs
distribution.
568

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

of pÌƒ over their domain diverges. For example, suppose we want to model a single
scalar variable x âˆˆ R with a single clique potential Ï†(x) = x2 . In this case,
î?š
Z = x 2dx.
(16.6)
Since this integral diverges, there is no probability distribution corresponding to
this choice of Ï†(x). Sometimes the choice of some parameter of the Ï† functions
determines whether
î€€
î€?the probability distribution is deï¬?ned. For example, for
2
Ï†(x; Î² ) = exp âˆ’Î²x , the Î² parameter determines whether Z exists. Positive Î²
results in a Gaussian distribution over x but all other values of Î² make Ï† impossible
to normalize.
One key diï¬€erence between directed modeling and undirected modeling is that
directed models are deï¬?ned directly in terms of probability distributions from
the start, while undirected models are deï¬?ned more loosely by Ï† functions that
are then converted into probability distributions. This changes the intuitions one
must develop in order to work with these models. One key idea to keep in mind
while working with undirected models is that the domain of each of the variables
has dramatic eï¬€ect on the kind of probability distribution that a given set of Ï†
functions corresponds to. For example, consider an n-dimensional vector-valued
random variable x and an undirected model parametrized by a vector of biases
b. Suppose we have one clique for each element of x, Ï†(i)(xi ) = exp(b ixi). What
kind of probability distribution does this result in? The answer is that we do
not have enough information, because we have not yet speciï¬?ed the domain of x.
If x âˆˆ Rn, then the integral deï¬?ning Z diverges and no probability distribution
exists. If x âˆˆ {0, 1} n, then p(x) factorizes into n independent distributions, with
p(xi = 1) = sigmoid (bi ). If the domain of x is the set of elementary basis vectors
({[1, 0, . . . , 0], [0 , 1, . . . , 0], . . . , [0, 0, . . . ,1]} ) then p(x) = softmax(b), so a large
value of bi actually reduces p(x j = 1) for j î€¶ = i. Often, it is possible to leverage
the eï¬€ect of a carefully chosen domain of a variable in order to obtain complicated
behavior from a relatively simple set of Ï† functions. We will explore a practical
application of this idea later, in section 20.6.

16.2.4

Energy-Based Models

Many interesting theoretical results about undirected models depend on the assumption that âˆ€x, pÌƒ(x) > 0. A convenient way to enforce this condition is to use
an energy-based model (EBM) where
pÌƒ(x) = exp(âˆ’E (x))
569

(16.7)

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a

b

c

d

e

f

Figure 16.4:
This graph implies that p(a, b, c, d, e, f) can be written as
1
Ï†
(a
)Ï†
(b
)Ï†a,d (a, d)Ï† b,e(b, e) Ï†e,f(e, f) for an appropriate choice of the Ï† func,
b
,
c
b,c
Z a,b
tions.

and E(x) is known as the energy function. Because exp(z ) is positive for all
z, this guarantees that no energy function will result in a probability of zero
for any state x. Being completely free to choose the energy function makes
learning simpler. If we learned the clique potentials directly, we would need to use
constrained optimization to arbitrarily impose some speciï¬?c minimal probability
value. By learning the energy function, we can use unconstrained optimization.5
The probabilities in an energy-based model can approach arbitrarily close to zero
but never reach it.
Any distribution of the form given by equation 16.7 is an example of a Boltzmann distribution. For this reason, many energy-based models are called
Boltzmann machines (Fahlman et al., 1983; Ackley et al., 1985; Hinton et al.,
1984; Hinton and Sejnowski, 1986). There is no accepted guideline for when to call
a model an energy-based model and when to call it a Boltzmann machine. The
term Boltzmann machine was ï¬?rst introduced to describe a model with exclusively
binary variables, but today many models such as the mean-covariance restricted
Boltzmann machine incorporate real-valued variables as well. While Boltzmann
machines were originally deï¬?ned to encompass both models with and without latent variables, the term Boltzmann machine is today most often used to designate
models with latent variables, while Boltzmann machines without latent variables
are more often called Markov random ï¬?elds or log-linear models.
Cliques in an undirected graph correspond to factors of the unnormalized
probability function. Because exp(a) exp(b) = exp( a+ b), this means that diï¬€erent
cliques in the undirected graph correspond to the diï¬€erent terms of the energy
function. In other words, an energy-based model is just a special kind of Markov
network: the exponentiation makes each term in the energy function correspond
to a factor for a diï¬€erent clique. See ï¬?gure 16.5 for an example of how to read the
5

For some models, we may still need to use constrained optimization to make sure Z exists.
570

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a

b

c

d

e

f

Figure 16.5: This graph implies that E(a, b, c, d, e, f ) can be written as Ea,b (a, b) +
Eb,c(b, c) + Ea,d (a, d) + Eb,e (b, e) + E e,f(e, f) for an appropriate choice of the per-clique
energy functions. Note that we can obtain the Ï† functions in ï¬?gure 16.4 by setting each Ï†
to the exponential of the corresponding negative energy, e.g.,Ï† a,b(a, b) = exp (âˆ’E (a, b)).

form of the energy function from an undirected graph structure. One can view an
energy-based model with multiple terms in its energy function as being a product
of experts (Hinton, 1999). Each term in the energy function corresponds to
another factor in the probability distribution. Each term of the energy function can
be thought of as an â€œexpertâ€? that determines whether a particular soft constraint
is satisï¬?ed. Each expert may enforce only one constraint that concerns only
a low-dimensional projection of the random variables, but when combined by
multiplication of probabilities, the experts together enforce a complicated highdimensional constraint.
One part of the deï¬?nition of an energy-based model serves no functional purpose
from a machine learning point of view: the âˆ’ sign in equation 16.7. This âˆ’ sign
could be incorporated into the deï¬?nition of E. For many choices of the function
E, the learning algorithm is free to determine the sign of the energy anyway. The
âˆ’ sign is present primarily to preserve compatibility between the machine learning
literature and the physics literature. Many advances in probabilistic modeling
were originally developed by statistical physicists, for whom E refers to actual,
physical energy and does not have arbitrary sign. Terminology such as â€œenergyâ€?
and â€œpartition functionâ€? remains associated with these techniques, even though
their mathematical applicability is broader than the physics context in which they
were developed. Some machine learning researchers (e.g., Smolensky (1986), who
referred to negative energy as harmony) have chosen to emit the negation, but
this is not the standard convention.
Many algorithms that operate on probabilistic models do not need to compute
pmodel (x) but only log pÌƒmodel(x). For energy-based models with latent variables h,
these algorithms are sometimes phrased in terms of the negative of this quantity,

571

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a

s

b

a

(a)

s

b

(b)

Figure 16.6: (a) The path between random variablea and random variable b through s is
active, because s is not observed. This means that a and b are not separated. (b) Here s
is shaded in, to indicate that it is observed. Because the only path between a and b is
through s , and that path is inactive, we can conclude that a and b are separated given s.

called the free energy:
F (x) = âˆ’ log

î?˜
h

exp (âˆ’E (x, h)) .

(16.8)

In this book, we usually prefer the more general log pÌƒmodel(x) formulation.

16.2.5

Separation and D-Separation

The edges in a graphical model tell us which variables directly interact. We often
need to know which variables indirectly interact. Some of these indirect interactions
can be enabled or disabled by observing other variables. More formally, we would
like to know which subsets of variables are conditionally independent from each
other, given the values of other subsets of variables.
Identifying the conditional independences in a graph is very simple in the case
of undirected models. In this case, conditional independence implied by the graph
is called separation . We say that a set of variables A is separated from another
set of variables B given a third set of variables S if the graph structure implies that
A is independent from B given S. If two variables a and b are connected by a path
involving only unobserved variables, then those variables are not separated. If no
path exists between them, or all paths contain an observed variable, then they are
separated. We refer to paths involving only unobserved variables as â€œactiveâ€? and
paths including an observed variable as â€œinactive.â€?
When we draw a graph, we can indicate observed variables by shading them in.
See ï¬?gure 16.6 for a depiction of how active and inactive paths in an undirected
model look when drawn in this way. See ï¬?gure 16.7 for an example of reading
separation from an undirected graph.
Similar concepts apply to directed models, except that in the context of
directed models, these concepts are referred to as d-separation. The â€œdâ€? stands
for â€œdependence.â€? D-separation for directed graphs is deï¬?ned the same as separation
572

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a
b

c

d

Figure 16.7: An example of reading separation properties from an undirected graph. Here
b is shaded to indicate that it is observed. Because observingb blocks the only path from
a to c, we say that a and c are separated from each other given b . The observation of b
also blocks one path between a and d, but there is a second, active path between them.
Therefore, a and d are not separated given b.

for undirected graphs: We say that a set of variables A is d-separated from another
set of variables B given a third set of variables S if the graph structure implies
that A is independent from B given S.
As with undirected models, we can examine the independences implied by the
graph by looking at what active paths exist in the graph. As before, two variables
are dependent if there is an active path between them, and d-separated if no such
path exists. In directed nets, determining whether a path is active is somewhat
more complicated. See ï¬?gure 16.8 for a guide to identifying active paths in a
directed model. See ï¬?gure 16.9 for an example of reading some properties from a
graph.
It is important to remember that separation and d-separation tell us only
about those conditional independences that are implied by the graph. There is no
requirement that the graph imply all independences that are present. In particular,
it is always legitimate to use the complete graph (the graph with all possible edges)
to represent any distribution. In fact, some distributions contain independences
that are not possible to represent with existing graphical notation. Contextspeciï¬?c independences are independences that are present dependent on the
value of some variables in the network. For example, consider a model of three
binary variables: a, b and c. Suppose that when a is 0, b and c are independent,
but when a is 1, b is deterministically equal to c. Encoding the behavior when
a = 1 requires an edge connecting b and c. The graph then fails to indicate that b
and c are independent when a = 0.
In general, a graph will never imply that an independence exists when it does
not. However, a graph may fail to encode an independence.

573

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a

s

b
a

a

s

b

b

(b)

(a)
a

s

b

a

s

s

c

(c)

(d)

b

Figure 16.8: All of the kinds of active paths of length two that can exist between random
variables a and b . (a)Any path with arrows proceeding directly froma to b or vice versa.
This kind of path becomes blocked if s is observed. We have already seen this kind of
path in the relay race example. (b)a and b are connected by a common cause s. For
example, suppose s is a variable indicating whether or not there is a hurricane and a and
b measure the wind speed at two diï¬€erent nearby weather monitoring outposts. If we
observe very high winds at station a , we might expect to also see high winds at b. This
kind of path can be blocked by observing s. If we already know there is a hurricane, we
expect to see high winds at b, regardless of what is observed at a. A lower than expected
wind at a (for a hurricane) would not change our expectation of winds at b (knowing
there is a hurricane). However, if s is not observed, then a and b are dependent, i.e., the
path is active. (c)a and b are both parents of s . This is called a V-structure or the
collider case. The V-structure causes a and b to be related by the explaining away
eï¬€ect. In this case, the path is actually active when s is observed. For example, suppose
s is a variable indicating that your colleague is not at work. The variable a represents
her being sick, while b represents her being on vacation. If you observe that she is not
at work, you can presume she is probably sick or on vacation, but it is not especially
likely that both have happened at the same time. If you ï¬?nd out that she is on vacation,
this fact is suï¬ƒcient to explain her absence. You can infer that she is probably not also
sick. (d)The explaining away eï¬€ect happens even if any descendant of s is observed! For
example, suppose that c is a variable representing whether you have received a report
from your colleague. If you notice that you have not received the report, this increases
your estimate of the probability that she is not at work today, which in turn makes it
more likely that she is either sick or on vacation. The only way to block a path through a
V-structure is to observe none of the descendants of the shared child.
574

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a

b

c

d

e

Figure 16.9: From this graph, we can read out several d-separation properties. Examples
include:
â€¢ a and b are d-separated given the empty set.
â€¢ a and e are d-separated given c.

â€¢ d and e are d-separated given c.

We can also see that some variables are no longer d-separated when we observe some
variables:
â€¢ a and b are not d-separated given c.

â€¢ a and b are not d-separated given d.

575

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

16.2.6

Converting between Undirected and Directed Graphs

We often refer to a speciï¬?c machine learning model as being undirected or directed.
For example, we typically refer to RBMs as undirected and sparse coding as directed.
This choice of wording can be somewhat misleading, because no probabilistic model
is inherently directed or undirected. Instead, some models are most easily described
using a directed graph, or most easily described using an undirected graph.
Directed models and undirected models both have the
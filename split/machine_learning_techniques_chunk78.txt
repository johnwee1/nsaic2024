s.

Let’s go ahead and build a simple GAN for Fashion MNIST.

First, we need to build the generator and the discriminator. The generator is similar
to  an  autoencoder’s  decoder,  and  the  discriminator  is  a  regular  binary  classifier  (it
takes  an  image  as  input  and  ends  with  a  Dense  layer  containing  a  single  unit  and
using the sigmoid activation function). For the second phase of each training itera‐
tion,  we  also  need  the  full  GAN  model  containing  the  generator  followed  by  the
discriminator:

codings_size = 30

generator = keras.models.Sequential([
    keras.layers.Dense(100, activation="selu", input_shape=[codings_size]),
    keras.layers.Dense(150, activation="selu"),
    keras.layers.Dense(28 * 28, activation="sigmoid"),
    keras.layers.Reshape([28, 28])
])

Generative Adversarial Networks 

| 

593

discriminator = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(150, activation="selu"),
    keras.layers.Dense(100, activation="selu"),
    keras.layers.Dense(1, activation="sigmoid")
])
gan = keras.models.Sequential([generator, discriminator])

Next, we need to compile these models. As the discriminator is a binary classifier, we
can  naturally  use  the  binary  cross-entropy  loss.  The  generator  will  only  be  trained
through the gan model, so we do not need to compile it at all. The gan model is also a
binary classifier, so it can use the binary cross-entropy loss. Importantly, the discrimi‐
nator  should  not  be  trained  during  the  second  phase,  so  we  make  it  non-trainable
before compiling the gan model:

discriminator.compile(loss="binary_crossentropy", optimizer="rmsprop")
discriminator.trainable = False
gan.compile(loss="binary_crossentropy", optimizer="rmsprop")

The trainable attribute is taken into account by Keras only when
compiling a model, so after running this code, the discriminator
is  trainable  if  we  call  its  fit()  method  or  its  train_on_batch()
method (which we will be using), while it is not trainable when we
call these methods on the gan model.

Since the training loop is unusual, we cannot use the regular fit() method. Instead,
we  will  write  a  custom  training  loop.  For  this,  we  first  need  to  create  a  Dataset  to
iterate through the images:

batch_size = 32
dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)

We are now ready to write the training loop. Let’s wrap it in a train_gan() function:

594 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):
    generator, discriminator = gan.layers
    for epoch in range(n_epochs):
        for X_batch in dataset:
            # phase 1 - training the discriminator
            noise = tf.random.normal(shape=[batch_size, codings_size])
            generated_images = generator(noise)
            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)
            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)
            discriminator.trainable = True
            discriminator.train_on_batch(X_fake_and_real, y1)
            # phase 2 - training the generator
            noise = tf.random.normal(shape=[batch_size, codings_size])
            y2 = tf.constant([[1.]] * batch_size)
            discriminator.trainable = False
            gan.train_on_batch(noise, y2)

train_gan(gan, dataset, batch_size, codings_size)

As discussed earlier, you can see the two phases at each iteration:

• In  phase  one  we  feed  Gaussian  noise  to  the  generator  to  produce  fake  images,
and  we  complete  this  batch  by  concatenating  an  equal  number  of  real  images.
The targets y1 are set to 0 for fake images and 1 for real images. Then we train
the  discriminator  on  this  batch.  Note  that  we  set  the  discriminator’s  trainable
attribute to True: this is only to get rid of a warning that Keras displays when it
notices that trainable is now False but was True when the model was compiled
(or vice versa).

• In phase two, we feed the GAN some Gaussian noise. Its generator will start by
producing  fake  images,  then  the  discriminator  will  try  to  guess  whether  these
images are fake or real. We want the discriminator to believe that the fake images
are real, so the targets y2 are set to 1. Note that we set the trainable attribute to
False, once again to avoid a warning.

That’s it! If you display the generated images (see Figure 17-16), you will see that at
the end of the first epoch, they already start to look like (very noisy) Fashion MNIST
images.

Unfortunately, the images never really get much better than that, and you may even
find epochs where the GAN seems to be forgetting what it learned. Why is that? Well,
it turns out that training a GAN can be challenging. Let’s see why.

Generative Adversarial Networks 

| 

595

Figure 17-16. Images generated by the GAN after one epoch of training

The Difficulties of Training GANs
During training, the generator and the discriminator constantly try to outsmart each
other, in a zero-sum game. As training advances, the game may end up in a state that
game  theorists  call  a  Nash  equilibrium,  named  after  the  mathematician  John  Nash:
this is when no player would be better off changing their own strategy, assuming the
other players do not change theirs. For example, a Nash equilibrium is reached when
everyone drives on the left side of the road: no driver would be better off being the
only  one  to  switch  sides.  Of  course,  there  is  a  second  possible  Nash  equilibrium:
when everyone drives on the right side of the road. Different initial states and dynam‐
ics may lead to one equilibrium or the other. In this example, there is a single optimal
strategy  once  an  equilibrium  is  reached  (i.e.,  driving  on  the  same  side  as  everyone
else), but a Nash equilibrium can involve multiple competing strategies (e.g., a preda‐
tor chases its prey, the prey tries to escape, and neither would be better off changing
their strategy).

So how does this apply to GANs? Well, the authors of the paper demonstrated that a
GAN  can  only  reach  a  single  Nash  equilibrium:  that’s  when  the  generator  produces
perfectly  realistic  images,  and  the  discriminator  is  forced  to  guess  (50%  real,  50%
fake).  This  fact  is  very  encouraging:  it  would  seem  that  you  just  need  to  train  the
GAN for long enough, and it will eventually reach this equilibrium, giving you a per‐
fect generator. Unfortunately, it’s not that simple: nothing guarantees that the equili‐
brium will ever be reached.

596 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

The  biggest  difficulty  is  called  mode  collapse:  this  is  when  the  generator’s  outputs
gradually become less diverse. How can this happen? Suppose that the generator gets
better at producing convincing shoes than any other class. It will fool the discrimina‐
tor a bit more with shoes, and this will encourage it to produce even more images of
shoes.  Gradually,  it  will  forget  how  to  produce  anything  else.  Meanwhile,  the  only
fake images that the discriminator will see will be shoes, so it will also forget how to
discriminate  fake  images  of  other  classes.  Eventually,  when  the  discriminator  man‐
ages to discriminate the fake shoes from the real ones, the generator will be forced to
move to another class. It may then become good at shirts, forgetting about shoes, and
the  discriminator  will  follow.  The  GAN  may  gradually  cycle  across  a  few  classes,
never really becoming very good at any of them.

Moreover, because the generator and the discriminator are constantly pushing against
each other, their parameters may end up oscillating and becoming unstable. Training
may begin properly, then suddenly diverge for no apparent reason, due to these insta‐
bilities. And since many factors affect these complex dynamics, GANs are very sensi‐
tive to the hyperparameters: you may have to spend a lot of effort fine-tuning them.

These problems have kept researchers very busy since 2014: many papers were pub‐
lished on this topic, some proposing new cost functions11 (though a 2018 paper12 by
Google researchers questions their efficiency) or techniques to stabilize training or to
avoid  the  mode  collapse  issue.  For  example,  a  popular  technique  called  experience
replay consists in storing the images produced by the generator at each iteration in a
replay buffer (gradually dropping older generated images) and training the discrimi‐
nator using real images plus fake images drawn from this buffer (rather than just fake
images  produced  by  the  current  generator).  This  reduces  the  chances  that  the  dis‐
criminator  will  overfit  the  latest  generator’s  outputs.  Another  common  technique  is
called mini-batch discrimination: it measures how similar images are across the batch
and provides this statistic to the discriminator, so it can easily reject a whole batch of
fake  images  that  lack  diversity.  This  encourages  the  generator  to  produce  a  greater
variety  of  images,  reducing  the  chance  of  mode  collapse.  Other  papers  simply  pro‐
pose specific architectures that happen to perform well.

In short, this is still a very active field of research, and the dynamics of GANs are still
not  perfectly  understood.  But  the  good  news  is  that  great  progress  has  been  made,
and some of the results are truly astounding! So let’s look at some of the most success‐
ful architectures, starting with deep convolutional GANs, which were the state of the
art just a few years ago. Then we will look at two more recent (and more complex)
architectures.

11 For a nice comparison of the main GAN losses, check out this great GitHub project by Hwalsuk Lee.

12 Mario Lucic et al., “Are GANs Created Equal? A Large-Scale Study,” Proceedings of the 32nd International Con‐

ference on Neural Information Processing Systems (2018): 698–707.

Generative Adversarial Networks 

| 

597

Deep Convolutional GANs
The  original  GAN  paper  in  2014  experimented  with  convolutional  layers,  but  only
tried  to  generate  small  images.  Soon  after,  many  researchers  tried  to  build  GANs
based  on  deeper  convolutional  nets  for  larger  images.  This  proved  to  be  tricky,  as
training was very unstable, but Alec Radford et al. finally succeeded in late 2015, after
experimenting  with  many  different  architectures  and  hyperparameters.  They  called
their  architecture  deep  convolutional  GANs  (DCGANs).13  Here  are  the  main  guide‐
lines they proposed for building stable convolutional GANs:

• Replace any pooling layers with strided convolutions (in the discriminator) and

transposed convolutions (in the generator).

• Use Batch Normalization in both the generator and the discriminator, except in

the generator’s output layer and the discriminator’s input layer.

• Remove fully connected hidden layers for deeper architectures.

• Use ReLU activation in the generator for all layers except the output layer, which

should use tanh.

• Use leaky ReLU activation in the discriminator for all layers.

These  guidelines  will  work  in  many  cases,  but  not  always,  so  you  may  still  need  to
experiment  with  different  hyperparameters  (in  fact,  just  changing  the  random  seed
and training the same model again will sometimes work). For example, here is a small
DCGAN that works reasonably well with Fashion MNIST:

13 Alec Radford et al., “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial

Networks,” arXiv preprint arXiv:1511.06434 (2015).

598 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

codings_size = 100

generator = keras.models.Sequential([
    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),
    keras.layers.Reshape([7, 7, 128]),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding="same",
                                 activation="selu"),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding="same",
                                 activation="tanh")
])
discriminator = keras.models.Sequential([
    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding="same",
                        activation=keras.layers.LeakyReLU(0.2),
                        input_shape=[28, 28, 1]),
    keras.layers.Dropout(0.4),
    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding="same",
                        activation=keras.layers.LeakyReLU(0.2)),
    keras.layers.Dropout(0.4),
    keras.layers.Flatten(),
    keras.layers.Dense(1, activation="sigmoid")
])
gan = keras.models.Sequential([generator, discriminator])

The generator takes codings of size 100, and it projects them to 6272 dimensions (7 *
7 * 128), and reshapes the result to get a 7 × 7 × 128 tensor. This tensor is batch nor‐
malized and fed to a transposed convolutional layer with a stride of 2, which upsam‐
ples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result is batch
normalized again and fed to another transposed convolutional layer with a stride of 2,
which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64 to 1. This
layer uses the tanh activation function, so the outputs will range from –1 to 1. For this
reason,  before  training  the  GAN,  we  need  to  rescale  the  training  set  to  that  same
range. We also need to reshape it to add the channel dimension:

X_train = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale

The  discriminator  looks  much  like  a  regular  CNN  for  binary  classification,  except
instead of using max pooling layers to downsample the image, we use strided convo‐
lutions (strides=2). Also note that we use the leaky ReLU activation function.

Overall, we respected the DCGAN guidelines, except we replaced the BatchNormali
zation layers in the discriminator with Dropout layers (otherwise training was unsta‐
ble in this case) and we replaced ReLU with SELU in the generator. Feel free to tweak
this  architecture:  you  will  see  how  sensitive  it  is  to  the  hyperparameters  (especially
the relative learning rates of the two networks).

Lastly, to build the dataset, then compile and train this model, we use the exact same
code as earlier. After 50 epochs of training, the generator produces images like those

Generative Adversarial Networks 

| 

599

shown  in  Figure  17-17.  It’s  still  not  perfect,  but  many  of  these  images  are  pretty
convincing.

Figure 17-17. Images generated by the DCGAN after 50 epochs of training

If you scale up this architecture and train it on a large dataset of faces, you can get
fairly realistic images. In fact, DCGANs can learn quite meaningful latent representa‐
tions, as you can see in Figure 17-18: many images were generated, and nine of them
were picked manually (top left), including three representing men with glasses, three
men without glasses, and three women without glasses. For each of these categories,
the codings that were used to generate the images were averaged, and an image was
generated based on the resultin
 and Storkey, A. J. (2011). Neuronal adaptation for samplingbased probabilistic inference in perceptual bistability. In Advances in Neural Information
Processing Systems, pages 2357â€“2365. 666
763

BIBLIOGRAPHY

Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation
and approximate inference in deep generative models. In ICMLâ€™2014 . Preprint:
arXiv:1401.4082. 652, 689, 696
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive
auto-encoders: Explicit invariance during feature extraction. In ICMLâ€™2011 . 521, 522,
523
Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X.
(2011b). Higher order contractive auto-encoder. In ECML PKDD . 521, 522
Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011c). The manifold
tangent classiï¬?er. In NIPSâ€™2011 . 271, 272, 523
Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for
sampling contractive auto-encoders. In ICMLâ€™2012 . 711
Ringach, D. and Shapley, R. (2004). Reverse correlation in neurophysiology. Cognitive
Science , 28(2), 147â€“166. 368
Roberts, S. and Everson, R. (2001). Independent component analysis: principles and
practice. Cambridge University Press. 493
Robinson, A. J. and Fallside, F. (1991). A recurrent error propagation network speech
recognition system. Computer Speech and Language, 5(3), 259â€“274. 27, 459
Rockafellar, R. T. (1997). Convex analysis. princeton landmarks in mathematics. 93
Romero, A., Ballas, N., Ebrahimi Kahou, S., Chassang, A., Gatta, C., and Bengio, Y.
(2015). Fitnets: Hints for thin deep nets. In ICLRâ€™2015, arXiv:1412.6550 . 325
Rosen, J. B. (1960). The gradient projection method for nonlinear programming. part i.
linear constraints. Journal of the Society for Industrial and Applied Mathematics, 8(1),
pp. 181â€“217. 93
Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and
organization in the brain. Psychological Review , 65, 386â€“408. 14, 15, 27
Rosenblatt, F. (1962). Principles of Neurodynamics. Spartan, New York. 15, 27
Roweis, S. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear
embedding. Science, 290(5500). 164, 518
Roweis, S., Saul, L., and Hinton, G. (2002). Global coordination of local linear models. In
T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information
Processing Systems 14 (NIPSâ€™01), Cambridge, MA. MIT Press. 489
Rubin, D. B. et al. (1984). Bayesianly justiï¬?able and relevant frequency calculations for
the applied statistician. The Annals of Statistics , 12(4), 1151â€“1172. 717
764

BIBLIOGRAPHY

Rumelhart, D., Hinton, G., and Williams, R. (1986a). Learning representations by
back-propagating errors. Nature, 323, 533â€“536. 14, 18, 23, 204, 225, 373, 476, 482
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986b). Learning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel
Distributed Processing, volume 1, chapter 8, pages 318â€“362. MIT Press, Cambridge. 21,
27, 225
Rumelhart, D. E., McClelland, J. L., and the PDP Research Group (1986c). Parallel
Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press,
Cambridge. 17
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,
A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2014a). ImageNet Large
Scale Visual Recognition Challenge. 21
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,
A., Khosla, A., Bernstein, M., et al. (2014b). Imagenet large scale visual recognition
challenge. arXiv preprint arXiv:1409.0575 . 28
Russel, S. J. and Norvig, P. (2003). Artiï¬?cial Intelligence: a Modern Approach. Prentice
Hall. 86
Rust, N., Schwartz, O., Movshon, J. A., and Simoncelli, E. (2005). Spatiotemporal
elements of macaque V1 receptive ï¬?elds. Neuron, 46(6), 945â€“956. 367
Sainath, T., Mohamed, A., Kingsbury, B., and Ramabhadran, B. (2013). Deep convolutional neural networks for LVCSR. In ICASSP 2013 . 460
Salakhutdinov, R. (2010). Learning in Markov random ï¬?elds using tempered transitions. In
Y. Bengio, D. Schuurmans, C. Williams, J. Laï¬€erty, and A. Culotta, editors, Advances
in Neural Information Processing Systems 22 (NIPSâ€™09). 603
Salakhutdinov, R. and Hinton, G. (2009a). Deep Boltzmann machines. In Proceedings of
the International Conference on Artiï¬?cial Intelligence and Statistics, volume 5, pages
448â€“455. 24, 27, 529, 663, 666, 671, 672
Salakhutdinov, R. and Hinton, G. (2009b). Semantic hashing. In International Journal of
Approximate Reasoning . 525
Salakhutdinov, R. and Hinton, G. E. (2007a). Learning a nonlinear embedding by
preserving class neighbourhood structure. In Proceedings of the Eleventh International
Conference on Artiï¬?cial Intelligence and Statistics (AISTATSâ€™07), San Juan, Porto
Rico. Omnipress. 527
Salakhutdinov, R. and Hinton, G. E. (2007b). Semantic hashing. In SIGIRâ€™2007 . 525

765

BIBLIOGRAPHY

Salakhutdinov, R. and Hinton, G. E. (2008). Using deep belief nets to learn covariance
kernels for Gaussian processes. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing Systems 20 (NIPSâ€™07), pages 1249â€“1256,
Cambridge, MA. MIT Press. 244
Salakhutdinov, R. and Larochelle, H. (2010). Eï¬ƒcient learning of deep Boltzmann machines.
In Proceedings of the Thirteenth International Conference on Artiï¬?cial Intelligence and
Statistics (AISTATS 2010), JMLR W&CP , volume 9, pages 693â€“700. 652
Salakhutdinov, R. and Mnih, A. (2008). Probabilistic matrix factorization. In NIPSâ€™2008 .
480
Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief
networks. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, Proceedings of
the Twenty-ï¬?fth International Conference on Machine Learning (ICMLâ€™08), volume 25,
pages 872â€“879. ACM. 628, 662
Salakhutdinov, R., Mnih, A., and Hinton, G. (2007). Restricted Boltzmann machines for
collaborative ï¬?ltering. In ICML. 480
Sanger, T. D. (1994). Neural network learning control of robot manipulators using
gradually increasing task diï¬ƒculty. IEEE Transactions on Robotics and Automation,
10(3). 328
Saul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in intractable
networks. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances in Neural
Information Processing Systems 8 (NIPSâ€™95). MIT Press, Cambridge, MA. 638
Saul, L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean ï¬?eld theory for sigmoid belief
networks. Journal of Artiï¬?cial Intelligence Research, 4, 61â€“76. 27, 693
Savich, A. W., Moussa, M., and Areibi, S. (2007). The impact of arithmetic representation
on implementing mlp-bp on fpgas: A study. Neural Networks, IEEE Transactions on ,
18(1), 240â€“252. 451
Saxe, A. M., Koh, P. W., Chen, Z., Bhand, M., Suresh, B., and Ng, A. (2011). On random
weights and unsupervised feature learning. In Proc. ICMLâ€™2011 . ACM. 363
Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. In ICLR . 285, 286, 303
Schaul, T., Antonoglou, I., and Silver, D. (2014). Unit tests for stochastic optimization.
In International Conference on Learning Representations . 309
Schmidhuber, J. (1992). Learning complex, extended sequences using the principle of
history compression. Neural Computation, 4(2), 234â€“242. 398
Schmidhuber, J. (1996). Sequential neural text compression. IEEE Transactions on Neural
Networks, 7(1), 142â€“146. 477
766

BIBLIOGRAPHY

Schmidhuber, J. (2012). Self-delimiting neural networks. arXiv preprint arXiv:1210.0118 .
390
SchÃ¶lkopf, B. and Smola, A. J. (2002). Learning with kernels: Support vector machines,
regularization, optimization, and beyond . MIT press. 704
SchÃ¶lkopf, B., Smola, A., and MÃ¼ller, K.-R. (1998). Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation, 10, 1299â€“1319. 164, 518
SchÃ¶lkopf, B., Burges, C. J. C., and Smola, A. J. (1999). Advances in Kernel Methods â€”
Support Vector Learning. MIT Press, Cambridge, MA. 18, 142
SchÃ¶lkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. (2012). On
causal and anticausal learning. In ICMLâ€™2012 , pages 1255â€“1262. 545
Schuster, M. (1999). On supervised learning from sequential data with applications for
speech recognition. 190
Schuster, M. and Paliwal, K. (1997). Bidirectional recurrent neural networks. IEEE
Transactions on Signal Processing , 45(11), 2673â€“2681. 395
Schwenk, H. (2007). Continuous space language models. Computer speech and language,
21, 492â€“518. 466
Schwenk, H. (2010). Continuous space language models for statistical machine translation.
The Prague Bulletin of Mathematical Linguistics , 93, 137â€“146. 473
Schwenk, H. (2014). Cleaned subset of WMT â€™14 dataset. 21
Schwenk, H. and Bengio, Y. (1998). Training methods for adaptive boosting of neural networks. In M. Jordan, M. Kearns, and S. Solla, editors, Advances in Neural Information
Processing Systems 10 (NIPSâ€™97), pages 647â€“653. MIT Press. 258
Schwenk, H. and Gauvain, J.-L. (2002). Connectionist language modeling for large
vocabulary continuous speech recognition. In International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 765â€“768, Orlando, Florida. 466
Schwenk, H., Costa-jussÃ , M. R., and Fonollosa, J. A. R. (2006). Continuous space
language models for the IWSLT 2006 task. In International Workshop on Spoken
Language Translation, pages 166â€“173. 473
Seide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using contextdependent deep neural networks. In Interspeech 2011 , pages 437â€“440. 23
Sejnowski, T. (1987). Higher-order Boltzmann machines. In AIP Conference Proceedings
151 on Neural Networks for Computing , pages 398â€“403. American Institute of Physics
Inc. 686
767

BIBLIOGRAPHY

Series, P., Reichert, D. P., and Storkey, A. J. (2010). Hallucinations in Charles Bonnet
syndrome induced by homeostasis: a deep Boltzmann machine model. In Advances in
Neural Information Processing Systems , pages 2020â€“2028. 666
Sermanet, P., Chintala, S., and LeCun, Y. (2012). Convolutional neural networks applied
to house numbers digit classiï¬?cation. CoRR , abs/1204.3968. 457
Sermanet, P., Kavukcuoglu, K., Chintala, S., and LeCun, Y. (2013). Pedestrian detection
with unsupervised multi-stage feature learning. In Proc. International Conference on
Computer Vision and Pattern Recognition (CVPRâ€™13). IEEE. 23, 201
Shilov, G. (1977). Linear Algebra. Dover Books on Mathematics Series. Dover Publications.
31
Siegelmann, H. (1995). Computation beyond the Turing limit. Science, 268(5210),
545â€“548. 379
Siegelmann, H. and Sontag, E. (1991). Turing computability with neural nets. Applied
Mathematics Letters, 4(6), 77â€“80. 379
Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural nets.
Journal of Computer and Systems Sciences, 50(1), 132â€“150. 379, 403
Sietsma, J. and Dow, R. (1991). Creating artiï¬?cial neural networks that generalize. Neural
Networks, 4(1), 67â€“79. 241
Simard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices for convolutional
neural networks. In ICDARâ€™2003 . 371
Simard, P. and Graf, H. P. (1994). Backpropagation without multiplication. In Advances
in Neural Information Processing Systems, pages 232â€“239. 451
Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent prop - A formalism
for specifying selected invariances in an adaptive network. In NIPSâ€™1991 . 270, 271, 272,
356
Simard, P. Y., LeCun, Y., and Denker, J. (1993). Eï¬ƒcient pattern recognition using a
new transformation distance. In NIPSâ€™92 . 270
Simard, P. Y., LeCun, Y. A., Denker, J. S., and Victorri, B. (1998). Transformation
invariance in pattern recognition â€” tangent distance and tangent propagation. Lecture
Notes in Computer Science, 1524. 270
Simons, D. J. and Levin, D. T. (1998). Failure to detect changes to people during a
real-world interaction. Psychonomic Bulletin & Review, 5(4), 644â€“649. 543
Simonyan, K. and Zisserman, A. (2015). Very deep convolutional networks for large-scale
image recognition. In ICLR . 323
768

BIBLIOGRAPHY

SjÃ¶berg, J. and Ljung, L. (1995). Overtraining, regularization and searching for a minimum,
with application to neural networks. International Journal of Control ,62(6), 1391â€“1407.
250
Skinner, B. F. (1958). Reinforcement today. American Psychologist , 13, 94â€“99. 328
Smolensky, P. (1986). Information processing in dynamical systems: Foundations of
harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed
Processing , volume 1, chapter 6, pages 194â€“281. MIT Press, Cambridge. 571, 587, 656
Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of
machine learning algorithms. In NIPSâ€™2012 . 436
Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a). Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. In NIPSâ€™2011 .
401
Socher, R., Manning, C., and Ng, A. Y. (2011b). Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the Twenty-Eighth International
Conference on Machine Learning (ICMLâ€™2011). 401
Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011c).
Semi-supervised recursive autoencoders for predicting sentiment distributions. In
EMNLPâ€™2011 . 401
Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., and Potts,
C. (2013a). Recursive deep models for semantic compositionality over a sentiment
treebank. In EMNLPâ€™2013 . 401
Socher, R., Ganjoo, M., Manning, C. D., and Ng, A. Y. (2013b). Zero-shot learning through
cross-modal transfer. In 27th Annual Conference on Neural Information Processing
Systems (NIPS 2013). 539
Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. (2015). Deep
unsupervised learning using nonequilibrium thermodynamics. 716
Sohn, K., Zhou, G., and Lee, H. (2013). Learning and selecting features jointly with
point-wise gated Boltzmann machines. In ICMLâ€™2013 . 687
Solomonoï¬€, R. J. (1989). A system for incremental learning based on algorithmic probability. 328
Sontag, E. D. (1998). VC dimension of neural networks. NATO ASI Series F Computer
and Systems Sciences, 168, 69â€“96. 547, 551
Sontag, E. D. and Sussman, H. J. (1989). Backpropagation can give rise to spurious local
minima even for networks without hidden layers. Complex Systems , 3, 91â€“106. 284
769

BIBLIOGRAPHY

Sparkes, B. (1996). The Red and the Black: Studies in Greek Pottery. Routledge. 1
Spitkovsky, V. I., Alshawi, H., and Jurafsky, D. (2010). From baby steps to leapfrog: how
â€œless is moreâ€? in unsupervised dependency parsing. In HLTâ€™10 . 328
Squire, W. and Trapp, G. (1998). Using complex variables to estimate derivatives of real
functions. SIAM Rev., 40(1), 110â€“â€“112. 439
Srebro, N. and Shraibman, A. (2005). Rank, trace-norm and max-norm. In Proceedings of
the 18th Annual Conference on Learning Theory, pages 545â€“560. Springer-Verlag. 238
Srivastava, N. (2013). Improving Neural Networks With Dropout. Masterâ€™s thesis, U.
Toronto. 535
Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann
machines. In NIPSâ€™2012 . 541
Srivastava, N., Salakhutdinov, R. R., and Hinton, G. E.
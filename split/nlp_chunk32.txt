igure 8.16 Some NER features for a sample sentence, assuming that Chicago and Vil-
lanueva are listed as locations in a gazetteer. We assume features only take on the values 0 or
1, so the ﬁrst POS feature, for example, would be represented as 1
POS = NNP
{

BIO Label
B-PER
I-PER
O
B-ORG
I-ORG
I-ORG
O
O
B-LOC
O
O

Gazetteer
0
1
0
0
0
0
0
0
1
0
0

x
.

.
}

Inference and Training for CRFs

8.5.3
How do we ﬁnd the best tag sequence ˆY for a given input X? We start with Eq. 8.22:

ˆY = argmax

Y

Y

∈

= argmax
Y

Y

∈

= argmax
Y

Y

∈

= argmax
Y

Y

∈

P(Y

X)
|

1
Z(X)

exp

exp

K

K

(cid:32)

(cid:88)k=1
n

wk

(cid:88)k=1
n

K

(cid:88)i=1

K

(cid:32)

(cid:88)k=1
n

wk

(cid:88)i=1

wkFk(X,Y )

(cid:33)

fk(yi

1, yi, X, i)

−

(cid:33)

fk(yi

1, yi, X, i)

−

= argmax
Y

Y

∈

wk fk(yi

1, yi, X, i)

−

(cid:88)i=1

(cid:88)k=1

(8.27)

(8.28)

(8.29)

(8.30)

We can ignore the exp function and the denominator Z(X), as we do above, because
exp doesn’t change the argmax, and the denominator Z(X) is constant for a given
observation sequence X.

How should we decode to ﬁnd this optimal tag sequence ˆy? Just as with HMMs,
we’ll turn to the Viterbi algorithm, which works because, like the HMM, the linear-
chain CRF depends at each timestep on only one previous output token yi

1.

Concretely, this involves ﬁlling an N

T array with the appropriate values, main-
×
taining backpointers as we proceed. As with HMM Viterbi, when the table is ﬁlled,
we simply follow pointers back from the maximum value in the ﬁnal column to
retrieve the desired set of labels.

−

8.6

• EVALUATION OF NAMED ENTITY RECOGNITION

181

The requisite changes from HMM Viterbi have to do only with how we ﬁll each
cell. Recall from Eq. 8.19 that the recursive step of the Viterbi equation computes
the Viterbi value of time t for state j as

vt ( j) =

N
max
i=1

vt

1(i) ai j b j(ot ); 1

−

j

≤

≤

N, 1 < t

T

≤

(8.31)

which is the HMM implementation of

N
max
i=1

vt ( j) =

si) P(ot |
The CRF requires only a slight change to this latter formula, replacing the a and b
prior and likelihood probabilities with the CRF features:

1(i) P(s j|

N, 1 < t

s j) 1

(8.32)

≤

≤

≤

vt

T

−

j

vt ( j) =

N
max
i=1

vt

−

1(i)

K

(cid:88)k=1

wk fk(yt

1, yt , X,t) 1

−

j

≤

≤

N, 1 < t

T

≤

(8.33)

Learning in CRFs relies on the same supervised learning algorithms we presented
for logistic regression. Given a sequence of observations, feature functions, and cor-
responding outputs, we use stochastic gradient descent to train the weights to maxi-
mize the log-likelihood of the training corpus. The local nature of linear-chain CRFs
means that the forward-backward algorithm introduced for HMMs in Appendix A
can be extended to a CRF version that will efﬁciently compute the necessary deriva-
tives. As with logistic regression, L1 or L2 regularization is important.

8.6 Evaluation of Named Entity Recognition

Part-of-speech taggers are evaluated by the standard metric of accuracy. Named
entity recognizers are evaluated by recall, precision, and F1 measure. Recall that
recall is the ratio of the number of correctly labeled responses to the total that should
have been labeled; precision is the ratio of the number of correctly labeled responses
to the total labeled; and F-measure is the harmonic mean of the two.

To know if the difference between the F1 scores of two NER systems is a signif-
icant difference, we use the paired bootstrap test, or the similar randomization test
(Section 4.9).

For named entity tagging, the entity rather than the word is the unit of response.
Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Air-
lines Holding and the non-entity discussed would each count as a single response.

The fact that named entity tagging has a segmentation component which is not
present in tasks like text categorization or part-of-speech tagging causes some prob-
lems with evaluation. For example, a system that labeled Jane but not Jane Vil-
lanueva as a person would cause two errors, a false positive for O and a false nega-
tive for I-PER. In addition, using entities as the unit of response but words as the unit
of training means that there is a mismatch between the training and test conditions.

8.7 Further Details

In this section we summarize a few remaining details of the data and models for
part-of-speech tagging and NER, beginning with data. Since the algorithms we have

182 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

presented are supervised, having labeled data is essential for training and testing. A
wide variety of datasets exist for part-of-speech tagging and/or NER. The Universal
Dependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in
over a hundred languages, as do the Penn Treebanks in English, Chinese, and Arabic.
OntoNotes has corpora labeled for named entities in English, Chinese, and Arabic
(Hovy et al., 2006). Named entity tagged corpora are also available in particular
domains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al.,
2019).

8.7.1 Rule-based Methods

While machine learned (neural or CRF) sequence models are the norm in academic
research, commercial approaches to NER are often based on pragmatic combina-
tions of lists and rules, with some smaller amount of supervised machine learning
(Chiticariu et al., 2013). For example in the IBM System T architecture, a user
speciﬁes declarative constraints for tagging tasks in a formal query language that
includes regular expressions, dictionaries, semantic constraints, and other operators,
which the system compiles into an efﬁcient extractor (Chiticariu et al., 2018).

One common approach is to make repeated rule-based passes over a text, starting
with rules with very high precision but low recall, and, in subsequent stages, using
machine learning methods that take the output of the ﬁrst pass into account (an
approach ﬁrst worked out for coreference (Lee et al., 2017a)):

1. First, use high-precision rules to tag unambiguous entity mentions.
2. Then, search for substring matches of the previously detected names.
3. Use application-speciﬁc name lists to ﬁnd likely domain-speciﬁc mentions.
4. Finally, apply supervised sequence labeling techniques that use tags from pre-

vious stages as additional features.

Rule-based methods were also the earliest methods for part-of-speech tagging.
Rule-based taggers like the English Constraint Grammar system (Karlsson et al.
1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:
(1) a morphological analyzer with tens of thousands of word stem entries returns all
parts of speech for a word, then (2) a large set of thousands of constraints are applied
to the input sentence to rule out parts of speech inconsistent with the context.

8.7.2 POS Tagging for Morphologically Rich Languages

Augmentations to tagging algorithms become necessary when dealing with lan-
guages with rich morphology like Czech, Hungarian and Turkish.

These productive word-formation processes result in a large vocabulary for these
languages: a 250,000 word token corpus of Hungarian has more than twice as many
word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while
a 10 million word token corpus of Turkish contains four times as many word types
as a similarly sized English corpus (Hakkani-T¨ur et al., 2002). Large vocabular-
ies mean many unknown words, and these unknown words cause signiﬁcant per-
formance degradations in a wide variety of languages (including Czech, Slovene,
Estonian, and Romanian) (Hajiˇc, 2000).

Highly inﬂectional languages also have much more information than English
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). Because this information is important for tasks like pars-
ing and coreference resolution, part-of-speech taggers for morphologically rich lan-

8.8

• SUMMARY

183

guages need to label words with case and gender information. Tagsets for morpho-
logically rich languages are therefore sequences of morphological tags rather than a
single primitive tag. Here’s a Turkish example, in which the word izin has three pos-
sible morphological/part-of-speech tags and meanings (Hakkani-T¨ur et al., 2002):

1. Yerdeki izin temizlenmesi gerek.

The trace on the ﬂoor should be cleaned.

2.

¨Uzerinde parmak izin kalmis¸.
Your ﬁnger print is left on (it).

3. Ic¸eri girmek ic¸in izin alman gerekiyor.

You need permission to enter.

iz + Noun+A3sg+Pnon+Gen

iz + Noun+A3sg+P2sg+Nom

izin + Noun+A3sg+Pnon+Nom

Using a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-
of-speech tag greatly increases the number of parts of speech, and so tagsets can
be 4 to 10 times larger than the 50–100 tags we have seen for English. With such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. The
role of the tagger is then to disambiguate among these tags. This method also helps
with unknown words since morphological parsers can accept unknown stems and
still segment the afﬁxes properly.

8.8 Summary

This chapter introduced parts of speech and named entities, and the tasks of part-
of-speech tagging and named entity recognition:

• Languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words, and open-class words like
nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40
and 200 tags.

• Part-of-speech tagging is the process of assigning a part-of-speech label to

each of a sequence of words.

• Named entities are words for proper nouns referring mainly to people, places,
and organizations, but extended to many other types that aren’t strictly entities
or even proper nouns.

• Two common approaches to sequence modeling are a generative approach,
HMM tagging, and a discriminative approach, CRF tagging. We will see a
neural approach in following chapters.

• The probabilities in HMM taggers are estimated by maximum likelihood es-
timation on tag-labeled training corpora. The Viterbi algorithm is used for
decoding, ﬁnding the most likely tag sequence

• Conditional Random Fields or CRF taggers train a log-linear model that can
choose the best tag sequence given an observation sequence, based on features
that condition on the output tag, the prior output tag, the entire input sequence,
and the current timestep. They use the Viterbi algorithm for inference, to
choose the best sequence of tags, and a version of the Forward-Backward
algorithm (see Appendix A) for training,

184 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

Bibliographical and Historical Notes

What is probably the earliest part-of-speech tagger was part of the parser in Zellig
Harris’s Transformations and Discourse Analysis Project (TDAP), implemented be-
tween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),
although earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quences and the relative frequency of tags for a word preﬁgures modern algorithms.
The parser was implemented essentially as a cascade of ﬁnite-state transducers; see
Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.

The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had
three components: a lexicon, a morphological analyzer, and a context disambigua-
tor. The small 1500-word lexicon listed only function words and other irregular
words. The morphological analyzer used inﬂectional and derivational sufﬁxes to as-
sign part-of-speech classes. These were run over words to produce candidate parts
of speech which were then disambiguated by a set of 500 context rules by relying on
surrounding islands of unambiguous words. For example, one rule said that between
an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-
ADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used
the same architecture as Klein and Simmons (1963), with a bigger dictionary and
more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis
and Kuˇcera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the
Brown corpus was then tagged by hand. All these early algorithms were based on
a two-stage architecture in which a dictionary was ﬁrst used to assign each word a
set of potential parts of speech, and then lists of handwritten disambiguation rules
winnowed the set down to a single part of speech per word.

Probabilities were used in tagging by Stolz et al. (1965) and a complete proba-
bilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The
Lancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown cor-
pus, was tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Mar-
shall 1987; Garside 1987), a probabilistic algorithm that approximated a simpliﬁed
HMM tagger. The algorithm used tag bigram probabilities, but instead of storing the
word likelihood of each tag, the algorithm marked tags either as rare (P(tag
word) <
|
word) > .10).
word) < .10) or normally frequent (P(tag
.01) infrequent (P(tag
|
|
DeRose (1988) developed a quasi-HMM algorithm, including the use of dy-
namic programming, although computing P(t
t)P(w). The
w)P(w) instead of P(w
|
|
same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the
ﬁrst implemented HMM tagger, described correctly in Church (1989), although
w)P(w) instead
Church (1988) also described the computation incorrectly as P(t
|
t)P(w). Church (p.c.) explained that he had simpliﬁed for pedagogical pur-
of P(w
|
poses because using the probability P(t
w) made the idea seem more understandable
|
as “storing a lexicon in an almost standard form”.

Later taggers explicitly introduced the use of the hidden Markov model (Kupiec
1992; Weischedel et al. 1993; Sch¨utze and Singer 1994). Merialdo (1994) showed
that fully unsupervised EM didn’t work well for the tagging task and that reliance
on hand-labeled data was important. Charniak et al. (1993) showed the importance
of the most frequent tag baseline; the 92.3% number we give above was from Abney
et al. (1999). See Brants (2000) for HMM tagger implementation details, includ-
ing the extension to trigram contexts, and the use of sophisticated unknown word
features; its performance is still close to state of the art taggers.

EXERCISES

185

Log-linear models for POS tagging were introduced by Ratnaparkhi (1996),
who introduced a system called MXPOST which implemented a maximum entropy
Markov model (MEMM), a slightly simpler version of a CRF. Around the same
time, sequence labelers were applied to the task of named entity tagging, ﬁrst with
HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once
CRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-
Callum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005).
Neural approaches to NER mainly follow from the pioneering results of Collobert
et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word
and character-based embeddings as input followed shortly and became a standard
neural algorithm for NER (Huang et al. 2015, Ma
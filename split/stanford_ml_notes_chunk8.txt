
n
(cid:88)

Î±i âˆ’

1
2

n
(cid:88)

i,j=1

y(i)y(j)Î±iÎ±j(cid:104)x(i), x(j)(cid:105).

(6.19)

i=1
s.t. 0 â‰¤ Î±i â‰¤ C,

i = 1, . . . , n

(6.20)

(6.21)

n
(cid:88)

i=1

Î±iy(i) = 0.

Letâ€™s say we have set of Î±iâ€™s that satisfy the constraints (6.20-6.21). Now,
suppose we want to hold Î±2, . . . , Î±n ï¬xed, and take a coordinate ascent step
and reoptimize the objective with respect to Î±1. Can we make any progress?
The answer is no, because the constraint (6.21) ensures that

Î±1y(1) = âˆ’

n
(cid:88)

i=2

Î±iy(i).

âˆ’2âˆ’1.5âˆ’1âˆ’0.500.511.522.5âˆ’2âˆ’1.5âˆ’1âˆ’0.500.511.522.576

Or, by multiplying both sides by y(1), we equivalently have

Î±1 = âˆ’y(1)

n
(cid:88)

i=2

Î±iy(i).

(This step used the fact that y(1) âˆˆ {âˆ’1, 1}, and hence (y(1))2 = 1.) Hence,
Î±1 is exactly determined by the other Î±iâ€™s, and if we were to hold Î±2, . . . , Î±n
ï¬xed, then we canâ€™t make any change to Î±1 without violating the con-
straint (6.21) in the optimization problem.

Thus, if we want to update some subject of the Î±iâ€™s, we must update at
least two of them simultaneously in order to keep satisfying the constraints.
This motivates the SMO algorithm, which simply does the following:

Repeat till convergence {

1. Select some pair Î±i and Î±j to update next (using a heuristic that
tries to pick the two that will allow us to make the biggest progress
towards the global maximum).

2. Reoptimize W (Î±) with respect to Î±i and Î±j, while holding all the

other Î±kâ€™s (k (cid:54)= i, j) ï¬xed.

}

To test for convergence of this algorithm, we can check whether the KKT
conditions (Equations 6.16-6.18) are satisï¬ed to within some tol. Here, tol is
the convergence tolerance parameter, and is typically set to around 0.01 to
0.001. (See the paper and pseudocode for details.)

The key reason that SMO is an eï¬ƒcient algorithm is that the update to
Î±i, Î±j can be computed very eï¬ƒciently. Letâ€™s now brieï¬‚y sketch the main
ideas for deriving the eï¬ƒcient update.

Letâ€™s say we currently have some setting of the Î±iâ€™s that satisfy the con-
straints (6.20-6.21), and suppose weâ€™ve decided to hold Î±3, . . . , Î±n ï¬xed, and
want to reoptimize W (Î±1, Î±2, . . . , Î±n) with respect to Î±1 and Î±2 (subject to
the constraints). From (6.21), we require that

Î±1y(1) + Î±2y(2) = âˆ’

n
(cid:88)

i=3

Î±iy(i).

Since the right hand side is ï¬xed (as weâ€™ve ï¬xed Î±3, . . . Î±n), we can just let
it be denoted by some constant Î¶:

Î±1y(1) + Î±2y(2) = Î¶.

(6.22)

We can thus picture the constraints on Î±1 and Î±2 as follows:

77

From the constraints (6.20), we know that Î±1 and Î±2 must lie within the box
[0, C] Ã— [0, C] shown. Also plotted is the line Î±1y(1) + Î±2y(2) = Î¶, on which we
know Î±1 and Î±2 must lie. Note also that, from these constraints, we know
L â‰¤ Î±2 â‰¤ H; otherwise, (Î±1, Î±2) canâ€™t simultaneously satisfy both the box
and the straight line constraint. In this example, L = 0. But depending on
what the line Î±1y(1) + Î±2y(2) = Î¶ looks like, this wonâ€™t always necessarily be
the case; but more generally, there will be some lower-bound L and some
upper-bound H on the permissible values for Î±2 that will ensure that Î±1, Î±2
lie within the box [0, C] Ã— [0, C].

Using Equation (6.22), we can also write Î±1 as a function of Î±2:

Î±1 = (Î¶ âˆ’ Î±2y(2))y(1).

(Check this derivation yourself; we again used the fact that y(1) âˆˆ {âˆ’1, 1} so
that (y(1))2 = 1.) Hence, the objective W (Î±) can be written

W (Î±1, Î±2, . . . , Î±n) = W ((Î¶ âˆ’ Î±2y(2))y(1), Î±2, . . . , Î±n).

Treating Î±3, . . . , Î±n as constants, you should be able to verify that this is
just some quadratic function in Î±2. I.e., this can also be expressed in the
form aÎ±2
2 + bÎ±2 + c for some appropriate a, b, and c. If we ignore the â€œboxâ€
constraints (6.20) (or, equivalently, that L â‰¤ Î±2 â‰¤ H), then we can easily
maximize this quadratic function by setting its derivative to zero and solving.
Weâ€™ll let Î±new,unclipped
denote the resulting value of Î±2. You should also be
2
able to convince yourself that if we had instead wanted to maximize W with
respect to Î±2 but subject to the box constraint, then we can ï¬nd the resulting
value optimal simply by taking Î±new,unclipped
and â€œclippingâ€ it to lie in the

2

Î±2Î±1Î±1Î±2CC(1)+(2)yy=Î¶HL[L, H] interval, to get

Î±new
2

=

ï£±
ï£²

ï£³

H
Î±new,unclipped
2
L

2

if Î±new,unclipped
> H
if L â‰¤ Î±new,unclipped
2
if Î±new,unclipped
< L

2

78

â‰¤ H

Finally, having found the Î±new
ï¬nd the optimal value of Î±new
.

2

1

, we can use Equation (6.22) to go back and

Thereâ€™re a couple more details that are quite easy but that weâ€™ll leave you
to read about yourself in Plattâ€™s paper: One is the choice of the heuristics
used to select the next Î±i, Î±j to update; the other is how to update b as the
SMO algorithm is run.

Part II

Deep learning

79

Chapter 7

Deep learning

We now begin our study of deep learning. In this set of notes, we give an
overview of neural networks, discuss vectorization and discuss training neural
networks with backpropagation.

7.1 Supervised learning with non-linear mod-

els

In the supervised learning setting (predicting y from the input x), suppose
our model/hypothesis is hÎ¸(x). In the past lectures, we have considered the
cases when hÎ¸(x) = Î¸(cid:62)x (in linear regression) or hÎ¸(x) = Î¸(cid:62)Ï†(x) (where Ï†(x)
is the feature map). A commonality of these two models is that they are
linear in the parameters Î¸. Next we will consider learning general family of
models that are non-linear in both the parameters Î¸ and the inputs x. The
most common non-linear models are neural networks, which we will deï¬ne
staring from the next section. For this section, it suï¬ƒces to think hÎ¸(x) as
an abstract non-linear model.1
Suppose {(x(i), y(i))}n

i=1 are the training examples. We will deï¬ne the

nonlinear model and the loss/cost function for learning it.

Regression problems. For simplicity, we start with the case where the
output is a real number, that is, y(i) âˆˆ R, and thus the model hÎ¸ also outputs
a real number hÎ¸(x) âˆˆ R. We deï¬ne the least square cost function for the

1If a concrete example is helpful, perhaps think about the model hÎ¸(x) = Î¸2

1x2

1 + Î¸2

2x2

2 +

Â· Â· Â· + Î¸2

dx2

d in this subsection, even though itâ€™s not a neural network.

80

i-th example (x(i), y(i)) as

J (i)(Î¸) =

1
2

(hÎ¸(x(i)) âˆ’ y(i))2 ,

and deï¬ne the mean-square cost function for the dataset as

J(Î¸) =

1
n

n
(cid:88)

i=1

J (i)(Î¸) ,

81

(7.1)

(7.2)

which is same as in linear regression except that we introduce a constant
1/n in front of the cost function to be consistent with the convention. Note
that multiplying the cost function with a scalar will not change the local
minima or global minima of the cost function. Also note that the underlying
parameterization for hÎ¸(x) is diï¬€erent from the case of linear regression,
even though the form of the cost function is the same mean-squared loss.
Throughout the notes, we use the words â€œlossâ€ and â€œcostâ€ interchangeably.

Binary classiï¬cation. Next we deï¬ne the model and loss function for
binary classiï¬cation. Suppose the inputs x âˆˆ Rd. Let Â¯hÎ¸ : Rd â†’ R be a
parameterized model (the analog of Î¸(cid:62)x in logistic linear regression). We
call the output Â¯hÎ¸(x) âˆˆ R the logit. Analogous to Section 2.1, we use the
logistic function g(Â·) to turn the logit Â¯hÎ¸(x) to a probability hÎ¸(x) âˆˆ [0, 1]:

hÎ¸(x) = g(Â¯hÎ¸(x)) = 1/(1 + exp(âˆ’Â¯hÎ¸(x)) .

(7.3)

We model the conditional distribution of y given x and Î¸ by

P (y = 1 | x; Î¸) = hÎ¸(x)
P (y = 0 | x; Î¸) = 1 âˆ’ hÎ¸(x)

Following the same derivation in Section 2.1 and using the derivation in
Remark 2.1.1, the negative likelihood loss function is equal to:

J (i)(Î¸) = âˆ’ log p(y(i) | x(i); Î¸) = (cid:96)logistic(Â¯hÎ¸(x(i)), y(i))

(7.4)

As done in equation (7.2), the total loss function is also deï¬ned as the average
of the loss function over individual training examples, J(Î¸) = 1
i=1 J (i)(Î¸).
n

(cid:80)n

82

Multi-class classiï¬cation. Following Section 2.3, we consider a classiï¬ca-
tion problem where the response variable y can take on any one of k values,
i.e. y âˆˆ {1, 2, . . . , k}. Let Â¯hÎ¸ : Rd â†’ Rk be a parameterized model. We
call the outputs Â¯hÎ¸(x) âˆˆ Rk the logits. Each logit corresponds to the predic-
tion for one of the k classes. Analogous to Section 2.3, we use the softmax
function to turn the logits Â¯hÎ¸(x) into a probability vector with non-negative
entries that sum up to 1:

P (y = j | x; Î¸) =

exp(Â¯hÎ¸(x)j)
s=1 exp(Â¯hÎ¸(x)s)

(cid:80)k

,

(7.5)

where Â¯hÎ¸(x)s denotes the s-th coordinate of Â¯hÎ¸(x).

Similarly to Section 2.3, the loss function for a single training example

(x(i), y(i)) is its negative log-likelihood:

J (i)(Î¸) = âˆ’ log p(y(i) | x(i); Î¸) = âˆ’ log

(cid:32) exp(Â¯hÎ¸(x(i))y(i))
s=1 exp(Â¯hÎ¸(x(i))s)

(cid:80)k

(cid:33)

.

(7.6)

Using the notations of Section 2.3, we can simply write in an abstract way:
J (i)(Î¸) = (cid:96)ce(Â¯hÎ¸(x(i)), y(i)).

(7.7)

The loss function is also deï¬ned as the average of the loss function of indi-
(cid:80)n
vidual training examples, J(Î¸) = 1
n

i=1 J (i)(Î¸).

We also note that the approach above can also be generated to any con-
ditional probabilistic model where we have an exponential distribution for
y, Exponential-family(y; Î·), where Î· = Â¯hÎ¸(x) is a parameterized nonlinear
function of x. However, the most widely used situations are the three cases
discussed above.

Optimizers (SGD). Commonly, people use gradient descent (GD), stochas-
tic gradient (SGD), or their variants to optimize the loss function J(Î¸). GDâ€™s
update rule can be written as2

Î¸ := Î¸ âˆ’ Î±âˆ‡Î¸J(Î¸)

(7.8)

where Î± > 0 is often referred to as the learning rate or step size. Next, we
introduce a version of the SGD (Algorithm 1), which is lightly diï¬€erent from
that in the ï¬rst lecture notes.

2Recall that, as deï¬ned in the previous lecture notes, we use the notation â€œa := bâ€ to
denote an operation (in a computer program) in which we set the value of a variable a to
be equal to the value of b. In other words, this operation overwrites a with the value of
b. In contrast, we will write â€œa = bâ€ when we are asserting a statement of fact, that the
value of a is equal to the value of b.

83

Algorithm 1 Stochastic Gradient Descent
1: Hyperparameter: learning rate Î±, number of total iteration niter.
2: Initialize Î¸ randomly.
3: for i = 1 to niter do
4:

Sample j uniformly from {1, . . . , n}, and update Î¸ by

Î¸ := Î¸ âˆ’ Î±âˆ‡Î¸J (j)(Î¸)

(7.9)

Oftentimes computing the gradient of B examples simultaneously for the
parameter Î¸ can be faster than computing B gradients separately due to
hardware parallelization. Therefore, a mini-batch version of SGD is most
commonly used in deep learning, as shown in Algorithm 2. There are also
other variants of the SGD or mini-batch SGD with slightly diï¬€erent sampling
schemes.

Algorithm 2 Mini-batch Stochastic Gradient Descent
1: Hyperparameters: learning rate Î±, batch size B, # iterations niter.
2: Initialize Î¸ randomly
3: for i = 1 to niter do
4:

Sample B examples j1, . . . , jB (without replacement) uniformly from

{1, . . . , n}, and update Î¸ by

Î¸ := Î¸ âˆ’

Î±
B

B
(cid:88)

k=1

âˆ‡Î¸J (jk)(Î¸)

(7.10)

With these generic algorithms, a typical deep learning model is learned
with the following steps. 1. Deï¬ne a neural network parametrization hÎ¸(x),
which we will introduce in Section 7.2, and 2. write the backpropagation
algorithm to compute the gradient of the loss function J (j)(Î¸) eï¬ƒciently,
which will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (or
other gradient-based optimizers) with the loss function J(Î¸).

84

7.2 Neural networks

Neural networks refer to a broad type of non-linear models/parametrizations
Â¯hÎ¸(x) that involve combinations of matrix multiplications and other entry-
wise non-linear operations. To have a uniï¬ed treatment for regression prob-
lem and classiï¬cation problem, here we consider Â¯hÎ¸(x) as the output of the
neural network. For regression problem, the ï¬nal prediction hÎ¸(x) = Â¯hÎ¸(x),
and for classiï¬cation problem, Â¯hÎ¸(x) is the logits and the predicted probability
will be hÎ¸(x) = 1/(1+exp(âˆ’Â¯hÎ¸(x)) (see equation 7.3) for binary classiï¬cation
or hÎ¸(x) = softmax(Â¯hÎ¸(x)) for multi-class classiï¬cation (see equation 7.5).
We will start small and slowly build up a neural network, step by step.

A Neural Network with a Single Neuron.
Recall the housing price
prediction problem from before: given the size of the house, we want to
predict the price. We will use it as a running example in this subsection.

Previously, we ï¬t a straight line to the graph of size vs. housing price.
Now, instead of ï¬tting a straight line, we wish to prevent negative housing
prices by setting the absolute minimum price as zero. This produces a â€œkinkâ€
in the graph as shown in Figure 7.1. How do we represent such a function
with a single kink as Â¯hÎ¸(x) with unknown parameter? (After doing so, we
can invoke the machinery in Section 7.1.)

We deï¬ne a parameterized function Â¯hÎ¸(x) with input x, parameterized by
Î¸, which outputs the price of the house y. Formally, Â¯hÎ¸ : x â†’ y. Perhaps
one of the simplest parametrization would be

Â¯hÎ¸(x) = max(wx + b, 0), where Î¸ = (w, b) âˆˆ R2

(7.11)

Here Â¯hÎ¸(x) returns a single value: (wx+b) or zero, whichever is greater. In
the context of neural networks, the function max{t, 0} is called a ReLU (pro-
nounced â€œray-luâ€), or rectiï¬ed linear unit, and often denoted by ReLU(t) (cid:44)
max{t, 0}.

Generally, a one-dimensional non-linear function that maps R to R such as
ReLU is often referred to as an activation function. The model Â¯hÎ¸(x) is said
to have a single neuron partly because it has a single non-linear activation
function. (We will discuss more about why a non-linear activation is called
neuron.)

When the input x âˆˆ Rd has multiple dimensions, a neural network with

a single neuron can be written as

Â¯hÎ¸(x) = ReLU(w(cid:62)x + b), where w âˆˆ Rd, b âˆˆ R, and Î¸ = (w, b)

(7.12)

85

Figure 7.1: Housing prices with a â€œkinkâ€ in the graph.

The term b is often referred to as the â€œbiasâ€, and the vector w is referred
to as the weight vector. Such a neural network has 1 layer. (We will deï¬ne
what multiple layers mean in the sequel.)

Stacking Neurons. A more complex neural network may take the single
neuron described above and â€œstackâ€ them together such that one neuron
passes its output as input into the next neuron, resulting in a more complex
function.

Let us now deepen the housing prediction example. In addition to the size
of the house, suppose that you know the number of bedrooms, the zip code
and the wealth of the neighborhood. Building neural networks is analogous
to Lego bricks: you take individual bricks and stack them together to build
complex structures. The same applies to neural networks: we take individual
neurons and stack them together to create complex neural networks.

Given these features (size, number of bedrooms, zip code, and wealth),
we might then decide that the price of the house depends on the maximum
family size it can accommodate. Suppose the family size is a function of the
size of the house and number of bedrooms (see Figure 7.2). The zip code
may provide additional information such as how walkable the neighborhood
is (i.e., can you walk to the
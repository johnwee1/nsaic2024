), which is called the KKT dual
complementarity condition. Speciﬁcally, it implies that if α∗
i > 0, then
gi(w∗) = 0. (I.e., the “gi(w) ≤ 0” constraint is active, meaning it holds with
equality rather than with inequality.) Later on, this will be key for showing
that the SVM has only a small number of “support vectors”; the KKT dual
complementarity condition will also give us our convergence test when we
talk about the SMO algorithm.

6.6 Optimal margin classiﬁers: the dual form

(option reading)

Note: The equivalence of optimization problem (6.8) and the optimization
problem (6.12), and the relationship between the primary and dual variables
in equation (6.10) are the most important take home messages of this section.

Previously, we posed the following (primal) optimization problem for ﬁnd-

ing the optimal margin classiﬁer:

minw,b

1
2

||w||2

s.t. y(i)(wT x(i) + b) ≥ 1,

i = 1, . . . , n

We can write the constraints as

gi(w) = −y(i)(wT x(i) + b) + 1 ≤ 0.

(6.8)

69

We have one such constraint for each training example. Note that from the
KKT dual complementarity condition, we will have αi > 0 only for the train-
ing examples that have functional margin exactly equal to one (i.e., the ones
corresponding to constraints that hold with equality, gi(w) = 0). Consider
the ﬁgure below, in which a maximum margin separating hyperplane is shown
by the solid line.

The points with the smallest margins are exactly the ones closest to the
decision boundary; here, these are the three points (one negative and two pos-
itive examples) that lie on the dashed lines parallel to the decision boundary.
Thus, only three of the αi’s—namely, the ones corresponding to these three
training examples—will be non-zero at the optimal solution to our optimiza-
tion problem. These three points are called the support vectors in this
problem. The fact that the number of support vectors can be much smaller
than the size the training set will be useful later.

Let’s move on. Looking ahead, as we develop the dual form of the prob-
lem, one key idea to watch out for is that we’ll try to write our algorithm
in terms of only the inner product (cid:104)x(i), x(j)(cid:105) (think of this as (x(i))T x(j))
between points in the input feature space. The fact that we can express our
algorithm in terms of these inner products will be key when we apply the
kernel trick.

When we construct the Lagrangian for our optimization problem we have:

L(w, b, α) =

n
(cid:88)

||w||2 −

(cid:2)y(i)(wT x(i) + b) − 1(cid:3) .

αi

(6.9)

1
2

i=1
Note that there’re only “αi” but no “βi” Lagrange multipliers, since the
problem has only inequality constraints.

70

Let’s ﬁnd the dual form of the problem. To do so, we need to ﬁrst
minimize L(w, b, α) with respect to w and b (for ﬁxed α), to get θD, which
we’ll do by setting the derivatives of L with respect to w and b to zero. We
have:

∇wL(w, b, α) = w −

αiy(i)x(i) = 0

n
(cid:88)

This implies that

i=1

w =

n
(cid:88)

i=1

αiy(i)x(i).

As for the derivative with respect to b, we obtain

∂
∂b

L(w, b, α) =

n
(cid:88)

i=1

αiy(i) = 0.

(6.10)

(6.11)

If we take the deﬁnition of w in Equation (6.10) and plug that back into

the Lagrangian (Equation 6.9), and simplify, we get

L(w, b, α) =

n
(cid:88)

i=1

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(x(i))T x(j) − b

n
(cid:88)

i=1

αiy(i).

But from Equation (6.11), the last term must be zero, so we obtain

L(w, b, α) =

n
(cid:88)

i=1

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(x(i))T x(j).

Recall that we got to the equation above by minimizing L with respect to
w and b. Putting this together with the constraints αi ≥ 0 (that we always
had) and the constraint (6.11), we obtain the following dual optimization
problem:

maxα W (α) =

n
(cid:88)

αi −

n
(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105).

(6.12)

1
2

i=1
i = 1, . . . , n

i,j=1

s.t. αi ≥ 0,
n
(cid:88)

αiy(i) = 0,

i=1

You should also be able to verify that the conditions required for p∗ = d∗
and the KKT conditions (Equations 6.3–6.7) to hold are indeed satisﬁed in

71

our optimization problem. Hence, we can solve the dual in lieu of solving
the primal problem. Speciﬁcally, in the dual problem above, we have a
maximization problem in which the parameters are the αi’s. We’ll talk later
about the speciﬁc algorithm that we’re going to use to solve the dual problem,
but if we are indeed able to solve it (i.e., ﬁnd the α’s that maximize W (α)
subject to the constraints), then we can use Equation (6.10) to go back and
ﬁnd the optimal w’s as a function of the α’s. Having found w∗, by considering
the primal problem, it is also straightforward to ﬁnd the optimal value for
the intercept term b as

b∗ = −

maxi:y(i)=−1 w∗T x(i) + mini:y(i)=1 w∗T x(i)
2

.

(6.13)

(Check for yourself that this is correct.)

Before moving on, let’s also take a more careful look at Equation (6.10),
which gives the optimal value of w in terms of (the optimal value of) α.
Suppose we’ve ﬁt our model’s parameters to a training set, and now wish to
make a prediction at a new point input x. We would then calculate wT x + b,
and predict y = 1 if and only if this quantity is bigger than zero. But
using (6.10), this quantity can also be written:

wT x + b =

(cid:33)T

αiy(i)x(i)

x + b

(cid:32) n

(cid:88)

i=1

=

n
(cid:88)

i=1

αiy(i)(cid:104)x(i), x(cid:105) + b.

(6.14)

(6.15)

Hence, if we’ve found the αi’s, in order to make a prediction, we have to
calculate a quantity that depends only on the inner product between x and
the points in the training set. Moreover, we saw earlier that the αi’s will all
be zero except for the support vectors. Thus, many of the terms in the sum
above will be zero, and we really need to ﬁnd only the inner products between
x and the support vectors (of which there is often only a small number) in
order calculate (6.15) and make our prediction.

By examining the dual form of the optimization problem, we gained sig-
niﬁcant insight into the structure of the problem, and were also able to write
the entire algorithm in terms of only inner products between input feature
vectors. In the next section, we will exploit this property to apply the ker-
nels to our classiﬁcation problem. The resulting algorithm, support vector
machines, will be able to eﬃciently learn in very high dimensional spaces.

72

6.7 Regularization and the non-separable case

(optional reading)

The derivation of the SVM as presented so far assumed that the data is
linearly separable. While mapping data to a high dimensional feature space
via φ does generally increase the likelihood that the data is separable, we
can’t guarantee that it always will be so. Also, in some cases it is not clear
that ﬁnding a separating hyperplane is exactly what we’d want to do, since
that might be susceptible to outliers. For instance, the left ﬁgure below
shows an optimal margin classiﬁer, and when a single outlier is added in the
upper-left region (right ﬁgure), it causes the decision boundary to make a
dramatic swing, and the resulting classiﬁer has a much smaller margin.

To make the algorithm work for non-linearly separable datasets as well
as be less sensitive to outliers, we reformulate our optimization (using (cid:96)1
regularization) as follows:

minγ,w,b

1
2

||w||2 + C

n
(cid:88)

i=1

ξi

s.t. y(i)(wT x(i) + b) ≥ 1 − ξi,

i = 1, . . . , n

ξi ≥ 0,

i = 1, . . . , n.

Thus, examples are now permitted to have (functional) margin less than 1,
and if an example has functional margin 1 − ξi (with ξ > 0), we would pay
a cost of the objective function being increased by Cξi. The parameter C
controls the relative weighting between the twin goals of making the ||w||2
small (which we saw earlier makes the margin large) and of ensuring that
most examples have functional margin at least 1.

As before, we can form the Lagrangian:

L(w, b, ξ, α, r) =

1
2

wT w + C

n
(cid:88)

n
(cid:88)

ξi −

i=1

i=1

(cid:2)y(i)(xT w + b) − 1 + ξi

(cid:3) −

αi

73

n
(cid:88)

i=1

riξi.

Here, the αi’s and ri’s are our Lagrange multipliers (constrained to be ≥ 0).
We won’t go through the derivation of the dual again in detail, but after
setting the derivatives with respect to w and b to zero as before, substituting
them back in, and simplifying, we obtain the following dual form of the
problem:

maxα W (α) =

n
(cid:88)

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

i=1
s.t. 0 ≤ αi ≤ C,

i = 1, . . . , n

n
(cid:88)

i=1

αiy(i) = 0,

As before, we also have that w can be expressed in terms of the αi’s as
given in Equation (6.10), so that after solving the dual problem, we can con-
tinue to use Equation (6.15) to make our predictions. Note that, somewhat
surprisingly, in adding (cid:96)1 regularization, the only change to the dual prob-
lem is that what was originally a constraint that 0 ≤ αi has now become
0 ≤ αi ≤ C. The calculation for b∗ also has to be modiﬁed (Equation 6.13 is
no longer valid); see the comments in the next section/Platt’s paper.

Also, the KKT dual-complementarity conditions (which in the next sec-
tion will be useful for testing for the convergence of the SMO algorithm)
are:

αi = 0 ⇒ y(i)(wT x(i) + b) ≥ 1
αi = C ⇒ y(i)(wT x(i) + b) ≤ 1
0 < αi < C ⇒ y(i)(wT x(i) + b) = 1.

(6.16)
(6.17)
(6.18)

Now, all that remains is to give an algorithm for actually solving the dual

problem, which we will do in the next section.

6.8 The SMO algorithm (optional reading)

The SMO (sequential minimal optimization) algorithm, due to John Platt,
gives an eﬃcient way of solving the dual problem arising from the derivation

74

of the SVM. Partly to motivate the SMO algorithm, and partly because it’s
interesting in its own right, let’s ﬁrst take another digression to talk about
the coordinate ascent algorithm.

6.8.1 Coordinate ascent

Consider trying to solve the unconstrained optimization problem

max
α

W (α1, α2, . . . , αn).

Here, we think of W as just some function of the parameters αi’s, and for now
ignore any relationship between this problem and SVMs. We’ve already seen
two optimization algorithms, gradient ascent and Newton’s method. The
new algorithm we’re going to consider here is called coordinate ascent:

Loop until convergence: {

For i = 1, . . . , n, {

αi := arg max ˆαi W (α1, . . . , αi−1, ˆαi, αi+1, . . . , αn).

}

}

Thus, in the innermost loop of this algorithm, we will hold all the variables
except for some αi ﬁxed, and reoptimize W with respect to just the parameter
αi. In the version of this method presented here, the inner-loop reoptimizes
the variables in order α1, α2, . . . , αn, α1, α2, . . .. (A more sophisticated version
might choose other orderings; for instance, we may choose the next variable
to update according to which one we expect to allow us to make the largest
increase in W (α).)

When the function W happens to be of such a form that the “arg max”
in the inner loop can be performed eﬃciently, then coordinate ascent can be
a fairly eﬃcient algorithm. Here’s a picture of coordinate ascent in action:

75

The ellipses in the ﬁgure are the contours of a quadratic function that
we want to optimize. Coordinate ascent was initialized at (2, −2), and also
plotted in the ﬁgure is the path that it took on its way to the global maximum.
Notice that on each step, coordinate ascent takes a step that’s parallel to one
of the axes, since only one variable is being optimized at a time.

6.8.2 SMO

We close oﬀ the discussion of SVMs by sketching the derivation of the SMO
algorithm.

Here’s the (dual) optimization problem that we want to solve:

maxα W (α) =

n
(cid:88)

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105).

(6.19)

i=1
s.t. 0 ≤ αi ≤ C,

i = 1, . . . , n

(6.20)

(6.21)

n
(cid:88)

i=1

αiy(i) = 0.

Let’s say we have set of αi’s that satisfy the constraints (6.20-6.21). Now,
suppose we want to hold α2, . . . , αn ﬁxed, and take a coordinate ascent step
and reoptimize the objective with respect to α1. Can we make any progress?
The answer is no, because the constraint (6.21) ensures that

α1y(1) = −

n
(cid:88)

i=2

αiy(i).

−2−1.5−1−0.500.511.522.5−2−1.5−1−0.500.511.522.576

Or, by multiplying both sides by y(1), we equivalently have

α1 = −y(1)

n
(cid:88)

i=2

αiy(i).

(This step used the fact that y(1) ∈ {−1, 1}, and hence (y(1))2 = 1.) Hence,
α1 is exactly determined by the other αi’s, and if we were to hold α2, . . . , αn
ﬁxed, then we can’t make any change to α1 without violating the con-
straint (6.21) in the optimization problem.

Thus, if we want to update some subject of the αi’s, we must update at
least two of them simultaneously in order to keep satisfying the constraints.
This motivates the SMO algorithm, which simply does the following:

Repeat till convergence {

1. Select some pair αi and αj to update next (using a heuristic that
tries to pick the two that will allow us to make the biggest progress
towards the global maximum).

2. Reoptimize W (α) with respect to αi and αj, while holding all the

other αk’s (k (cid:54)= i, j) ﬁxed.

}

To test for convergence of this algorithm, we can check whether the KKT
conditions (Equations 6.16-6.18) are satisﬁed to within some tol. Here, tol is
the convergence tolerance parameter, and is typically set to around 0.01 to
0.001. (See the paper and pseudocode for details.)

The key reason that SMO is an eﬃcient algorithm is that the update to
αi, αj can be computed very eﬃciently. Let’s now brieﬂy sketch the main
ideas for deriving the eﬃcient update.

Let’s say we currently have some setting of the αi’s that satisfy the con-
straints (6.20-6.21), and suppose we’ve decided to hold α3, . . . , αn ﬁxed, and
want to reoptimize W (α1, α2, . . . , αn) with respect to α1 and α2 (subject to
the constraints). From (6.21), we require that

α1y(1) + α2y(2) = −

n
(cid:88)

i=3

αiy(i).

Since the right hand side is ﬁxed (as we’ve ﬁxed α3, . . . αn), we can just let
it be denoted by some constant ζ:

α1y(1) + α2y(2) = ζ.

(6.22)

We can thus picture the constraints on α1 and α2 as follows:

77

From the constraints (6.20), we know that α1 and α2 must lie within the box
[0, C] × [0, C] shown. Also plotted is the line α1y(1) + α2y(2) = ζ, on which we
know α1 and α2 must lie. Note also that, from these constraints, we know
L ≤ α2 ≤ H; otherwise, (α1, α2) can’t simultaneously satisfy both the box
and the straight line constraint. In this example, L = 0. But depending on
what the line α1y(1) + α2y(2) = ζ looks like, this won’t always necessarily be
the case; but more generally, there will be some lower-bound L and some
upper-bound H on the permissible values for α2 that will ensure that α1, α2
lie within the box [0, C] × [0, C].

Using Equation (6.22), we can also write α1 as a function of α2:

α1 = (ζ − α2y(2))y(1).

(Check this derivation yourself; we again used the fact that y(1) ∈ {−1, 1} so
that (y(1))2 = 1.) Hence, the objective W (α) can be written

W (α1, α2, . . . , αn) = W ((ζ − α2y(2))y(1), α2, . . . , αn).

Treating α3, . . . , αn as constants, you should be able to verify that this is
just some quadratic function in α2. I.e., this can also be expressed in the
form aα2
2 + bα2 + c for some appropriate a, b, and c. If we ignore the “box”
constraints (6.20) (or, equivalently, that L ≤ α2 ≤ H), then we can easily
maximize this quadratic function by setting its derivative to zero and solving.
We’ll let αnew,unclipped
denote the resulting value of α2. You should also be
2
able to convince yourself that if we had instead wanted to m
ethod (RL). BDCL (Zhang and Yum, 1989) was
the best dynamic channel allocation method they found in the literature. It is
a heuristic method that assigns channels to cells as in FA, but channels can be
borrowed from neighboring cells when needed. It orders the channels in each
cell and uses this ordering to determine which channels to borrow and how
calls are dynamically reassigned channels within a cell.

Figure 14.10 shows the blocking probabilities of these methods for mean
arrival rates of 150, 200, and 350 calls/hour as well as for a case in which

294

CHAPTER 14. APPLICATIONS AND CASE STUDIES

Figure 14.10: Performance of FA, BDCL, and RL channel allocation methods
for diﬀerent mean call arrival rates.

diﬀerent cells had diﬀerent mean arrival rates. The reinforcement learning
method learned on-line. The data shown are for its asymptotic performance,
but in fact learning was rapid. The RL method blocked calls less frequently
than did the other methods for all arrival rates and soon after starting to learn.
Note that the diﬀerences between the methods decreased as the call arrival rate
increased. This is to be expected because as the system gets saturated with
calls there are fewer opportunities for a dynamic allocation method to set up
favorable usage patterns. In practice, however, it is the performance of the
unsaturated system that is most important. For marketing reasons, cellular
telephone systems are built with enough capacity that more than 10% blocking
is rare.

Nie and Haykin (1996) also studied the application of reinforcement learn-
ing to dynamic channel allocation. They formulated the problem somewhat
diﬀerently than Singh and Bertsekas did. Instead of trying to minimize the
probability of blocking a call directly, their system tried to minimize a more
indirect measure of system performance. Cost was assigned to patterns of
channel use depending on the distances between calls using the same chan-
nels. Patterns in which channels were being used by multiple calls that were
close to each other were favored over patterns in which channel-sharing calls
were far apart. Nie and Haykin compared their system with a method called
MAXAVAIL (Sivarajan, McEliece, and Ketchum, 1990), considered to be one
of the best dynamic channel allocation methods. For each new call, it selects
the channel that maximizes the total number of channels available in the en-
tire system. Nie and Haykin showed that the blocking probability achieved by
their reinforcement learning system was closely comparable to that of MAX-
AVAIL under a variety of conditions in a 49-cell, 70-channel simulation. A

01020304050Nonuniform350 calls/hour200 calls/hour150 calls/hourRLBDCLFA% BlockingTraffic14.6. JOB-SHOP SCHEDULING

295

key point, however, is that the allocation policy produced by reinforcement
learning can be implemented on-line much more eﬃciently than MAXAVAIL,
which requires so much on-line computation that it is not feasible for large
systems.

The studies we described in this section are so recent that the many ques-
tions they raise have not yet been answered. We can see, though, that there
can be diﬀerent ways to apply reinforcement learning to the same real-world
problem.
In the near future, we expect to see many reﬁnements of these
applications, as well as many new applications of reinforcement learning to
problems arising in communication systems.

14.6 Job-Shop Scheduling

Many jobs in industry and elsewhere require completing a collection of tasks
while satisfying temporal and resource constraints. Temporal constraints say
that some tasks have to be ﬁnished before others can be started; resource
constraints say that two tasks requiring the same resource cannot be done
simultaneously (e.g., the same machine cannot do two tasks at once). The
objective is to create a schedule specifying when each task is to begin and
what resources it will use that satisﬁes all the constraints while taking as
little overall time as possible. This is the job-shop scheduling problem. In its
general form, it is NP-complete, meaning that there is probably no eﬃcient
procedure for exactly ﬁnding shortest schedules for arbitrary instances of the
problem. Job-shop scheduling is usually done using heuristic algorithms that
take advantage of special properties of each speciﬁc instance.

Zhang and Dietterich (1995, 1996; Zhang, 1996) were motivated to apply
reinforcement learning to job-shop scheduling because the design of domain-
speciﬁc, heuristic algorithms can be expensive and time-consuming. Their
goal was to show how reinforcement learning can be used to learn how to
quickly ﬁnd constraint-satisfying schedules of short duration in speciﬁc do-
mains, thereby reducing the amount of hand engineering required. They ad-
dressed the NASA space shuttle payload processing problem (SSPP), which
requires scheduling the tasks required for installation and testing of shuttle
cargo bay payloads. An SSPP typically requires scheduling for two to six
shuttle missions, each requiring between 34 and 164 tasks. An example of
a task is MISSION-SEQUENCE-TEST, which has a duration of 7200 time
units and requires the following resources: two quality control oﬃcers, two
technicians, one ATE, one SPCDS, and one HITS. Some resources are divided
into pools, and if a task needs more than one resource of a speciﬁc type, the
resources must belong to the same pool, and the pool has to be the right one.

296

CHAPTER 14. APPLICATIONS AND CASE STUDIES

For example, if a task needs two quality control oﬃcers, they both have to be
in the pool of quality control oﬃcers working on the same shift at the right
site.
It is not too hard to ﬁnd a conﬂict-free schedule for a job, one that
meets all the temporal and resource constraints, but the objective is to ﬁnd a
conﬂict-free schedule with the shortest possible total duration, which is much
more diﬃcult.

How can you do this using reinforcement learning? Job-shop scheduling
is usually formulated as a search in the space of schedules, what is called a
discrete, or combinatorial, optimization problem. A typical solution method
would sequentially generate schedules, attempting to improve each over its
predecessor in terms of constraint violations and duration (a hill-climbing, or
local search, method). You could think of this as a nonassociative reinforce-
ment learning problem of the type we discussed in Chapter 2 with a very large
number of possible actions: all the possible schedules! But aside from the
problem of having so many actions, any solution obtained this way would just
be a single schedule for a single job instance. In contrast, what Zhang and
Dietterich wanted their learning system to end up with was a policy that could
quickly ﬁnd good schedules for any SSPP. They wanted it to learn a skill for
job-shop scheduling in this speciﬁc domain.

For clues about how to do this, they looked to an existing optimization
approach to SSPP, in fact, the one actually in use by NASA at the time of
their research: the iterative repair method developed by Zweben and Daun
(1994). The starting point for the search is a critical path schedule, a schedule
that meets the temporal constraints but ignores the resource constraints. This
schedule can be constructed eﬃciently by scheduling each task prior to launch
as late as the temporal constraints permit, and each task after landing as early
as these constraints permit. Resource pools are assigned randomly. Two types
of operators are used to modify schedules. They can be applied to any task
that violates a resource constraint. A Reassign-Pool operator changes the
pool assigned to one of the task’s resources. This type of operator applies only
if it can reassign a pool so that the resource requirement is satisﬁed. A Move
operator moves a task to the ﬁrst earlier or later time at which its resource
needs can be satisﬁed and uses the critical path method to reschedule all of
the task’s temporal dependents.

At each step of the iterative repair search, one operator is applied to the
current schedule, selected according to the following rules. The earliest task
with a resource constraint violation is found, and a Reassign-Pool oper-
ator is applied to this task if possible. If more than one applies, that is, if
several diﬀerent pool reassignments are possible, one is selected at random.
If no Reassign-Pool operator applies, then a Move operator is selected at
random based on a heuristic that prefers short-distance moves of tasks having

14.6. JOB-SHOP SCHEDULING

297

few temporal dependents and whose resource requirements are close to the
task’s overallocation. After an operator is applied, the number of constraint
violations of the resulting schedule is determined. A simulated annealing pro-
cedure is used decide whether to accept or reject this new schedule. If ∆V
denotes the number of constraint violations removed by the repair, then the
∆V /T ), where T is the cur-
new schedule is accepted with probability exp(
rent computational temperature that is gradually decreased throughout the
search. If accepted, the new schedule becomes the current schedule for the
next iteration; otherwise, the algorithm attempts to repair the old schedule
again, which will usually produce diﬀerent results due to the random decisions
involved. Search stops when all constraints are satisﬁed. Short schedules are
obtained by running the algorithm several times and selecting the shortest of
the resulting conﬂict-free schedules.

−

−

Zhang and Dietterich treated entire schedules as states in the sense of
reinforcement learning. The actions were the applicable Reassign-Pool and
Move operators, typically numbering about 20. The problem was treated
as episodic, each episode starting with the same critical path schedule that
the iterative repair algorithm would start with and ending when a schedule
was found that did not violate any constraint. The initial state—a critical
path schedule—is denoted S0. The rewards were designed to promote the
quick construction of conﬂict-free schedules of short duration. The system
0.001) on each step that resulted in a
received a small negative reward (
schedule that still violated a constraint. This encouraged the agent to ﬁnd
conﬂict-free schedules quickly, that is, with a small number of repairs to S0.
Encouraging the system to ﬁnd short schedules is more diﬃcult because what
it means for a schedule to be short depends on the speciﬁc SSPP instance. The
shortest schedule for a diﬃcult instance, one with a lot of tasks and constraints,
will be longer than the shortest schedule for a simpler instance. Zhang and
Dietterich devised a formula for a resource dilation factor (RDF), intended to
be an instance-independent measure of a schedule’s duration. To account for
an instance’s intrinsic diﬃculty, the formula makes use of a measure of the
resource overallocation of S0. Since longer schedules tend to produce larger
RDFs, the negative of the RDF of the ﬁnal conﬂict-free schedule was used as
a reward at the end of each episode. With this reward function, if it takes N
repairs starting from a schedule s to obtain a ﬁnal conﬂict-free schedule, Sf ,
0.001(N
the return from s is

RDF (Sf )

1).

−

−

−

This reward function was designed to try to make a system learn to satisfy
the two goals of ﬁnding conﬂict-free schedules of short duration and ﬁnding
conﬂict-free schedules quickly. But the reinforcement learning system really
has only one goal—maximizing expected return—so the particular reward val-
ues determine how a learning system will tend to trade oﬀ these two goals.

298

CHAPTER 14. APPLICATIONS AND CASE STUDIES

0.001 means that the
Setting the immediate reward to the small value of
learning system will regard one repair, one step in the scheduling process, as
being worth 0.001 units of RDF. So, for example, if from some schedule it is
possible to produce a conﬂict-free schedule with one repair or with two, an
optimal policy will take extra repair only if it promises a reduction in ﬁnal
RDF of more than 0.001.

−

Zhang and Dietterich used TD(λ) to learn the value function. Function
approximation was by a multilayer neural network trained by backpropagating
TD errors. Actions were selected by an ε-greedy policy, with ε decreasing
during learning. One-step lookahead search was used to ﬁnd the greedy action.
Their knowledge of the problem made it easy to predict the schedules that
would result from each repair operation. They experimented with a number of
modiﬁcations to this basic procedure to improve its performance. One was to
use the TD(λ) algorithm backward after each episode, with the eligibility trace
extending to future rather than to past states. Their results suggested that this
was more accurate and eﬃcient than forward learning. In updating the weights
of the network, they also sometimes performed multiple weight updates when
the TD error was large. This is apparently equivalent to dynamically varying
the step-size parameter in an error-dependent way during learning.

They also tried an experience replay technique due to Lin (1992). At any
point in learning, the agent remembered the best episode up to that point.
After every four episodes, it replayed this remembered episode, learning from
it as if it were a new episode. At the start of training, they similarly allowed
the system to learn from episodes generated by a good scheduler, and these
could also be replayed later in learning. To make the lookahead search faster
for large-scale problems, which typically had a branching factor of about 20,
they used a variant they called random sample greedy search that estimated
the greedy action by considering only random samples of actions, increasing
the sample size until a preset conﬁdence was reached that the greedy action of
the sample was the true greedy action. Finally, having discovered that learning
could be slowed considerably by excessive looping in the scheduling process,
they made their system explicitly check for loops and alter action selections
when a loop was detected. Although all of these techniques could improve the
eﬃciency of learning, it is not clear how crucial all of them were for the success
of the system.

Zhang and Dietterich experimented with two diﬀerent network architec-
tures. In the ﬁrst version of their system, each schedule was represented using
a set of 20 handcrafted features. To deﬁne these features, they studied small
scheduling problems to ﬁnd features that had some ability to predict RDF. For
example, experience with small problems showed that only four of the resource
pools tended to cause allocation problems. The mean and standard deviation

14.6. JOB-SHOP SCHEDULING

299

of each of these pools’ unused portions over the entire schedule were computed,
resulting in 10 real-valued features. Two other features were the RDF of the
current schedule and the percentage of its duration during which it violated
resource constraints. The network had 20 input units, one for each feature, a
hidden layer of 40 sigmoidal units, and an output layer of 8 sigmoidal units.
The output units coded the value of a schedule using a code in which, roughly,
the location of the activity peak over the 8 units represented the value. Us-
ing the appropriate TD error, the network weights were updated using error
backpropagation, with the multiple weight-update technique mentioned above.

The second version of the system (Zhang and Dietterich, 1996) used a
more
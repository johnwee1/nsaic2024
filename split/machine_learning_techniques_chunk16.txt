wiggles around to get as close as possi‐
ble to the training instances.

130 

| 

Chapter 4: Training Models

Figure 4-14. High-degree Polynomial Regression

This  high-degree  Polynomial  Regression  model  is  severely  overfitting  the  training
data, while the linear model is underfitting it. The model that will generalize best in
this case is the quadratic model, which makes sense because the data was generated
using a quadratic model. But in general you won’t know what function generated the
data, so how can you decide how complex your model should be? How can you tell
that your model is overfitting or underfitting the data?

In Chapter 2 you used cross-validation to get an estimate of a model’s generalization
performance.  If  a  model  performs  well  on  the  training  data  but  generalizes  poorly
according  to  the  cross-validation  metrics,  then  your  model  is  overfitting.  If  it  per‐
forms poorly on both, then it is underfitting. This is one way to tell when a model is
too simple or too complex.

Another way to tell is to look at the learning curves: these are plots of the model’s per‐
formance on the training set and the validation set as a function of the training set
size (or the training iteration). To generate the plots, train the model several times on
different sized subsets of the training set. The following code defines a function that,
given some training data, plots the learning curves of a model:

Learning Curves 

| 

131

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model, X, y):
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
    train_errors, val_errors = [], []
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))
    plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train")
    plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val")

Let’s look at the learning curves of the plain Linear Regression model (a straight line;
see Figure 4-15):

lin_reg = LinearRegression()
plot_learning_curves(lin_reg, X, y)

Figure 4-15. Learning curves

This model that’s underfitting deserves a bit of explanation. First, let’s look at the per‐
formance on the training data: when there are just one or two instances in the train‐
ing set, the model can fit them perfectly, which is why the curve starts at zero. But as
new instances are added to the training set, it becomes impossible for the model to fit
the training data perfectly, both because the data is noisy and because it is not linear
at  all.  So  the  error  on  the  training  data  goes  up  until  it  reaches  a  plateau,  at  which
point adding new instances to the training set doesn’t make the average error much
better  or  worse.  Now  let’s  look  at  the  performance  of  the  model  on  the  validation
data. When the model is trained on very few training instances, it is incapable of gen‐
eralizing properly, which is why the validation error is initially quite big. Then, as the

132 

| 

Chapter 4: Training Models

model  is  shown  more  training  examples,  it  learns,  and  thus  the  validation  error
slowly goes down. However, once again a straight line cannot do a good job modeling
the data, so the error ends up at a plateau, very close to the other curve.

These  learning  curves  are  typical  of  a  model  that’s  underfitting.  Both  curves  have
reached a plateau; they are close and fairly high.

If your model is underfitting the training data, adding more train‐
ing examples will not help. You need to use a more complex model
or come up with better features.

Now let’s look at the learning curves of a 10th-degree polynomial model on the same
data (Figure 4-16):

from sklearn.pipeline import Pipeline

polynomial_regression = Pipeline([
        ("poly_features", PolynomialFeatures(degree=10, include_bias=False)),
        ("lin_reg", LinearRegression()),
    ])

plot_learning_curves(polynomial_regression, X, y)

Figure 4-16. Learning curves for the 10th-degree polynomial model

These learning curves look a bit like the previous ones, but there are two very impor‐
tant differences:

• The  error  on  the  training  data  is  much  lower  than  with  the  Linear  Regression

model.

Learning Curves 

| 

133

• There is a gap between the curves. This means that the model performs signifi‐
cantly better on the training data than on the validation data, which is the hall‐
mark of an overfitting model. If you used a much larger training set, however, the
two curves would continue to get closer.

One way to improve an overfitting model is to feed it more training
data until the validation error reaches the training error.

The Bias/Variance Trade-off
An  important  theoretical  result  of  statistics  and  Machine  Learning  is  the  fact  that  a
model’s  generalization  error  can  be  expressed  as  the  sum  of  three  very  different
errors:

Bias

This part of the generalization error is due to wrong assumptions, such as assum‐
ing that the data is linear when it is actually quadratic. A high-bias model is most
likely to underfit the training data.8

Variance

This  part  is  due  to  the  model’s  excessive  sensitivity  to  small  variations  in  the
training data. A model with many degrees of freedom (such as a high-degree pol‐
ynomial model) is likely to have high variance and thus overfit the training data.

Irreducible error

This  part  is  due  to  the  noisiness  of  the  data  itself.  The  only  way  to  reduce  this
part of the error is to clean up the data (e.g., fix the data sources, such as broken
sensors, or detect and remove outliers).

Increasing a model’s complexity will typically increase its variance and reduce its bias.
Conversely, reducing a model’s complexity increases its bias and reduces its variance.
This is why it is called a trade-off.

Regularized Linear Models
As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the
model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be

8 This notion of bias is not to be confused with the bias term of linear models.

134 

| 

Chapter 4: Training Models

for it to overfit the data. A simple way to regularize a polynomial model is to reduce
the number of polynomial degrees.

For a linear model, regularization is typically achieved by constraining the weights of
the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,
which implement three different ways to constrain the weights.

Ridge Regression
Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin‐
2 is added to the cost function.
ear Regression: a regularization term equal to α∑i = 1
This  forces  the  learning  algorithm  to  not  only  fit  the  data  but  also  keep  the  model
weights as small as possible. Note that the regularization term should only be added
to the cost function during training. Once the model is trained, you want to use the
unregularized performance measure to evaluate the model’s performance.

θi

n

It is quite common for the cost function used during training to be
different  from  the  performance  measure  used  for  testing.  Apart
from regularization, another reason they might be different is that a
good  training  cost  function  should  have  optimization-friendly
derivatives, while the performance measure used for testing should
be as close as possible to the final objective. For example, classifiers
are  often  trained  using  a  cost  function  such  as  the  log  loss  (dis‐
cussed in a moment) but evaluated using precision/recall.

The hyperparameter α controls how much you want to regularize the model. If α = 0,
then Ridge Regression is just Linear Regression. If α is very large, then all weights end
up very close to zero and the result is a flat line going through the data’s mean. Equa‐
tion 4-8 presents the Ridge Regression cost function.9

Equation 4-8. Ridge Regression cost function

J θ = MSE θ + α

1
n
2 ∑i = 1

2

θi

Note  that  the  bias  term  θ0  is  not  regularized  (the  sum  starts  at  i  =  1,  not  0).  If  we
define  w  as  the  vector  of  feature  weights  (θ1  to  θn),  then  the  regularization  term  is

9 It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this
notation throughout the rest of this book. The context will make it clear which cost function is being dis‐
cussed.

Regularized Linear Models 

| 

135

equal to ½(∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.10 For
Gradient Descent, just add αw to the MSE gradient vector (Equation 4-6).

It  is  important  to  scale  the  data  (e.g.,  using  a  StandardScaler)
before performing Ridge Regression, as it is sensitive to the scale of
the input features. This is true of most regularized models.

Figure 4-17 shows several Ridge models trained on some linear data using different α
values. On the left, plain Ridge models are used, leading to linear predictions. On the
right,  the  data  is  first  expanded  using  PolynomialFeatures(degree=10),  then  it  is
scaled using a StandardScaler, and finally the Ridge models are applied to the result‐
ing  features:  this  is  Polynomial  Regression  with  Ridge  regularization.  Note  how
increasing  α  leads  to  flatter  (i.e.,  less  extreme,  more  reasonable)  predictions,  thus
reducing the model’s variance but increasing its bias.

Figure 4-17. A linear model (left) and a polynomial model (right), both with various lev‐
els of Ridge regularization

As with Linear Regression, we can perform Ridge Regression either by computing a
closed-form equation or by performing Gradient Descent. The pros and cons are the

10 Norms are discussed in Chapter 2.

136 

| 

Chapter 4: Training Models

same. Equation 4-9 shows the closed-form solution, where A is the (n + 1) × (n + 1)
identity matrix,11 except with a 0 in the top-left cell, corresponding to the bias term.

Equation 4-9. Ridge Regression closed-form solution

θ = X⊺X + αA

−1

  X⊺   y

Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐
tion (a variant of Equation 4-9 that uses a matrix factorization technique by André-
Louis Cholesky):

>>> from sklearn.linear_model import Ridge
>>> ridge_reg = Ridge(alpha=1, solver="cholesky")
>>> ridge_reg.fit(X, y)
>>> ridge_reg.predict([[1.5]])
array([[1.55071465]])

And using Stochastic Gradient Descent:12

>>> sgd_reg = SGDRegressor(penalty="l2")
>>> sgd_reg.fit(X, y.ravel())
>>> sgd_reg.predict([[1.5]])
array([1.47012588])

The  penalty  hyperparameter  sets  the  type  of  regularization  term  to  use.  Specifying
"l2" indicates that you want SGD to add a regularization term to the cost function
equal  to  half  the  square  of  the  ℓ2  norm  of  the  weight  vector:  this  is  simply  Ridge
Regression.

Lasso Regression
Least  Absolute  Shrinkage  and  Selection  Operator  Regression  (usually  simply  called
Lasso Regression) is another regularized version of Linear Regression: just like Ridge
Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm
of the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10).

Equation 4-10. Lasso Regression cost function

J θ = MSE θ + α∑i = 1

n

θi

11 A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).
12 Alternatively you can use the Ridge class with the "sag" solver. Stochastic Average GD is a variant of Stochas‐
tic GD. For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient
Algorithm” by Mark Schmidt et al. from the University of British Columbia.

Regularized Linear Models 

| 

137

Figure  4-18  shows  the  same  thing  as  Figure  4-17  but  replaces  Ridge  models  with
Lasso models and uses smaller α values.

Figure 4-18. A linear model (left) and a polynomial model (right), both using various
levels of Lasso regularization

An  important  characteristic  of  Lasso  Regression  is  that  it  tends  to  eliminate  the
weights  of  the  least  important  features  (i.e.,  set  them  to  zero).  For  example,  the
dashed  line  in  the  righthand  plot  in  Figure  4-18  (with  α  =  10-7)  looks  quadratic,
almost  linear:  all  the  weights  for  the  high-degree  polynomial  features  are  equal  to
zero. In other words, Lasso Regression automatically performs feature selection and
outputs a sparse model (i.e., with few nonzero feature weights).

You can get a sense of why this is the case by looking at Figure 4-19: the axes repre‐
sent  two  model  parameters,  and  the  background  contours  represent  different  loss
functions.  In  the  top-left  plot,  the  contours  represent  the  ℓ1  loss  (|θ1|  +  |θ2|),  which
drops linearly as you get closer to any axis. For example, if you initialize the model
parameters  to  θ1  =  2  and  θ2  =  0.5,  running  Gradient  Descent  will  decrement  both
parameters equally (as represented by the dashed yellow line); therefore θ2 will reach
0 first (since it was closer to 0 to begin with). After that, Gradient Descent will roll
down the gutter until it reaches θ1 = 0 (with a bit of bouncing around, since the gradi‐
ents of ℓ1 never get close to 0: they are either –1 or 1 for each parameter). In the top-
right plot, the contours represent Lasso’s cost function (i.e., an MSE cost function plus
an ℓ1 loss). The small white circles show the path that Gradient Descent takes to opti‐
mize  some  model  parameters  that  were  initialized  around  θ1  =  0.25  and  θ2  =  –1:
notice once again how the path quickly reaches θ2 = 0, then rolls down the gutter and
ends up bouncing around the global optimum (represented by the red square). If we
increased α, the global optimum would move left along the dashed yellow line, while

138 

| 

Chapter 4: Training Models

if we decreased α, the global optimum would move right (in this example, the optimal
parameters for the unregularized MSE are θ1 = 2 and θ2 = 0.5).

Figure 4-19. Lasso versus Ridge regularization

The  two  bottom  plots  show  the  same  thing  but  with  an  ℓ2  penalty  instead.  In  the
bottom-left plot, you can see that the ℓ2 loss decreases with the distance to the origin,
so Gradient Descent just takes a straight path toward that point. In the bottom-right
plot, the contours represent Ridge Regression’s cost function (i.e., an MSE cost func‐
tion plus an ℓ2 loss). There are two main differences with Lasso. First, the gradients
get smaller as the parameters approach the global optimum, so Gradient Descent nat‐
urally slows down, which helps convergence (as there is no bouncing around). Sec‐
ond, the optimal parameters (represented by the red square) get closer and closer to
the origin when you increase α, but they never get eliminated entirely.

To avoid Gradient Descent from bouncing around the optimum at
the end when using Lasso, you need to gradually reduce the learn‐
ing  rate  during  training  (it  will  still  bounce  around  the  optimum,
but the steps will get smaller and smaller, so it will converge).

Regularized Linear Models 


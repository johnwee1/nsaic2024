

For every state, update

V (s) := R(s) + max
aâˆˆA

Î³

(cid:88)

s(cid:48)

Psa(s(cid:48))V (s(cid:48)).

(15.4)

This algorithm can be thought of as repeatedly trying to update the

estimated value function using Bellman Equations (15.2).

There are two possible ways of performing the updates in the inner loop of
the algorithm. In the ï¬rst, we can ï¬rst compute the new values for V (s) for
every state s, and then overwrite all the old values with the new values. This
is called a synchronous update. In this case, the algorithm can be viewed as
implementing a â€œBellman backup operatorâ€ that takes a current estimate of
the value function, and maps it to a new estimate. (See homework problem
for details.) Alternatively, we can also perform asynchronous updates.
Here, we would loop over the states (in some order), updating the values one
at a time.

Under either synchronous or asynchronous updates, it can be shown that
value iteration will cause V to converge to V âˆ—. Having found V âˆ—, we can
then use Equation (15.3) to ï¬nd the optimal policy.

Apart from value iteration, there is a second standard algorithm for ï¬nd-
ing an optimal policy for an MDP. The policy iteration algorithm proceeds
as follows:

Thus, the inner-loop repeatedly computes the value function for the cur-
rent policy, and then updates the policy using the current value function.
(The policy Ï€ found in step (b) is also called the policy that is greedy with
respect to V .) Note that step (a) can be done via solving Bellmanâ€™s equa-
tions as described earlier, which in the case of a ï¬xed policy, is just a set of
|S| linear equations in |S| variables.

After at most a ï¬nite number of iterations of this algorithm, V will con-

verge to V âˆ—, and Ï€ will converge to Ï€âˆ—.2

2Note that value iteration cannot reach the exact V âˆ— in a ï¬nite number of iterations,

192

Algorithm 5 Policy Iteration

1: Initialize Ï€ randomly.
2: for until convergence do
Let V := V Ï€.
3:
For each state s, let
4:

(cid:46) typically by linear system solver

Ï€(s) := arg max
aâˆˆA

(cid:88)

s(cid:48)

Psa(s(cid:48))V (s(cid:48)).

Both value iteration and policy iteration are standard algorithms for solv-
ing MDPs, and there isnâ€™t currently universal agreement over which algo-
rithm is better. For small MDPs, policy iteration is often very fats and
converges with very few iterations. However, for MDPs with large state
spaces, solving for V Ï€ explicitly would involve solving a large system of lin-
ear equations, and could be diï¬ƒcult (and note that one has to solve the
linear system multiple times in policy iteration). In these problems, value
iteration may be preferred. For this reason, in practice value iteration seems
to be used more often than policy iteration. For some more discussions on
the comparison and connection of value iteration and policy iteration, please
see Section 15.5.

15.3 Learning a model for an MDP

So far, we have discussed MDPs and algorithms for MDPs assuming that the
state transition probabilities and rewards are known. In many realistic prob-
lems, we are not given state transition probabilities and rewards explicitly,
but must instead estimate them from data. (Usually, S, A and Î³ are known.)
For example, suppose that, for the inverted pendulum problem (see prob-

whereas policy iteration with an exact linear system solver, can. This is because when
the actions space and policy space are discrete and ï¬nite, and once the policy reaches the
optimal policy in policy iteration, then it will not change at all. On the other hand, even
though value iteration will converge to the V âˆ—, but there is always some non-zero error in
the learned value function.

lem set 4), we had a number of trials in the MDP, that proceeded as follows:

193

a(1)
0âˆ’â†’ s(1)
1
a(2)
0âˆ’â†’ s(2)
1

a(1)
1âˆ’â†’ s(1)
2
a(2)
1âˆ’â†’ s(2)
2

a(1)
2âˆ’â†’ s(1)
3
a(2)
2âˆ’â†’ s(2)
3

a(1)
3âˆ’â†’ . . .
a(2)
3âˆ’â†’ . . .

s(1)
0

s(2)
0
. . .

i

Here, s(j)

is the state we were at time i of trial j, and a(j)
is the cor-
responding action that was taken from that state. In practice, each of the
trials above might be run until the MDP terminates (such as if the pole falls
over in the inverted pendulum problem), or it might be run for some large
but ï¬nite number of timesteps.

i

Given this â€œexperienceâ€ in the MDP consisting of a number of trials,
we can then easily derive the maximum likelihood estimates for the state
transition probabilities:

Psa(s(cid:48)) =

#times took we action a in state s and got to s(cid:48)
#times we took action a in state s

(15.5)

Or, if the ratio above is â€œ0/0â€â€”corresponding to the case of never having
taken action a in state s beforeâ€”the we might simply estimate Psa(s(cid:48)) to be
1/|S|. (I.e., estimate Psa to be the uniform distribution over all states.)

Note that, if we gain more experience (observe more trials) in the MDP,
there is an eï¬ƒcient way to update our estimated state transition probabilities
using the new experience. Speciï¬cally, if we keep around the counts for both
the numerator and denominator terms of (15.5), then as we observe more
trials, we can simply keep accumulating those counts. Computing the ratio
of these counts then given our estimate of Psa.

Using a similar procedure, if R is unknown, we can also pick our estimate
of the expected immediate reward R(s) in state s to be the average reward
observed in state s.

Having learned a model for the MDP, we can then use either value it-
eration or policy iteration to solve the MDP using the estimated transition
probabilities and rewards. For example, putting together model learning and
value iteration, here is one possible algorithm for learning in an MDP with
unknown state transition probabilities:

1. Initialize Ï€ randomly.

2. Repeat {

(a) Execute Ï€ in the MDP for some number of trials.

194

(b) Using the accumulated experience in the MDP, update our esti-

mates for Psa (and R, if applicable).

(c) Apply value iteration with the estimated state transition probabil-

ities and rewards to get a new estimated value function V .

(d) Update Ï€ to be the greedy policy with respect to V .

}

We note that, for this particular algorithm, there is one simple optimiza-
tion that can make it run much more quickly. Speciï¬cally, in the inner loop
of the algorithm where we apply value iteration, if instead of initializing value
iteration with V = 0, we initialize it with the solution found during the pre-
vious iteration of our algorithm, then that will provide value iteration with
a much better initial starting point and make it converge more quickly.

15.4 Continuous state MDPs

So far, weâ€™ve focused our attention on MDPs with a ï¬nite number of states.
We now discuss algorithms for MDPs that may have an inï¬nite number of
states. For example, for a car, we might represent the state as (x, y, Î¸, Ë™x, Ë™y, Ë™Î¸),
comprising its position (x, y); orientation Î¸; velocity in the x and y directions
Ë™x and Ë™y; and angular velocity Ë™Î¸. Hence, S = R6 is an inï¬nite set of states,
because there is an inï¬nite number of possible positions and orientations
for the car.3 Similarly, the inverted pendulum you saw in PS4 has states
(x, Î¸, Ë™x, Ë™Î¸), where Î¸ is the angle of the pole. And, a helicopter ï¬‚ying in 3d
space has states of the form (x, y, z, Ï†, Î¸, Ïˆ, Ë™x, Ë™y, Ë™z, Ë™Ï†, Ë™Î¸, Ë™Ïˆ), where here the roll
Ï†, pitch Î¸, and yaw Ïˆ angles specify the 3d orientation of the helicopter.

In this section, we will consider settings where the state space is S = Rd,

and describe ways for solving such MDPs.

15.4.1 Discretization

Perhaps the simplest way to solve a continuous-state MDP is to discretize
the state space, and then to use an algorithm like value iteration or policy
iteration, as described previously.

For example, if we have 2d states (s1, s2), we can use a grid to discretize

the state space:

3Technically, Î¸ is an orientation and so the range of Î¸ is better written Î¸ âˆˆ [âˆ’Ï€, Ï€) than

Î¸ âˆˆ R; but for our purposes, this distinction is not important.

195

[t]

Here, each grid cell represents a separate discrete state Â¯s. We can
then approximate the continuous-state MDP via a discrete-state one
( Â¯S, A, {PÂ¯sa}, Î³, R), where Â¯S is the set of discrete states, {PÂ¯sa} are our state
transition probabilities over the discrete states, and so on. We can then use
value iteration or policy iteration to solve for the V âˆ—(Â¯s) and Ï€âˆ—(Â¯s) in the
discrete state MDP ( Â¯S, A, {PÂ¯sa}, Î³, R). When our actual system is in some
continuous-valued state s âˆˆ S and we need to pick an action to execute, we
compute the corresponding discretized state Â¯s, and execute action Ï€âˆ—(Â¯s).

This discretization approach can work well for many problems. However,
there are two downsides. First, it uses a fairly naive representation for V âˆ—
(and Ï€âˆ—). Speciï¬cally, it assumes that the value function is takes a constant
value over each of the discretization intervals (i.e., that the value function is
piecewise constant in each of the gridcells).

To better understand the limitations of such a representation, consider a

supervised learning problem of ï¬tting a function to this dataset:

[t]

123456781.522.533.544.555.5xyClearly, linear regression would do ï¬ne on this problem. However, if we
instead discretize the x-axis, and then use a representation that is piecewise
constant in each of the discretization intervals, then our ï¬t to the data would
look like this:

196

[t]

This piecewise constant representation just isnâ€™t a good representation for
many smooth functions. It results in little smoothing over the inputs, and no
generalization over the diï¬€erent grid cells. Using this sort of representation,
we would also need a very ï¬ne discretization (very small grid cells) to get a
good approximation.

A second downside of this representation is called the curse of dimen-
sionality. Suppose S = Rd, and we discretize each of the d dimensions of the
state into k values. Then the total number of discrete states we have is kd.
This grows exponentially quickly in the dimension of the state space d, and
thus does not scale well to large problems. For example, with a 10d state, if
we discretize each state variable into 100 values, we would have 10010 = 1020
discrete states, which is far too many to represent even on a modern desktop
computer.

As a rule of thumb, discretization usually works extremely well for 1d
and 2d problems (and has the advantage of being simple and quick to im-
plement). Perhaps with a little bit of cleverness and some care in choosing
the discretization method, it often works well for problems with up to 4d
states. If youâ€™re extremely clever, and somewhat lucky, you may even get it
to work for some 6d problems. But it very rarely works for problems any
higher dimensional than that.

123456781.522.533.544.555.5xy197

15.4.2 Value function approximation

We now describe an alternative method for ï¬nding policies in continuous-
state MDPs, in which we approximate V âˆ— directly, without resorting to dis-
cretization. This approach, called value function approximation, has been
successfully applied to many RL problems.

Using a model or simulator

To develop a value function approximation algorithm, we will assume that
we have a model, or simulator, for the MDP. Informally, a simulator is
a black-box that takes as input any (continuous-valued) state st and action
at, and outputs a next-state st+1 sampled according to the state transition
probabilities Pstat:

[t]

There are several ways that one can get such a model. One is to use
physics simulation. For example, the simulator for the inverted pendulum
in PS4 was obtained by using the laws of physics to calculate what position
and orientation the cart/pole will be in at time t + 1, given the current state
at time t and the action a taken, assuming that we know all the parameters
of the system such as the length of the pole, the mass of the pole, and so
on. Alternatively, one can also use an oï¬€-the-shelf physics simulation software
package which takes as input a complete physical description of a mechanical
system, the current state st and action at, and computes the state st+1 of the
system a small fraction of a second into the future.4

An alternative way to get a model is to learn one from data collected in
the MDP. For example, suppose we execute n trials in which we repeatedly
take actions in an MDP, each trial for T timesteps. This can be done picking
actions at random, executing some speciï¬c policy, or via some other way of

4Open Dynamics Engine (http://www.ode.com) is one example of a free/open-source
physics simulator that can be used to simulate systems like the inverted pendulum, and
that has been a reasonably popular choice among RL researchers.

choosing actions. We would then observe n state sequences like the following:

198

a(1)
0âˆ’â†’ s(1)
1
a(2)
0âˆ’â†’ s(2)
1

a(1)
1âˆ’â†’ s(1)
2
a(2)
1âˆ’â†’ s(2)
2

a(1)
2âˆ’â†’ Â· Â· Â·

a(2)
2âˆ’â†’ Â· Â· Â·

a(1)
T âˆ’1âˆ’â†’ s(1)
T
a(2)
T âˆ’1âˆ’â†’ s(2)
T

s(1)
0

s(2)
0
Â· Â· Â·

s(n)
0

a(n)
0âˆ’â†’ s(n)

1

a(n)
1âˆ’â†’ s(n)

2

a(n)
2âˆ’â†’ Â· Â· Â·

a(n)
T âˆ’1âˆ’â†’ s(n)
T

We can then apply a learning algorithm to predict st+1 as a function of st
and at.

For example, one may choose to learn a linear model of the form

st+1 = Ast + Bat,

(15.6)

using an algorithm similar to linear regression. Here, the parameters of the
model are the matrices A and B, and we can estimate them using the data
collected from our n trials, by picking

arg min
A,B

n
(cid:88)

T âˆ’1
(cid:88)

i=1

t=0

(cid:13)
(cid:13)s(i)
(cid:13)

t+1 âˆ’

(cid:16)

As(i)

t + Ba(i)

t

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

.

We could also potentially use other loss functions for learning the model.
For example, it has been found in recent work Luo et al. [2018] that using
(cid:107) Â· (cid:107)2 norm (without the square) may be helpful in certain cases.

Having learned A and B, one option is to build a deterministic model,
in which given an input st and at, the output st+1 is exactly determined.
Speciï¬cally, we always compute st+1 according to Equation (15.6). Alter-
natively, we may also build a stochastic model, in which st+1 is a random
function of the inputs, by modeling it as

st+1 = Ast + Bat + (cid:15)t,

where here (cid:15)t is a noise term, usually modeled as (cid:15)t âˆ¼ N (0, Î£). (The covari-
ance matrix Î£ can also be estimated from data in a straightforward way.)

Here, weâ€™ve written the next-state st+1 as a linear function of the current
state and action; but of course, non-linear functions are also possible. Specif-
ically, one can learn a model st+1 = AÏ†s(st) + BÏ†a(at), where Ï†s and Ï†a are
some non-linear feature mappings of the states and actions. Alternatively,
one can also use non-linear learning algorithms, such as locally weighted lin-
ear regression, to learn to estimate st+1 as a function of st and at. These
approaches can also be used to build either deterministic or stochastic sim-
ulators of an MDP.

199

Fitted value iteration

We now describe the ï¬tted value iteration algorithm for approximating
the value function of a continuous state MDP. In the sequel, we will assume
that the problem has a continuous state space S = Rd, but that the action
space A is small and discrete.5

Recall that in value iteration, we would like to perform the update

V (s)

:= R(s) + Î³ max

a

= R(s) + Î³ max

a

(cid:90)

Psa
ion we discuss several methods for directly estimating
the partition function.
Estimating the partition function can be important because we require it if
we wish to compute the normalized likelihood of data. This is often important in
evaluating the model, monitoring training performance, and comparing models to
each other.
For example, imagine we have two models: model MA deï¬?ning a probability distribution p A(x;Î¸ A ) = Z1A pÌƒA (x; Î¸A ) and model MB deï¬?ning a probability
distribution p B (x;Î¸ B ) = Z1B pÌƒB (x; Î¸B ). A common way to compare the models
is to evaluate and compare the likelihood that both models assign to an i.i.d.
test
Suppose
the test set consists of m examples {x(1) , . . . , x(m) }. If
î?‘ dataset.
î?‘
( i)
( i)
i pA (x ; Î¸ A ) >
i pB (x ; Î¸B ) or equivalently if
î?˜
î?˜
log pA (x(i); Î¸A ) âˆ’
log pB (x(i); Î¸B ) > 0,
(18.38)
i

i

then we say that M A is a better model than M B (or, at least, it is a better model
of the test set), in the sense that it has a better test log-likelihood. Unfortunately,
testing whether this condition holds requires knowledge of the partition function.
Unfortunately, equation 18.38 seems to require evaluating the log probability that
the model assigns to each point, which in turn requires evaluating the partition
function. We can simplify the situation slightly by re-arranging equation 18.38
into a form where we need to know only the ratio of the two modelâ€™s partition
functions:
î€ 
î€¡
(
i
)
î?˜
î?˜
î?˜
pÌƒA (x ; Î¸A )
Z(Î¸A )
log p A (x(i); Î¸A )âˆ’
log p B (x(i); Î¸B ) =
log
.
âˆ’m log
(
i
)
pÌƒ
(x
;
Î¸
)
Z(Î¸
)
B
B
B
i
i
i
(18.39)
623

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

We can thus determine whether M A is a better model than M B without knowing
the partition function of either model but only their ratio. As we will see shortly,
we can estimate this ratio using importance sampling, provided that the two models
are similar.
If, however, we wanted to compute the actual probability of the test data under
either MA or M B , we would need to compute the actual value of the partition
Z (Î¸ )
functions. That said, if we knew the ratio of two partition functions, r = Z (Î¸ B
,
A)
and we knew the actual value of just one of the two, say Z (Î¸A ), we could compute
the value of the other:
Z(Î¸B ) = rZ (Î¸A) =

Z(Î¸B )
Z(Î¸A).
Z(Î¸A )

(18.40)

A simple way to estimate the partition function is to use a Monte Carlo
method such as simple importance sampling. We present the approach in terms
of continuous variables using integrals, but it can be readily applied to discrete
variables by replacing the integrals with summation. We use a proposal distribution
p0(x) = Z10 pÌƒ0(x) which supports tractable sampling and tractable evaluation of
both the partition function Z0 and the unnormalized distribution pÌƒ 0(x).
î?š
Z1 = pÌƒ 1 (x) dx
(18.41)
î?š
p0(x)
=
pÌƒ1(x) dx
(18.42)
p0(x)
î?š
pÌƒ (x)
= Z 0 p0 (x) 1
dx
(18.43)
pÌƒ 0(x)
K

Z 0 î?˜ pÌƒ1 (x(k) )
ZÌ‚1 =
K
pÌƒ0 (x(k) )
k=1

s.t. : x(k) âˆ¼ p 0

(18.44)

In the last line, we make a Monte Carlo estimator, ZÌ‚ 1 , of the integral using samples
drawn from p 0(x) and then weight each sample with the ratio of the unnormalized
pÌƒ1 and the proposal p0 .
We see also that this approach allows us to estimate the ratio between the
partition functions as
K

1 î?˜ pÌƒ1(x (k))
K
pÌƒ0(x (k))
k=1

s.t. : x(k) âˆ¼ p 0.

(18.45)

This value can then be used directly to compare two models as described in
equation 18.39.
624

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

If the distribution p0 is close to p1, equation 18.44 can be an eï¬€ective way of
estimating the partition function (Minka, 2005). Unfortunately, most of the time
p1 is both complicated (usually multimodal) and deï¬?ned over a high dimensional
space. It is diï¬ƒcult to ï¬?nd a tractable p0 that is simple enough to evaluate while
still being close enough to p1 to result in a high quality approximation. If p0 and
p1 are not close, most samples from p0 will have low probability under p 1 and
therefore make (relatively) negligible contribution to the sum in equation 18.44.
Having few samples with signiï¬?cant weights in this sum will result in an
estimator that is of poor quality due to high variance. This can be understood
quantitatively through an estimate of the variance of our estimate ZÌ‚1 :
î€ 
î€¡2
K
î€? î€‘
(
k
)
î?˜
pÌƒ 1(x )
Ë† ZÌ‚1 = Z0
Var
(18.46)
âˆ’ ZÌ‚1 .
2
(k) )
K
pÌƒ
(x
0
k=1

This quantity is largest when there is signiï¬?cant deviation in the values of the
(k) )
importance weights pÌƒpÌƒ1(x
.
(x(k) )
0

We now turn to two related strategies developed to cope with the challenging task of estimating partition functions for complex distributions over highdimensional spaces: annealed importance sampling and bridge sampling. Both
start with the simple importance sampling strategy introduced above and both
attempt to overcome the problem of the proposal p0 being too far from p 1 by
introducing intermediate distributions that attempt to bridge the gap between p0
and p1.

18.7.1

Annealed Importance Sampling

In situations where D KL(p0î?«p1) is large (i.e., where there is little overlap between
p0 and p1), a strategy called annealed importance sampling (AIS) attempts
to bridge the gap by introducing intermediate distributions (Jarzynski, 1997; Neal,
2001). Consider a sequence of distributions p Î·0 , . . . , pÎ·n , with 0 = Î·0 < Î·1 < Â· Â· Â· <
Î·nâˆ’1 < Î· n = 1 so that the ï¬?rst and last distributions in the sequence are p0 and p1
respectively.
This approach allows us to estimate the partition function of a multimodal
distribution deï¬?ned over a high-dimensional space (such as the distribution deï¬?ned
by a trained RBM). We begin with a simpler model with a known partition function
(such as an RBM with zeroes for weights) and estimate the ratio between the two
modelâ€™s partition functions. The estimate of this ratio is based on the estimate
of the ratios of a sequence of many similar distributions, such as the sequence of
RBMs with weights interpolating between zero and the learned weights.
625

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

1
We can now write the ratio Z
Z 0 as

ZÎ·
Z1
Z1 ZÎ· 1
Â· Â· Â· nâˆ’1
=
Z0
Z0 ZÎ· 1
Z Î·nâˆ’1
ZÎ· ZÎ·
ZÎ·
Z1
= 1 2 Â· Â· Â· nâˆ’1
Z 0 ZÎ· 1
ZÎ·nâˆ’2 Z Î·nâˆ’1
=

nâˆ’1
î?™

ZÎ·j+1
ZÎ·j
j=0

(18.47)
(18.48)
(18.49)

Provided the distributions pÎ·j and pÎ·j +1 , for all 0 â‰¤ j â‰¤ n âˆ’ 1, are suï¬ƒciently

ZÎ· j+1
Z Î·j using simple importance
1
sampling and then use these to obtain an estimate of Z
Z0 .

close, we can reliably estimate each of the factors

Where do these intermediate distributions come from? Just as the original
proposal distribution p0 is a design choice, so is the sequence of distributions
pÎ·1 . . . pÎ·nâˆ’1. That is, it can be speciï¬?cally constructed to suit the problem domain.
One general-purpose and popular choice for the intermediate distributions is to
use the weighted geometric average of the target distribution p1 and the starting
proposal distribution (for which the partition function is known) p0:
Î·

1âˆ’Î· j

pÎ· j âˆ? p 1j p0

(18.50)

In order to sample from these intermediate distributions, we deï¬?ne a series of
Markov chain transition functions T Î· j(xî€° | x) that deï¬?ne the conditional probability
distribution of transitioning to xî€° given we are currently at x. The transition
operator TÎ·j (xî€° | x) is deï¬?ned to leave p Î·j (x) invariant:
î?š
pÎ·j (x) = p Î·j(x î€° )T Î·j (x | x î€°) dxî€°
(18.51)
These transitions may be constructed as any Markov chain Monte Carlo method
(e.g., Metropolis-Hastings, Gibbs), including methods involving multiple passes
through all of the random variables or other kinds of iterations.
The AIS sampling strategy is then to generate samples from p0 and then use
the transition operators to sequentially generate samples from the intermediate
distributions until we arrive at samples from the target distribution p1:
â€¢ for k = 1 . . . K

(k )

â€“ Sample xÎ·1 âˆ¼ p 0 (x)
626

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

(k )

(k )

(k)

â€“ Sample xÎ·2 âˆ¼ TÎ·1 (xÎ·2 | xÎ·1 )
â€“ ...

(k )

(k )

(k )

â€“ Sample xÎ·nâˆ’1 âˆ¼ TÎ·nâˆ’2(x Î·nâˆ’1 | x Î·nâˆ’2)
(k )

(k )

(k)

â€“ Sample xÎ·n âˆ¼ TÎ·nâˆ’1 (xÎ·n | x Î·nâˆ’1 )
â€¢ end
For sample k, we can derive the importance weight by chaining together the
importance weights for the jumps between the intermediate distributions given in
equation 18.49:
pÌƒÎ·1 (x (Î·1k) ) pÌƒÎ·2(x (Î·k2) )
pÌƒ1 (x(1k))
(k )
w =
...
.
(18.52)
(k )
(k )
(k )
pÌƒ0(x Î·1 ) pÌƒÎ·1(x Î·2 )
pÌƒ Î·nâˆ’1(x Î·n )
To avoid numerical issues such as overï¬‚ow, it is probably best to compute log w(k) by
adding and subtracting log probabilities, rather than computing w(k) by multiplying
and dividing probabilities.
With the sampling procedure thus deï¬?ned and the importance weights given
in equation 18.52, the estimate of the ratio of partition functions is given by:
K

Z1
1 î?˜ (k )
w
â‰ˆ
Z0
K

(18.53)

k=1

In order to verify that this procedure deï¬?nes a valid importance sampling
scheme, we can show (Neal, 2001) that the AIS procedure corresponds to simple
importance sampling on an extended state space with points sampled over the
product space [xÎ·1 , . . . , xÎ·nâˆ’1 , x1]. To do this, we deï¬?ne the distribution over the
extended space as:
pÌƒ(xÎ·1 , . . . , x Î·nâˆ’1, x1 )

(18.54)

=pÌƒ 1(x1)TÌƒÎ·nâˆ’1(x Î·nâˆ’1 | x 1) TÌƒÎ·nâˆ’2 (xÎ·nâˆ’2 | xÎ·nâˆ’1 ) . . . TÌƒÎ· 1 (xÎ·1 | xÎ·2 ),

(18.55)

where TÌƒa is the reverse of the transition operator deï¬?ned by T a (via an application
of Bayesâ€™ rule):
p a (xî€° )
pÌƒa (xî€°)
î€°
TÌƒ a (x | x) =
T a(x | x ) =
Ta (x | xî€° ).
pa (x)
pÌƒ a (x)
î€°

(18.56)

Plugging the above into the expression for the joint distribution on the extended
state space given in equation 18.55, we get:
pÌƒ(x Î·1 , . . . , xÎ·nâˆ’1 , x 1 )

(18.57)
627

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

nâˆ’2
î?™ pÌƒ Î· (x Î· )
pÌƒÎ· nâˆ’1(x Î·nâˆ’1 )
i
i
= pÌƒ 1(x 1)
T Î·nâˆ’1(x 1 | xÎ·nâˆ’1 )
TÎ· i (xÎ·i+1 | xÎ·i )
pÌƒÎ· nâˆ’1(x1 )
pÌƒ
(x
)
Î·
Î·
i
i+1
i=1

(18.58)

nâˆ’2
î?™ pÌƒÎ·i+1 (x Î·i+1 )
pÌƒ1(x 1 )
|
x
=
T Î· (x1 Î·nâˆ’1 ) pÌƒÎ·1 (xÎ· 1)
TÎ· i(x Î· i+1 | xÎ·i ).
pÌƒÎ·nâˆ’1 (x1 ) nâˆ’1
pÌƒ
(x
)
Î·
Î·
i
i+1
i=1

(18.59)

We now have means of generating samples from the joint proposal distribution
q over the extended sample via a sampling scheme given above, with the joint
distribution given by:
q(xÎ· 1 , . . . , xÎ·nâˆ’1 , x 1) = p 0(x Î·1 )TÎ·1 (x Î·2 | x Î·1) . . . T Î·nâˆ’1 (x1 | x Î·nâˆ’1 ).

(18.60)

We have a joint distribution on the extended space given by equation 18.59. Taking
q(xÎ·1 , . . . , xÎ· nâˆ’1, x 1 ) as the proposal distribution on the extended state space from
which we will draw samples, it remains to determine the importance weights:
w

(k )

(k )
(k )
(k )
pÌƒ(xÎ·1 , . . . , x Î·nâˆ’1, x 1)
pÌƒ1(x1 )
pÌƒÎ·2 (x Î·2 ) pÌƒÎ·1 (x Î·1 )
=
=
...
.
(k )
(k)
(k )
q(x Î·1 , . . . , xÎ· nâˆ’1, x1)
pÌƒÎ·nâˆ’1 (x Î·nâˆ’1 )
pÌƒ1(x Î· ) pÌƒ 0(x )
1

(18.61)

0

These weights are the same as proposed for AIS. Thus we can interpret AIS as
simple importance sampling applied to an extended state and its validity follows
immediately from the validity of importance sampling.
Annealed importance sampling (AIS) was ï¬?rst discovered by Jarzynski (1997)
and then again, independently, by Neal (2001). It is currently the most common
way of estimating the partition function for undirected probabilistic models. The
reasons for this may have more to do with the publication of an inï¬‚uential paper
(Salakhutdinov and Murray, 2008) describing its application to estimating the
partition function of restricted Boltzmann machines and deep belief networks than
with any inherent advantage the method has over the other method described
below.
A discussion of the properties of the AIS estimator (e.g.. its variance and
eï¬ƒciency) can be found in Neal (2001).

18.7.2

Bridge Sampling

Bridge sampling Bennett (1976) is another method that, like AIS, addresses the
shortcomings of importance sampling. Rather than chaining together a series of
628

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

intermediate distributions, bridge sampling relies on a single distribution p âˆ— , known
as the bridge, to interpolate between a distribution with known partition function,
p0, and a distribution p1 for which we are trying to estimate the partition function
Z1 .
Bridge sampling estimates the ratio Z1 /Z0 as the ratio of the expected importance weights between pÌƒ0 and pÌƒ âˆ— and between pÌƒ1 and pÌƒâˆ— :
î€¬ K
K
(k )
î?˜
î?˜ pÌƒâˆ— (x(k) )
Z1
pÌƒâˆ— (x0 )
1
(18.62)
â‰ˆ
(
k
)
(k )
Z0
pÌƒ0 (x )
pÌƒ1 (x )
k=1

0

k=1

1

If the bridge distribution pâˆ— is chosen carefully to have a large overlap of support
with both p0 and p 1, then bridge sampling can allow the distance between two
distributions (or more formally, DKL (p0î?«p 1)) to be much larger than with standard
importance sampling.
It can be shown that the optimal bridging distribution is given by p(âˆ—opt) (x) âˆ?
pÌƒ0 (x)pËœ1 (x)
rpÌƒ 0(x)+pËœ1 (x) where r = Z1 /Z 0 . At ï¬?rst, this appears to be an unworkable solution
as it would seem to require the very quantity we are trying to estimate, Z1 /Z0 .
However, it is possible to start with a coarse estimate of r and use the resulting
bridge distribution to reï¬?ne our estimate iteratively (Neal, 2005). That is, we
iteratively re-estimate the ratio and use each iteration to update the value of r.
Linked importance sampling Both AIS and bridge sampling have their advantages. If DKL (p 0î?«p1) is not too large (because p0 and p 1 are suï¬ƒciently close)
bridge sampling can be a more eï¬€ective means of estimating the ratio of partition
functions than AIS. If, however, the two distributions are too far apart for a single
distribution p âˆ— to bridge the gap then one can at least use AIS with potentially
many intermediate distributions to span the distance between p0 and p 1. Neal
(2005) showed how his linked importance sampling method leveraged the power of
the bridge sampling strategy to bridge the intermediate distributions used in AIS
to signiï¬?cantly improve the overall partition function estimates.
Estimating the partition function while training While AIS has become
accepted as the standard method for estimating the partition function for many
undirected models, it is suï¬ƒciently computationally intensive that it remains
infeasible to use during training. However, alternative strategies that have been
explored to maintain an estimate of the partition function throughout training
Using a combination of bridge sampling, short-chain AIS and parallel tempering,
Desjardins et al. (2011) devised a scheme to track the partition function of an
629

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

RBM throughout the training process. The strategy is based on the maintenance of
independent estimates of the partition functions of the RBM at every temperature
operating in the parallel tempering scheme. The authors combined bridge sampling
estimates of the ratios of partition functions of neighboring chains (i.e. from
parallel tempering) with AIS estimates across time to come up with a low variance
estimate of the partition functions at every iteration of learning.
The tools described in this chapter provide many diï¬€erent ways of overcoming
the problem of intractable partition functions, but there can be several other
diï¬ƒculties involved in training and using generative models. Foremost among
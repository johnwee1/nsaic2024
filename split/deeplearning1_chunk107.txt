,
35(8), 1798â€“1828. 555
Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative
stochastic networks trainable by backprop. In ICMLâ€™2014 . 711, 712, 713, 714, 715
Bennett, C. (1976). Eï¬ƒcient estimation of free energy diï¬€erences from Monte Carlo data.
Journal of Computational Physics, 22(2), 245â€“268. 628
Bennett, J. and Lanning, S. (2007). The Netï¬‚ix prize. 479
Berger, A. L., Della Pietra, V. J., and Della Pietra, S. A. (1996). A maximum entropy
approach to natural language processing. Computational Linguistics , 22, 39â€“71. 473
Berglund, M. and Raiko, T. (2013). Stochastic gradient estimate variance in contrastive
divergence and persistent contrastive divergence. CoRR , abs/1312.6002. 614
Bergstra, J. (2011). Incorporating Complex Cells into Neural Networks for Pattern
Classiï¬?cation . Ph.D. thesis, UniversitÃ© de MontrÃ©al. 255
Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex
cell-like networks. In NIPSâ€™2009 . 494
Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. J.
Machine Learning Res., 13, 281â€“305. 433, 434, 435
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,
J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression
compiler. In Proc. SciPy . 25, 82, 214, 222, 446
725

BIBLIOGRAPHY

Bergstra, J., Bardenet, R., Bengio, Y., and KÃ©gl, B. (2011). Algorithms for hyper-parameter
optimization. In NIPSâ€™2011 . 436
Berkes, P. and Wiskott, L. (2005). Slow feature analysis yields a rich repertoire of complex
cell properties. Journal of Vision , 5(6), 579â€“602. 495
Bertsekas, D. P. and Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientiï¬?c.
106
Besag, J. (1975). Statistical analysis of non-lattice data. The Statistician , 24(3), 179â€“195.
615
Bishop, C. M. (1994). Mixture density networks. 189
Bishop, C. M. (1995a). Regularization and complexity control in feed-forward networks.
In Proceedings International Conference on Artiï¬?cial Neural Networks ICANNâ€™95 ,
volume 1, page 141â€“148. 242, 250
Bishop, C. M. (1995b). Training with noise is equivalent to Tikhonov regularization.
Neural Computation, 7(1), 108â€“116. 242
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. 98, 146
Blum, A. L. and Rivest, R. L. (1992). Training a 3-node neural network is NP-complete.
293
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1989). Learnability and
the Vapnikâ€“Chervonenkis dimension. Journal of the ACM , 36(4), 929â€“â€“865. 114
Bonnet, G. (1964). Transformations des signaux alÃ©atoires Ã  travers les systÃ¨mes non
linÃ©aires sans mÃ©moire. Annales des TÃ©lÃ©communications , 19(9â€“10), 203â€“220. 689
Bordes, A., Weston, J., Collobert, R., and Bengio, Y. (2011). Learning structured
embeddings of knowledge bases. In AAAI 2011 . 484
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012). Joint learning of words and
meaning representations for open-text semantic parsing. AISTATSâ€™2012 . 401, 484, 485
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2013a). A semantic matching energy
function for learning with multi-relational data. Machine Learning: Special Issue on
Learning Semantics . 483
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013b).
Translating embeddings for modeling multi-relational data. In C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information
Processing Systems 26 , pages 2787â€“2795. Curran Associates, Inc. 484
Bornschein, J. and Bengio, Y. (2015).
arXiv:1406.2751 . 693

Reweighted wake-sleep.

726

In ICLRâ€™2015,

BIBLIOGRAPHY

Bornschein, J., Shabanian, S., Fischer, A., and Bengio, Y. (2015). Training bidirectional
Helmholtz machines. Technical report, arXiv:1506.03877. 693
Boser, B. E., Guyon, I. M., and Vapnik, V. N. (1992). A training algorithm for optimal margin classiï¬?ers. In COLT â€™92: Proceedings of the ï¬?fth annual workshop on
Computational learning theory, pages 144â€“152, New York, NY, USA. ACM. 18, 141
Bottou, L. (1998). Online algorithms and stochastic approximations. In D. Saad, editor,
Online Learning in Neural Networks. Cambridge University Press, Cambridge, UK. 296
Bottou, L. (2011). From machine learning to machine reasoning. Technical report,
arXiv.1102.1808. 401
Bottou, L. (2015). Multilayer neural networks. Deep Learning Summer School. 440
Bottou, L. and Bousquet, O. (2008). The tradeoï¬€s of large scale learning. In NIPSâ€™2008 .
282, 295
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal
dependencies in high-dimensional sequences: Application to polyphonic music generation
and transcription. In ICMLâ€™12 . 685, 686
Boureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysis of feature pooling in
vision algorithms. In Proc. International Conference on Machine learning (ICMLâ€™10).
345
Boureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011). Ask the locals:
multi-way local pooling for image recognition. In Proc. International Conference on
Computer Vision (ICCVâ€™11). IEEE. 345
Bourlard, H. and Kamp, Y. (1988). Auto-association by multilayer perceptrons and
singular value decomposition. Biological Cybernetics , 59, 291â€“294. 502
Bourlard, H. and Wellekens, C. (1989). Speech pattern discrimination and multi-layered
perceptrons. Computer Speech and Language , 3, 1â€“19. 459
Boyd, S. and Vandenberghe, L. (2004). Convex Optimization . Cambridge University
Press, New York, NY, USA. 93
Brady, M. L., Raghavan, R., and Slawny, J. (1989). Back-propagation fails to separate
where perceptrons succeed. IEEE Transactions on Circuits and Systems, 36, 665â€“674.
284
Brakel, P., Stroobandt, D., and Schrauwen, B. (2013). Training energy-based models for
time-series imputation. Journal of Machine Learning Research , 14, 2771â€“2797. 674,
698
Brand, M. (2003). Charting a manifold. In NIPSâ€™2002 , pages 961â€“968. MIT Press. 164,
518
727

BIBLIOGRAPHY

Breiman, L. (1994). Bagging predictors. Machine Learning , 24(2), 123â€“140. 256
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984). Classiï¬?cation and
Regression Trees. Wadsworth International Group, Belmont, CA. 146
Bridle, J. S. (1990). Alphanets: a recurrent â€˜neuralâ€™ network architecture with a hidden
Markov model interpretation. Speech Communication, 9(1), 83â€“92. 186
Briggman, K., Denk, W., Seung, S., Helmstaedter, M. N., and Turaga, S. C. (2009).
Maximin aï¬ƒnity learning of image segmentation. In NIPSâ€™2009 , pages 1865â€“1873. 360
Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Laï¬€erty, J. D.,
Mercer, R. L., and Roossin, P. S. (1990). A statistical approach to machine translation.
Computational linguistics , 16(2), 79â€“85. 21
Brown, P. F., Pietra, V. J. D., DeSouza, P. V., Lai, J. C., and Mercer, R. L. (1992). Classbased n-gram models of natural language. Computational Linguistics , 18, 467â€“479.
463
Bryson, A. and Ho, Y. (1969). Applied optimal control: optimization, estimation, and
control . Blaisdell Pub. Co. 225
Bryson, Jr., A. E. and Denham, W. F. (1961). A steepest-ascent method for solving
optimum programming problems. Technical Report BR-1303, Raytheon Company,
Missle and Space Division. 225
BuciluaÌŒ, C., Caruana, R., and Niculescu-Mizil, A. (2006). Model compression. In
Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery
and data mining , pages 535â€“541. ACM. 448
Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders.
arXiv preprint arXiv:1509.00519 . 698
Cai, M., Shi, Y., and Liu, J. (2013). Deep maxout neural networks for speech recognition.
In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop
on, pages 291â€“296. IEEE. 194
Carreira-PerpiÃ±an, M. A. and Hinton, G. E. (2005). On contrastive divergence learning.
In R. G. Cowell and Z. Ghahramani, editors, Proceedings of the Tenth International
Workshop on Artiï¬?cial Intelligence and Statistics (AISTATSâ€™05), pages 33â€“40. Society
for Artiï¬?cial Intelligence and Statistics. 611
Caruana, R. (1993). Multitask connectionist learning. In Proc. 1993 Connectionist Models
Summer School , pages 372â€“379. 244
Cauchy, A. (1847). MÃ©thode gÃ©nÃ©rale pour la rÃ©solution de systÃ¨mes dâ€™Ã©quations simultanÃ©es. In Compte rendu des sÃ©ances de lâ€™acadÃ©mie des sciences, pages 536â€“538. 83,
225
728

BIBLIOGRAPHY

Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923,
UCSD. 164
Chandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey. ACM
computing surveys (CSUR), 41(3), 15. 102
Chapelle, O., Weston, J., and SchÃ¶lkopf, B. (2003). Cluster kernels for semi-supervised
learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural
Information Processing Systems 15 (NIPSâ€™02), pages 585â€“592, Cambridge, MA. MIT
Press. 244
Chapelle, O., SchÃ¶lkopf, B., and Zien, A., editors (2006). Semi-Supervised Learning. MIT
Press, Cambridge, MA. 244, 541
Chellapilla, K., Puri, S., and Simard, P. (2006). High Performance Convolutional Neural
Networks for Document Processing. In Guy Lorette, editor, Tenth International
Workshop on Frontiers in Handwriting Recognition, La Baule (France). UniversitÃ© de
Rennes 1, Suvisoft. http://www.suvisoft.com. 24, 27, 445
Chen, B., Ting, J.-A., Marlin, B. M., and de Freitas, N. (2010). Deep learning of invariant
spatio-temporal features from video. NIPS*2010 Deep Learning and Unsupervised
Feature Learning Workshop. 360
Chen, S. F. and Goodman, J. T. (1999). An empirical study of smoothing techniques for
language modeling. Computer, Speech and Language, 13(4), 359â€“393. 462, 463, 473
Chen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., and Temam, O. (2014a). DianNao:
A small-footprint high-throughput accelerator for ubiquitous machine-learning. In Proceedings of the 19th international conference on Architectural support for programming
languages and operating systems, pages 269â€“284. ACM. 451
Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C.,
and Zhang, Z. (2015). MXNet: A ï¬‚exible and eï¬ƒcient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274 . 25
Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N.,
et al. (2014b). DaDianNao: A machine-learning supercomputer. In Microarchitecture
(MICRO), 2014 47th Annual IEEE/ACM International Symposium on, pages 609â€“622.
IEEE. 451
Chilimbi, T., Suzue, Y., Apacible, J., and Kalyanaraman, K. (2014). Project Adam:
Building an eï¬ƒcient and scalable deep learning training system. In 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDIâ€™14). 447
Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is eï¬ƒcient for learning restricted
Boltzmann machines. In IJCNNâ€™2010 . 603, 614

729

BIBLIOGRAPHY

Cho, K., Raiko, T., and Ilin, A. (2011). Enhanced gradient and adaptive learning rate for
training restricted Boltzmann machines. In ICMLâ€™2011 , pages 105â€“112. 674
Cho, K., van MerriÃ«nboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y.
(2014a). Learning phrase representations using RNN encoder-decoder for statistical
machine translation. In Proceedings of the Empiricial Methods in Natural Language
Processing (EMNLP 2014). 397, 474, 475
Cho, K., Van MerriÃ«nboer, B., Bahdanau, D., and Bengio, Y. (2014b). On the properties of neural machine translation: Encoder-decoder approaches. ArXiv e-prints ,
abs/1409.1259. 412
Choromanska, A., Henaï¬€, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2014). The
loss surface of multilayer networks. 285, 286
Chorowski, J., Bahdanau, D., Cho, K., and Bengio, Y. (2014). End-to-end continuous
speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602.
461
Christianson, B. (1992). Automatic Hessians by reverse accumulation. IMA Journal of
Numerical Analysis, 12(2), 135â€“150. 224
Chrupala, G., Kadar, A., and Alishahi, A. (2015). Learning language through pictures.
arXiv 1506.03694. 412
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated
recurrent neural networks on sequence modeling. NIPSâ€™2014 Deep Learning workshop,
arXiv 1412.3555. 412, 460
Chung, J., GÃ¼lÃ§ehre, Ã‡., Cho, K., and Bengio, Y. (2015a). Gated feedback recurrent
neural networks. In ICMLâ€™15 . 412
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A., and Bengio, Y. (2015b). A
recurrent latent variable model for sequential data. In NIPSâ€™2015 . 698
Ciresan, D., Meier, U., Masci, J., and Schmidhuber, J. (2012). Multi-column deep neural
network for traï¬ƒc sign classiï¬?cation. Neural Networks , 32, 333â€“338. 23, 201
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big
simple neural nets for handwritten digit recognition. Neural Computation, 22 , 1â€“14.
24, 27, 446
Coates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse
coding and vector quantization. In ICMLâ€™2011 . 27, 256, 498
Coates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in
unsupervised feature learning. In Proceedings of the Thirteenth International Conference
on Artiï¬?cial Intelligence and Statistics (AISTATS 2011). 363, 364, 455
730

BIBLIOGRAPHY

Coates, A., Huval, B., Wang, T., Wu, D., Catanzaro, B., and Andrew, N. (2013).
Deep learning with COTS HPC systems. In S. Dasgupta and D. McAllester, editors,
Proceedings of the 30th International Conference on Machine Learning (ICML-13),
volume 28 (3), pages 1337â€“1345. JMLR Workshop and Conference Proceedings. 24, 27,
364, 447
Cohen, N., Sharir, O., and Shashua, A. (2015). On the expressive power of deep learning:
A tensor analysis. arXiv:1509.05009. 554
Collobert, R. (2004). Large Scale Machine Learning. Ph.D. thesis, UniversitÃ© de Paris VI,
LIP6. 197
Collobert, R. (2011). Deep learning for eï¬ƒcient discriminative parsing. In AISTATSâ€™2011 .
101, 477
Collobert, R. and Weston, J. (2008a). A uniï¬?ed architecture for natural language processing:
Deep neural networks with multitask learning. In ICMLâ€™2008 . 471, 477
Collobert, R. and Weston, J. (2008b). A uniï¬?ed architecture for natural language
processing: Deep neural networks with multitask learning. In ICMLâ€™2008 . 535
Collobert, R., Bengio, S., and Bengio, Y. (2001). A parallel mixture of SVMs for very
large scale problems. Technical Report IDIAP-RR-01-12, IDIAP. 450
Collobert, R., Bengio, S., and Bengio, Y. (2002). Parallel mixture of SVMs for very large
scale problems. Neural Computation, 14(5), 1105â€“1114. 450
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011a).
Natural language processing (almost) from scratch. The Journal of Machine Learning
Research , 12, 2493â€“2537. 328, 477, 535, 536
Collobert, R., Kavukcuoglu, K., and Farabet, C. (2011b). Torch7: A Matlab-like environment for machine learning. In BigLearn, NIPS Workshop. 25, 214, 446
Comon, P. (1994). Independent component analysis - a new concept? Signal Processing,
36, 287â€“314. 491
Cortes, C. and Vapnik, V. (1995). Support vector networks. Machine Learning , 20,
273â€“297. 18, 141
Couprie, C., Farabet, C., Najman, L., and LeCun, Y. (2013). Indoor semantic segmentation
using depth information. In International Conference on Learning Representations
(ICLR2013). 23, 201
Courbari
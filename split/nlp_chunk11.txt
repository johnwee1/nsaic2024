 that have each been hand-
(d1, c1), ...., (dN, cN)
. Our goal is to learn a classiﬁer that is
labeled with a class:
}
{
C, where C is
capable of mapping from a new document d to its correct class c
some set of useful document classes. A probabilistic classiﬁer additionally will tell
us the probability of the observation being in the class. This full distribution over
the classes can be useful information for downstream decisions; avoiding making
discrete decisions early on can be useful when combining systems.

∈

∈

Many kinds of machine learning algorithms are used to build classiﬁers. This
chapter introduces naive Bayes; the following one introduces logistic regression.
These exemplify two ways of doing classiﬁcation. Generative classiﬁers like naive
Bayes build a model of how a class could generate some input data. Given an ob-
servation, they return the class most likely to have generated the observation. Dis-
criminative classiﬁers like logistic regression instead learn what features from the
input are most useful to discriminate between the different possible classes. While
discriminative systems are often more accurate and hence more commonly used,
generative classiﬁers still have a role.

4.1 Naive Bayes Classiﬁers

naive Bayes
classiﬁer

In this section we introduce the multinomial naive Bayes classiﬁer, so called be-
cause it is a Bayesian classiﬁer that makes a simplifying (naive) assumption about

62 CHAPTER 4

• NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

bag of words

how the features interact.

The intuition of the classiﬁer is shown in Fig. 4.1. We represent a text document
as if it were a bag of words, that is, an unordered set of words with their position
ignored, keeping only their frequency in the document. In the example in the ﬁgure,
instead of representing the word order in all the phrases like “I love this movie” and
“I would recommend it”, we simply note that the word I occurred 5 times in the
entire excerpt, the word it 6 times, the words love, recommend, and movie once, and
so on.

Figure 4.1
words is ignored (the bag-of-words assumption) and we make use of the frequency of each word.

Intuition of the multinomial naive Bayes classiﬁer applied to a movie review. The position of the

Naive Bayes is a probabilistic classiﬁer, meaning that for a document d, out of
all classes c
C the classiﬁer returns the class ˆc which has the maximum posterior
probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our
estimate of the correct class”.

∈

ˆ

Bayesian
inference

ˆc = argmax

P(c

c

C

d)
|

(4.1)

∈
This idea of Bayesian inference has been known since the work of Bayes (1763),
and was ﬁrst applied to text classiﬁcation by Mosteller and Wallace (1964). The
intuition of Bayesian classiﬁcation is to use Bayes’ rule to transform Eq. 4.1 into
other probabilities that have some useful properties. Bayes’ rule is presented in
y) into
Eq. 4.2; it gives us a way to break down any conditional probability P(x
|
three other probabilities:

P(x

y) =
|

x)P(x)
P(y
|
P(y)

We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:

ˆc = argmax
c
∈

C

P(c

d) = argmax
|

C

c
∈

P(d

c)P(c)
|
P(d)

(4.2)

(4.3)

ititititititIIIIIloverecommendmoviethethethethetototoandandandseenseenyetwouldwithwhowhimsicalwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat…6 54332111111111111…4.1

• NAIVE BAYES CLASSIFIERS

63

We can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This
is possible because we will be computing P(d
for each possible class. But P(d)
doesn’t change for each class; we are always asking about the most likely class for
the same document d, which must have the same probability P(d). Thus, we can
choose the class that maximizes this simpler formula:

c)P(c)
|
P(d)

ˆc = argmax
c
∈

C

d) = argmax
P(c
|

P(d

c)P(c)
|

(4.4)

C

c
∈

We call Naive Bayes a generative model because we can read Eq. 4.4 as stating
a kind of implicit assumption about how a document is generated: ﬁrst a class is
sampled from P(c), and then the words are generated by sampling from P(d
c). (In
|
fact we could imagine generating artiﬁcial documents, or at least their word counts,
by following this process). We’ll say more about this intuition of generative models
in Chapter 5.

prior
probability
likelihood

To return to classiﬁcation: we compute the most probable class ˆc given some
document d by choosing the class which has the highest product of two probabilities:
c):
the prior probability of the class P(c) and the likelihood of the document P(d
|

likelihood

prior

c)
|
(cid:122) (cid:125)(cid:124) (cid:123)
Without loss of generalization, we can represent a document d as a set of features

ˆc = argmax
c
∈

(4.5)

P(c)

(cid:122)(cid:125)(cid:124)(cid:123)

P(d

C

f1, f2, ..., fn:

prior

ˆc = argmax
c

likelihood
c)
P( f1, f2, ...., fn|
(cid:125)(cid:124)
(cid:122)
Unfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the probability of every possible combination of
features (for example, every possible set of words and positions) would require huge
numbers of parameters and impossibly large training sets. Naive Bayes classiﬁers
therefore make two simplifying assumptions.

(4.6)

P(c)

(cid:122)(cid:125)(cid:124)(cid:123)

(cid:123)

C

∈

The ﬁrst is the bag-of-words assumption discussed intuitively above: we assume
position doesn’t matter, and that the word “love” has the same effect on classiﬁcation
whether it occurs as the 1st, 20th, or last word in the document. Thus we assume
that the features f1, f2, ..., fn only encode word identity and not position.

The second is commonly called the naive Bayes assumption: this is the condi-
c) are independent given

tional independence assumption that the probabilities P( fi|
the class c and hence can be ‘naively’ multiplied as follows:

naive Bayes
assumption

c)
c) = P( f1|
The ﬁnal equation for the class chosen by a naive Bayes classiﬁer is thus:

P( f1, f2, ...., fn|

c)
P( fn|

c)
P( f2|

...

·

·

·

cNB = argmax

C

c
∈

P(c)

P( f

c)
|

F
(cid:89)f
∈

(4.7)

(4.8)

To apply the naive Bayes classiﬁer to text, we need to consider word positions, by
simply walking an index through every word position in the document:

positions

all word positions in test document

←

cNB = argmax

c

C

∈

P(c)

c)
P(wi|

positions

(cid:89)i
∈

(4.9)

64 CHAPTER 4

• NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

Naive Bayes calculations, like calculations for language modeling, are done in log
space, to avoid underﬂow and increase speed. Thus Eq. 4.9 is generally instead
expressed1 as

cNB = argmax

C

c
∈

log P(c) +

c)
log P(wi|

positions

(cid:88)i
∈

(4.10)

By considering features in log space, Eq. 4.10 computes the predicted class as a lin-
ear function of input features. Classiﬁers that use a linear combination of the inputs
to make a classiﬁcation decision —like naive Bayes and also logistic regression—
are called linear classiﬁers.

linear
classiﬁers

4.2 Training the Naive Bayes Classiﬁer

c)? Let’s ﬁrst consider the maxi-
How can we learn the probabilities P(c) and P( fi|
mum likelihood estimate. We’ll simply use the frequencies in the data. For the class
prior P(c) we ask what percentage of the documents in our training set are in each
class c. Let Nc be the number of documents in our training data with class c and
Ndoc be the total number of documents. Then:

ˆP(c) =

Nc
Ndoc

(4.11)

c), we’ll assume a feature is just the existence of a word
To learn the probability P( fi|
c), which we compute as
in the document’s bag of words, and so we’ll want P(wi|
the fraction of times the word wi appears among all words in all documents of topic
c. We ﬁrst concatenate all documents with category c into one big “category c” text.
Then we use the frequency of wi in this concatenated document to give a maximum
likelihood estimate of the probability:

ˆP(wi|

c) =

count(wi, c)

w

V count(w, c)
∈

(4.12)

Here the vocabulary V consists of the union of all the word types in all classes, not
just the words in one class c.

(cid:80)

There is a problem, however, with maximum likelihood training. Imagine we
are trying to estimate the likelihood of the word “fantastic” given class positive, but
suppose there are no training documents that both contain the word “fantastic” and
are classiﬁed as positive. Perhaps the word “fantastic” happens to occur (sarcasti-
cally?) in the class negative. In such a case the probability for this feature will be
zero:

ˆP(“fantastic”
positive) =
|

count(“fantastic”, positive)
V count(w, positive)
∈

w

= 0

(4.13)

(cid:80)
But since naive Bayes naively multiplies all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the probability of the
class to be zero, no matter the other evidence!

The simplest solution is the add-one (Laplace) smoothing introduced in Chap-
ter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing

1

In practice throughout this book, we’ll use log to mean natural log (ln) when the base is not speciﬁed.

4.2

• TRAINING THE NAIVE BAYES CLASSIFIER

65

algorithms in language modeling, it is commonly used in naive Bayes text catego-
rization:

ˆP(wi|

c) =

count(wi, c) + 1
V (count(w, c) + 1)
∈

w

=

count(wi, c) + 1
V count(w, c)
∈

+

w

V
|

|

(4.14)

(cid:80)

Note once again that it is crucial that the vocabulary V consists of the union of all the
word types in all classes, not just the words in one class c (try to convince yourself
why this must be true; see the exercise at the end of the chapter).

(cid:0)(cid:80)

(cid:1)

unknown word

stop words

What do we do about words that occur in our test data but are not in our vocab-
ulary at all because they did not occur in any training document in any class? The
solution for such unknown words is to ignore them—remove them from the test
document and not include any probability for them at all.

Finally, some systems choose to completely ignore another class of words: stop
words, very frequent words like the and a. This can be done by sorting the vocabu-
lary by frequency in the training set, and deﬁning the top 10–100 vocabulary entries
as stop words, or alternatively by using one of the many predeﬁned stop word lists
available online. Then each instance of these stop words is simply removed from
both training and test documents as if it had never occurred. In most text classiﬁca-
tion applications, however, using a stop word list doesn’t improve performance, and
so it is more common to make use of the entire vocabulary and not use a stop word
list.

Fig. 4.2 shows the ﬁnal algorithm.

function TRAIN NAIVE BAYES(D, C) returns log P(c) and log P(w

c)
|

# Calculate P(c) terms

∈

for each class c

C
Ndoc = number of documents in D
Nc = number of documents from D in class c
logprior[c]

log

Nc
Ndoc

←
vocabulary of D

V
bigdoc[c]
←
for each word w in V

←

append(d) for d

count(w,c)

←

# of occurrences of w in bigdoc[c]

loglikelihood[w,c]

log

←

return logprior, loglikelihood, V

(cid:80)

∈

D with class c

# Calculate P(w

c) terms
|

count(w, c) + 1
w(cid:48) in V (count (w(cid:48), c) + 1)

function TEST NAIVE BAYES(testdoc, logprior, loglikelihood, C, V) returns best c

for each class c

C
∈
logprior[c]

←

sum[c]
for each position i in testdoc
testdoc[i]

word
←
if word

V

∈
sum[c]
return argmaxc sum[c]

←

sum[c]+ loglikelihood[word,c]

Figure 4.2 The naive Bayes algorithm, using add-1 smoothing. To use add-α smoothing
instead, change the +1 to +α for loglikelihood counts in training.

66 CHAPTER 4

• NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

4.3 Worked example

Let’s walk through an example of training and testing naive Bayes with add-one
smoothing. We’ll use a sentiment analysis domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simpliﬁed from actual movie reviews.

Cat

Documents

Training -
-
-
+
+
?

Test

just plain boring
entirely predictable and lacks energy
no surprises and very few laughs
very powerful
the most fun ﬁlm of the summer
predictable with no fun

The prior P(c) for the two classes is computed via Eq. 4.11 as Nc
Ndoc

:

P(

) =

−

3
5

P(+) =

2
5

The word with doesn’t occur in the training set, so we drop it completely (as
mentioned above, we don’t use unknown word models for naive Bayes). The like-
lihoods from the training set for the remaining three words “predictable”, “no”, and
“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder
of the words in the training set is left as an exercise for the reader):

P(“predictable”

) =

|−

P(“no”

) =

|−

P(“fun”

) =

|−

1 + 1
14 + 20
1 + 1
14 + 20
0 + 1
14 + 20

P(“no”

+) =
P(“predictable”
|
0 + 1
9 + 20
1 + 1
9 + 20

+) =
|

+) =
|

P(“fun”

0 + 1
9 + 20

For the test sentence S = “predictable with no fun”, after removing the word ‘with’,
the chosen class, via Eq. 4.9, is therefore computed as follows:

P(

)P(S

−

) =

|−

P(+)P(S

+) =
|

2

1

3
5 ×
2
5 ×

×

2
1
343 = 6.1
×
1
2
293 = 3.2
×

×

5
10−

5
10−

×

×

The model thus predicts the class negative for the test sentence.

4.4 Optimizing for Sentiment Analysis

While standard naive Bayes text classiﬁcation can work well for sentiment analysis,
some small changes are generally employed that improve performance.

First, for sentiment classiﬁcation and a number of other text classiﬁcation tasks,
whether a word occurs or not seems to matter more than its frequency. Thus it often
improves performance to clip the word counts in each document at 1 (see the end

4.4

• OPTIMIZING FOR SENTIMENT ANALYSIS

67

binary naive
Bayes

of the chapter for pointers to these results). This variant is called binary multino-
mial naive Bayes or binary naive Bayes. The variant uses the same algorithm as
in Fig. 4.2 except that for each document we remove all duplicate words before con-
catenating them into the single big document during training and we also remove
duplicate words from test documents. Fig. 4.3 shows an example in which a set
of four documents (shortened and text-normalized for this example) are remapped
to binary, with the modiﬁed counts shown in the table on the right. The example
is worked without add-1 smoothing to make the differences clearer. Note that the
results counts need not be 1; the word great has a count of 2 even for binary naive
Bayes, because it appears in multiple documents.

Four original documents:

−

it was pathetic the worst part was the
boxing scenes
no plot twists or great scenes
−
+ and satire and great plot twists
+ great scenes great ﬁlm

After per-document binarization:

−

it was pathetic the worst part boxing
scenes
no plot twists or great scenes

−
+ and satire great plot twists
+ great scenes ﬁlm

NB

Binary
Counts Counts
+
2
and
0
boxing
1
ﬁlm
3
great
0
it
0
no
0
or
p
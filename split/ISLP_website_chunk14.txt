 of this choice. However, the
coefficients and their p-values do depend on the choice of dummy variable
coding. Rather than rely on the individual coefficients, we can use an F -test
to test H0 : β1 = β2 = 0; this does not depend on the coding. This F -test
has a p-value of 0.96, indicating that we cannot reject the null hypothesis
that there is no relationship between balance and region.
Using this dummy variable approach presents no difficulties when incorporating both quantitative and qualitative predictors. For example, to
regress balance on both a quantitative variable such as income and a qualitative variable such as student, we must simply create a dummy variable
for student and then fit a multiple regression model using income and the
dummy variable as predictors for credit card balance.
There are many different ways of coding qualitative variables besides
the dummy variable approach taken here. All of these approaches lead to
equivalent model fits, but the coefficients are different and have different
interpretations, and are designed to measure particular contrasts. This topic contrast
is beyond the scope of the book.

3.3.2

Extensions of the Linear Model

The standard linear regression model (3.19) provides interpretable results
and works quite well on many real-world problems. However, it makes several highly restrictive assumptions that are often violated in practice. Two
of the most important assumptions state that the relationship between the
predictors and response are additive and linear. The additivity assumption
additive
means that the association between a predictor Xj and the response Y does linear
not depend on the values of the other predictors. The linearity assumption
states that the change in the response Y associated with a one-unit change
in Xj is constant, regardless of the value of Xj . In later chapters of this
book, we examine a number of sophisticated methods that relax these two
12 There could still in theory be a difference between South and West, although the
data here does not suggest any difference.

3.3 Other Considerations in the Regression Model

95

assumptions. Here, we briefly examine some common classical approaches
for extending the linear model.
Removing the Additive Assumption
In our previous analysis of the Advertising data, we concluded that both TV
and radio seem to be associated with sales. The linear models that formed
the basis for this conclusion assumed that the effect on sales of increasing
one advertising medium is independent of the amount spent on the other
media. For example, the linear model (3.20) states that the average increase
in sales associated with a one-unit increase in TV is always β1 , regardless
of the amount spent on radio.
However, this simple model may be incorrect. Suppose that spending
money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.
In this situation, given a fixed budget of $100,000, spending half on radio
and half on TV may increase sales more than allocating the entire amount
to either TV or to radio. In marketing, this is known as a synergy effect,
and in statistics it is referred to as an interaction effect. Figure 3.5 suggests that such an effect may be present in the advertising data. Notice
that when levels of either TV or radio are low, then the true sales are lower
than predicted by the linear model. But when advertising is split between
the two media, then the model tends to underestimate sales.
Consider the standard linear regression model with two variables,
Y = β0 + β1 X1 + β2 X2 + ".
According to this model, a one-unit increase in X1 is associated with an
average increase in Y of β1 units. Notice that the presence of X2 does
not alter this statement—that is, regardless of the value of X2 , a oneunit increase in X1 is associated with a β1 -unit increase in Y . One way of
extending this model is to include a third predictor, called an interaction
term, which is constructed by computing the product of X1 and X2 . This
results in the model
Y = β0 + β1 X1 + β2 X2 + β3 X1 X2 + ".

(3.31)

How does inclusion of this interaction term relax the additive assumption?
Notice that (3.31) can be rewritten as
Y

=
=

β0 + (β1 + β3 X2 )X1 + β2 X2 + "
β0 + β̃1 X1 + β2 X2 + "

(3.32)

where β̃1 = β1 + β3 X2 . Since β̃1 is now a function of X2 , the association
between X1 and Y is no longer constant: a change in the value of X2 will
change the association between X1 and Y . A similar argument shows that
a change in the value of X1 changes the association between X2 and Y .
For example, suppose that we are interested in studying the productivity of a factory. We wish to predict the number of units produced on the
basis of the number of production lines and the total number of workers.
It seems likely that the effect of increasing the number of production lines

96

3. Linear Regression

Intercept
TV
radio
TV×radio

Coefficient
6.7502
0.0191
0.0289
0.0011

Std. error
0.248
0.002
0.009
0.000

t-statistic
27.23
12.70
3.24
20.73

p-value
< 0.0001
< 0.0001
0.0014
< 0.0001

TABLE 3.9. For the Advertising data, least squares coefficient estimates associated with the regression of sales onto TV and radio, with an interaction term,
as in (3.33).

will depend on the number of workers, since if no workers are available
to operate the lines, then increasing the number of lines will not increase
production. This suggests that it would be appropriate to include an interaction term between lines and workers in a linear model to predict units.
Suppose that when we fit the model, we obtain
units

≈
=

1.2 + 3.4 × lines + 0.22 × workers + 1.4 × (lines × workers)
1.2 + (3.4 + 1.4 × workers) × lines + 0.22 × workers.

In other words, adding an additional line will increase the number of units
produced by 3.4 + 1.4 × workers. Hence the more workers we have, the
stronger will be the effect of lines.
We now return to the Advertising example. A linear model that uses
radio, TV, and an interaction between the two to predict sales takes the
form
sales

=
=

β0 + β1 × TV + β2 × radio + β3 × (radio × TV) + "
β0 + (β1 + β3 × radio) × TV + β2 × radio + ".
(3.33)

We can interpret β3 as the increase in the effectiveness of TV advertising
associated with a one-unit increase in radio advertising (or vice-versa). The
coefficients that result from fitting the model (3.33) are given in Table 3.9.
The results in Table 3.9 strongly suggest that the model that includes the
interaction term is superior to the model that contains only main effects.
main effect
The p-value for the interaction term, TV×radio, is extremely low, indicating
that there is strong evidence for Ha : β3 =
% 0. In other words, it is clear that
the true relationship is not additive. The R2 for the model (3.33) is 96.8 %,
compared to only 89.7 % for the model that predicts sales using TV and
radio without an interaction term. This means that (96.8 − 89.7)/(100 −
89.7) = 69 % of the variability in sales that remains after fitting the additive model has been explained by the interaction term. The coefficient
estimates in Table 3.9 suggest that an increase in TV advertising of $1,000 is
associated with increased sales of (β̂1 + β̂3 × radio)×1,000 = 19+1.1× radio
units. And an increase in radio advertising of $1,000 will be associated with
an increase in sales of (β̂2 + β̂3 × TV) × 1,000 = 29 + 1.1 × TV units.
In this example, the p-values associated with TV, radio, and the interaction term all are statistically significant (Table 3.9), and so it is obvious
that all three variables should be included in the model. However, it is
sometimes the case that an interaction term has a very small p-value, but
the associated main effects (in this case, TV and radio) do not. The hierarchical principle states that if we include an interaction in a model, we

hierarchical
principle

3.3 Other Considerations in the Regression Model

97

should also include the main effects, even if the p-values associated with
their coefficients are not significant. In other words, if the interaction between X1 and X2 seems important, then we should include both X1 and
X2 in the model even if their coefficient estimates have large p-values. The
rationale for this principle is that if X1 × X2 is related to the response,
then whether or not the coefficients of X1 or X2 are exactly zero is of little interest. Also X1 × X2 is typically correlated with X1 and X2 , and so
leaving them out tends to alter the meaning of the interaction.
In the previous example, we considered an interaction between TV and
radio, both of which are quantitative variables. However, the concept of
interactions applies just as well to qualitative variables, or to a combination
of quantitative and qualitative variables. In fact, an interaction between
a qualitative variable and a quantitative variable has a particularly nice
interpretation. Consider the Credit data set from Section 3.3.1, and suppose
that we wish to predict balance using the income (quantitative) and student
(qualitative) variables. In the absence of an interaction term, the model
takes the form
=
β2
if ith person is a student
balancei ≈ β0 + β1 × incomei +
0
if ith person is not a student
=
β0 + β2
if ith person is a student
= β1 × incomei +
β0
if ith person is not a student.
(3.34)
Notice that this amounts to fitting two parallel lines to the data, one for
students and one for non-students. The lines for students and non-students
have different intercepts, β0 + β2 versus β0 , but the same slope, β1 . This
is illustrated in the left-hand panel of Figure 3.7. The fact that the lines
are parallel means that the average effect on balance of a one-unit increase
in income does not depend on whether or not the individual is a student.
This represents a potentially serious limitation of the model, since in fact a
change in income may have a very different effect on the credit card balance
of a student versus a non-student.
This limitation can be addressed by adding an interaction variable, created by multiplying income with the dummy variable for student. Our
model now becomes
=
β2 + β3 × incomei if student
balancei ≈ β0 + β1 × incomei +
0
if not student
=
(β0 + β2 ) + (β1 + β3 ) × incomei if student
=
β0 + β1 × incomei
if not student.
(3.35)

Once again, we have two different regression lines for the students and
the non-students. But now those regression lines have different intercepts,
β0 +β2 versus β0 , as well as different slopes, β1 +β3 versus β1 . This allows for
the possibility that changes in income may affect the credit card balances
of students and non-students differently. The right-hand panel of Figure 3.7

1000

student
non−student

200

600

Balance

1000
600
200

Balance

1400

3. Linear Regression
1400

98

0

50

100
Income

150

0

50

100

150

Income

FIGURE 3.7. For the Credit data, the least squares lines are shown for prediction of balance from income for students and non-students. Left: The model
(3.34) was fit. There is no interaction between income and student. Right: The
model (3.35) was fit. There is an interaction term between income and student.

shows the estimated relationships between income and balance for students
and non-students in the model (3.35). We note that the slope for students
is lower than the slope for non-students. This suggests that increases in
income are associated with smaller increases in credit card balance among
students as compared to non-students.
Non-linear Relationships
As discussed previously, the linear regression model (3.19) assumes a linear
relationship between the response and predictors. But in some cases, the
true relationship between the response and the predictors may be nonlinear. Here we present a very simple way to directly extend the linear model
to accommodate non-linear relationships, using polynomial regression. In
polynomial
later chapters, we will present more complex approaches for performing regression
non-linear fits in more general settings.
Consider Figure 3.8, in which the mpg (gas mileage in miles per gallon)
versus horsepower is shown for a number of cars in the Auto data set. The
orange line represents the linear regression fit. There is a pronounced relationship between mpg and horsepower, but it seems clear that this relationship is in fact non-linear: the data suggest a curved relationship. A simple
approach for incorporating non-linear associations in a linear model is to
include transformed versions of the predictors. For example, the points in
Figure 3.8 seem to have a quadratic shape, suggesting that a model of the
quadratic
form
mpg = β0 + β1 × horsepower + β2 × horsepower2 + "

(3.36)

may provide a better fit. Equation 3.36 involves predicting mpg using a
non-linear function of horsepower. But it is still a linear model! That is,
(3.36) is simply a multiple linear regression model with X1 = horsepower
and X2 = horsepower2 . So we can use standard linear regression software to
estimate β0 , β1 , and β2 in order to produce a non-linear fit. The blue curve
in Figure 3.8 shows the resulting quadratic fit to the data. The quadratic

50

3.3 Other Considerations in the Regression Model

99

30
10

20

Miles per gallon

40

Linear
Degree 2
Degree 5

50

100

150

200

Horsepower

FIGURE 3.8. The Auto data set. For a number of cars, mpg and horsepower are
shown. The linear regression fit is shown in orange. The linear regression fit for a
model that includes horsepower2 is shown as a blue curve. The linear regression
fit for a model that includes all polynomials of horsepower up to fifth-degree is
shown in green.

Intercept
horsepower
horsepower2

Coefficient
56.9001
−0.4662
0.0012

Std. error
1.8004
0.0311
0.0001

t-statistic
31.6
−15.0
10.1

p-value
< 0.0001
< 0.0001
< 0.0001

TABLE 3.10. For the Auto data set, least squares coefficient estimates associated
with the regression of mpg onto horsepower and horsepower2 .

fit appears to be substantially better than the fit obtained when just the
linear term is included. The R2 of the quadratic fit is 0.688, compared to
0.606 for the linear fit, and the p-value in Table 3.10 for the quadratic term
is highly significant.
If including horsepower2 led to such a big improvement in the model, why
not include horsepower3 , horsepower4 , or even horsepower5 ? The green curve
in Figure 3.8 displays the fit that results from including all polynomials up
to fifth degree in the model (3.36). The resulting fit seems unnecessarily
wiggly—that is, it is unclear that including the additional terms really has
led to a better fit to the data.
The approach that we have just described for extending the linear model
to accommodate non-linear relationships is known as polynomial regression, since we have included polynomial functions of the predictors in the
regression model. We further explore this approach and other non-linear
extensions of the linear model in Chapter 7.

100

3. Linear Regression

3.3.3

Potential Problems

When we fit a linear regression model to a particular data set, many problems may occur. Most common among these are the following:
1. Non-linearity of the response-predictor relationships.
2. Correlation of error terms.
3. Non-constant variance of error terms.
4. Outliers.
5. High-leverage points.
6. Collinearity.
In practice, identifying and overcoming these problems is as much an
art as a science. Many pages in countless books have been written on this
topic. Since
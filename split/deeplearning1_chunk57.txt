 (x(1), . . . , x (Ï„ )) = P (Ï„ )P (x (1) , . . . , x(Ï„ ) | Ï„ ).

(10.34)

The strategy of predicting Ï„ directly is used for example by Goodfellow et al.
(2014d).

10.2.4

Modeling Sequences Conditioned on Context with RNNs

In the previous section we described how an RNN could correspond to a directed
graphical model over a sequence of random variables y (t) with no inputs x. Of
course, our development of RNNs as in equation 10.8 included a sequence of
inputs x(1), x (2) , . . . , x (Ï„ ). In general, RNNs allow the extension of the graphical
model view to represent not only a joint distribution over the y variables but
also a conditional distribution over y given x. As discussed in the context of
feedforward networks in section 6.2.1.1, any model representing a variable P (y; Î¸)
can be reinterpreted as a model representing a conditional distribution P (y|Ï‰ )
with Ï‰ = Î¸. We can extend such a model to represent a distribution P (y | x) by
using the same P (y | Ï‰) as before, but making Ï‰ a function of x. In the case of
an RNN, this can be achieved in diï¬€erent ways. We review here the most common
and obvious choices.
Previously, we have discussed RNNs that take a sequence of vectors x(t) for
t = 1, . . . , Ï„ as input. Another option is to take only a single vector x as input.
When x is a ï¬?xed-size vector, we can simply make it an extra input of the RNN
that generates the y sequence. Some common ways of providing an extra input to
an RNN are:
1. as an extra input at each time step, or
2. as the initial state h(0), or
3. both.
The ï¬?rst and most common approach is illustrated in ï¬?gure 10.9. The interaction
between the input x and each hidden unit vector h(t) is parametrized by a newly
introduced weight matrix R that was absent from the model of only the sequence
of y values. The same product xî€¾R is added as additional input to the hidden
units at every time step. We can think of the choice of x as determining the value
391

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

of xî€¾ R that is eï¬€ectively a new bias parameter used for each of the hidden units.
The weights remain independent of the input. We can think of this model as taking
the parameters Î¸ of the non-conditional model and turning them into Ï‰, where
the bias parameters within Ï‰ are now a function of the input.
y (tâˆ’1)

U

y (t )

U

L(tâˆ’1)

W

U

L(t)

o(t)

o(t+1)

V

V

V

R

W

R

h(t)

R

W

R

y (...)

L(t+1)

o(tâˆ’1)

h(tâˆ’1)

s(... )

y (t+1)

h(t+1)

W
h(... )

R

x

Figure 10.9: An RNN that maps a ï¬?xed-length vectorx into a distribution over sequences
Y. This RNN is appropriate for tasks such as image captioning, where a single image is
used as input to a model that then produces a sequence of words describing the image.
Each element y(t) of the observed output sequence serves both as input (for the current
time step) and, during training, as target (for the previous time step).

Rather than receiving only a single vector x as input, the RNN may receive
a sequence of vectors x(t) as input. The RNN described in equation 10.8 corresponds to a conditional distribution P (y(1) , . . . , y (Ï„ ) | x (1) , . . . , x(Ï„ ) ) that makes a
conditional independence assumption that this distribution factorizes as
î?™
P (y(t) | x(1) , . . . , x(t)).
(10.35)
t

To remove the conditional independence assumption, we can add connections from
the output at time t to the hidden unit at time t + 1, as shown in ï¬?gure 10.10. The
model can then represent arbitrary probability distributions over the y sequence.
This kind of model representing a distribution over a sequence given another
392

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

y (tâˆ’1)

y ( t)

y (t+1)

L(tâˆ’1)

L(t)

L(t+1)

R

R

o(tâˆ’1)

o(t)

o(t+1)

V

V

V

W
h(... )

R

W

W

W

h(tâˆ’1)

h(t)

h(t+1)

U

U

U

x(tâˆ’1)

x ( t)

x(t+1)

h(... )

Figure 10.10: A conditional recurrent neural network mapping a variable-length sequence
of x values into a distribution over sequences of y values of the same length. Compared to
ï¬?gure 10.3, this RNN contains connections from the previous output to the current state.
These connections allow this RNN to model an arbitrary distribution over sequences ofy
given sequences of x of the same length. The RNN of ï¬?gure 10.3 is only able to represent
distributions in which the y values are conditionally independent from each other given
the x values.

393

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

sequence still has one restriction, which is that the length of both sequences must
be the same. We describe how to remove this restriction in section 10.4.
y (tâˆ’1)

y (t)

y (t+1)

L(tâˆ’1)

L(t)

L(t+1)

o(tâˆ’1)

o(t)

o(t+1)

g (tâˆ’1)

g (t)

g (t+1)

h(tâˆ’1)

h(t)

h(t+1)

x(tâˆ’1)

x (t)

x (t+1)

Figure 10.11: Computation of a typical bidirectional recurrent neural network, meant
to learn to map input sequences x to target sequences y, with loss L(t) at each step t.
The h recurrence propagates information forward in time (towards the right) while the
g recurrence propagates information backward in time (towards the left). Thus at each
point t , the output units o(t) can beneï¬?t from a relevant summary of the past in its h(t)
input and from a relevant summary of the future in its g(t) input.

10.3

Bidirectional RNNs

All of the recurrent networks we have considered up to now have a â€œcausalâ€? structure, meaning that the state at time t only captures information from the past,
x(1) , . . . , x (tâˆ’1), and the present input x (t) . Some of the models we have discussed
also allow information from past y values to aï¬€ect the current state when the y
values are available.
However, in many applications we want to output a prediction of y(t) which may
394

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

depend on the whole input sequence. For example, in speech recognition, the correct
interpretation of the current sound as a phoneme may depend on the next few
phonemes because of co-articulation and potentially may even depend on the next
few words because of the linguistic dependencies between nearby words: if there
are two interpretations of the current word that are both acoustically plausible, we
may have to look far into the future (and the past) to disambiguate them. This is
also true of handwriting recognition and many other sequence-to-sequence learning
tasks, described in the next section.
Bidirectional recurrent neural networks (or bidirectional RNNs) were invented
to address that need (Schuster and Paliwal, 1997). They have been extremely successful (Graves, 2012) in applications where that need arises, such as handwriting
recognition (Graves et al., 2008; Graves and Schmidhuber, 2009), speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013) and bioinformatics (Baldi
et al., 1999).
As the name suggests, bidirectional RNNs combine an RNN that moves forward
through time beginning from the start of the sequence with another RNN that
moves backward through time beginning from the end of the sequence. Figure 10.11
illustrates the typical bidirectional RNN, with h(t) standing for the state of the
sub-RNN that moves forward through time and g(t) standing for the state of the
sub-RNN that moves backward through time. This allows the output units o (t)
to compute a representation that depends on both the past and the future but
is most sensitive to the input values around time t, without having to specify a
ï¬?xed-size window around t (as one would have to do with a feedforward network,
a convolutional network, or a regular RNN with a ï¬?xed-size look-ahead buï¬€er).
This idea can be naturally extended to 2-dimensional input, such as images, by
having four RNNs, each one going in one of the four directions: up, down, left,
right. At each point (i, j) of a 2-D grid, an output Oi,j could then compute a
representation that would capture mostly local information but could also depend
on long-range inputs, if the RNN is able to learn to carry that information.
Compared to a convolutional network, RNNs applied to images are typically more
expensive but allow for long-range lateral interactions between features in the
same feature map (Visin et al., 2015; Kalchbrenner et al., 2015). Indeed, the
forward propagation equations for such RNNs may be written in a form that shows
they use a convolution that computes the bottom-up input to each layer, prior
to the recurrent propagation across the feature map that incorporates the lateral
interactions.

395

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

10.4

Encoder-Decoder Sequence-to-Sequence Architectures

We have seen in ï¬?gure 10.5 how an RNN can map an input sequence to a ï¬?xed-size
vector. We have seen in ï¬?gure 10.9 how an RNN can map a ï¬?xed-size vector to a
sequence. We have seen in ï¬?gures 10.3, 10.4, 10.10 and 10.11 how an RNN can
map an input sequence to an output sequence of the same length.
Encoder
â€¦

x(1)

x(2)

x(...)

x(n x )

C

Decoder
â€¦

y (1)

y (2)

y (...)

y (n y )

Figure 10.12: Example of an encoder-decoder or sequence-to-sequence RNN architecture,
for learning to generate an output sequence (y (1) , . . . , y(n y) ) given an input sequence
(x(1), x (2) , . . . , x(nx )). It is composed of an encoder RNN that reads the input sequence
and a decoder RNN that generates the output sequence (or computes the probability of a
given output sequence). The ï¬?nal hidden state of the encoder RNN is used to compute a
generally ï¬?xed-size context variable C which represents a semantic summary of the input
sequence and is given as input to the decoder RNN.

Here we discuss how an RNN can be trained to map an input sequence to an
output sequence which is not necessarily of the same length. This comes up in
many applications, such as speech recognition, machine translation or question
396

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

answering, where the input and output sequences in the training set are generally
not of the same length (although their lengths might be related).
We often call the input to the RNN the â€œcontext.â€? We want to produce a
representation of this context, C . The context C might be a vector or sequence of
vectors that summarize the input sequence X = (x(1), . . . , x (nx )).
The simplest RNN architecture for mapping a variable-length sequence to
another variable-length sequence was ï¬?rst proposed by Cho et al. (2014a) and
shortly after by Sutskever et al. (2014), who independently developed that architecture and were the ï¬?rst to obtain state-of-the-art translation using this approach.
The former system is based on scoring proposals generated by another machine
translation system, while the latter uses a standalone recurrent network to generate
the translations. These authors respectively called this architecture, illustrated
in ï¬?gure 10.12, the encoder-decoder or sequence-to-sequence architecture. The
idea is very simple: (1) an encoder or reader or input RNN processes the input
sequence. The encoder emits the context C , usually as a simple function of its
ï¬?nal hidden state. (2) a decoder or writer or output RNN is conditioned on
that ï¬?xed-length vector (just like in ï¬?gure 10.9) to generate the output sequence
Y = (y(1) , . . . , y (ny )). The innovation of this kind of architecture over those
presented in earlier sections of this chapter is that the lengths n x and ny can
vary from each other, while previous architectures constrained nx = ny = Ï„. In a
sequence-to-sequence architecture, the two RNNs are trained jointly to maximize
the average of log P (y(1), . . . , y (ny ) | x(1) , . . . , x(nx ) ) over all the pairs of x and y
sequences in the training set. The last state hnx of the encoder RNN is typically
used as a representation C of the input sequence that is provided as input to the
decoder RNN.
If the context C is a vector, then the decoder RNN is simply a vector-tosequence RNN as described in section 10.2.4. As we have seen, there are at least
two ways for a vector-to-sequence RNN to receive input. The input can be provided
as the initial state of the RNN, or the input can be connected to the hidden units
at each time step. These two ways can also be combined.
There is no constraint that the encoder must have the same size of hidden layer
as the decoder.
One clear limitation of this architecture is when the context C output by the
encoder RNN has a dimension that is too small to properly summarize a long
sequence. This phenomenon was observed by Bahdanau et al. (2015) in the context
of machine translation. They proposed to make C a variable-length sequence rather
than a ï¬?xed-size vector. Additionally, they introduced an attention mechanism
that learns to associate elements of the sequence C to elements of the output
397

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

sequence. See section 12.4.5.1 for more details.

10.5

Deep Recurrent Networks

The computation in most RNNs can be decomposed into three blocks of parameters
and associated transformations:
1. from the input to the hidden state,
2. from the previous hidden state to the next hidden state, and
3. from the hidden state to the output.
With the RNN architecture of ï¬?gure 10.3, each of these three blocks is associated
with a single weight matrix. In other words, when the network is unfolded, each
of these corresponds to a shallow transformation. By a shallow transformation,
we mean a transformation that would be represented by a single layer within
a deep MLP. Typically this is a transformation represented by a learned aï¬ƒne
transformation followed by a ï¬?xed nonlinearity.
Would it be advantageous to introduce depth in each of these operations?
Experimental evidence (Graves et al., 2013; Pascanu et al., 2014a) strongly suggests
so. The experimental evidence is in agreement with the idea that we need enough
depth in order to perform the required mappings. See also Schmidhuber (1992),
El Hihi and Bengio (1996), or Jaeger (2007a) for earlier work on deep RNNs.
Graves et al. (2013) were the ï¬?rst to show a signiï¬?cant beneï¬?t of decomposing
the state of an RNN into multiple layers as in ï¬?gure 10.13 (left). We can think
of the lower layers in the hierarchy depicted in ï¬?gure 10.13a as playing a role
in transforming the raw input into a representation that is more appropriate, at
the higher levels of the hidden state. Pascanu et al. (2014a) go a step further
and propose to have a separate MLP (possibly deep) for each of the three blocks
enumerated above, as illustrated in ï¬?gure 10.13b. Considerations of representational
capacity suggest to allocate enough capacity in each of these three steps, but doing
so by adding depth may hurt learning by making optimization diï¬ƒcult. In general,
it is easier to optimize shallower architectures, and adding the extra depth of
ï¬?gure 10.13b makes the shortest path from a variable in time step t to a variable
in time step t + 1 become longer. For example, if an MLP with a single hidden
layer is used for the state-to-state transition, we have doubled the length of the
shortest path between variables in any two diï¬€erent time steps, compared with the
ordinary RNN of ï¬?gure 10.3. However, as argued by Pascanu et al. (2014a), this
398

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

y

y

h

h

x

x

x

(a)

(b)

(c)

y

z

h

Figure 10.13: A recurrent neural network can be made d
)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)169

unit vector u and a point x, the length of the projection of x onto u is given
by xT u. I.e., if x(i) is a point in our dataset (one of the crosses in the plot),
then its projection onto u (the corresponding circle in the ï¬gure) is distance
xT u from the origin. Hence, to maximize the variance of the projections, we
would like to choose a unit-length u so as to maximize:

1
n

n
(cid:88)

i=1

(x(i)T

u)2 =

1
n

n
(cid:88)

uT x(i)x(i)T

u

= uT

i=1
(cid:32)

1
n

n
(cid:88)

i=1

x(i)x(i)T

(cid:33)

u.

We easily recognize that the maximizing this subject to (cid:107)u(cid:107)2 = 1 gives the
i=1 x(i)x(i)T , which is just the empirical
principal eigenvector of Î£ = 1
n
covariance matrix of the data (assuming it has zero mean).1

(cid:80)n

To summarize, we have found that if we wish to ï¬nd a 1-dimensional
subspace with with to approximate the data, we should choose u to be the
principal eigenvector of Î£. More generally, if we wish to project our data
into a k-dimensional subspace (k < d), we should choose u1, . . . , uk to be the
top k eigenvectors of Î£. The uiâ€™s now form a new, orthogonal basis for the
data.2

Then, to represent x(i) in this basis, we need only compute the corre-

sponding vector

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

y(i) =

uT
1 x(i)
2 x(i)
uT
...
k x(i)
uT
Thus, whereas x(i) âˆˆ Rd, the vector y(i) now gives a lower, k-dimensional,
approximation/representation for x(i). PCA is therefore also referred to as
a dimensionality reduction algorithm. The vectors u1, . . . , uk are called
the ï¬rst k principal components of the data.

âˆˆ Rk.

ï£º
ï£º
ï£º
ï£»

Remark. Although we have shown it formally only for the case of k = 1,
using well-known properties of eigenvectors it is straightforward to show that

1If you havenâ€™t seen this before, try using the method of Lagrange multipliers to max-
imize uT Î£u subject to that uT u = 1. You should be able to show that Î£u = Î»u, for some
Î», which implies u is an eigenvector of Î£, with eigenvalue Î».

2Because Î£ is symmetric, the uiâ€™s will (or always can be chosen to be) orthogonal to

each other.

170

of all possible orthogonal bases u1, . . . , uk, the one that we have chosen max-
imizes (cid:80)
2. Thus, our choice of a basis preserves as much variability
as possible in the original data.

i (cid:107)y(i)(cid:107)2

PCA can also be derived by picking the basis that minimizes the ap-
proximation error arising from projecting the data onto the k-dimensional
subspace spanned by them. (See more in homework.)

PCA has many applications; we will close our discussion with a few exam-
ples. First, compressionâ€”representing x(i)â€™s with lower dimension y(i)â€™sâ€”is
an obvious application. If we reduce high dimensional data to k = 2 or 3 di-
mensions, then we can also plot the y(i)â€™s to visualize the data. For instance,
if we were to reduce our automobiles data to 2 dimensions, then we can plot
it (one point in our plot would correspond to one car type, say) to see what
cars are similar to each other and what groups of cars may cluster together.
Another standard application is to preprocess a dataset to reduce its
dimension before running a supervised learning learning algorithm with the
x(i)â€™s as inputs. Apart from computational beneï¬ts, reducing the dataâ€™s
dimension can also reduce the complexity of the hypothesis class considered
and help avoid overï¬tting (e.g., linear classiï¬ers over lower dimensional input
spaces will have smaller VC dimension).

Lastly, as in our RC pilot example, we can also view PCA as a noise
In our example it, estimates the intrinsic â€œpiloting
reduction algorithm.
karmaâ€ from the noisy measures of piloting skill and enjoyment. In class, we
also saw the application of this idea to face images, resulting in eigenfaces
method. Here, each point x(i) âˆˆ R100Ã—100 was a 10000 dimensional vector,
with each coordinate corresponding to a pixel intensity value in a 100x100
image of a face. Using PCA, we represent each image x(i) with a much lower-
dimensional y(i).
In doing so, we hope that the principal components we
found retain the interesting, systematic variations between faces that capture
what a person really looks like, but not the â€œnoiseâ€ in the images introduced
by minor lighting variations, slightly diï¬€erent imaging conditions, and so on.
We then measure distances between faces i and j by working in the reduced
dimension, and computing (cid:107)y(i) âˆ’ y(j)(cid:107)2. This resulted in a surprisingly good
face-matching and retrieval algorithm.

Chapter 13

Independent components
analysis

Our next topic is Independent Components Analysis (ICA). Similar to PCA,
this will ï¬nd a new basis in which to represent our data. However, the goal
is very diï¬€erent.

As a motivating example, consider the â€œcocktail party problem.â€ Here, d
speakers are speaking simultaneously at a party, and any microphone placed
in the room records only an overlapping combination of the d speakersâ€™ voices.
But lets say we have d diï¬€erent microphones placed in the room, and because
each microphone is a diï¬€erent distance from each of the speakers, it records a
diï¬€erent combination of the speakersâ€™ voices. Using these microphone record-
ings, can we separate out the original d speakersâ€™ speech signals?

To formalize this problem, we imagine that there is some data s âˆˆ Rd

that is generated via d independent sources. What we observe is

x = As,

where A is an unknown square matrix called the mixing matrix. Repeated
observations gives us a dataset {x(i); i = 1, . . . , n}, and our goal is to recover
the sources s(i) that had generated our data (x(i) = As(i)).

In our cocktail party problem, s(i) is an d-dimensional vector, and s(i)
is
j
the sound that speaker j was uttering at time i. Also, x(i) in an d-dimensional
vector, and x(i)
is the acoustic reading recorded by microphone j at time i.
j
Let W = Aâˆ’1 be the unmixing matrix. Our goal is to ï¬nd W , so
that given our microphone recordings x(i), we can recover the sources by
computing s(i) = W x(i). For notational convenience, we also let wT
i denote

171

172

the i-th row of W , so that

W =

ï£®

ï£¯
ï£°

â€” wT
1 â€”
...
â€” wT
d â€”

ï£¹

ï£º
ï£» .

Thus, wi âˆˆ Rd, and the j-th source can be recovered as s(i)

j = wT

j x(i).

13.1

ICA ambiguities

To what degree can W = Aâˆ’1 be recovered? If we have no prior knowledge
about the sources and the mixing matrix, it is easy to see that there are some
inherent ambiguities in A that are impossible to recover, given only the x(i)â€™s.
Speciï¬cally, let P be any d-by-d permutation matrix. This means that
each row and each column of P has exactly one â€œ1.â€ Here are some examples
of permutation matrices:

P =

ï£®

ï£°

0 1 0
1 0 0
0 0 1

ï£¹

ï£» ; P =

(cid:21)

(cid:20) 0 1
1 0

; P =

(cid:20) 1 0
0 1

(cid:21)

.

If z is a vector, then P z is another vector that contains a permuted version
of zâ€™s coordinates. Given only the x(i)â€™s, there will be no way to distinguish
between W and P W . Speciï¬cally, the permutation of the original sources is
ambiguous, which should be no surprise. Fortunately, this does not matter
for most applications.

Further, there is no way to recover the correct scaling of the wiâ€™s. For in-
stance, if A were replaced with 2A, and every s(i) were replaced with (0.5)s(i),
then our observed x(i) = 2A Â· (0.5)s(i) would still be the same. More broadly,
if a single column of A were scaled by a factor of Î±, and the corresponding
source were scaled by a factor of 1/Î±, then there is again no way to determine
that this had happened given only the x(i)â€™s. Thus, we cannot recover the
â€œcorrectâ€ scaling of the sources. However, for the applications that we are
concerned withâ€”including the cocktail party problemâ€”this ambiguity also
does not matter. Speciï¬cally, scaling a speakerâ€™s speech signal s(i)
j by some
positive factor Î± aï¬€ects only the volume of that speakerâ€™s speech. Also, sign
changes do not matter, and s(i)
sound identical when played on a
speaker. Thus, if the wi found by an algorithm is scaled by any non-zero real
number, the corresponding recovered source si = wT
i x will be scaled by the

j and âˆ’s(i)

j

173

same factor; but this usually does not matter. (These comments also apply
to ICA for the brain/MEG data that we talked about in class.)

Are these the only sources of ambiguity in ICA? It turns out that they
are, so long as the sources si are non-Gaussian. To see what the diï¬ƒculty is
with Gaussian data, consider an example in which n = 2, and s âˆ¼ N (0, I).
Here, I is the 2x2 identity matrix. Note that the contours of the density of
the standard normal distribution N (0, I) are circles centered on the origin,
and the density is rotationally symmetric.

Now, suppose we observe some x = As, where A is our mixing matrix.

Then, the distribution of x will be Gaussian, x âˆ¼ N (0, AAT ), since

Esâˆ¼N (0,I)[x] = E[As] = AE[s] = 0
Cov[x] = Esâˆ¼N (0,I)[xxT ] = E[AssT AT ] = AE[ssT ]AT = A Â· Cov[s] Â· AT = AAT

Now, let R be an arbitrary orthogonal (less formally, a rotation/reï¬‚ection)
matrix, so that RRT = RT R = I, and let A(cid:48) = AR. Then if the data had
been mixed according to A(cid:48) instead of A, we would have instead observed
x(cid:48) = A(cid:48)s. The distribution of x(cid:48) is also Gaussian, x(cid:48) âˆ¼ N (0, AAT ), since
Esâˆ¼N (0,I)[x(cid:48)(x(cid:48))T ] = E[A(cid:48)ssT (A(cid:48))T ] = E[ARssT (AR)T ] = ARRT AT = AAT .
Hence, whether the mixing matrix is A or A(cid:48), we would observe data from
a N (0, AAT ) distribution. Thus, there is no way to tell if the sources were
mixed using A and A(cid:48). There is an arbitrary rotational component in the
mixing matrix that cannot be determined from the data, and we cannot
recover the original sources.

Our argument above was based on the fact that the multivariate standard
normal distribution is rotationally symmetric. Despite the bleak picture that
this paints for ICA on Gaussian data, it turns out that, so long as the data is
not Gaussian, it is possible, given enough data, to recover the d independent
sources.

13.2 Densities and linear transformations

Before moving on to derive the ICA algorithm proper, we ï¬rst digress brieï¬‚y
to talk about the eï¬€ect of linear transformations on densities.

Suppose a random variable s is drawn according to some density ps(s).
For simplicity, assume for now that s âˆˆ R is a real number. Now, let the
random variable x be deï¬ned according to x = As (here, x âˆˆ R, A âˆˆ R). Let
px be the density of x. What is px?

Let W = Aâˆ’1. To calculate the â€œprobabilityâ€ of a particular value of x,
it is tempting to compute s = W x, then then evaluate ps at that point, and

174

conclude that â€œpx(x) = ps(W x).â€ However, this is incorrect. For example,
let s âˆ¼ Uniform[0, 1], so ps(s) = 1{0 â‰¤ s â‰¤ 1}. Now, let A = 2, so x = 2s.
Clearly, x is distributed uniformly in the interval [0, 2]. Thus, its density is
given by px(x) = (0.5)1{0 â‰¤ x â‰¤ 2}. This does not equal ps(W x), where
W = 0.5 = Aâˆ’1. Instead, the correct formula is px(x) = ps(W x)|W |.

More generally, if s is a vector-valued distribution with density ps, and

x = As for a square, invertible matrix A, then the density of x is given by

px(x) = ps(W x) Â· |W |,

where W = Aâˆ’1.

Remark. If youâ€™re seen the result that A maps [0, 1]d to a set of volume |A|,
then hereâ€™s another way to remember the formula for px given above, that also
generalizes our previous 1-dimensional example. Speciï¬cally, let A âˆˆ RdÃ—d be
given, and let W = Aâˆ’1 as usual. Also let C1 = [0, 1]d be the d-dimensional
hypercube, and deï¬ne C2 = {As : s âˆˆ C1} âŠ† Rd to be the image of C1
under the mapping given by A. Then it is 
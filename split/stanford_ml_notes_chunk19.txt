ed in the room, and because
each microphone is a diﬀerent distance from each of the speakers, it records a
diﬀerent combination of the speakers’ voices. Using these microphone record-
ings, can we separate out the original d speakers’ speech signals?

To formalize this problem, we imagine that there is some data s ∈ Rd

that is generated via d independent sources. What we observe is

x = As,

where A is an unknown square matrix called the mixing matrix. Repeated
observations gives us a dataset {x(i); i = 1, . . . , n}, and our goal is to recover
the sources s(i) that had generated our data (x(i) = As(i)).

In our cocktail party problem, s(i) is an d-dimensional vector, and s(i)
is
j
the sound that speaker j was uttering at time i. Also, x(i) in an d-dimensional
vector, and x(i)
is the acoustic reading recorded by microphone j at time i.
j
Let W = A−1 be the unmixing matrix. Our goal is to ﬁnd W , so
that given our microphone recordings x(i), we can recover the sources by
computing s(i) = W x(i). For notational convenience, we also let wT
i denote

171

172

the i-th row of W , so that

W =






— wT
1 —
...
— wT
d —




 .

Thus, wi ∈ Rd, and the j-th source can be recovered as s(i)

j = wT

j x(i).

13.1

ICA ambiguities

To what degree can W = A−1 be recovered? If we have no prior knowledge
about the sources and the mixing matrix, it is easy to see that there are some
inherent ambiguities in A that are impossible to recover, given only the x(i)’s.
Speciﬁcally, let P be any d-by-d permutation matrix. This means that
each row and each column of P has exactly one “1.” Here are some examples
of permutation matrices:

P =





0 1 0
1 0 0
0 0 1



 ; P =

(cid:21)

(cid:20) 0 1
1 0

; P =

(cid:20) 1 0
0 1

(cid:21)

.

If z is a vector, then P z is another vector that contains a permuted version
of z’s coordinates. Given only the x(i)’s, there will be no way to distinguish
between W and P W . Speciﬁcally, the permutation of the original sources is
ambiguous, which should be no surprise. Fortunately, this does not matter
for most applications.

Further, there is no way to recover the correct scaling of the wi’s. For in-
stance, if A were replaced with 2A, and every s(i) were replaced with (0.5)s(i),
then our observed x(i) = 2A · (0.5)s(i) would still be the same. More broadly,
if a single column of A were scaled by a factor of α, and the corresponding
source were scaled by a factor of 1/α, then there is again no way to determine
that this had happened given only the x(i)’s. Thus, we cannot recover the
“correct” scaling of the sources. However, for the applications that we are
concerned with—including the cocktail party problem—this ambiguity also
does not matter. Speciﬁcally, scaling a speaker’s speech signal s(i)
j by some
positive factor α aﬀects only the volume of that speaker’s speech. Also, sign
changes do not matter, and s(i)
sound identical when played on a
speaker. Thus, if the wi found by an algorithm is scaled by any non-zero real
number, the corresponding recovered source si = wT
i x will be scaled by the

j and −s(i)

j

173

same factor; but this usually does not matter. (These comments also apply
to ICA for the brain/MEG data that we talked about in class.)

Are these the only sources of ambiguity in ICA? It turns out that they
are, so long as the sources si are non-Gaussian. To see what the diﬃculty is
with Gaussian data, consider an example in which n = 2, and s ∼ N (0, I).
Here, I is the 2x2 identity matrix. Note that the contours of the density of
the standard normal distribution N (0, I) are circles centered on the origin,
and the density is rotationally symmetric.

Now, suppose we observe some x = As, where A is our mixing matrix.

Then, the distribution of x will be Gaussian, x ∼ N (0, AAT ), since

Es∼N (0,I)[x] = E[As] = AE[s] = 0
Cov[x] = Es∼N (0,I)[xxT ] = E[AssT AT ] = AE[ssT ]AT = A · Cov[s] · AT = AAT

Now, let R be an arbitrary orthogonal (less formally, a rotation/reﬂection)
matrix, so that RRT = RT R = I, and let A(cid:48) = AR. Then if the data had
been mixed according to A(cid:48) instead of A, we would have instead observed
x(cid:48) = A(cid:48)s. The distribution of x(cid:48) is also Gaussian, x(cid:48) ∼ N (0, AAT ), since
Es∼N (0,I)[x(cid:48)(x(cid:48))T ] = E[A(cid:48)ssT (A(cid:48))T ] = E[ARssT (AR)T ] = ARRT AT = AAT .
Hence, whether the mixing matrix is A or A(cid:48), we would observe data from
a N (0, AAT ) distribution. Thus, there is no way to tell if the sources were
mixed using A and A(cid:48). There is an arbitrary rotational component in the
mixing matrix that cannot be determined from the data, and we cannot
recover the original sources.

Our argument above was based on the fact that the multivariate standard
normal distribution is rotationally symmetric. Despite the bleak picture that
this paints for ICA on Gaussian data, it turns out that, so long as the data is
not Gaussian, it is possible, given enough data, to recover the d independent
sources.

13.2 Densities and linear transformations

Before moving on to derive the ICA algorithm proper, we ﬁrst digress brieﬂy
to talk about the eﬀect of linear transformations on densities.

Suppose a random variable s is drawn according to some density ps(s).
For simplicity, assume for now that s ∈ R is a real number. Now, let the
random variable x be deﬁned according to x = As (here, x ∈ R, A ∈ R). Let
px be the density of x. What is px?

Let W = A−1. To calculate the “probability” of a particular value of x,
it is tempting to compute s = W x, then then evaluate ps at that point, and

174

conclude that “px(x) = ps(W x).” However, this is incorrect. For example,
let s ∼ Uniform[0, 1], so ps(s) = 1{0 ≤ s ≤ 1}. Now, let A = 2, so x = 2s.
Clearly, x is distributed uniformly in the interval [0, 2]. Thus, its density is
given by px(x) = (0.5)1{0 ≤ x ≤ 2}. This does not equal ps(W x), where
W = 0.5 = A−1. Instead, the correct formula is px(x) = ps(W x)|W |.

More generally, if s is a vector-valued distribution with density ps, and

x = As for a square, invertible matrix A, then the density of x is given by

px(x) = ps(W x) · |W |,

where W = A−1.

Remark. If you’re seen the result that A maps [0, 1]d to a set of volume |A|,
then here’s another way to remember the formula for px given above, that also
generalizes our previous 1-dimensional example. Speciﬁcally, let A ∈ Rd×d be
given, and let W = A−1 as usual. Also let C1 = [0, 1]d be the d-dimensional
hypercube, and deﬁne C2 = {As : s ∈ C1} ⊆ Rd to be the image of C1
under the mapping given by A. Then it is a standard result in linear algebra
(and, indeed, one of the ways of deﬁning determinants) that the volume of
C2 is given by |A|. Now, suppose s is uniformly distributed in [0, 1]d, so its
density is ps(s) = 1{s ∈ C1}. Then clearly x will be uniformly distributed
in C2. Its density is therefore found to be px(x) = 1{x ∈ C2}/vol(C2) (since
it must integrate over C2 to 1). But using the fact that the determinant
of the inverse of a matrix is just the inverse of the determinant, we have
1/vol(C2) = 1/|A| = |A−1| = |W |. Thus, px(x) = 1{x ∈ C2}|W | = 1{W x ∈
C1}|W | = ps(W x)|W |.

13.3

ICA algorithm

We are now ready to derive an ICA algorithm. We describe an algorithm
by Bell and Sejnowski, and we give an interpretation of their algorithm as a
method for maximum likelihood estimation. (This is diﬀerent from their orig-
inal interpretation involving a complicated idea called the infomax principal
which is no longer necessary given the modern understanding of ICA.)

We suppose that the distribution of each source sj is given by a density

ps, and that the joint distribution of the sources s is given by

p(s) =

d
(cid:89)

j=1

ps(sj).

175

Note that by modeling the joint distribution as a product of marginals, we
capture the assumption that the sources are independent. Using our formulas
from the previous section, this implies the following density on x = As =
W −1s:

d
(cid:89)

p(x) =

ps(wT

j x) · |W |.

j=1

All that remains is to specify a density for the individual sources ps.

Recall that, given a real-valued random variable z, its cumulative distri-
−∞ pz(z)dz and

bution function (cdf) F is deﬁned by F (z0) = P (z ≤ z0) = (cid:82) z0
the density is the derivative of the cdf: pz(z) = F (cid:48)(z).

Thus, to specify a density for the si’s, all we need to do is to specify some
cdf for it. A cdf has to be a monotonic function that increases from zero
to one. Following our previous discussion, we cannot choose the Gaussian
cdf, as ICA doesn’t work on Gaussian data. What we’ll choose instead as
a reasonable “default” cdf that slowly increases from 0 to 1, is the sigmoid
function g(s) = 1/(1 + e−s). Hence, ps(s) = g(cid:48)(s).1

The square matrix W is the parameter in our model. Given a training

set {x(i); i = 1, . . . , n}, the log likelihood is given by

(cid:96)(W ) =

n
(cid:88)

(cid:32) d

(cid:88)

i=1

j=1

log g(cid:48)(wT

j x(i)) + log |W |

.

(cid:33)

We would like to maximize this in terms W . By taking derivatives and using
the fact (from the ﬁrst set of notes) that ∇W |W | = |W |(W −1)T , we easily
derive a stochastic gradient ascent learning rule. For a training example x(i),
the update rule is:

W := W + α















1 − 2g(wT
1 − 2g(wT

1 x(i))
2 x(i))

...

1 − 2g(wT

d x(i))








x(i)T + (W T )−1








,

1If you have prior knowledge that the sources’ densities take a certain form, then it
is a good idea to substitute that in here. But in the absence of such knowledge, the
sigmoid function can be thought of as a reasonable default that seems to work well for
many problems. Also, the presentation here assumes that either the data x(i) has been
preprocessed to have zero mean, or that it can naturally be expected to have zero mean
(such as acoustic signals). This is necessary because our assumption that ps(s) = g(cid:48)(s)
implies E[s] = 0 (the derivative of the logistic function is a symmetric function, and
hence gives a density corresponding to a random variable with zero mean), which implies
E[x] = E[As] = 0.

176

where α is the learning rate.

After the algorithm converges, we then compute s(i) = W x(i) to recover

the original sources.

Remark. When writing down the likelihood of the data, we implicitly as-
sumed that the x(i)’s were independent of each other (for diﬀerent values
of i; note this issue is diﬀerent from whether the diﬀerent coordinates of
x(i) are independent), so that the likelihood of the training set was given
by (cid:81)
i p(x(i); W ). This assumption is clearly incorrect for speech data and
other time series where the x(i)’s are dependent, but it can be shown that
having correlated training examples will not hurt the performance of the al-
gorithm if we have suﬃcient data. However, for problems where successive
training examples are correlated, when implementing stochastic gradient as-
cent, it sometimes helps accelerate convergence if we visit training examples
(I.e., run stochastic gradient ascent on a
in a randomly permuted order.
randomly shuﬄed copy of the training set.)

Chapter 14

Self-supervised learning and
foundation models

Despite its huge success, supervised learning with neural networks typically
relies on the availability of a labeled dataset of decent size, which is some-
times costly to collect. Recently, AI and machine learning are undergoing a
paradigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and
GPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and
are adaptable to a wide range of downstream tasks. These models, called
foundation models by Bommasani et al. [2021], oftentimes leverage massive
unlabeled data so that much fewer labeled data in the downstream tasks are
needed. Moreover, though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent capabil-
ities. These models are typically (pre-)trained by self-supervised learning
methods where the supervisions/labels come from parts of the inputs.

This chapter will introduce the paradigm of foundation models and basic

related concepts.

14.1 Pretraining and adaptation

The foundation models paradigm consists of two phases: pretraining (or sim-
ply training) and adaptation. We ﬁrst pretrain a large model on a massive
unlabeled dataset (e.g., billions of unlabeled images).1 Then, we adapt the
pretrained model to a downstream task (e.g., detecting cancer from scan im-
ages). These downstream tasks are often prediction tasks with limited or

1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-

geNet dataset).

177

178

even no labeled data. The intuition is that the pretrained models learn good
representations that capture intrinsic semantic structure/ information about
the data, and the adaptation phase customizes the model to a particular
downstream task by, e.g., retrieving the information speciﬁc to it. For ex-
ample, a model pretrained on massive unlabeled image data may learn good
general visual representations/features, and we adapt the representations to
solve biomedical imagining tasks.

We formalize the two phases below.

Pretraining.
Suppose we have an unlabeled pretraining dataset
{x(1), x(2) · · · , x(n)} that consists of n examples in Rd. Let φθ be a model that
is parameterized by θ and maps the input x to some m-dimensional represen-
tation φθ(x). (People also call φθ(x) ∈ Rm the embedding or features of the
example x.) We pretrain the model θ with a pretraining loss, which is often
an average of loss functions on all the examples: Lpre(θ) = 1
i=1 (cid:96)pre(θ, x(i)).
n
Here (cid:96)pre is a so-called self-supervised loss on a single datapoint x(i), because
as shown later, e.g., in Section 14.3, the “supervision” comes from the data
point x(i) itself.
It is also possible that the pretraining loss is not a sum
of losses on individual examples. We will discuss two pretraining losses in
Section 14.2 and Section 14.3.

(cid:80)n

We use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,
2014]) to minimize Lpre(θ). We denote the obtained pretrained model by ˆθ.

task

, y(ntask)
task

task, y(1)

task), · · · , (x(ntask)

Adaptation. For a downstream task, we usually have a labeled dataset
{(x(1)
)} with ntask examples. The setting when
ntask = 0 is called zero-shot learning—the downstream task doesn’t have any
labeled examples. When ntask is relatively small (say, between 1 and 50), the
setting is called few-shot learning. It’s also pretty common to have a larger
ntask on the order of ranging from hundreds to tens of thousands.

An adaptation algorithm generally takes in a downstream dataset and the
pretrained model ˆθ, and outputs a variant of ˆθ that solves the downstream
task. We will discuss below two popular and general adaptation methods,
linear probe and ﬁnetuning. In addition, two other methods speciﬁc to lan-
guage problems are introduced in 14.3.2.

The linear probe approach uses a linear head on top of the representation
to predict the downstream labels. Mathematically, the adapted model out-
puts w(cid:62)φˆθ(x), where w ∈ Rm is a parameter to be learned, and ˆθ is exactly
the pretrained model (ﬁxed). We can use SGD (or other optimizers) to train

w on the downstream task loss to predict the task label

min
w∈Rm

1
ntask

ntask(cid:88)

i=1

(cid:96)task(y(i)

task, w(cid:62)φˆθ(x(i)

task))

179

(14.1)

if the downstream task is a regression problem, we will have
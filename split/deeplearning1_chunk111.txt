loring strategies for
training deep neural networks. Journal of Machine Learning Research, 10, 1â€“40. 535
Lasserre, J. A., Bishop, C. M., and Minka, T. P. (2006). Principled hybrids of generative and
discriminative models. In Proceedings of the Computer Vision and Pattern Recognition
Conference (CVPRâ€™06), pages 87â€“94, Washington, DC, USA. IEEE Computer Society.
244, 253
Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010). Tiled
convolutional neural networks. In J. Laï¬€erty, C. K. I. Williams, J. Shawe-Taylor,
R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems
23 (NIPSâ€™10), pages 1279â€“1287. 352
Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. (2011). On optimization
methods for deep learning. In Proc. ICMLâ€™2011 . ACM. 316
Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng,
A. (2012). Building high-level features using large scale unsupervised learning. In
ICMLâ€™2012 . 24, 27
Le Roux, N. and Bengio, Y. (2008). Representational power of restricted Boltzmann
machines and deep belief networks. Neural Computation, 20(6), 1631â€“1649. 553, 655
Le Roux, N. and Bengio, Y. (2010). Deep belief networks are compact universal approximators. Neural Computation , 22(8), 2192â€“2207. 553
LeCun, Y. (1985). Une procÃ©dure dâ€™apprentissage pour RÃ©seau Ã  seuil assymÃ©trique. In
Cognitiva 85: A la FrontiÃ¨re de lâ€™Intelligence Artiï¬?cielle, des Sciences de la Connaissance
et des Neurosciences, pages 599â€“604, Paris 1985. CESTA, Paris. 225
LeCun, Y. (1986). Learning processes in an asymmetric threshold network. In F. FogelmanSouliÃ©, E. Bienenstock, and G. Weisbuch, editors, Disordered Systems and Biological
Organization , pages 233â€“240. Springer-Verlag, Les Houches, France. 352
LeCun, Y. (1987). ModÃ¨les connexionistes de lâ€™apprentissage. Ph.D. thesis, UniversitÃ© de
Paris VI. 18, 502, 515
LeCun, Y. (1989). Generalization and network design strategies. Technical Report
CRG-TR-89-4, University of Toronto. 330, 352

751

BIBLIOGRAPHY

LeCun, Y., Jackel, L. D., Boser, B., Denker, J. S., Graf, H. P., Guyon, I., Henderson, D.,
Howard, R. E., and Hubbard, W. (1989). Handwritten digit recognition: Applications
of neural network chips and automatic learning. IEEE Communications Magazine,
27(11), 41â€“46. 368
LeCun, Y., Bottou, L., Orr, G. B., and MÃ¼ller, K.-R. (1998a). Eï¬ƒcient backprop. In
Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524.
Springer Verlag. 310, 429
LeCun, Y., Bottou, L., Bengio, Y., and Haï¬€ner, P. (1998b). Gradient based learning
applied to document recognition. Proc. IEEE . 16, 18, 21, 27, 371, 458, 460
LeCun, Y., Kavukcuoglu, K., and Farabet, C. (2010). Convolutional networks and
applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE
International Symposium on , pages 253â€“256. IEEE. 371
Lâ€™Ecuyer, P. (1994). Eï¬ƒciency improvement and variance reduction. In Proceedings of
the 1994 Winter Simulation Conference, pages 122â€“â€“132. 690
Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2014). Deeply-supervised nets.
arXiv preprint arXiv:1409.5185 . 326
Lee, H., Battle, A., Raina, R., and Ng, A. (2007). Eï¬ƒcient sparse coding algorithms.
In B. SchÃ¶lkopf, J. Platt, and T. Hoï¬€man, editors, Advances in Neural Information
Processing Systems 19 (NIPSâ€™06), pages 801â€“808. MIT Press. 637
Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area
V2. In NIPSâ€™07 . 255
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief
networks for scalable unsupervised learning of hierarchical representations. In L. Bottou
and M. Littman, editors, Proceedings of the Twenty-sixth International Conference on
Machine Learning (ICMLâ€™09). ACM, Montreal, Canada. 363, 683, 684
Lee, Y. J. and Grauman, K. (2011). Learning the easy things ï¬?rst: self-paced visual
category discovery. In CVPRâ€™2011 . 328
Leibniz, G. W. (1676). Memoir using the chain rule. (Cited in TMME 7:2&3 p 321-332,
2010). 225
Lenat, D. B. and Guha, R. V. (1989). Building large knowledge-based systems; representation and inference in the Cyc project . Addison-Wesley Longman Publishing Co., Inc.
2
Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. (1993). Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function.
Neural Networks , 6, 861â€“â€“867. 198, 199
752

BIBLIOGRAPHY

Levenberg, K. (1944). A method for the solution of certain non-linear problems in least
squares. Quarterly Journal of Applied Mathematics , II(2), 164â€“168. 312
Lâ€™HÃ´pital, G. F. A. (1696). Analyse des inï¬?niment petits, pour lâ€™intelligence des lignes
courbes. Paris: Lâ€™Imprimerie Royale. 225
Li, Y., Swersky, K., and Zemel, R. S. (2015). Generative moment matching networks.
CoRR, abs/1502.02761. 703
Lin, T., Horne, B. G., Tino, P., and Giles, C. L. (1996). Learning long-term dependencies
is not as diï¬ƒcult with NARX recurrent neural networks. IEEE Transactions on Neural
Networks, 7(6), 1329â€“1338. 407
Lin, Y., Liu, Z., Sun, M., Liu, Y., and Zhu, X. (2015). Learning entity and relation
embeddings for knowledge graph completion. In Proc. AAAIâ€™15 . 484
Linde, N. (1992). The machine that changed the world, episode 3. Documentary miniseries.
2
Lindsey, C. and Lindblad, T. (1994). Review of hardware neural networks: a userâ€™s
perspective. In Proc. Third Workshop on Neural Networks: From Biology to High
Energy Physics , pages 195â€“â€“202, Isola dâ€™Elba, Italy. 451
Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. BIT
Numerical Mathematics , 16(2), 146â€“160. 225
LISA (2008). Deep learning tutorials: Restricted Boltzmann machines. Technical report,
LISA Lab, UniversitÃ© de MontrÃ©al. 589
Long, P. M. and Servedio, R. A. (2010). Restricted Boltzmann machines are hard to
approximately evaluate or simulate. In Proceedings of the 27th International Conference
on Machine Learning (ICMLâ€™10). 658
Lotter, W., Kreiman, G., and Cox, D. (2015). Unsupervised learning of visual structure
using predictive generative networks. arXiv preprint arXiv:1511.06380 . 544, 545
Lovelace, A. (1842). Notes upon L. F. Menabreaâ€™s â€œ Sketch of the Analytical Engine
invented by Charles Babbageâ€?. 1
Lu, L., Zhang, X., Cho, K., and Renals, S. (2015). A study of the recurrent neural network
encoder-decoder for large vocabulary speech recognition. In Proc. Interspeech . 461
Lu, T., PÃ¡l, D., and PÃ¡l, M. (2010). Contextual multi-armed bandits. In International
Conference on Artiï¬?cial Intelligence and Statistics , pages 485â€“492. 480
Luenberger, D. G. (1984). Linear and Nonlinear Programming. Addison Wesley. 316
LukoÅ¡eviÄ?ius, M. and Jaeger, H. (2009). Reservoir computing approaches to recurrent
neural network training. Computer Science Review, 3(3), 127â€“149. 404
753

BIBLIOGRAPHY

Luo, H., Shen, R., Niu, C., and Ullrich, C. (2011). Learning class-relevant features and
class-irrelevant features via a hybrid third-order RBM. In International Conference on
Artiï¬?cial Intelligence and Statistics, pages 470â€“478. 686
Luo, H., Carrier, P. L., Courville, A., and Bengio, Y. (2013). Texture modeling with
convolutional spike-and-slab RBMs and deep extensions. In AISTATSâ€™2013 . 102
Lyu, S. (2009). Interpretation and generalization of score matching. In Proceedings of the
Twenty-ï¬?fth Conference in Uncertainty in Artiï¬?cial Intelligence (UAIâ€™09). 618
Ma, J., Sheridan, R. P., Liaw, A., Dahl, G. E., and Svetnik, V. (2015). Deep neural nets
as a method for quantitative structure â€“ activity relationships. J. Chemical information
and modeling. 530
Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectiï¬?er nonlinearities improve neural
network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech, and
Language Processing. 193
Maass, W. (1992). Bounds for the computational power and learning complexity of analog
neural nets (extended abstract). In Proc. of the 25th ACM Symp. Theory of Computing ,
pages 335â€“344. 199
Maass, W., Schnitger, G., and Sontag, E. D. (1994). A comparison of the computational
power of sigmoid and Boolean threshold circuits. Theoretical Advances in Neural
Computation and Learning , pages 127â€“151. 199
Maass, W., Natschlaeger, T., and Markram, H. (2002). Real-time computing without
stable states: A new framework for neural computation based on perturbations. Neural
Computation, 14(11), 2531â€“2560. 404
MacKay, D. (2003). Information Theory, Inference and Learning Algorithms. Cambridge
University Press. 73
Maclaurin, D., Duvenaud, D., and Adams, R. P. (2015). Gradient-based hyperparameter
optimization through reversible learning. arXiv preprint arXiv:1502.03492 . 435
Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., and Yuille, A. L. (2015). Deep captioning
with multimodal recurrent neural networks. In ICLRâ€™2015 . arXiv:1410.1090. 102
Marcotte, P. and Savard, G. (1992). Novel approaches to the discrimination problem.
Zeitschrift fÃ¼r Operations Research (Theory), 36, 517â€“545. 276
Marlin, B. and de Freitas, N. (2011). Asymptotic eï¬ƒciency of deterministic estimators for
discrete energy-based models: Ratio matching and pseudolikelihood. In UAIâ€™2011 . 617,
619

754

BIBLIOGRAPHY

Marlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010). Inductive principles for
restricted Boltzmann machine learning. In Proceedings of The Thirteenth International
Conference on Artiï¬?cial Intelligence and Statistics (AISTATSâ€™10), volume 9, pages
509â€“516. 613, 618, 619
Marquardt, D. W. (1963). An algorithm for least-squares estimation of non-linear parameters. Journal of the Society of Industrial and Applied Mathematics, 11(2), 431â€“441.
312
Marr, D. and Poggio, T. (1976). Cooperative computation of stereo disparity. Science,
194. 367
Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and
M. Littman, editors, Proceedings of the Twenty-seventh International Conference on
Machine Learning (ICML-10), pages 735â€“742. ACM. 304
Martens, J. and Medabalimi, V. (2014). On the expressive eï¬ƒciency of sum product
networks. arXiv:1411.7717 . 554
Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free
optimization. In Proc. ICMLâ€™2011 . ACM. 413
Mase, S. (1995). Consistency of the maximum pseudo-likelihood estimator of continuous
state space Gibbsian processes. The Annals of Applied Probability , 5(3), pp. 603â€“612.
616
McClelland, J., Rumelhart, D., and Hinton, G. (1995). The appeal of parallel distributed
processing. In Computation & intelligence, pages 305â€“341. American Association for
Artiï¬?cial Intelligence. 17
McCulloch, W. S. and Pitts, W. (1943). A logical calculus of ideas immanent in nervous
activity. Bulletin of Mathematical Biophysics, 5, 115â€“133. 14, 15
Mead, C. and Ismail, M. (2012). Analog VLSI implementation of neural systems , volume 80.
Springer Science & Business Media. 451
Melchior, J., Fischer, A., and Wiskott, L. (2013). How to center binary deep Boltzmann
machines. arXiv preprint arXiv:1311.1354 . 674
Memisevic, R. and Hinton, G. E. (2007). Unsupervised learning of image transformations.
In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPRâ€™07).
686
Memisevic, R. and Hinton, G. E. (2010). Learning to represent spatial transformations
with factored higher-order Boltzmann machines. Neural Computation ,22(6), 1473â€“1492.
686

755

BIBLIOGRAPHY

Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E.,
Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra,
J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In
JMLR W&CP: Proc. Unsupervised and Transfer Learning , volume 7. 201, 532, 538
Mesnil, G., Rifai, S., Dauphin, Y., Bengio, Y., and Vincent, P. (2012). Surï¬?ng on the
manifold. Learning Workshop, Snowbird. 711
Miikkulainen, R. and Dyer, M. G. (1991). Natural language processing with modular
PDP networks and distributed lexicon. Cognitive Science, 15, 343â€“399. 477
Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis,
Brno University of Technology. 414
Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011a). Empirical
evaluation and combination of advanced language modeling techniques. In Proc. 12th annual conference of the international speech communication association (INTERSPEECH
2011). 472
Mikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. (2011b). Strategies for
training large scale neural network language models. In Proc. ASRUâ€™2011 . 328, 472
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Eï¬ƒcient estimation of word representations in vector space. In International Conference on Learning Representations:
Workshops Track . 536
Mikolov, T., Le, Q. V., and Sutskever, I. (2013b). Exploiting similarities among languages
for machine translation. Technical report, arXiv:1309.4168. 539
Minka, T. (2005). Divergence measures and message passing. Microsoft Research Cambridge
UK Tech Rep MSRTR2005173 , 72(TR-2005-173). 625
Minsky, M. L. and Papert, S. A. (1969). Perceptrons. MIT Press, Cambridge. 15
Mirza, M. and Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784 . 702
Mishkin, D. and Matas, J. (2015).
arXiv:1511.06422 . 305

All you need is a good init.

arXiv preprint

Misra, J. and Saha, I. (2010). Artiï¬?cial neural networks in hardware: A survey of two
decades of progress. Neurocomputing, 74(1), 239â€“255. 451
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill, New York. 99
Miyato, T., Maeda, S., Koyama, M., Nakae, K., and Ishii, S. (2015). Distributional
smoothing with virtual adversarial training. In ICLR . Preprint: arXiv:1507.00677. 269
756

BIBLIOGRAPHY

Mnih, A. and Gregor, K. (2014). Neural variational inference and learning in belief
networks. In ICMLâ€™2014 . 691, 692, 693
Mnih, A. and Hinton, G. E. (2007). Three new graphical models for statistical language
modelling. In Z. Ghahramani, editor, Proceedings of the Twenty-fourth International
Conference on Machine Learning (ICMLâ€™07), pages 641â€“648. ACM. 465
Mnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model.
In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21 (NIPSâ€™08), pages 1081â€“1088. 467
Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings eï¬ƒciently with noisecontrastive estimation. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Weinberger, editors, Advances in Neural Information Processing Systems 26 , pages
2265â€“2273. Curran Associates, Inc. 472, 622
Mnih, A. and Teh, Y. W. (2012). A fast and simple algorithm for training neural
probabilistic language models. In ICMLâ€™2012 , pages 1751â€“1758. 472
Mnih, V. and Hinton, G. (2010). Learning to detect roads in high-resolution aerial images.
In Proceedings of the 11th European Conference on Computer Vision (ECCV). 102
Mnih, V., Larochelle, H., and Hinton, G. (2011). Conditional restricted Boltzmann
machines for structure output prediction. In Proc. Conf. on Uncertainty in Artiï¬?cial
Intelligence (UAI). 685
Mnih, V., Kavukcuoglo, K., Silver, D., Graves, A., Antonoglou, I., and Wierstra, D. (2013).
Playing Atari with deep reinforcement learning. Technical report, arXiv:1312.5602. 106
Mnih, V., Heess
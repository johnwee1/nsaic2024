ismappedtoasingleentryinthegrid(seethecolumnintroducedbyMicrosoftinTable1).Table1Afragmentoftheentitygrid.Nounphrasesarerepresentedbytheirheadnouns.Gridcellscorrespondtogrammaticalroles:subjects(S),objects(O),orneither(X).DepartmentTrialMicrosoftEvidenceCompetitorsMarketsProductsBrandsCaseNetscapeSoftwareTacticsGovernmentSuitEarnings1SOSXO––––––––––12––O––XSO–––––––23––SO––––SOO––––34––S––––––––S–––45––––––––––––SO–56–XS–––––––––––O66BarzilayandLapataModelingLocalCoherenceTable2Summaryaugmentedwithsyntacticannotationsforgridcomputation.1[TheJusticeDepartment]Sisconductingan[anti-trusttrial]Oagainst[MicrosoftCorp.]Xwith[evidence]Xthat[thecompany]Sisincreasinglyattemptingtocrush[competitors]O.2[Microsoft]Oisaccusedoftryingtoforcefullybuyinto[markets]Xwhere[itsownproducts]Sarenotcompetitiveenoughtounseat[establishedbrands]O.3[Thecase]Srevolvesaround[evidence]Oof[Microsoft]Saggressivelypressuring[Netscape]Ointomerging[browsersoftware]O.4[Microsoft]Sclaims[itstactics]Sarecommonplaceandgoodeconomically.5[Thegovernment]Smayﬁle[acivilsuit]Orulingthat[conspiracy]Stocurb[competition]Othrough[collusion]Xis[aviolationoftheShermanAct]O.6[Microsoft]Scontinuestoshow[increasedearnings]Odespite[thetrial]X.Whenanounisattestedmorethanoncewithadifferentgrammaticalroleinthesamesentence,wedefaulttotherolewiththehighestgrammaticalranking:subjectsarerankedhigherthanobjects,whichinturnarerankedhigherthantherest.Forexample,theentityMicrosoftismentionedtwiceinSentence1withthegrammaticalrolesx(forMicrosoftCorp.)ands(forthecompany),butisrepresentedonlybysinthegrid(seeTables1and2).3.2EntityGridsasFeatureVectorsAfundamentalassumptionunderlyingourapproachisthatthedistributionofentitiesincoherenttextsexhibitscertainregularitiesreﬂectedingridtopology.SomeoftheseregularitiesareformalizedinCenteringTheoryasconstraintsontransitionsofthelocalfocusinadjacentsentences.Gridsofcoherenttextsarelikelytohavesomedensecolumns(i.e.,columnswithjustafewgaps,suchasMicrosoftinTable1)andmanysparsecolumnswhichwillconsistmostlyofgaps(seemarketsandearningsinTable1).Onewouldfurtherexpectthatentitiescorrespondingtodensecolumnsaremoreoftensubjectsorobjects.Thesecharacteristicswillbelesspronouncedinlow-coherencetexts.InspiredbyCenteringTheory,ouranalysisrevolvesaroundpatternsoflocalentitytransitions.Alocalentitytransitionisasequence{S,O,X,–}nthatrepresentsentityoccurrencesandtheirsyntacticrolesinnadjacentsentences.Localtransitionscanbeeasilyobtainedfromagridascontinuoussubsequencesofeachcolumn.Eachtransitionwillhaveacertainprobabilityinagivengrid.Forinstance,theprobabilityofthetransition[S–]inthegridfromTable1is0.08(computedasaratioofitsfrequency[i.e.,six]dividedbythetotalnumberoftransitionsoflengthtwo[i.e.,75]).Eachtextcanthusbeviewedasadistributiondeﬁnedovertransitiontypes.Wecannowgoonestepfurtherandrepresenteachtextbyaﬁxedsetoftransitionsequencesusingastandardfeaturevectornotation.EachgridrenderingjofadocumentdicorrespondstoafeaturevectorΦ(xij)=(p1(xij),p2(xij),...,pm(xij)),wheremisthenumberofallpredeﬁnedentitytransitions,andpt(xij)theprobabilityoftransitiontingridxij.Thisfeaturevectorrepresentationisusefullyamenabletomachinelearningalgorithms(seeourexperimentsinSections4–6).Furthermore,itallowstheconsid-erationoflargenumbersoftransitionswhichcouldpotentiallyuncovernovelentitydistributionpatternsrelevantforcoherenceassessmentorothercoherence-relatedtasks.Notethatconsiderablelatitudeisavailablewhenspecifyingthetransitiontypestobeincludedinafeaturevector.Thesecanbealltransitionsofagivenlength(e.g.,twoorthree)orthemostfrequenttransitionswithinadocumentcollection.Anexampleof7ComputationalLinguisticsVolume34,Number1afeaturespacewithtransitionsoflengthtwoisillustratedinTable3.Thesecondrow(introducedbyd1)isthefeaturevectorrepresentationofthegridinTable1.3.3GridConstruction:LinguisticDimensionsOneofthecentralresearchissuesindevelopingentity-basedmodelsofcoherenceisdeterminingwhatsourcesoflinguisticknowledgeareessentialforaccurateprediction,andhowtoencodethemsuccinctlyinadiscourserepresentation.Previousapproachestendtoagreeonthefeaturesofentitydistributionrelatedtolocalcoherence—thedisagreementliesinthewaythesefeaturesaremodeled.Ourstudyofalternativeencodingsisnotamereduplicationofpreviousef-forts(Poesioetal.2004)thatfocusonlinguisticaspectsofparameterization.Becauseweareinterestedinanautomaticallyconstructedmodel,wehavetotakeintoaccountcom-putationalandlearningissueswhenconsideringalternativerepresentations.Therefore,ourexplorationoftheparameterspaceisguidedbythreeconsiderations:thelinguisticimportanceofaparameter,theaccuracyofitsautomaticcomputation,andthesizeoftheresultingfeaturespace.Fromthelinguisticside,wefocusonpropertiesofentitydistri-butionthataretightlylinkedtolocalcoherence,andatthesametimeallowformultipleinterpretationsduringtheencodingprocess.Computationalconsiderationspreventusfromconsideringdiscourserepresentationsthatcannotbecomputedreliablybyexist-ingtools.Forinstance,wecouldnotexperimentwiththegranularityofanutterance—sentenceversusclause—becauseavailableclauseseparatorsintroducesubstantialnoiseintoagridconstruction.Finally,weexcluderepresentationsthatwillexplodethesizeofthefeaturespace,therebyincreasingtheamountofdatarequiredfortrainingthemodel.EntityExtraction.Theaccuratecomputationofentityclassesiskeytocomputingmean-ingfulentitygrids.Inpreviousimplementationsofentity-basedmodels,classesofcoref-erentnounshavebeenextractedmanually(MiltsakakiandKukich2000;Karamanisetal.2004;Poesioetal.2004),butthisisnotanoptionforourmodel.Anobvioussolutionforidentifyingentityclassesistoemployanautomaticcoreferenceresolutiontoolthatdetermineswhichnounphrasesrefertothesameentityinadocument.Currentapproachesrecastcoreferenceresolutionasaclassiﬁcationtask.ApairofNPsisclassiﬁedascoreferringornotbasedonconstraintsthatarelearnedfromanannotatedcorpus.AseparateclusteringmechanismthencoordinatesthepossiblycontradictorypairwiseclassiﬁcationsandconstructsapartitiononthesetofNPs.Inourexperiments,weemployNgandCardie’s(2002)coreferenceresolutionsystem.ThesystemdecideswhethertwoNPsarecoreferentbyexploitingawealthoflexical,grammatical,semantic,andpositionalfeatures.ItistrainedontheMUC(6–7)datasetsandyieldsstate-of-the-artperformance(70.4F-measureonMUC-6and63.4onMUC-7).Table3Exampleofafeature-vectordocumentrepresentationusingalltransitionsoflengthtwogivensyntacticcategoriesS,O,X,and–.SSSOSXS–OSOOOXO–XSXOXXX––S–O–X––d1.01.010.08.0100.09000.03.05.07.03.59d2.02.01.01.020.070.02.14.14.06.04.03.070.1.36d3.0200.03.090.09.06000.05.03.07.17.398524 CHAPTER 23

• DISCOURSE COHERENCE

a modiﬁed order (such as a randomized order). We turn to these evaluations in the
next section.

23.3.3 Evaluating Neural and Entity-based coherence

Entity-based coherence models, as well as the neural models we introduce in the
next section, are generally evaluated in one of two ways.

First, we can have humans rate the coherence of a document and train a classiﬁer
to predict these human ratings, which can be categorial (high/low, or high/mid/low)
or continuous. This is the best evaluation to use if we have some end task in mind,
like essay grading, where human raters are the correct deﬁnition of the ﬁnal label.

Alternatively, since it’s very expensive to get human labels, and we might not
yet have an end-task in mind, we can use natural texts to do self-supervision. In
self-supervision we pair up a natural discourse with a pseudo-document created by
changing the ordering. Since naturally-ordered discourses are more coherent than
random permutation (Lin et al., 2011), a successful coherence algorithm should pre-
fer the original ordering.

Self-supervision has been implemented in 3 ways. In the sentence order dis-
crimination task (Barzilay and Lapata, 2005), we compare a document to a random
permutation of its sentence. A model is considered correct for an (original, per-
muted) test pair if it ranks the original document higher. Given k documents, we can
compute n permutations, resulting in kn pairs each with one original document and
one permutation, to use in training and testing.

In the sentence insertion task (Chen et al., 2007) we take a document, remove
one of the n sentences s, and create n
1 copies of the document with s inserted into
each position. The task is to decide which of the n documents is the one with the
original ordering, distinguishing the original position for s from all other positions.
Insertion is harder than discrimination since we are comparing documents that differ
by only one sentence.

−

Finally, in the sentence order reconstruction task (Lapata, 2003), we take a
document, randomize the sentences, and train the model to put them back in the
correct order. Again given k documents, we can compute n permutations, resulting
in kn pairs each with one original document and one permutation, to use in training
and testing. Reordering is of course a much harder task than simple classiﬁcation.

23.4 Representation learning models for local coherence

The third kind of local coherence is topical or semantic ﬁeld coherence. Discourses
cohere by talking about the same topics and subtopics, and drawing on the same
semantic ﬁelds in doing so.

The ﬁeld was pioneered by a series of unsupervised models in the 1990s of this
kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976):
the sharing of identical or semantically related words in nearby sentences. Morris
and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that
occurred through a discourse and that were related in Roget’s Thesaurus (by being in
the same category, or linked categories). They showed that the number and density
of chain correlated with the topic structure. The TextTiling algorithm of Hearst
(1997) computed the cosine between neighboring text spans (the normalized dot
product of vectors of raw word counts), again showing that sentences or paragraph in

lexical cohesion

TextTiling

23.4

• REPRESENTATION LEARNING MODELS FOR LOCAL COHERENCE

525

a subtopic have high cosine with each other, but not with sentences in a neighboring
subtopic.

A third early model, the LSA Coherence method of Foltz et al. (1998) was the
ﬁrst to use embeddings, modeling the coherence between two sentences as the co-
sine between their LSA sentence embedding vectors1, computing embeddings for a
sentence s by summing the embeddings of its words w:

sim(s,t) = cos(s, t)

= cos(

w,

w)

(23.31)

and deﬁning the overall coherence of a text as the average similarity over all pairs of
adjacent sentences si and si+1:

s
w
(cid:88)
∈

t
w
(cid:88)
∈

n
1
−

1

n

1

coherence(T ) =

cos(si, si+1)

(23.32)

(cid:88)i=1

−
Modern neural representation-learning coherence models, beginning with Li et al.
(2014), draw on the intuitions of these early unsupervised models for learning sen-
tence representations and measuring how they change between neighboring sen-
tences. But the new models also draw on the idea pioneered by Barzilay and Lapata
(2005) of self-supervision. That is, unlike say coherence relation models, which
train on hand-labeled representations for RST or PDTB, these models are trained to
distinguish natural discourses from unnatural discourses formed by scrambling the
order of sentences, thus using representation learning to discover the features that
matter for at least the ordering aspect of coherence.

Here we present one such model, the local coherence discriminator (LCD) (Xu
et al., 2019). Like early models, LCD computes the coherence of a text as the av-
erage of coherence scores between consecutive pairs of sentences. But unlike the
early unsupervised models, LCD is a self-supervised model trained to discriminate
consecutive sentence pairs (si, si+1) in the training documents (assumed to be coher-
ent) from (constructed) incoherent pairs (si, s(cid:48)). All consecutive pairs are positive
examples, and the negative (incoherent) partner for a sentence si is another sentence
uniformly sampled from the same document as si.

Fig. 23.11 describes the architecture of the model fθ , which takes a sentence
pair and returns a score, higher scores for more coherent pairs. Given an input
sentence pair s and t, the model computes sentence embeddings s and t (using any
sentence embeddings algorithm), and then concatenates four features of the pair: (1)
t; (3) the absolute value
the concatenation of the two vectors (2) their difference s
t. These are passed
of their difference
through a one-layer feedforward network to output the coherence score.

−
; (4) their element-wise product s
|

s
|

(cid:12)

−

t

The model is trained to make this coherence score higher for real pairs than for
negative pairs. More formally, the training objective for a corpus C of documents d,
each of which consists of a list of sentences si, is:

Lθ =

d
C (cid:88)si∈
(cid:88)d
∈

[L( fθ (si, si+1), fθ (si, s(cid:48)))]

E
p(s(cid:48)|

si)

(23.33)

si) is the expectation with respect to the negative sampling distribution con-
Ep(s(cid:48)|
ditioned on si: given a sentence si the algorithms samples a negative sentence s(cid:48)

1 See Chapter 6 for more on LSA embeddings; they are computed by applying SVD to the term-
document matrix (each cell weighted by log frequency and normalized by entropy), and then the ﬁrst
300 dimensions are used as the embedding.

526 CHAPTER 23

• DISCOURSE COHERENCE

Figure 23.11 The architecture of the LCD model of document coherence, showing the
computation of the score for a pair of sentences s and t. Figure from Xu et al. (2019).

uniformly over the other sentences in the same document. L is a loss function that
takes two scores, one for a positive pair and one for a negative pair, with the goal of
encouraging f + = fθ (si, si+1) to be high and f − = fθ (si, s(cid:48))) to be low. Fig. 23.11
use the margin loss l( f +, f −) = max(0, η
f + + f −) where η is the margin hyper-
parameter.

−

Xu et al. (2019) also give a useful baseline algorithm that itself has quite high
performance in measuring perplexity: train an RNN language model on the data,
and compute the log likelihood of sentence si in two ways, once given the preceding
context (conditional log likelihood) and once with no context (marginal log likeli-
hood). The difference between these values tells us how much the preceding context
improved the predictability of si, a predictability measure of coherence.

Training models to predict longer contexts than just consecutive pairs of sen-
tences can result in even stronger discourse representations. For example a Trans-
former language model trained with a contrastive sentence objective to predict text
up to a distance of
2 sentences improves performance on various discourse coher-
ence tasks (Iter et al., 2020).

±

Language-model style models are generally evaluated by the methods of Sec-
tion 23.3.3, although they can also be evaluated on the RST and PDTB coherence
relation tasks.

23.5 Global Coherence

A discourse must also cohere globally rather than just at the level of pairs of sen-
tences. Consider stories, for example. The narrative structure of stories is one of
the oldest kinds of global coherence to be studied. In his inﬂuential Morphology of
the Folktale, Propp (1968) models the discourse structure of Russian folktales via
a kind of plot grammar. His model includes a set of character categories he called
dramatis personae, like Hero, Villain, Donor, or Helper, and a set of events he
called functions (like “Villain commits kidnapping”, “Donor tests Hero”, or “Hero
is pursued”) 
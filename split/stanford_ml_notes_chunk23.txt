(s(cid:48))V (s(cid:48))ds(cid:48)

s(cid:48)
Es(cid:48)âˆ¼Psa[V (s(cid:48))]

(15.7)

(15.8)

(In Section 15.2, we had written the value iteration update with a summation
s(cid:48) Psa(s(cid:48))V (s(cid:48)) rather than an integral over states;
V (s) := R(s) + Î³ maxa
the new notation reï¬‚ects that we are now working in continuous states rather
than discrete states.)

(cid:80)

The main idea of ï¬tted value iteration is that we are going to approxi-
mately carry out this step, over a ï¬nite sample of states s(1), . . . , s(n). Specif-
ically, we will use a supervised learning algorithmâ€”linear regression in our
description belowâ€”to approximate the value function as a linear or non-linear
function of the states:

V (s) = Î¸T Ï†(s).

Here, Ï† is some appropriate feature mapping of the states.

For each state s in our ï¬nite sample of n states, ï¬tted value iteration
will ï¬rst compute a quantity y(i), which will be our approximation to R(s) +
Î³ maxa Es(cid:48)âˆ¼Psa[V (s(cid:48))] (the right hand side of Equation 15.8). Then, it will
apply a supervised learning algorithm to try to get V (s) close to R(s) +
Î³ maxa Es(cid:48)âˆ¼Psa[V (s(cid:48))] (or, in other words, to try to get V (s) close to y(i)).

In detail, the algorithm is as follows:

1. Randomly sample n states s(1), s(2), . . . s(n) âˆˆ S.

2. Initialize Î¸ := 0.

3. Repeat {

For i = 1, . . . , n {

5In practice, most MDPs have much smaller action spaces than state spaces. E.g., a car
has a 6d state space, and a 2d action space (steering and velocity controls); the inverted
pendulum has a 4d state space, and a 1d action space; a helicopter has a 12d state space,
and a 4d action space. So, discretizing this set of actions is usually less of a problem than
discretizing the state space would have been.

For each action a âˆˆ A {

200

1, . . . , s(cid:48)
(cid:80)k

Sample s(cid:48)
Set q(a) = 1
k
// Hence, q(a) is an estimate of R(s(i)) +

k âˆ¼ Ps(i)a (using a model of the MDP).
j=1 R(s(i)) + Î³V (s(cid:48)
j)

Î³Es(cid:48)âˆ¼P

s(i)a

[V (s(cid:48))].

}
Set y(i) = maxa q(a).

// Hence, y(i)

is an estimate of R(s(i)) +

Î³ maxa Es(cid:48)âˆ¼P

s(i)a

[V (s(cid:48))].

}

// In the original value iteration algorithm (over discrete states)
// we updated the value function according to V (s(i)) := y(i).
// In this algorithm, we want V (s(i)) â‰ˆ y(i), which weâ€™ll achieve

// using supervised learning (linear regression).

Set Î¸ := arg minÎ¸

(cid:80)n

i=1

1
2

(cid:0)Î¸T Ï†(s(i)) âˆ’ y(i)(cid:1)2

}

Above, we had written out ï¬tted value iteration using linear regression
as the algorithm to try to make V (s(i)) close to y(i). That step of the algo-
rithm is completely analogous to a standard supervised learning (regression)
problem in which we have a training set (x(1), y(1)), (x(2), y(2)), . . . , (x(n), y(n)),
and want to learn a function mapping from x to y; the only diï¬€erence is that
here s plays the role of x. Even though our description above used linear re-
gression, clearly other regression algorithms (such as locally weighted linear
regression) can also be used.

Unlike value iteration over a discrete set of states, ï¬tted value iteration
cannot be proved to always to converge. However, in practice, it often does
converge (or approximately converge), and works well for many problems.
Note also that if we are using a deterministic simulator/model of the MDP,
then ï¬tted value iteration can be simpliï¬ed by setting k = 1 in the algorithm.
This is because the expectation in Equation (15.8) becomes an expectation
over a deterministic distribution, and so a single example is suï¬ƒcient to
exactly compute that expectation. Otherwise, in the algorithm above, we
had to draw k samples, and average to try to approximate that expectation
(see the deï¬nition of q(a), in the algorithm pseudo-code).

201

Finally, ï¬tted value iteration outputs V , which is an approximation to
V âˆ—. This implicitly deï¬nes our policy. Speciï¬cally, when our system is in
some state s, and we need to choose an action, we would like to choose the
action

arg max

a

Es(cid:48)âˆ¼Psa[V (s(cid:48))]

(15.9)

The process for computing/approximating this is similar to the inner-loop of
ï¬tted value iteration, where for each action, we sample s(cid:48)
k âˆ¼ Psa to
approximate the expectation. (And again, if the simulator is deterministic,
we can set k = 1.)

1, . . . , s(cid:48)

In practice, there are often other ways to approximate this step as well.
For example, one very common case is if the simulator is of the form st+1 =
f (st, at) + (cid:15)t, where f is some deterministic function of the states (such as
f (st, at) = Ast + Bat), and (cid:15) is zero-mean Gaussian noise. In this case, we
can pick the action given by

arg max

a

V (f (s, a)).

In other words, here we are just setting (cid:15)t = 0 (i.e., ignoring the noise in
the simulator), and setting k = 1. Equivalent, this can be derived from
Equation (15.9) using the approximation

Es(cid:48)[V (s(cid:48))] â‰ˆ V (Es(cid:48)[s(cid:48)])
= V (f (s, a)),

(15.10)
(15.11)

where here the expectation is over the random s(cid:48) âˆ¼ Psa. So long as the noise
terms (cid:15)t are small, this will usually be a reasonable approximation.

However, for problems that donâ€™t lend themselves to such approximations,
having to sample k|A| states using the model, in order to approximate the
expectation above, can be computationally expensive.

15.5 Connections between Policy and Value

Iteration (Optional)

In the policy iteration, line 3 of Algorithm 5, we typically use linear system
solver to compute V Ï€. Alternatively, one can also the iterative Bellman
updates, similarly to the value iteration, to evaluate V Ï€, as in the Procedure
VE(Â·) in Line 1 of Algorithm 6 below. Here if we take option 1 in Line 2 of
the Procedure VE, then the diï¬€erence between the Procedure VE from the

202

Algorithm 6 Variant of Policy Iteration
1: procedure VE(Ï€, k)
2:

(cid:46) To evaluate V Ï€
Option 1: initialize V (s) := 0; Option 2: Initialize from the current

V in the main algorithm.
for i = 0 to k âˆ’ 1 do

For every state s, update

V (s) := R(s) + Î³

(cid:88)

s(cid:48)

PsÏ€(s)(s(cid:48))V (s(cid:48)).

(15.12)

return V

3:
4:

5:

Require: hyperparameter k.

6: Initialize Ï€ randomly.
7: for until convergence do
8:
9:

Let V = VE(Ï€, k).
For each state s, let

Ï€(s) := arg max
aâˆˆA

(cid:88)

s(cid:48)

Psa(s(cid:48))V (s(cid:48)).

(15.13)

203

value iteration (Algorithm 4) is that on line 4, the procedure is using the
action from Ï€ instead of the greedy action.

Using the Procedure VE, we can build Algorithm 6, which is a variant
of policy iteration that serves an intermediate algorithm that connects pol-
icy iteration and value iteration. Here we are going to use option 2 in VE
to maximize the re-use of knowledge learned before. One can verify indeed
that if we take k = 1 and use option 2 in Line 2 in Algorithm 6, then Algo-
rithm 6 is semantically equivalent to value iteration (Algorithm 4). In other
words, both Algorithm 6 and value iteration interleave the updates in (15.13)
and (15.12). Algorithm 6 alternate between k steps of update (15.12) and
one step of (15.13), whereas value iteration alternates between 1 steps of up-
date (15.12) and one step of (15.13). Therefore generally Algorithm 6 should
not be faster than value iteration, because assuming that update (15.12)
and (15.13) are equally useful and time-consuming, then the optimal balance
of the update frequencies could be just k = 1 or k â‰ˆ 1.

On the other hand, if k steps of update (15.12) can be done much faster
than k times a single step of (15.12), then taking additional steps of equa-
tion (15.12) in group might be useful. This is what policy iteration is lever-
aging â€” the linear system solver can give us the result of Procedure VE with
k = âˆž much faster than using the Procedure VE for a large k. On the ï¬‚ip
side, when such a speeding-up eï¬€ect no longer exists, e.g.,, when the state
space is large and linear system solver is also not fast, then value iteration is
more preferable.

Chapter 16

LQR, DDP and LQG

16.1 Finite-horizon MDPs

In Chapter 15, we deï¬ned Markov Decision Processes (MDPs) and covered
Value Iteration / Policy Iteration in a simpliï¬ed setting. More speciï¬cally we
introduced the optimal Bellman equation that deï¬nes the optimal value
function V Ï€âˆ— of the optimal policy Ï€âˆ—.

V Ï€âˆ—(s) = R(s) + max
aâˆˆA

Î³

Psa(s(cid:48))V Ï€âˆ—(s(cid:48))

(cid:88)

s(cid:48)âˆˆS

Recall that from the optimal value function, we were able to recover the

optimal policy Ï€âˆ— with

Ï€âˆ—(s) = argmaxaâˆˆA

Psa(s(cid:48))V âˆ—(s(cid:48))

(cid:88)

s(cid:48)âˆˆS

In this chapter, weâ€™ll place ourselves in a more general setting:

1. We want to write equations that make sense for both the discrete and

the continuous case. Weâ€™ll therefore write

Es(cid:48)âˆ¼Psa
(cid:88)

(cid:2)V Ï€âˆ—(s(cid:48))(cid:3)
Psa(s(cid:48))V Ï€âˆ—(s(cid:48))

s(cid:48)âˆˆS

instead of

meaning that we take the expectation of the value function at the next
state. In the ï¬nite case, we can rewrite the expectation as a sum over

204

205

In the continuous case, we can rewrite the expectation as an
states.
integral. The notation s(cid:48) âˆ¼ Psa means that the state s(cid:48) is sampled from
the distribution Psa.

2. Weâ€™ll assume that the rewards depend on both states and actions. In
other words, R : S Ã— A â†’ R. This implies that the previous mechanism
for computing the optimal action is changed into

Ï€âˆ—(s) = argmaxaâˆˆA R(s, a) + Î³Es(cid:48)âˆ¼Psa

(cid:2)V Ï€âˆ—(s(cid:48))(cid:3)

3. Instead of considering an inï¬nite horizon MDP, weâ€™ll assume that we

have a ï¬nite horizon MDP that will be deï¬ned as a tuple

(S, A, Psa, T, R)

with T > 0 the time horizon (for instance T = 100). In this setting,
our deï¬nition of payoï¬€ is going to be (slightly) diï¬€erent:

R(s0, a0) + R(s1, a1) + Â· Â· Â· + R(sT , aT )

instead of (inï¬nite horizon case)

R(s0, a0) + Î³R(s1, a1) + Î³2R(s2, a2) + . . .
âˆž
(cid:88)

R(st, at)Î³t

t=0

What happened to the discount factor Î³? Remember that the intro-
duction of Î³ was (partly) justiï¬ed by the necessity of making sure that
If the rewards are
the inï¬nite sum would be ï¬nite and well-deï¬ned.
bounded by a constant Â¯R, the payoï¬€ is indeed bounded by

âˆž
(cid:88)

|

t=0

R(st)Î³t| â‰¤ Â¯R

âˆž
(cid:88)

t=0

Î³t

and we recognize a geometric sum! Here, as the payoï¬€ is a ï¬nite sum,
the discount factor Î³ is not necessary anymore.

206

In this new setting, things behave quite diï¬€erently. First, the optimal
policy Ï€âˆ— might be non-stationary, meaning that it changes over time.
In other words, now we have

Ï€(t) : S â†’ A

where the superscript (t) denotes the policy at time step t. The dynam-
ics of the ï¬nite horizon MDP following policy Ï€(t) proceeds as follows:
we start in some state s0, take some action a0 := Ï€(0)(s0) according to
our policy at time step 0. The MDP transitions to a successor s1, drawn
according to Ps0a0. Then, we get to pick another action a1 := Ï€(1)(s1)
following our new policy at time step 1 and so on...

Why does the optimal policy happen to be non-stationary in the ï¬nite-
horizon setting? Intuitively, as we have a ï¬nite numbers of actions to
take, we might want to adopt diï¬€erent strategies depending on where
we are in the environment and how much time we have left. Imagine
a grid with 2 goals with rewards +1 and +10. At the beginning, we
might want to take actions to aim for the +10 goal. But if after some
steps, dynamics somehow pushed us closer to the +1 goal and we donâ€™t
have enough steps left to be able to reach the +10 goal, then a better
strategy would be to aim for the +1 goal...

4. This observation allows us to use time dependent dynamics

st+1 âˆ¼ P (t)
st,at

meaning that the transitionâ€™s distribution P (t)
st,at changes over time. The
same thing can be said about R(t). Note that this setting is a better
In a car, the gas tank empties, traï¬ƒc changes,
model for real life.
etc. Combining the previous remarks, weâ€™ll use the following general
formulation for our ï¬nite horizon MDP

(cid:0)S, A, P (t)

sa , T, R(t)(cid:1)

Remark: notice that the above formulation would be equivalent to
adding the time into the state.

207

The value function at time t for a policy Ï€ is then deï¬ned in the same
way as before, as an expectation over trajectories generated following
policy Ï€ starting in state s.

Vt(s) = E (cid:2)R(t)(st, at) + Â· Â· Â· + R(T )(sT , aT )|st = s, Ï€(cid:3)

Now, the question is

In this ï¬nite-horizon setting, how do we ï¬nd the optimal value function

V âˆ—
t (s) = max

Ï€

V Ï€
t (s)

It turns out that Bellmanâ€™s equation for Value Iteration is made for Dy-
namic Programming. This may come as no surprise as Bellman is one of
the fathers of dynamic programming and the Bellman equation is strongly
related to the ï¬eld. To understand how we can simplify the problem by
adopting an iteration-based approach, we make the following observations:

1. Notice that at the end of the game (for time step T ), the optimal value

is obvious

âˆ€s âˆˆ S : V âˆ—

T (s) := max
aâˆˆA

R(T )(s, a)

(16.1)

2. For another time step 0 â‰¤ t < T , if we suppose that we know the

optimal value function for the next time step V âˆ—

t+1, then we have

âˆ€t < T, s âˆˆ S : V âˆ—

t (s) := max
aâˆˆA

(cid:104)

R(t)(s, a) + E

s(cid:48)âˆ¼P (t)
sa

(cid:2)V âˆ—

t+1(s(cid:48))(cid:3)(cid:105)

(16.2)

With these observations in mind, we can come up with a clever algorithm

to solve for the optimal value function:

1. compute V âˆ—

T using equation (16.1).

2. for t = T âˆ’ 1, . . . , 0:

compute V âˆ—

t using V âˆ—

t+1 using equation (16.2)

208

Side note We can interpret standard value iteration as a special case
of this general case, but without keeping track of time.
It turns out that
in the standard setting, if we run value iteration for T steps, we get a Î³T
approximation of the optimal value iteration (geometric convergence). See
problem set 4 for a proof of the following result:

Theorem Let B denote the Bellman update and ||f (x)||âˆž := supx |f (x)|.

If Vt denotes the value function at the t-th step, then

||Vt+1 âˆ’ V âˆ—||âˆž = ||B(Vt) âˆ’ V âˆ—||âˆž

â‰¤ Î³||Vt âˆ’ V âˆ—||âˆž
â‰¤ Î³t||V1 âˆ’ V âˆ—||âˆž

In other words, the Bellman operator B is a Î³-contracting operator.

16.2 Linear Quadratic Regulation (LQR)

In this section, weâ€™ll cover a special case of the ï¬nite-horizon setting described
in Section 16.1, for which the exact solution is (easily) tractable. This
model is widely used in robotics, and a common technique in many problems
is to reduce the formulation to this framework.

First, letâ€™s describe the modelâ€™s assumptions. We place ourselves in the

continuous setting, with

and weâ€™ll assume linear transitions (with noise)

S = Rd, A = Rd

st+1 = Atst + Btat + wt

where At âˆˆ RdÃ—d, Bt âˆˆ RdÃ—d are matrices and wt âˆ¼ N (0, Î£t) is some
gaussian noise (with zero mean). As weâ€™ll show in the following paragraphs,
it turns out that the noise, as long as it has zero mean, does not impact the
optimal policy!

Weâ€™ll also assume quadratic rewards

R(t)(st, at) = âˆ’s(cid:62)

t Utst âˆ’ a(cid:62)

t Wtat

209

where Ut âˆˆ RdÃ—n, Wt âˆˆ RdÃ—d are positive deï¬nite matrices (meaning that

the reward is always negative).

Remark Note that the quadratic formulation of the reward is equivalent
to saying that we want our state to be close to the 
d
Chess Champion . Princeton University Press, Princeton, NJ, USA. 2
Huang, F. and Ogata, Y. (2002). Generalized pseudo-likelihood estimates for Markov
random ï¬?elds on lattice. Annals of the Institute of Statistical Mathematics, 54(1), 1â€“18.
616
Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. (2013). Learning deep
structured semantic models for web search using clickthrough data. In Proceedings of
the 22nd ACM international conference on Conference on information & knowledge
management , pages 2333â€“2338. ACM. 480
Hubel, D. and Wiesel, T. (1968). Receptive ï¬?elds and functional architecture of monkey
striate cortex. Journal of Physiology (London), 195, 215â€“243. 364
744

BIBLIOGRAPHY

Hubel, D. H. and Wiesel, T. N. (1959). Receptive ï¬?elds of single neurons in the catâ€™s
striate cortex. Journal of Physiology , 148, 574â€“591. 364
Hubel, D. H. and Wiesel, T. N. (1962). Receptive ï¬?elds, binocular interaction, and
functional architecture in the catâ€™s visual cortex. Journal of Physiology (London), 160 ,
106â€“154. 364
Huszar, F. (2015). How (not) to train your generative model: schedule sampling, likelihood,
adversary? arXiv:1511.05101 . 698
Hutter, F., Hoos, H., and Leyton-Brown, K. (2011). Sequential model-based optimization
for general algorithm conï¬?guration. In LION-5 . Extended version as UBC Tech report
TR-2010-10. 436
Hyotyniemi, H. (1996). Turing machines are recurrent neural networks. In STePâ€™96 , pages
13â€“24. 379
HyvÃ¤rinen, A. (1999). Survey on independent component analysis. Neural Computing
Surveys, 2, 94â€“128. 491
HyvÃ¤rinen, A. (2005). Estimation of non-normalized statistical models using score matching.
Journal of Machine Learning Research, 6, 695â€“709. 513, 617
HyvÃ¤rinen, A. (2007a). Connections between score matching, contrastive divergence,
and pseudolikelihood for continuous-valued variables. IEEE Transactions on Neural
Networks, 18, 1529â€“1531. 618
HyvÃ¤rinen, A. (2007b). Some extensions of score matching. Computational Statistics and
Data Analysis, 51, 2499â€“2512. 618
HyvÃ¤rinen, A. and Hoyer, P. O. (1999). Emergence of topography and complex cell
properties from natural images using extensions of ica. In NIPS , pages 827â€“833. 493
HyvÃ¤rinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis:
Existence and uniqueness results. Neural Networks, 12(3), 429â€“439. 493
HyvÃ¤rinen, A., Karhunen, J., and Oja, E. (2001a). Independent Component Analysis.
Wiley-Interscience. 491
HyvÃ¤rinen, A., Hoyer, P. O., and Inki, M. O. (2001b). Topographic independent component
analysis. Neural Computation , 13(7), 1527â€“1558. 493
HyvÃ¤rinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural Image Statistics: A probabilistic
approach to early computational vision. Springer-Verlag. 370
Iba, Y. (2001). Extended ensemble Monte Carlo. International Journal of Modern Physics,
C12, 623â€“656. 603
745

BIBLIOGRAPHY

Inayoshi, H. and Kurita, T. (2005). Improved generalization by adding both autoassociation and hidden-layer noise to neural-network-based-classiï¬?ers. IEEE Workshop
on Machine Learning for Signal Processing, pages 141â€”-146. 515
Ioï¬€e, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training
by reducing internal covariate shift. 100, 317, 320
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation.
Neural networks , 1(4), 295â€“307. 307
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures
of local experts. Neural Computation, 3, 79â€“87. 189, 450
Jaeger, H. (2003). Adaptive nonlinear system identiï¬?cation with echo state networks. In
Advances in Neural Information Processing Systems 15 . 404
Jaeger, H. (2007a). Discovering multiscale dynamical features with hierarchical echo state
networks. Technical report, Jacobs University. 398
Jaeger, H. (2007b). Echo state network. Scholarpedia, 2(9), 2330. 404
Jaeger, H. (2012). Long short-term memory in echo state networks: Details of a simulation
study. Technical report, Technical report, Jacobs University Bremen. 405
Jaeger, H. and Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and
saving energy in wireless communication. Science, 304(5667), 78â€“80. 27, 404
Jaeger, H., Lukosevicius, M., Popovici, D., and Siewert, U. (2007). Optimization and
applications of echo state networks with leaky- integrator neurons. Neural Networks ,
20(3), 335â€“352. 407
Jain, V., Murray, J. F., Roth, F., Turaga, S., Zhigulin, V., Briggman, K. L., Helmstaedter,
M. N., Denk, W., and Seung, H. S. (2007). Supervised learning of image restoration
with convolutional networks. In Computer Vision, 2007. ICCV 2007. IEEE 11th
International Conference on, pages 1â€“8. IEEE. 359
Jaitly, N. and Hinton, G. (2011). Learning a better representation of speech soundwaves
using restricted Boltzmann machines. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on, pages 5884â€“5887. IEEE. 458
Jaitly, N. and Hinton, G. E. (2013). Vocal tract length perturbation (VTLP) improves
speech recognition. In ICMLâ€™2013 . 241
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best
multi-stage architecture for object recognition? In ICCVâ€™09 . 16, 24, 27, 174, 193, 226,
363, 364, 523
Jarzynski, C. (1997). Nonequilibrium equality for free energy diï¬€erences. Phys. Rev. Lett.,
78, 2690â€“2693. 625, 628
746

BIBLIOGRAPHY

Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge University
Press. 53
Jean, S., Cho, K., Memisevic, R., and Bengio, Y. (2014). On using very large target
vocabulary for neural machine translation. arXiv:1412.2007. 474, 475
Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov source parameters
from sparse data. In E. S. Gelsema and L. N. Kanal, editors, Pattern Recognition in
Practice. North-Holland, Amsterdam. 462, 473
Jia, Y. (2013). Caï¬€e: An open source convolutional architecture for fast feature embedding.
http://caffe.berkeleyvision.org/. 25, 214
Jia, Y., Huang, C., and Darrell, T. (2012). Beyond spatial pyramids: Receptive ï¬?eld
learning for pooled image features. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on , pages 3370â€“3377. IEEE. 345
Jim, K.-C., Giles, C. L., and Horne, B. G. (1996). An analysis of noise in recurrent neural
networks: convergence and generalization. IEEE Transactions on Neural Networks,
7(6), 1424â€“1438. 242
Jordan, M. I. (1998). Learning in Graphical Models. Kluwer, Dordrecht, Netherlands. 18
Joulin, A. and Mikolov, T. (2015). Inferring algorithmic patterns with stack-augmented
recurrent nets. arXiv preprint arXiv:1503.01007 . 418
Jozefowicz, R., Zaremba, W., and Sutskever, I. (2015). An empirical evaluation of recurrent
network architectures. In ICMLâ€™2015 . 306, 412
Judd, J. S. (1989). Neural Network Design and the Complexity of Learning . MIT press.
293
Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: an adaptive
algorithm based on neuromimetic architecture. Signal Processing, 24, 1â€“10. 491
Kahou, S. E., Pal, C., Bouthillier, X., Froumenty, P., GÃ¼lÃ§ehre, c., Memisevic, R., Vincent,
P., Courville, A., Bengio, Y., Ferrari, R. C., Mirza, M., Jean, S., Carrier, P. L., Dauphin,
Y., Boulanger-Lewandowski, N., Aggarwal, A., Zumer, J., Lamblin, P., Raymond,
J.-P., Desjardins, G., Pascanu, R., Warde-Farley, D., Torabi, A., Sharma, A., Bengio,
E., CÃ´tÃ©, M., Konda, K. R., and Wu, Z. (2013). Combining modality speciï¬?c deep
neural networks for emotion recognition in video. In Proceedings of the 15th ACM on
International Conference on Multimodal Interaction. 201
Kalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In
EMNLPâ€™2013 . 474, 475
Kalchbrenner, N., Danihelka, I., and Graves, A. (2015). Grid long short-term memory.
arXiv preprint arXiv:1507.01526 . 395
747

BIBLIOGRAPHY

Kamyshanska, H. and Memisevic, R. (2015). The potential energy of an autoencoder.
IEEE Transactions on Pattern Analysis and Machine Intelligence. 515
Karpathy, A. and Li, F.-F. (2015). Deep visual-semantic alignments for generating image
descriptions. In CVPRâ€™2015 . arXiv:1412.2306. 102
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014).
Large-scale video classiï¬?cation with convolutional neural networks. In CVPR . 21
Karush, W. (1939). Minima of Functions of Several Variables with Inequalities as Side
Constraints . Masterâ€™s thesis, Dept. of Mathematics, Univ. of Chicago. 95
Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model
component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal
Processing , ASSP-35(3), 400â€“401. 462, 473
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference in sparse coding
algorithms with applications to object recognition. Technical report, Computational and
Biological Learning Lab, Courant Institute, NYU. Tech Report CBLL-TR-2008-12-01.
523
Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009). Learning invariant
features through topographic ï¬?lter maps. In CVPRâ€™2009 . 523
Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y.
(2010). Learning convolutional feature hierarchies for visual recognition. In NIPSâ€™2010 .
364, 523
Kelley, H. J. (1960). Gradient theory of optimal ï¬‚ight paths. ARS Journal , 30(10),
947â€“954. 225
Khan, F., Zhu, X., and Mutlu, B. (2011). How do humans teach: On curriculum learning
and teaching dimension. In Advances in Neural Information Processing Systems 24
(NIPSâ€™11), pages 1449â€“1457. 328
Kim, S. K., McAfee, L. C., McMahon, P. L., and Olukotun, K. (2009). A highly scalable
restricted Boltzmann machine FPGA implementation. In Field Programmable Logic
and Applications, 2009. FPL 2009. International Conference on, pages 367â€“372. IEEE.
451
Kindermann, R. (1980). Markov Random Fields and Their Applications (Contemporary
Mathematics ; V. 1). American Mathematical Society. 566
Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 . 308
Kingma, D. and LeCun, Y. (2010). Regularized estimation of image statistics by score
matching. In NIPSâ€™2010 . 513, 620
748

BIBLIOGRAPHY

Kingma, D., Rezende, D., Mohamed, S., and Welling, M. (2014). Semi-supervised learning
with deep generative models. In NIPSâ€™2014 . 426
Kingma, D. P. (2013). Fast gradient-based inference with continuous latent variable
models in auxiliary form. Technical report, arxiv:1306.0733. 652, 689, 696
Kingma, D. P. and Welling, M. (2014a). Auto-encoding variational bayes. In Proceedings
of the International Conference on Learning Representations (ICLR). 689, 700
Kingma, D. P. and Welling, M. (2014b). Eï¬ƒcient gradient-based inference through
transformations between bayes nets and neural nets. Technical report, arxiv:1402.0480.
689
Kirkpatrick, S., Jr., C. D. G., , and Vecchi, M. P. (1983). Optimization by simulated
annealing. Science, 220, 671â€“680. 327
Kiros, R., Salakhutdinov, R., and Zemel, R. (2014a). Multimodal neural language models.
In ICMLâ€™2014 . 102
Kiros, R., Salakhutdinov, R., and Zemel, R. (2014b). Unifying visual-semantic embeddings
with multimodal neural language models. arXiv:1411.2539 [cs.LG]. 102, 410
Klementiev, A., Titov, I., and Bhattarai, B. (2012). Inducing crosslingual distributed
representations of words. In Proceedings of COLING 2012 . 476, 539
Knowles-Barley, S., Jones, T. R., Morgan, J., Lee, D., Kasthuri, N., Lichtman, J. W., and
Pï¬?ster, H. (2014). Deep learning for the connectome. GPU Technology Conference. 26
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and
Techniques. MIT Press. 583, 595, 645
Konig, Y., Bourlard, H., and Morgan, N. (1996). REMAP: Recursive estimation and
maximization of a posteriori probabilities â€“ application to transition-based connectionist
speech recognition. In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances in
Neural Information Processing Systems 8 (NIPSâ€™95). MIT Press, Cambridge, MA. 459
Koren, Y. (2009). The BellKor solution to the Netï¬‚ix grand prize. 258, 480
Kotzias, D., Denil, M., de Freitas, N., and Smyth, P. (2015). From group to individual
labels using deep features. In ACM SIGKDD . 106
Koutnik, J., Greï¬€, K., Gomez, F., and Schmidhuber, J. (2014). A clockwork RNN. In
ICMLâ€™2014 . 408
KoÄ?iskÃ½, T., Hermann, K. M., and Blunsom, P. (2014). Learning Bilingual Word Representations by Marginalizing Alignments. In Proceedings of ACL. 476
Krause, O., Fischer, A., Glasmachers, T., and Igel, C. (2013). Approximation properties
of DBNs with binary hidden units and real-valued visible units. In ICMLâ€™2013 . 553
749

BIBLIOGRAPHY

Krizhevsky, A. (2010). Convolutional deep belief networks on CIFAR-10. Technical report,
University of Toronto. Unpublished Manuscript: http://www.cs.utoronto.ca/ kriz/convcifar10-aug2010.pdf. 446
Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny
images. Technical report, University of Toronto. 21, 561
Krizhevsky, A. and Hinton, G. E. (2011). Using very deep autoencoders for content-based
image retrieval. In ESANN . 525
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiï¬?cation with deep
convolutional neural networks. In NIPSâ€™2012 . 23, 24, 27, 100, 201, 371, 454, 458
Krueger, K. A. and Dayan, P. (2009). Flexible shaping: how learning in small steps helps.
Cognition , 110, 380â€“394. 328
Kuhn, H. W. and Tucker, A. W. (1951). Nonlinear programming. In Proceedings of the
Second Berkeley Symposium on Mathematical Statistics and Probability , pages 481â€“492,
Berkeley, Calif. University of California Press. 95
Kumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ondruska, P., Iyyer,
M., Gulrajani, I., and Socher, R. (2015). Ask me anything: Dynamic memory networks
for natural language processing. arXiv:1506.07285 . 418, 485
Kumar, M. P., Packer, B., and Koller, D. (2010). Self-paced learning for latent variable
models. In NIPSâ€™2010 . 328
Lang, K. J. and Hinton, G. E. (1988). The development of the time-delay neural network
architecture for speech recognition. Technical Report CMU-CS-88-152, Carnegie-Mellon
University. 367, 374, 407
Lang, K. J., Waibel, A. H., and Hinton, G. E. (1990). A time-delay neural network
architecture for isolated word recognition. Neural networks, 3(1), 23â€“43. 374
Langford, J. and Zhang, T. (2008). The epoch-greedy algorithm for contextual multi-armed
bandits. In NIPSâ€™2008 , pages 1096â€“â€“1103. 480
Lappalainen, H., Giannakopoulos, X., Honkela, A., and Karhunen, J. (2000). Nonlinear
independent component analysis using ensemble learning: Experiments and discussion.
In Proc. ICA. Citeseer. 493
Larochelle, H. and Bengio, Y. (2008). Classiï¬?cation using discriminative restricted
Boltzmann machines. In ICMLâ€™2008 . 244, 255, 530, 686, 716
Larochelle, H. and Hinton, G. E. (2010). Learning to combine foveal glimpses with a
third-order Boltzmann machine. In Advances in Neural Information Processing Systems
23 , pages 1243â€“1251. 367
750

BIBLIOGRAPHY

Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator.
In AISTATSâ€™2011 . 705, 708, 709
Larochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks. In
AAAI Conference on Artiï¬?cial Intelligence. 539
Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009). Exp
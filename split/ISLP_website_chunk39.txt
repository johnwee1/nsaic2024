200

253

0

Cross−Validation Error

6.3 Dimension Reduction Methods

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

!β̂λL!1/!β̂!1

0.4

0.6

0.8

1.0

!β̂λL!1/!β̂!1

FIGURE 6.13. Left: Ten-fold cross-validation MSE for the lasso, applied to
the sparse simulated data set from Figure 6.9. Right: The corresponding lasso
coefficient estimates are displayed. The two signal variables are shown in color,
and the noise variables are in gray. The vertical dashed lines indicate the lasso
fit for which the cross-validation error is smallest.

observations. In contrast, the least squares solution—displayed on the far
right of the right-hand panel of Figure 6.13—assigns a large coefficient
estimate to only one of the two signal variables.

6.3

Dimension Reduction Methods

The methods that we have discussed so far in this chapter have controlled
variance in two different ways, either by using a subset of the original variables, or by shrinking their coefficients toward zero. All of these methods
are defined using the original predictors, X1 , X2 , . . . , Xp . We now explore
a class of approaches that transform the predictors and then fit a least
squares model using the transformed variables. We will refer to these techniques as dimension reduction methods.
dimension
Let Z1 , Z2 , . . . , ZM represent M < p linear combinations of our original reduction
p predictors. That is,
linear
p
0
combination
Zm =
φjm Xj
(6.16)
j=1

for some constants φ1m , φ2m . . . , φpm , m = 1, . . . , M . We can then fit the
linear regression model
yi = θ 0 +

M
0

θm zim + "i ,

i = 1, . . . , n,

(6.17)

m=1

using least squares. Note that in (6.17), the regression coefficients are given
by θ0 , θ1 , . . . , θM . If the constants φ1m , φ2m , . . . , φpm are chosen wisely, then
such dimension reduction approaches can often outperform least squares
regression. In other words, fitting (6.17) using least squares can lead to
better results than fitting (6.1) using least squares.
The term dimension reduction comes from the fact that this approach
reduces the problem of estimating the p + 1 coefficients β0 , β1 , . . . , βp to the

6. Linear Model Selection and Regularization

25
20
15
10
0

5

Ad Spending

30

35

254

10

20

30

40

50

60

70

Population

FIGURE 6.14. The population size (pop) and ad spending (ad) for 100 different
cities are shown as purple circles. The green solid line indicates the first principal
component, and the blue dashed line indicates the second principal component.

simpler problem of estimating the M + 1 coefficients θ0 , θ1 , . . . , θM , where
M < p. In other words, the dimension of the problem has been reduced
from p + 1 to M + 1.
Notice that from (6.16),
M
0

m=1

where

θm zim =

M
0

m=1

θm

p
0

φjm xij =

θm φjm xij =

j=1 m=1

j=1

βj =

p 0
M
0

M
0

θm φjm .

p
0

βj xij ,

j=1

(6.18)

m=1

Hence (6.17) can be thought of as a special case of the original linear
regression model given by (6.1). Dimension reduction serves to constrain
the estimated βj coefficients, since now they must take the form (6.18).
This constraint on the form of the coefficients has the potential to bias the
coefficient estimates. However, in situations where p is large relative to n,
selecting a value of M 0 p can significantly reduce the variance of the fitted
coefficients. If M = p, and all the Zm are linearly independent, then (6.18)
poses no constraints. In this case, no dimension reduction occurs, and so
fitting (6.17) is equivalent to performing least squares on the original p
predictors.
All dimension reduction methods work in two steps. First, the transformed predictors Z1 , Z2 , . . . , ZM are obtained. Second, the model is fit
using these M predictors. However, the choice of Z1 , Z2 , . . . , ZM , or equivalently, the selection of the φjm ’s, can be achieved in different ways. In this
chapter, we will consider two approaches for this task: principal components
and partial least squares.

6.3.1

Principal Components Regression

Principal components analysis (PCA) is a popular approach for deriving

principal
components
analysis

6.3 Dimension Reduction Methods

255

a low-dimensional set of features from a large set of variables. PCA is
discussed in greater detail as a tool for unsupervised learning in Chapter 12.
Here we describe its use as a dimension reduction technique for regression.
An Overview of Principal Components Analysis
PCA is a technique for reducing the dimension of an n × p data matrix X.
The first principal component direction of the data is that along which the
observations vary the most. For instance, consider Figure 6.14, which shows
population size (pop) in tens of thousands of people, and ad spending for a
particular company (ad) in thousands of dollars, for 100 cities.6 The green
solid line represents the first principal component direction of the data. We
can see by eye that this is the direction along which there is the greatest
variability in the data. That is, if we projected the 100 observations onto
this line (as shown in the left-hand panel of Figure 6.15), then the resulting
projected observations would have the largest possible variance; projecting
the observations onto any other line would yield projected observations
with lower variance. Projecting a point onto a line simply involves finding
the location on the line which is closest to the point.
The first principal component is displayed graphically in Figure 6.14, but
how can it be summarized mathematically? It is given by the formula
Z1 = 0.839 × (pop − pop) + 0.544 × (ad − ad).

(6.19)

Here φ11 = 0.839 and φ21 = 0.544 are the principal component loadings,
which define the direction referred to above. In (6.19), pop indicates the
mean of all pop values in this data set, and ad indicates the mean of all advertising spending. The idea is that out of every possible linear combination
of pop and ad such that φ211 + φ221 = 1, this particular linear combination
yields the highest variance: i.e. this is the linear combination for which
Var(φ11 × (pop − pop) + φ21 × (ad − ad)) is maximized. It is necessary to
consider only linear combinations of the form φ211 +φ221 = 1, since otherwise
we could increase φ11 and φ21 arbitrarily in order to blow up the variance.
In (6.19), the two loadings are both positive and have similar size, and so
Z1 is almost an average of the two variables.
Since n = 100, pop and ad are vectors of length 100, and so is Z1 in
(6.19). For instance,
zi1 = 0.839 × (popi − pop) + 0.544 × (adi − ad).

(6.20)

The values of z11 , . . . , zn1 are known as the principal component scores, and
can be seen in the right-hand panel of Figure 6.15.
There is also another interpretation of PCA: the first principal component vector defines the line that is as close as possible to the data. For
instance, in Figure 6.14, the first principal component line minimizes the
sum of the squared perpendicular distances between each point and the
line. These distances are plotted as dashed line segments in the left-hand
6 This dataset is distinct from the Advertising data discussed in Chapter 3.

30

40

Population

50

10
5
0
−5

25
20
15

Ad Spending

10
5

20

−10

2nd Principal Component

6. Linear Model Selection and Regularization

30

256

−20

−10

0

10

20

1st Principal Component

FIGURE 6.15. A subset of the advertising data. The mean pop and ad budgets
are indicated with a blue circle. Left: The first principal component direction is
shown in green. It is the dimension along which the data vary the most, and it also
defines the line that is closest to all n of the observations. The distances from each
observation to the principal component are represented using the black dashed line
segments. The blue dot represents (pop, ad). Right: The left-hand panel has been
rotated so that the first principal component direction coincides with the x-axis.

panel of Figure 6.15, in which the crosses represent the projection of each
point onto the first principal component line. The first principal component
has been chosen so that the projected observations are as close as possible
to the original observations.
In the right-hand panel of Figure 6.15, the left-hand panel has been
rotated so that the first principal component direction coincides with the
x-axis. It is possible to show that the first principal component score for
the ith observation, given in (6.20), is the distance in the x-direction of the
ith cross from zero. So for example, the point in the bottom-left corner of
the left-hand panel of Figure 6.15 has a large negative principal component
score, zi1 = −26.1, while the point in the top-right corner has a large
positive score, zi1 = 18.7. These scores can be computed directly using
(6.20).
We can think of the values of the principal component Z1 as singlenumber summaries of the joint pop and ad budgets for each location. In
this example, if zi1 = 0.839 × (popi − pop) + 0.544 × (adi − ad) < 0,
then this indicates a city with below-average population size and belowaverage ad spending. A positive score suggests the opposite. How well can a
single number represent both pop and ad? In this case, Figure 6.14 indicates
that pop and ad have approximately a linear relationship, and so we might
expect that a single-number summary will work well. Figure 6.16 displays
zi1 versus both pop and ad.7 The plots show a strong relationship between
the first principal component and the two features. In other words, the first
principal component appears to capture most of the information contained
in the pop and ad predictors.
So far we have concentrated on the first principal component. In general, one can construct up to p distinct principal components. The second
7 The principal components were calculated after first standardizing both pop and ad,
a common approach. Hence, the x-axes on Figures 6.15 and 6.16 are not on the same
scale.

257

25
20
15

Ad Spending

5

10

40
30
20

Population

50

30

60

6.3 Dimension Reduction Methods

−3

−2

−1

0

1

2

3

−3

1st Principal Component

−2

−1

0

1

2

3

1st Principal Component

FIGURE 6.16. Plots of the first principal component scores zi1 versus pop and
ad. The relationships are strong.

principal component Z2 is a linear combination of the variables that is uncorrelated with Z1 , and has largest variance subject to this constraint. The
second principal component direction is illustrated as a dashed blue line in
Figure 6.14. It turns out that the zero correlation condition of Z1 with Z2
is equivalent to the condition that the direction must be perpendicular, or perpenorthogonal, to the first principal component direction. The second principal dicular
component is given by the formula
orthogonal
Z2 = 0.544 × (pop − pop) − 0.839 × (ad − ad).
Since the advertising data has two predictors, the first two principal components contain all of the information that is in pop and ad. However, by
construction, the first component will contain the most information. Consider, for example, the much larger variability of zi1 (the x-axis) versus
zi2 (the y-axis) in the right-hand panel of Figure 6.15. The fact that the
second principal component scores are much closer to zero indicates that
this component captures far less information. As another illustration, Figure 6.17 displays zi2 versus pop and ad. There is little relationship between
the second principal component and these two predictors, again suggesting
that in this case, one only needs the first principal component in order to
accurately represent the pop and ad budgets.
With two-dimensional data, such as in our advertising example, we can
construct at most two principal components. However, if we had other
predictors, such as population age, income level, education, and so forth,
then additional components could be constructed. They would successively
maximize variance, subject to the constraint of being uncorrelated with the
preceding components.
The Principal Components Regression Approach
The principal components regression (PCR) approach involves constructprincipal
ing the first M principal components, Z1 , . . . , ZM , and then using these components
components as the predictors in a linear regression model that is fit us- regression
ing least squares. The key idea is that often a small number of principal
components suffice to explain most of the variability in the data, as well
as the relationship with the response. In other words, we assume that the

6. Linear Model Selection and Regularization

25
20
15

Ad Spending

5

10

40
30
20

Population

50

30

60

258

−1.0

−0.5

0.0

0.5

1.0

−1.0

2nd Principal Component

−0.5

0.0

0.5

1.0

2nd Principal Component

100

150

Squared Bias
Test MSE
Variance

0

50

Mean Squared Error

60
50
40
30
20
10
0

Mean Squared Error

70

FIGURE 6.17. Plots of the second principal component scores zi2 versus pop
and ad. The relationships are weak.

0

10

20

30

Number of Components

40

0

10

20

30

40

Number of Components

FIGURE 6.18. PCR was applied to two simulated data sets. In each panel, the
horizontal dashed line represents the irreducible error. Left: Simulated data from
Figure 6.8. Right: Simulated data from Figure 6.9.

directions in which X1 , . . . , Xp show the most variation are the directions
that are associated with Y . While this assumption is not guaranteed to be
true, it often turns out to be a reasonable enough approximation to give
good results.
If the assumption underlying PCR holds, then fitting a least squares
model to Z1 , . . . , ZM will lead to better results than fitting a least squares
model to X1 , . . . , Xp , since most or all of the information in the data that
relates to the response is contained in Z1 , . . . , ZM , and by estimating only
M 0 p coefficients we can mitigate overfitting. In the advertising data, the
first principal component explains most of the variance in both pop and ad,
so a principal component regression that uses this single variable to predict
some response of interest, such as sales, will likely perform quite well.
Figure 6.18 displays the PCR fits on the simulated data sets from Figures 6.8 and 6.9. Recall that both data sets were generated using n = 50
observations and p = 45 predictors. However, while the response in the
first data set was a function of all the predictors, the response in the second data set was generated using only two of the predictors. The curves are
plotted as a function of M , the number of principal components used as
predictors in the regression model. As more principal components are used

6.3 Dimension Reduction Methods

60
50
40
30
20

Mean Squared Error

10
0

10

20

30

40

50

60

Squared Bias
Test MSE
Variance

70

Ridge Regression and Lasso

0

Mean Squared Error

70

PCR

259

0

10

20

30

Number of Components

40

0.0

0.2

0.4

0.6

0.8

1.0

Shrinkage Factor

FIGURE 6.19. PCR, ridge regression, and the lasso were applied to a simulated data set in which the first five principal components of X contain all the
information about the response Y . In each panel, the irreducible error Var(!) is
shown as a horizontal dashed line. Left: Results for PCR. Right: Results for lasso
(solid) and ridge regression (dotted). The x-axis displays the shrinkage factor
of the coefficient estimates, defined as the &2 norm of the shrunken coefficient
estimates divided by the &2 norm of the least squares estimate.

in the regression model, the bias decreases, but the variance increases. This
results in a typical U-shape for t
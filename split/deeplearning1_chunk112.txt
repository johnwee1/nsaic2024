, N., Graves, A., and Kavukcuoglu, K. (2014). Recurrent models of visual
attention. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger,
editors, NIPSâ€™2014 , pages 2204â€“2212. 691
Mnih, V., Kavukcuoglo, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,
A., Riedmiller, M., Fidgeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,
Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015).
Human-level control through deep reinforcement learning. Nature , 518, 529â€“533. 25
Mobahi, H. and Fisher, III, J. W. (2015). A theoretical analysis of optimization by
Gaussian continuation. In AAAIâ€™2015 . 327
Mobahi, H., Collobert, R., and Weston, J. (2009). Deep learning from temporal coherence
in video. In L. Bottou and M. Littman, editors, Proceedings of the 26th International
Conference on Machine Learning, pages 737â€“744, Montreal. Omnipress. 494
Mohamed, A., Dahl, G., and Hinton, G. (2009). Deep belief networks for phone recognition.
459
757

BIBLIOGRAPHY

Mohamed, A., Sainath, T. N., Dahl, G., Ramabhadran, B., Hinton, G. E., and Picheny,
M. A. (2011). Deep belief networks using discriminative features for phone recognition. In
Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference
on, pages 5060â€“5063. IEEE. 459
Mohamed, A., Dahl, G., and Hinton, G. (2012a). Acoustic modeling using deep belief
networks. IEEE Trans. on Audio, Speech and Language Processing , 20(1), 14â€“22. 459
Mohamed, A., Hinton, G., and Penn, G. (2012b). Understanding how deep belief networks
perform acoustic modelling. In Acoustics, Speech and Signal Processing (ICASSP),
2012 IEEE International Conference on , pages 4273â€“4276. IEEE. 459
Moller, M. F. (1993). A scaled conjugate gradient algorithm for fast supervised learning.
Neural Networks , 6, 525â€“533. 316
Montavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering
trick. In G. Montavon, G. Orr, and K.-R. MÃ¼ller, editors, Neural Networks: Tricks of
the Trade, volume 7700 of Lecture Notes in Computer Science , pages 621â€“637. Preprint:
http://arxiv.org/abs/1203.3783. 673
MontÃºfar, G. (2014). Universal approximation depth and errors of narrow belief networks
with discrete units. Neural Computation, 26. 553
MontÃºfar, G. and Ay, N. (2011). Reï¬?nements of universal approximation results for
deep belief networks and restricted Boltzmann machines. Neural Computation,23(5),
1306â€“1319. 553
Montufar, G. F., Pascanu, R., Cho, K., and Bengio, Y. (2014). On the number of linear
regions of deep neural networks. In NIPSâ€™2014 . 19, 199, 200
Mor-Yosef, S., Samueloï¬€, A., Modan, B., Navot, D., and Schenker, J. G. (1990). Ranking
the risk factors for cesarean: logistic regression analysis of a nationwide study. Obstet
Gynecol , 75(6), 944â€“7. 3
Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language
model. In AISTATSâ€™2005 . 467, 469
Mozer, M. C. (1992). The induction of multiscale temporal structure. In J. M. S. Hanson
and R. Lippmann, editors, Advances in Neural Information Processing Systems 4
(NIPSâ€™91), pages 275â€“282, San Mateo, CA. Morgan Kaufmann. 407, 408
Murphy, K. P. (2012). Machine Learning: a Probabilistic Perspective . MIT Press,
Cambridge, MA, USA. 62, 98, 146
Murray, B. U. I. and Larochelle, H. (2014). A deep and tractable density estimator. In
ICMLâ€™2014 . 190, 710
Nair, V. and Hinton, G. (2010). Rectiï¬?ed linear units improve restricted Boltzmann
machines. In ICMLâ€™2010 . 16, 174, 197
758

BIBLIOGRAPHY

Nair, V. and Hinton, G. E. (2009). 3d object recognition with deep belief nets. In Y. Bengio,
D. Schuurmans, J. D. Laï¬€erty, C. K. I. Williams, and A. Culotta, editors, Advances in
Neural Information Processing Systems 22 , pages 1339â€“1347. Curran Associates, Inc.
686
Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis.
In NIPSâ€™2010 . 164
Naumann, U. (2008). Optimal Jacobian accumulation is NP-complete. Mathematical
Programming , 112(2), 427â€“441. 222
Navigli, R. and Velardi, P. (2005). Structural semantic interconnections: a knowledgebased approach to word sense disambiguation. IEEE Trans. Pattern Analysis and
Machine Intelligence, 27(7), 1075â€“â€“1086. 485
Neal, R. and Hinton, G. (1999). A view of the EM algorithm that justiï¬?es incremental,
sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models . MIT
Press, Cambridge, MA. 634
Neal, R. M. (1990). Learning stochastic feedforward networks. Technical report. 692
Neal, R. M. (1993). Probabilistic inference using Markov chain Monte-Carlo methods.
Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto. 680
Neal, R. M. (1994). Sampling from multimodal distributions using tempered transitions.
Technical Report 9421, Dept. of Statistics, University of Toronto. 603
Neal, R. M. (1996). Bayesian Learning for Neural Networks . Lecture Notes in Statistics.
Springer. 265
Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing , 11(2),
125â€“139. 625, 627, 628
Neal, R. M. (2005). Estimating ratios of normalizing constants using linked importance
sampling. 629
Nesterov, Y. (1983). A method of solving a convex programming problem with convergence
rate O(1/k 2 ). Soviet Mathematics Doklady , 27, 372â€“376. 300
Nesterov, Y. (2004). Introductory lectures on convex optimization : a basic course. Applied
optimization. Kluwer Academic Publ., Boston, Dordrecht, London. 300
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading
digits in natural images with unsupervised feature learning. Deep Learning and
Unsupervised Feature Learning Workshop, NIPS. 21
Ney, H. and Kneser, R. (1993). Improved clustering techniques for class-based statistical
language modelling. In European Conference on Speech Communication and Technology
(Eurospeech), pages 973â€“976, Berlin. 463
759

BIBLIOGRAPHY

Ng,
A.
(2015).
Advice
for
applying
machine
https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf. 421

learning.

Niesler, T. R., Whittaker, E. W. D., and Woodland, P. C. (1998). Comparison of part-ofspeech and automatically derived category-based language models for speech recognition.
In International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pages 177â€“180. 463
Ning, F., Delhomme, D., LeCun, Y., Piano, F., Bottou, L., and Barbano, P. E. (2005).
Toward automatic phenotyping of developing embryos from videos. Image Processing,
IEEE Transactions on, 14(9), 1360â€“1371. 360
Nocedal, J. and Wright, S. (2006). Numerical Optimization. Springer. 92, 96
Norouzi, M. and Fleet, D. J. (2011). Minimal loss hashing for compact binary codes. In
ICMLâ€™2011 . 525
Nowlan, S. J. (1990). Competing experts: An experimental investigation of associative
mixture models. Technical Report CRG-TR-90-5, University of Toronto. 450
Nowlan, S. J. and Hinton, G. E. (1992). Simplifying neural networks by soft weight-sharing.
Neural Computation, 4(4), 473â€“493. 139
Olshausen, B. and Field, D. J. (2005). How close are we to understanding V1? Neural
Computation, 17, 1665â€“1699. 16
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive ï¬?eld properties
by learning a sparse code for natural images. Nature, 381, 607â€“609. 147, 255, 370, 496
Olshausen, B. A., Anderson, C. H., and Van Essen, D. C. (1993). A neurobiological
model of visual attention and invariant pattern recognition based on dynamic routing
of information. J. Neurosci., 13(11), 4700â€“4719. 450
Opper, M. and Archambeau, C. (2009). The variational Gaussian approximation revisited.
Neural computation, 21(3), 786â€“792. 689
Oquab, M., Bottou, L., Laptev, I., and Sivic, J. (2014). Learning and transferring mid-level
image representations using convolutional neural networks. In Computer Vision and
Pattern Recognition (CVPR), 2014 IEEE Conference on , pages 1717â€“1724. IEEE. 536
Osindero, S. and Hinton, G. E. (2008). Modeling image patches with a directed hierarchy
of Markov random ï¬?elds. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing Systems 20 (NIPSâ€™07), pages 1121â€“1128,
Cambridge, MA. MIT Press. 632
Ovid and Martin, C. (2004). Metamorphoses . W.W. Norton. 1

760

BIBLIOGRAPHY

Paccanaro, A. and Hinton, G. E. (2000). Extracting distributed representations of concepts
and relations from positive and negative propositions. In International Joint Conference
on Neural Networks (IJCNN), Como, Italy. IEEE, New York. 484
Paine, T. L., Khorrami, P., Han, W., and Huang, T. S. (2014). An analysis of unsupervised
pre-training in light of recent advances. arXiv preprint arXiv:1412.6597 . 532
Palatucci, M., Pomerleau, D., Hinton, G. E., and Mitchell, T. M. (2009). Zero-shot
learning with semantic output codes. In Y. Bengio, D. Schuurmans, J. D. Laï¬€erty,
C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing
Systems 22 , pages 1410â€“1418. Curran Associates, Inc. 539
Parker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research
in Economics and Management Sci., MIT. 225
Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the diï¬ƒculty of training recurrent
neural networks. In ICMLâ€™2013 . 289, 402, 403, 408, 414, 416
Pascanu, R., GÃ¼lÃ§ehre, Ã‡., Cho, K., and Bengio, Y. (2014a). How to construct deep
recurrent neural networks. In ICLRâ€™2014 . 19, 265, 398, 399, 410, 460
Pascanu, R., Montufar, G., and Bengio, Y. (2014b). On the number of inference regions
of deep feed forward networks with piece-wise linear activations. In ICLRâ€™2014 . 550
Pati, Y., Rezaiifar, R., and Krishnaprasad, P. (1993). Orthogonal matching pursuit:
Recursive function approximation with applications to wavelet decomposition. In Proceedings of the 27 th Annual Asilomar Conference on Signals, Systems, and Computers,
pages 40â€“44. 255
Pearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential
reasoning. In Proceedings of the 7th Conference of the Cognitive Science Society,
University of California, Irvine, pages 329â€“334. 563
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann. 54
Perron, O. (1907). Zur theorie der matrices. Mathematische Annalen, 64(2), 248â€“263. 597
Petersen, K. B. and Pedersen, M. S. (2006). The matrix cookbook. Version 20051003. 31
Peterson, G. B. (2004). A day of great illumination: B. F. Skinnerâ€™s discovery of shaping.
Journal of the Experimental Analysis of Behavior , 82(3), 317â€“328. 328
Pham, D.-T., Garat, P., and Jutten, C. (1992). Separation of a mixture of independent
sources through a maximum likelihood approach. In EUSIPCO , pages 771â€“774. 491

761

BIBLIOGRAPHY

Pham, P.-H., Jelaca, D., Farabet, C., Martini, B., LeCun, Y., and Culurciello, E. (2012).
NeuFlow: dataï¬‚ow vision processing system-on-a-chip. In Circuits and Systems (MWSCAS), 2012 IEEE 55th International Midwest Symposium on, pages 1044â€“1047. IEEE.
451
Pinheiro, P. H. O. and Collobert, R. (2014). Recurrent convolutional neural networks for
scene labeling. In ICMLâ€™2014 . 359
Pinheiro, P. H. O. and Collobert, R. (2015). From image-level to pixel-level labeling with
convolutional networks. In Conference on Computer Vision and Pattern Recognition
(CVPR). 359
Pinto, N., Cox, D. D., and DiCarlo, J. J. (2008). Why is real-world visual object recognition
hard? PLoS Comput Biol , 4. 456
Pinto, N., Stone, Z., Zickler, T., and Cox, D. (2011). Scaling up biologically-inspired
computer vision: A case study in unconstrained face recognition on facebook. In
Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer
Society Conference on, pages 35â€“42. IEEE. 363
Pollack, J. B. (1990). Recursive distributed representations. Artiï¬?cial Intelligence,46 (1),
77â€“105. 401
Polyak, B. and Juditsky, A. (1992). Acceleration of stochastic approximation by averaging.
SIAM J. Control and Optimization, 30(4), 838â€“855. 322
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.
USSR Computational Mathematics and Mathematical Physics, 4(5), 1â€“17. 296
Poole, B., Sohl-Dickstein, J., and Ganguli, S. (2014). Analyzing noise in autoencoders
and deep networks. CoRR , abs/1406.1831. 241
Poon, H. and Domingos, P. (2011). Sum-product networks: A new deep architecture. In
Proceedings of the Twenty-seventh Conference in Uncertainty in Artiï¬?cial Intelligence
(UAI), Barcelona, Spain. 554
Presley, R. K. and Haggard, R. L. (1994). A ï¬?xed point implementation of the backpropagation learning algorithm. In Southeastconâ€™94. Creative Technology Transfer-A Global
Aï¬€air., Proceedings of the 1994 IEEE , pages 136â€“138. IEEE. 451
Price, R. (1958). A useful theorem for nonlinear devices having Gaussian inputs. IEEE
Transactions on Information Theory, 4(2), 69â€“72. 689
Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and Fried, I. (2005). Invariant visual
representation by single neurons in the human brain. Nature, 435(7045), 1102â€“1107.
366

762

BIBLIOGRAPHY

Radford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 .
552, 701, 702
Raiko, T., Yao, L., Cho, K., and Bengio, Y. (2014). Iterative neural autoregressive
distribution estimator (NADE-k). Technical report, arXiv:1406.1485. 676, 709
Raina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning
using graphics processors. In L. Bottou and M. Littman, editors, Proceedings of the
Twenty-sixth International Conference on Machine Learning (ICMLâ€™09), pages 873â€“880,
New York, NY, USA. ACM. 27, 446
Ramsey, F. P. (1926). Truth and probability. In R. B. Braithwaite, editor, The Foundations
of Mathematics and other Logical Essays , chapter 7, pages 156â€“198. McMaster University
Archive for the History of Economic Thought. 56
Ranzato, M. and Hinton, G. H. (2010). Modeling pixel means and covariances using
factorized third-order Boltzmann machines. In CVPRâ€™2010 , pages 2551â€“2558. 680
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007a). Eï¬ƒcient learning of sparse
representations with an energy-based model. In NIPSâ€™2006 . 14, 19, 507, 528, 530
Ranzato, M., Huang, F., Boureau, Y., and LeCun, Y. (2007b). Unsupervised learning of
invariant feature hierarchies with applications to object recognition. In Proceedings of
the Computer Vision and Pattern Recognition Conference (CVPRâ€™07). IEEE Press. 364
Ranzato, M., Boureau, Y., and LeCun, Y. (2008). Sparse feature learning for deep belief
networks. In NIPSâ€™2007 . 507
Ranzato, M., Krizhevsky, A., and Hinton, G. E. (2010a). Factored 3-way restricted
Boltzmann machines for modeling natural images. In Proceedings of AISTATS 2010 .
678, 679
Ranzato, M., Mnih, V., and Hinton, G. (2010b). Generating more realistic images using
gated MRFs. In NIPSâ€™2010 . 680
Rao, C. (1945). Information and the accuracy attainable in the estimation of statistical
parameters. Bulletin of the Calcutta Mathematical Society, 37, 81â€“89. 135, 295
Rasmus, A., Valpola, H., Honkala, M., Berglund, M., and Raiko, T. (2015). Semi-supervised
learning with ladder network. arXiv preprint arXiv:1507.02672 . 426, 530
Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In NIPSâ€™2011 . 447
Reichert, D. P., SeriÃ¨s, P.,
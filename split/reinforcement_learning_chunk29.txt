hat direct methods
are responsible for most human and animal learning. Related debates in psy-
chology and AI concern the relative importance of cognition as opposed to
trial-and-error learning, and of deliberative planning as opposed to reactive
decision-making. Our view is that the contrast between the alternatives in
all these debates has been exaggerated, that more insight can be gained by
recognizing the similarities between these two sides than by opposing them.
For example, in this book we have emphasized the deep similarities between
dynamic programming and temporal-diﬀerence methods, even though one was
designed for planning and the other for modelfree learning.

Dyna-Q includes all of the processes shown in Figure 8.2—planning, act-
ing, model-learning, and direct RL—all occurring continually. The planning
method is the random-sample one-step tabular Q-planning method given in
Figure 8.1. The direct RL method is one-step tabular Q-learning. The model-
learning method is also table-based and assumes the world is deterministic.
After each transition St, At (cid:32) Rt+1, St+1, the model records in its table entry
for St, At the prediction that Rt+1, St+1 will deterministically follow. Thus, if
the model is queried with a state–action pair that has been experienced before,
it simply returns the last-observed next state and next reward as its predic-
tion. During planning, the Q-planning algorithm randomly samples only from
state–action pairs that have previously been experienced (in Step 1), so the
model is never queried with a pair about which it has no information.

The overall architecture of Dyna agents, of which the Dyna-Q algorithm is

planningvalue/policyexperiencemodelmodellearningactingdirectRL200CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS

Figure 8.3: The general Dyna Architecture

one example, is shown in Figure 8.3. The central column represents the basic
interaction between agent and environment, giving rise to a trajectory of real
experience. The arrow on the left of the ﬁgure represents direct reinforcement
learning operating on real experience to improve the value function and the
policy. On the right are model-based processes. The model is learned from real
experience and gives rise to simulated experience. We use the term search con-
trol to refer to the process that selects the starting states and actions for the
simulated experiences generated by the model. Finally, planning is achieved
by applying reinforcement learning methods to the simulated experiences just
as if they had really happened. Typically, as in Dyna-Q, the same reinforce-
ment learning method is used both for learning from real experience and for
planning from simulated experience. The reinforcement learning method is
thus the “ﬁnal common path” for both learning and planning. Learning and
planning are deeply integrated in the sense that they share almost all the same
machinery, diﬀering only in the source of their experience.

Conceptually, planning, acting, model-learning, and direct RL occur simul-
taneously and in parallel in Dyna agents. For concreteness and implementation
on a serial computer, however, we fully specify the order in which they occur
within a time step. In Dyna-Q, the acting, model-learning, and direct RL pro-
cesses require little computation, and we assume they consume just a fraction
of the time. The remaining time in each step can be devoted to the planning
process, which is inherently computation-intensive. Let us assume that there
is time in each step, after acting, model-learning, and direct RL, to complete
n iterations (Steps 1–3) of the Q-planning algorithm. Figure 8.4 shows the
complete algorithm for Dyna-Q.

realdirect RLupdateModelplanning updatesearchcontrolPolicy/value functionsexperiencemodellearningEnvironmentsimulatedexperience8.2.

INTEGRATING PLANNING, ACTING, AND LEARNING

201

∈

A(s)

S and a

current (nonterminal) state
(cid:15)-greedy(S, Q)

Initialize Q(s, a) and M odel(s, a) for all s
Do forever:
(a) S
(b) A
(c) Execute action A; observe resultant reward, R, and state, S(cid:48)
(d) Q(S, A)
(e) M odel(S, A)
←
(f) Repeat n times:

R, S(cid:48) (assuming deterministic environment)

R + γ maxa Q(S(cid:48), a)

Q(S, A) + α

Q(S, A)

←
←

←

−

∈

(cid:2)

(cid:3)

S
←
A
←
R, S(cid:48) ←
Q(S, A)

random previously observed state
random action previously taken in S

M odel(S, A)

Q(S, A) + α

R + γ maxa Q(S(cid:48), a)

Q(S, A)

−

←

(cid:2)

(cid:3)

Figure 8.4: Dyna-Q Algorithm. M odel(s, a) denotes the contents of the model
(predicted next state and reward) for state–action pair s, a. Direct reinforce-
ment learning, model-learning, and planning are implemented by steps (d),
(e), and (f), respectively. If (e) and (f) were omitted, the remaining algorithm
would be one-step tabular Q-learning.

Example 8.1: Dyna Maze
Consider the simple maze shown inset in
Figure 8.5. In each of the 47 states there are four actions, up, down, right, and
left, which take the agent deterministically to the corresponding neighboring
states, except when movement is blocked by an obstacle or the edge of the
maze, in which case the agent remains where it is. Reward is zero on all
transitions, except those into the goal state, on which it is +1. After reaching
the goal state (G), the agent returns to the start state (S) to begin a new
episode. This is a discounted, episodic task with γ = 0.95.

The main part of Figure 8.5 shows average learning curves from an ex-
periment in which Dyna-Q agents were applied to the maze task. The initial
action values were zero, the step-size parameter was α = 0.1, and the explo-
ration parameter was (cid:15) = 0.1. When selecting greedily among actions, ties
were broken randomly. The agents varied in the number of planning steps,
n, they performed per real step. For each n, the curves show the number of
steps taken by the agent in each episode, averaged over 30 repetitions of the
experiment. In each repetition, the initial seed for the random number gen-
erator was held constant across algorithms. Because of this, the ﬁrst episode
was exactly the same (about 1700 steps) for all values of n, and its data are
not shown in the ﬁgure. After the ﬁrst episode, performance improved for all
values of n, but much more rapidly for larger values. Recall that the n = 0
agent is a nonplanning agent, utilizing only direct reinforcement learning (one-
step tabular Q-learning). This was by far the slowest agent on this problem,
despite the fact that the parameter values (α and ε) were optimized for it. The

202CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS

Figure 8.5: A simple maze (inset) and the average learning curves for Dyna-Q
agents varying in their number of planning steps (n) per real step. The task
is to travel from S to G as quickly as possible.

nonplanning agent took about 25 episodes to reach (ε-)optimal performance,
whereas the n = 5 agent took about ﬁve episodes, and the n = 50 agent took
only three episodes.

Figure 8.6 shows why the planning agents found the solution so much faster
than the nonplanning agent. Shown are the policies found by the n = 0 and
n = 50 agents halfway through the second episode. Without planning (n = 0),
each episode adds only one additional step to the policy, and so only one step
(the last) has been learned so far. With planning, again only one step is learned
during the ﬁrst episode, but here during the second episode an extensive policy
has been developed that by the episode’s end will reach almost back to the
start state. This policy is built by the planning process while the agent is still
wandering near the start state. By the end of the third episode a complete
optimal policy will have been found and perfect performance attained.

In Dyna-Q, learning and planning are accomplished by exactly the same al-
gorithm, operating on real experience for learning and on simulated experience
for planning. Because planning proceeds incrementally, it is trivial to intermix
planning and acting. Both proceed as fast as they can. The agent is always
reactive and always deliberative, responding instantly to the latest sensory
information and yet always planning in the background. Also ongoing in the

28006004002001420103040500 planning steps(direct RL only)EpisodesStepsperepisode5 planning steps50 planning stepsSGactions8.3. WHEN THE MODEL IS WRONG

203

Figure 8.6: Policies found by planning and nonplanning Dyna-Q agents
halfway through the second episode. The arrows indicate the greedy action in
each state; no arrow is shown for a state if all of its action values are equal.
The black square indicates the location of the agent.

background is the model-learning process. As new information is gained, the
model is updated to better match reality. As the model changes, the ongoing
planning process will gradually compute a diﬀerent way of behaving to match
the new model.

8.3 When the Model Is Wrong

In the maze example presented in the previous section, the changes in the
model were relatively modest. The model started out empty, and was then
ﬁlled only with exactly correct information. In general, we cannot expect to be
so fortunate. Models may be incorrect because the environment is stochastic
and only a limited number of samples have been observed, because the model
was learned using function approximation that has generalized imperfectly, or
simply because the environment has changed and its new behavior has not
yet been observed. When the model is incorrect, the planning process will
compute a suboptimal policy.

In some cases, the suboptimal policy computed by planning quickly leads
to the discovery and correction of the modeling error. This tends to happen
when the model is optimistic in the sense of predicting greater reward or better
state transitions than are actually possible. The planned policy attempts to
exploit these opportunities and in doing so discovers that they do not exist.

Example 8.2: Blocking Maze A maze example illustrating this relatively
minor kind of modeling error and recovery from it is shown in Figure 8.7.
Initially, there is a short path from start to goal, to the right of the barrier,
as shown in the upper left of the ﬁgure. After 1000 time steps, the short
path is “blocked,” and a longer path is opened up along the left-hand side of

SGSGWITHOUT PLANNING (N=0)WITH PLANNING (N=50)nn204CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS

Figure 8.7: Average performance of Dyna agents on a blocking task. The left
environment was used for the ﬁrst 1000 steps, the right environment for the
rest. Dyna-Q+ is Dyna-Q with an exploration bonus that encourages explo-
ration. Dyna-AC is a Dyna agent that uses an actor–critic learning method
instead of Q-learning.

the barrier, as shown in upper right of the ﬁgure. The graph shows average
cumulative reward for Dyna-Q and two other Dyna agents. The ﬁrst part of the
graph shows that all three Dyna agents found the short path within 1000 steps.
When the environment changed, the graphs become ﬂat, indicating a period
during which the agents obtained no reward because they were wandering
around behind the barrier. After a while, however, they were able to ﬁnd the
new opening and the new optimal behavior.

Greater diﬃculties arise when the environment changes to become better
than it was before, and yet the formerly correct policy does not reveal the
improvement.
In these cases the modeling error may not be detected for a
long time, if ever, as we see in the next example.

Example 8.3: Shortcut Maze The problem caused by this kind of en-
vironmental change is illustrated by the maze example shown in Figure 8.8.
Initially, the optimal path is to go around the left side of the barrier (upper
left). After 3000 steps, however, a shorter path is opened up along the right
side, without disturbing the longer path (upper right). The graph shows that
two of the three Dyna agents never switched to the shortcut. In fact, they
never realized that it existed. Their models said that there was no short-
cut, so the more they planned, the less likely they were to step to the right

Cumulativereward0100020003000Time steps1500Dyna-Q+Dyna-QDyna-ACSGGS8.3. WHEN THE MODEL IS WRONG

205

Figure 8.8: Average performance of Dyna agents on a shortcut task. The left
environment was used for the ﬁrst 3000 steps, the right environment for the
rest.

and discover it. Even with an ε-greedy policy, it is very unlikely that an
agent will take so many exploratory actions as to discover the shortcut.

The general problem here is another version of the conﬂict between ex-
ploration and exploitation. In a planning context, exploration means trying
actions that improve the model, whereas exploitation means behaving in the
optimal way given the current model. We want the agent to explore to ﬁnd
changes in the environment, but not so much that performance is greatly de-
graded. As in the earlier exploration/exploitation conﬂict, there probably is
no solution that is both perfect and practical, but simple heuristics are often
eﬀective.

The Dyna-Q+ agent that did solve the shortcut maze uses one such heuris-
tic. This agent keeps track for each state–action pair of how many time steps
have elapsed since the pair was last tried in a real interaction with the envi-
ronment. The more time that has elapsed, the greater (we might presume)
the chance that the dynamics of this pair has changed and that the model of
it is incorrect. To encourage behavior that tests long-untried actions, a spe-
cial “bonus reward” is given on simulated experiences involving these actions.
In particular, if the modeled reward for a transition is R, and the transition
has not been tried in τ time steps, then planning backups are done as if that
transition produced a reward of R + κ√τ , for some small κ. This encour-
ages the agent to keep testing all accessible state transitions and even to plan

CumulativerewardSGGS030006000Time steps4000Dyna-Q+Dyna-QDyna-AC206CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS

long sequences of actions in order to carry out such tests.1 Of course all this
testing has its cost, but in many cases, as in the shortcut maze, this kind of
computational curiosity is well worth the extra exploration.

8.4 Prioritized Sweeping

In the Dyna agents presented in the preceding sections, simulated transitions
are started in state–action pairs selected uniformly at random from all pre-
viously experienced pairs. But a uniform selection is usually not the best;
planning can be much more eﬃcient if simulated transitions and backups are
focused on particular state–action pairs. For example, consider what happens
during the second episode of the ﬁrst maze task (Figure 8.6). At the beginning
of the second episode, only the state–action pair leading directly into the goal
has a positive value; the values of all other pairs are still zero. This means
that it is pointless to back up along almost all transitions, because they take
the agent from one zero-valued state to another, and thus the backups would
have no eﬀect. Only a backup along a transition into the state just prior to
the goal, or from it into the goal, will change any values. If simulated transi-
tions are generated uniformly, then many wasteful backups will be made before
stumbling onto one of the two useful ones. As planning progresses, the region
of useful backups grows, but planning is still far less eﬃcient than it would
be if focused where
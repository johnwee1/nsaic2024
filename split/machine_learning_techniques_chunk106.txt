() function to print its input:

@tf.function
def tf_cube(x):
    print("x =", x)
    return x ** 3

Now let’s call it:

>>> result = tf_cube(tf.constant(2.0))
x = Tensor("x:0", shape=(), dtype=float32)
>>> result
<tf.Tensor: id=19068290, shape=(), dtype=float32, numpy=8.0>

The result looks good, but look at what was printed: x is a symbolic tensor! It has a
shape and a data type, but no value. Plus it has a name ("x:0"). This is because the
print() function is not a TensorFlow operation, so it will only run when the Python
function is traced, which happens in graph mode, with arguments replaced with sym‐
bolic tensors (same type and shape, but no value). Since the print() function was not
captured into the graph, the next times we call tf_cube() with float32 scalar tensors,
nothing is printed:

>>> result = tf_cube(tf.constant(3.0))
>>> result = tf_cube(tf.constant(4.0))

But  if  we  call  tf_cube()  with  a  tensor  of  a  different  type  or  shape,  or  with  a  new
Python value, the function will be traced again, so the print() function will be called:

>>> result = tf_cube(2) # new Python value: trace!
x = 2
>>> result = tf_cube(3) # new Python value: trace!
x = 3
>>> result = tf_cube(tf.constant([[1., 2.]])) # New shape: trace!
x = Tensor("x:0", shape=(1, 2), dtype=float32)
>>> result = tf_cube(tf.constant([[3., 4.], [5., 6.]])) # New shape: trace!
x = Tensor("x:0", shape=(None, 2), dtype=float32)
>>> result = tf_cube(tf.constant([[7., 8.], [9., 10.]])) # Same shape: no trace

If your function has Python side effects (e.g., it saves some logs to
disk),  be  aware  that  this  code  will  only  run  when  the  function  is
traced (i.e., every time the TF Function is called with a new input
signature).  It  best  to  assume  that  the  function  may  be  traced  (or
not) any time the TF Function is called.

TensorFlow Graphs 

| 

795

In some cases, you may want to restrict a TF Function to a specific input signature.
For  example,  suppose  you  know  that  you  will  only  ever  call  a  TF  Function  with
batches  of  28  ×  28–pixel  images,  but  the  batches  will  have  very  different  sizes.  You
may  not  want  TensorFlow  to  generate  a  different  concrete  function  for  each  batch
size, or count on it to figure out on its own when to use None. In this case, you can
specify the input signature like this:

@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])
def shrink(images):
    return images[:, ::2, ::2] # drop half the rows and columns

This TF Function will accept any float32 tensor of shape [*, 28, 28], and it will reuse
the same concrete function every time:

img_batch_1 = tf.random.uniform(shape=[100, 28, 28])
img_batch_2 = tf.random.uniform(shape=[50, 28, 28])
preprocessed_images = shrink(img_batch_1) # Works fine. Traces the function.
preprocessed_images = shrink(img_batch_2) # Works fine. Same concrete function.

However,  if  you  try  to  call  this  TF  Function  with  a  Python  value,  or  a  tensor  of  an
unexpected data type or shape, you will get an exception:

img_batch_3 = tf.random.uniform(shape=[2, 2, 2])
preprocessed_images = shrink(img_batch_3)  # ValueError! Unexpected signature.

Using AutoGraph to Capture Control Flow
If  your  function  contains  a  simple  for  loop,  what  do  you  expect  will  happen?  For
example, let’s write a function that will add 10 to its input, by just adding 1 10 times:

@tf.function
def add_10(x):
    for i in range(10):
        x += 1
    return x

It works fine, but when we look at its graph, we find that it does not contain a loop: it
just contains 10 addition operations!

>>> add_10(tf.constant(0))
<tf.Tensor: id=19280066, shape=(), dtype=int32, numpy=10>
>>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()
[<tf.Operation 'x' type=Placeholder>, [...],
 <tf.Operation 'add' type=Add>, [...],
 <tf.Operation 'add_1' type=Add>, [...],
 <tf.Operation 'add_2' type=Add>, [...],
 [...]
 <tf.Operation 'add_9' type=Add>, [...],
 <tf.Operation 'Identity' type=Identity>]

796 

|  Appendix G: TensorFlow Graphs

This actually makes sense: when the function got traced, the loop ran 10 times, so the
x += 1 operation was run 10 times, and since it was in graph mode, it recorded this
operation 10 times in the graph. You can think of this for loop as a “static” loop that
gets unrolled when the graph is created.

If you want the graph to contain a “dynamic” loop instead (i.e., one that runs when
the graph is executed), you can create one manually using the tf.while_loop() oper‐
ation, but it is not very intuitive (see the “Using AutoGraph to Capture Control Flow”
section of the Chapter 12 notebook for an example). Instead, it is much simpler to use
TensorFlow’s AutoGraph feature, discussed in Chapter 12. AutoGraph is actually acti‐
vated  by  default  (if  you  ever  need  to  turn  it  off,  you  can  pass  autograph=False  to
tf.function()).  So  if  it  is  on,  why  didn’t  it  capture  the  for  loop  in  the  add_10()
function? Well, it only captures for loops that iterate over tf.range(), not range().
This is to give you the choice:

• If you use range(), the for loop will be static, meaning it will only be executed
when the function is traced. The loop will be “unrolled” into a set of operations
for each iteration, as we saw.

• If you use tf.range(), the loop will be dynamic, meaning that it will be included

in the graph itself (but it will not run during tracing).

Let’s look at the graph that gets generated if you just replace range() with tf.range()
in the add_10() function:

>>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()
[<tf.Operation 'x' type=Placeholder>, [...],
 <tf.Operation 'range' type=Range>, [...],
 <tf.Operation 'while' type=While>, [...],
 <tf.Operation 'Identity' type=Identity>]

As you can see, the graph now contains a While loop operation, as if you had called
the tf.while_loop() function.

Handling Variables and Other Resources in TF Functions
In  TensorFlow,  variables  and  other  stateful  objects,  such  as  queues  or  datasets,  are
called resources. TF Functions treat them with special care: any operation that reads
or  updates  a  resource  is  considered  stateful,  and  TF  Functions  ensure  that  stateful
operations are executed in the order they appear (as opposed to stateless operations,
which may be run in parallel, so their order of execution is not guaranteed). More‐
over,  when  you  pass  a  resource  as  an  argument  to  a  TF  Function,  it  gets  passed  by
reference, so the function may modify it. For example:

TensorFlow Graphs 

| 

797

counter = tf.Variable(0)

@tf.function
def increment(counter, c=1):
    return counter.assign_add(c)

increment(counter) # counter is now equal to 1
increment(counter) # counter is now equal to 2

If you peek at the function definition, the first argument is marked as a resource:

>>> function_def = increment.get_concrete_function(counter).function_def
>>> function_def.signature.input_arg[0]
name: "counter"
type: DT_RESOURCE

It  is  also  possible  to  use  a  tf.Variable  defined  outside  of  the  function,  without
explicitly passing it as an argument:

counter = tf.Variable(0)

@tf.function
def increment(c=1):
    return counter.assign_add(c)

The TF Function will treat this as an implicit first argument, so it will actually end up
with the same signature (except for the name of the argument). However, using global
variables  can  quickly  become  messy,  so  you  should  generally  wrap  variables  (and
other  resources)  inside  classes.  The  good  news  is  @tf.function  works  fine  with
methods too:

class Counter:
    def __init__(self):
        self.counter = tf.Variable(0)

    @tf.function
    def increment(self, c=1):
        return self.counter.assign_add(c)

Do not use =, +=, -=, or any other Python assignment operator with
TF  variables.  Instead,  you  must  use  the  assign(),  assign_add(),
or  assign_sub()  methods.  If  you  try  to  use  a  Python  assignment
operator, you will get an exception when you call the method.

A good example of this object-oriented approach is, of course, tf.keras. Let’s see how
to use TF Functions with tf.keras.

798 

|  Appendix G: TensorFlow Graphs

Using TF Functions with tf.keras (or Not)
By default, any custom function, layer, or model you use with tf.keras will automati‐
cally be converted to a TF Function; you do not need to do anything at all! However,
in some cases you may want to deactivate this automatic conversion—for example, if
your custom code cannot be turned into a TF Function, or if you just want to debug
your  code,  which  is  much  easier  in  eager  mode.  To  do  this,  you  can  simply  pass
dynamic=True when creating the model or any of its layers:

model = MyModel(dynamic=True)

If your custom model or layer will always be dynamic, you can instead call the base
class’s constructor with dynamic=True:

class MyLayer(keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(dynamic=True, **kwargs)
        [...]

Alternatively, you can pass run_eagerly=True when calling the compile() method:

model.compile(loss=my_mse, optimizer="nadam", metrics=[my_mae],
              run_eagerly=True)

Now  you  know  how  TF  Functions  handle  polymorphism  (with  multiple  concrete
functions),  how  graphs  are  automatically  generated  using  AutoGraph  and  tracing,
what graphs look like, how to explore their symbolic operations and tensors, how to
handle variables and resources, and how to use TF Functions with tf.keras.

TensorFlow Graphs 

| 


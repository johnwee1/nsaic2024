ï£¯
ï£º
0
0
0
ï£¯
ï£º
ï£¯
ï£º ï£¯ âˆ’5 ï£º
(7.46)
ï£° âˆ’9 ï£»
ï£° 1 0 0 âˆ’1 0 âˆ’4 ï£» ï£¯
ï£º
ï£° 1 ï£»
âˆ’3
1 0 0
0 âˆ’5 0
4
m
m
Ã—
n
yâˆˆR
AâˆˆR
x âˆˆ Rn

254

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

ï£®

ï£¹

ï£®

âˆ’14
3
ï£¯ 1 ï£º
ï£¯ 4
ï£¯
ï£º
ï£¯
ï£¯ 19 ï£º = ï£¯ âˆ’1
ï£¯
ï£º
ï£¯
ï£° 2 ï£»
ï£° 3
23
âˆ’5
y âˆˆ Rm

ï£¹

âˆ’1 2 âˆ’5 4
1
2 âˆ’ 3 âˆ’1 1
3 ï£º
ï£º
5
4
2 âˆ’ 3 âˆ’2 ï£º
ï£º
1
2 âˆ’3 0 âˆ’3 ï£»
4 âˆ’ 2 2 âˆ’ 5 âˆ’1
B âˆˆ R mÃ—n

ï£®

ï£¹
0
ï£¯ 2 ï£º
ï£¯
ï£º
ï£¯ 0 ï£º
ï£¯
ï£º
ï£¯ 0 ï£º
ï£¯
ï£º
ï£° âˆ’3 ï£»
0
h âˆˆ Rn

(7.47)

In the ï¬?rst expression, we have an example of a sparsely parametrized linear
regression model. In the second, we have linear regression with a sparse representation h of the data x. That is, h is a function of x that, in some sense, represents
the information present in x, but does so with a sparse vector.
Representational regularization is accomplished by the same sorts of mechanisms
that we have used in parameter regularization.
Norm penalty regularization of representations is performed by adding to the
loss function J a norm penalty on the representation. This penalty is denoted
Ëœ
â„¦(h). As before, we denote the regularized loss function by J:
JËœ(Î¸; X , y) = J (Î¸; X , y) + Î±â„¦(h)

(7.48)

where Î± âˆˆ [0, âˆž) weights the relative contribution of the norm penalty term, with
larger values of Î± corresponding to more regularization.
Just as an L1 penalty on the parameters induces parameter sparsity, an L 1
penalty on the elements
of the representation induces representational sparsity:
î??
â„¦(h) = ||h||1 = i |h i |. Of course, the L 1 penalty is only one choice of penalty
that can result in a sparse representation. Others include the penalty derived from
a Student-t prior on the representation (Olshausen and Field, 1996; Bergstra, 2011)
and KL divergence penalties (Larochelle and Bengio, 2008) that are especially
useful for representations with elements constrained to lie on the unit interval.
Lee et al. (2008) and Goodfellow et al. (2009) both provide examples ofî??strategies
based on regularizing the average activation across several examples, m1 i h(i), to
be near some target value, such as a vector with .01 for each entry.

Other approaches obtain representational sparsity with a hard constraint on
the activation values. For example, orthogonal matching pursuit (Pati et al.,
1993) encodes an input x with the representation h that solves the constrained
optimization problem
arg min î?«x âˆ’ W hî?«2 ,
(7.49)
h,î?«hî?«0 <k

where î?«hî?«0 is the number of non-zero entries of h . This problem can be solved
eï¬ƒciently when W is constrained to be orthogonal. This method is often called
255

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

OMP-k with the value of k speciï¬?ed to indicate the number of non-zero features
allowed. Coates and Ng (2011) demonstrated that OMP-1 can be a very eï¬€ective
feature extractor for deep architectures.
Essentially any model that has hidden units can be made sparse. Throughout
this book, we will see many examples of sparsity regularization used in a variety of
contexts.

7.11

Bagging and Other Ensemble Methods

Bagging (short for bootstrap aggregating) is a technique for reducing generalization error by combining several models (Breiman, 1994). The idea is to
train several diï¬€erent models separately, then have all of the models vote on the
output for test examples. This is an example of a general strategy in machine
learning called model averaging. Techniques employing this strategy are known
as ensemble methods.
The reason that model averaging works is that diï¬€erent models will usually
not make all the same errors on the test set.
Consider for example a set of k regression models. Suppose that each model
makes an error î€? i on each example, with the errors drawn from a zero-mean
multivariate normal distribution with variances E[î€? 2i ] = v and covariances E[î€?iî€?j ] =
c. î??
Then the error made by the average prediction of all the ensemble models is
1
i î€?i . The expected squared error of the ensemble predictor is
k
ï£®î€ 
ï£®
ï£«
ï£¶ï£¹
î€¡2ï£¹
î?˜ 2 î?˜
1î?˜
1
ï£­î€?i +
î€?i ï£» = 2 Eï£°
î€?iî€?j ï£¸ï£»
(7.50)
Eï£°
k
k
i

i

=

1
kâˆ’1
v+
c.
k
k

j î€¶=i

(7.51)

In the case where the errors are perfectly correlated and c = v, the mean squared
error reduces to v, so the model averaging does not help at all. In the case where
the errors are perfectly uncorrelated and c = 0, the expected squared error of the
ensemble is only 1k v. This means that the expected squared error of the ensemble
decreases linearly with the ensemble size. In other words, on average, the ensemble
will perform at least as well as any of its members, and if the members make
independent errors, the ensemble will perform signiï¬?cantly better than its members.
Diï¬€erent ensemble methods construct the ensemble of models in diï¬€erent ways.
For example, each member of the ensemble could be formed by training a completely
256

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

Original dataset

First ensemble member

First resampled dataset

8

Second resampled dataset

Second ensemble member
8

Figure 7.5: A cartoon depiction of how bagging works. Suppose we train an 8 detector on
the dataset depicted above, containing an 8, a 6 and a 9. Suppose we make two diï¬€erent
resampled datasets. The bagging training procedure is to construct each of these datasets
by sampling with replacement. The ï¬?rst dataset omits the 9 and repeats the 8. On this
dataset, the detector learns that a loop on top of the digit corresponds to an 8. On
the second dataset, we repeat the 9 and omit the 6. In this case, the detector learns
that a loop on the bottom of the digit corresponds to an 8. Each of these individual
classiï¬?cation rules is brittle, but if we average their output then the detector is robust,
achieving maximal conï¬?dence only when both loops of the 8 are present.

diï¬€erent kind of model using a diï¬€erent algorithm or objective function. Bagging
is a method that allows the same kind of model, training algorithm and objective
function to be reused several times.
Speciï¬?cally, bagging involves constructing k diï¬€erent datasets. Each dataset
has the same number of examples as the original dataset, but each dataset is
constructed by sampling with replacement from the original dataset. This means
that, with high probability, each dataset is missing some of the examples from the
original dataset and also contains several duplicate examples (on average around
2/3 of the examples from the original dataset are found in the resulting training
set, if it has the same size as the original). Model i is then trained on dataset
i. The diï¬€erences between which examples are included in each dataset result in
diï¬€erences between the trained models. See ï¬?gure 7.5 for an example.
Neural networks reach a wide enough variety of solution points that they can
often beneï¬?t from model averaging even if all of the models are trained on the same
dataset. Diï¬€erences in random initialization, random selection of minibatches,
diï¬€erences in hyperparameters, or diï¬€erent outcomes of non-deterministic implementations of neural networks are often enough to cause diï¬€erent members of the
257

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

ensemble to make partially independent errors.
Model averaging is an extremely powerful and reliable method for reducing
generalization error. Its use is usually discouraged when benchmarking algorithms
for scientiï¬?c papers, because any machine learning algorithm can beneï¬?t substantially from model averaging at the price of increased computation and memory.
For this reason, benchmark comparisons are usually made using a single model.
Machine learning contests are usually won by methods using model averaging over dozens of models. A recent prominent example is the Netï¬‚ix Grand
Prize (Koren, 2009).
Not all techniques for constructing ensembles are designed to make the ensemble
more regularized than the individual models. For example, a technique called
boosting (Freund and Schapire, 1996b,a) constructs an ensemble with higher
capacity than the individual models. Boosting has been applied to build ensembles
of neural networks (Schwenk and Bengio, 1998) by incrementally adding neural
networks to the ensemble. Boosting has also been applied interpreting an individual
neural network as an ensemble (Bengio et al., 2006a), incrementally adding hidden
units to the neural network.

7.12

Dropout

Dropout (Srivastava et al., 2014) provides a computationally inexpensive but
powerful method of regularizing a broad family of models. To a ï¬?rst approximation,
dropout can be thought of as a method of making bagging practical for ensembles
of very many large neural networks. Bagging involves training multiple models,
and evaluating multiple models on each test example. This seems impractical
when each model is a large neural network, since training and evaluating such
networks is costly in terms of runtime and memory. It is common to use ensembles
of ï¬?ve to ten neural networksâ€”Szegedy et al. (2014a) used six to win the ILSVRCâ€”
but more than this rapidly becomes unwieldy. Dropout provides an inexpensive
approximation to training and evaluating a bagged ensemble of exponentially many
neural networks.
Speciï¬?cally, dropout trains the ensemble consisting of all sub-networks that
can be formed by removing non-output units from an underlying base network,
as illustrated in ï¬?gure 7.6. In most modern neural networks, based on a series of
aï¬ƒne transformations and nonlinearities, we can eï¬€ectively remove a unit from a
network by multiplying its output value by zero. This procedure requires some
slight modiï¬?cation for models such as radial basis function networks, which take
258

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

the diï¬€erence between the unitâ€™s state and some reference value. Here, we present
the dropout algorithm in terms of multiplication by zero for simplicity, but it can
be trivially modiï¬?ed to work with other operations that remove a unit from the
network.
Recall that to learn with bagging, we deï¬?ne k diï¬€erent models, construct k
diï¬€erent datasets by sampling from the training set with replacement, and then
train model i on dataset i. Dropout aims to approximate this process, but with an
exponentially large number of neural networks. Speciï¬?cally, to train with dropout,
we use a minibatch-based learning algorithm that makes small steps, such as
stochastic gradient descent. Each time we load an example into a minibatch, we
randomly sample a diï¬€erent binary mask to apply to all of the input and hidden
units in the network. The mask for each unit is sampled independently from all of
the others. The probability of sampling a mask value of one (causing a unit to be
included) is a hyperparameter ï¬?xed before training begins. It is not a function
of the current value of the model parameters or the input example. Typically,
an input unit is included with probability 0.8 and a hidden unit is included with
probability 0.5. We then run forward propagation, back-propagation, and the
learning update as usual. Figure 7.7 illustrates how to run forward propagation
with dropout.
More formally, suppose that a mask vector Âµ speciï¬?es which units to include,
and J (Î¸, Âµ) deï¬?nes the cost of the model deï¬?ned by parameters Î¸ and mask Âµ.
Then dropout training consists in minimizing EÂµ J( Î¸, Âµ). The expectation contains
exponentially many terms but we can obtain an unbiased estimate of its gradient
by sampling values of Âµ.
Dropout training is not quite the same as bagging training. In the case of
bagging, the models are all independent. In the case of dropout, the models share
parameters, with each model inheriting a diï¬€erent subset of parameters from the
parent neural network. This parameter sharing makes it possible to represent an
exponential number of models with a tractable amount of memory. In the case of
bagging, each model is trained to convergence on its respective training set. In the
case of dropout, typically most models are not explicitly trained at allâ€”usually,
the model is large enough that it would be infeasible to sample all possible subnetworks within the lifetime of the universe. Instead, a tiny fraction of the possible
sub-networks are each trained for a single step, and the parameter sharing causes
the remaining sub-networks to arrive at good settings of the parameters. These
are the only diï¬€erences. Beyond these, dropout follows the bagging algorithm. For
example, the training set encountered by each sub-network is indeed a subset of
the original training set sampled with replacement.
259

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

y

y

h1

h2

x1

x2

y

h1

y

h1

h1

x2

x1

y

h2

h2
x1

y

x2
y

h2

h2

h2
x1

x2

x1

y
x1

h2

y

h1
h1

y

y

x2

x2

y

y

x2

h1

h1

h2

Base network
x1

x2
y

x1

y

x1
y

h2

y

h1

x2

Ensemble of subnetworks

Figure 7.6: Dropout trains an ensemble consisting of all sub-networks that can be
constructed by removing non-output units from an underlying base network. Here, we
begin with a base network with two visible units and two hidden units. There are sixteen
possible subsets of these four units. We show all sixteen subnetworks that may be formed
by dropping out diï¬€erent subsets of units from the original network. In this small example,
a large proportion of the resulting networks have no input units or no path connecting
the input to the output. This problem becomes insigniï¬?cant for networks with wider
layers, where the probability of dropping all possible paths from inputs to outputs becomes
smaller.

260

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

y

h1

h2

x1

x2

y

hË†1

Âµ h1

hË†2

h1

xË†2

xË†1

Âµ x1

Âµ h2

h2

x1

x2

Âµ x2

Figure 7.7: An example of forward propagation through a feedforward network using
dropout. (Top)In this example, we use a feedforward network with two input units, one
hidden layer with two hidden units, and one output unit. (Bottom)To perform forward
propagation with dropout, we randomly sample a vector Âµ with one entry for each input
or hidden unit in the network. The entries of Âµ are binary and are sampled independently
from each other. The probability of each entry being 1 is a hyperparameter, usually 0.5
for the hidden layers and 0.8 for the input. Each unit in the network is multiplied by
the corresponding mask, and then forward propagation continues through the rest of the
network as usual. This is equivalent to randomly selecting one of the sub-networks from
ï¬?gure 7.6 and running forward propagation through it.
261

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

To make a prediction, a bagged ensemble must accumulate votes from all of
its members. We refer to this process as inference in this context. So far, our
description of bagging and dropout has not required that the model be explicitly
probabilistic. Now, we assume that the modelâ€™s role is to output a probability
distribution. In the case of bagging, each model i produces a probability distribution
p(i) (y | x). The prediction of the ensemble is given by the arithmetic mean of all
of these distributions,
k
1 î?˜ ( i)
p ( y | x ).
(7.52)
k
i=1

In the case of dropout, each sub-model deï¬?ned by mask vector Âµ deï¬?nes a probability distribution p(y | x, Âµ). The arithmetic mean over all masks is given
by
î?˜
p(Âµ)p(y | x, Âµ)
(7.53)
Âµ

wh
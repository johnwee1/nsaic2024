words in
the context.

10.1.1 Transformers: the intuition

The intuition of a transformer is that across a series of layers, we build up richer and
richer contextualized representations of the meanings of input words or tokens (we
will refer to the input as a sequence of words for convenience, although technically
the input is ﬁrst tokenized by an algorithm like BPE, so it is a series of tokens rather
than words). At each layer of a transformer, to compute the representation of a
word i we combine information from the representation of i at the previous layer
with information from the representations of the neighboring words. The goal is to
produce a contextualized representation for each word at each position. We can think
of these representations as a contextualized version of the static vectors we saw in
Chapter 6, which each represented the meaning of a word type. By contrast, our goal
in transformers is to produce a contextualized version, something that represents
what this word means in the particular context in which it occurs.

We thus need a mechanism that tells us how to weigh and combine the represen-
tations of the different words from the context at the prior level in order to compute
our representation at this layer. This mechanism must be able to look broadly in the
context, since words have rich linguistic relationships with words that can be many
sentences away. Even within the sentence, words have important linguistic relation-
ships with contextual words. Consider these examples, each exhibiting linguistic
relationships that we’ll discuss in more depth in later chapters:

(10.1) The keys to the cabinet are on the table.
(10.2) The chicken crossed the road because it wanted to get to the other side.
(10.3) I walked along the pond, and noticed that one of the trees along the bank

had fallen into the water after the storm.

In (10.1), the phrase The keys is the subject of the sentence, and in English and
many languages, must agree in grammatical number with the verb are; in this case
both are plural. In English we can’t use a singular verb like is with a plural sub-
ject like keys; we’ll discuss agreement more in Chapter 17. In (10.2), the pronoun
it corefers to the chicken; it’s the chicken that wants to get to the other side. We’ll
discuss coreference more in Chapter 26. In (10.3), the way we know that bank refers
to the side of a pond or river and not a ﬁnancial institution is from the context, in-
cluding words like pond and water. We’ll discuss word senses more in Chapter 23.
These helpful contextual words can be quite far way in the sentence or paragraph,

216 CHAPTER 10

• TRANSFORMERS AND LARGE LANGUAGE MODELS

so we need a mechanism that can look broadly in the context to help compute rep-
resentations for words.

Self-attention is just such a mechanism: it allows us to look broadly in the con-
text and tells us how to integrate the representation from words in that context from
layer k

1 to build the representation for words in layer k.

−

Figure 10.1 The self-attention weight distribution α that is part of the computation of the
representation for the word it at layer 6. In computing the representation for it, we attend
differently to the various words at layer 5, with darker shades indicating higher self-attention
values. Note that the transformer is attending highly to animal, a sensible result, since in this
example it corefers with the animal, and so we’d like the representation for it to draw on the
representation for animal. Figure simpliﬁed from (Uszkoreit, 2017).

Fig. 10.1 shows an schematic example simpliﬁed from a real transformer (Uszko-
reit, 2017). Here we want to compute a contextual representation for the word it, at
layer 6 of the transformer, and we’d like that representation to draw on the represen-
tations of all the prior words, from layer 5. The ﬁgure uses color to represent the
attention distribution over the contextual words: the word animal has a high atten-
tion weight, meaning that as we are computing the representation for it, we will draw
most heavily on the representation for animal. This will be useful for the model to
build a representation that has the correct meaning for it, which indeed is corefer-
ent here with the word animal. (We say that a pronoun like it is coreferent with a
noun like animal if they both refer to the same thing; we’ll return to coreference in
Chapter 26.)

10.1.2 Causal or backward-looking self-attention

The concept of context can be used in two ways in self-attention.
In causal, or
backward looking self-attention, the context is any of the prior words. In general
bidirectional self-attention, the context can include future words.
In this chapter
we focus on causal, backward looking self-attention; we’ll introduce bidirectional
self-attention in Chapter 11.

Fig. 10.2 thus illustrates the ﬂow of information in a single causal, or backward
looking, self-attention layer. As with the overall transformer, a self-attention layer
maps input sequences (x1, ..., xn) to output sequences of the same length (a1, ..., an).
When processing each item in the input, the model has access to all of the inputs
up to and including the one under consideration, but no access to information about
inputs beyond the current one. In addition, the computation performed for each item
is independent of all the other computations. The ﬁrst point ensures that we can use
this approach to create language models and use them for autoregressive generation,
and the second point means that we can easily parallelize both forward inference
and training of such models.

Theanimaldidn’tcrossthestreetbecauseitwastootiredTheanimaldidn’tcrossthestreetbecauseitwastootiredLayer 6Layer 5self-attention distribution10.1

• THE TRANSFORMER: A SELF-ATTENTION NETWORK

217

Information ﬂow in a causal (or masked) self-attention model. In processing
Figure 10.2
each element of the sequence, the model attends to all the inputs up to, and including, the
current one. Unlike RNNs, the computations at each time step are independent of all the
other steps and therefore can be performed in parallel.

10.1.3 Self-attention more formally

We’ve given the intuition of self-attention (as a way to compute representations of a
word at a given layer by integrating information from words at the previous layer)
and we’ve deﬁned context as all the prior words in the input. Let’s now introduce
the self-attention computation itself.

The core intuition of attention is the idea of comparing an item of interest to a
collection of other items in a way that reveals their relevance in the current context.
In the case of self-attention for language, the set of comparisons are to other words
(or tokens) within a given sequence. The result of these comparisons is then used to
compute an output sequence for the current input sequence. For example, returning
to Fig. 10.2, the computation of a3 is based on a set of comparisons between the
input x3 and its preceding elements x1 and x2, and to x3 itself.

How shall we compare words to other words? Since our representations for
words are vectors, we’ll make use of our old friend the dot product that we used
for computing word similarity in Chapter 6, and also played a role in attention in
Chapter 9. Let’s refer to the result of this comparison between words i and j as a
score (we’ll be updating this equation to add attention to the computation of this
score):

Verson 1:

score(xi, x j) = xi ·

x j

(10.4)

The result of a dot product is a scalar value ranging from

∞ to ∞, the larger
the value the more similar the vectors that are being compared. Continuing with our
example, the ﬁrst step in computing y3 would be to compute three scores: x3 ·
x1,
x3. Then to make effective use of these scores, we’ll normalize them
x3 ·
with a softmax to create a vector of weights, αi j, that indicates the proportional
relevance of each input to the input element i that is the current focus of attention.

x2 and x3 ·

−

αi j = softmax(score(xi, x j))
exp(score(xi, x j))
i
k=1 exp(score(xi, xk)) ∀

=

∀

j

≤
j

i

i

≤

(10.5)

(10.6)

(cid:80)

Of course, the softmax weight will likely be highest for the current focus element
i, since vecxi is very similar to itself, resulting in a high dot product. But other
context words may also be similar to i, and the softmax will also assign some weight
to those words.

Given the proportional scores in α, we generate an output value ai by summing

Self-AttentionLayerx1a1x2a2a3a4a5x3x4x5218 CHAPTER 10

• TRANSFORMERS AND LARGE LANGUAGE MODELS

the inputs seen so far, each weighted by its α value.

ai =

αi jx j

(10.7)

i
(cid:88)j
≤
The steps embodied in Equations 10.4 through 10.7 represent the core of an
attention-based approach: a set of comparisons to relevant items in some context,
a normalization of those scores to provide a probability distribution, followed by a
weighted sum using this distribution. The output a is the result of this straightfor-
ward computation over the inputs.

This kind of simple attention can be useful, and indeed we saw in Chapter 9
how to use this simple idea of attention for LSTM-based encoder-decoder models
for machine translation. But transformers allow us to create a more sophisticated
way of representing how words can contribute to the representation of longer inputs.
Consider the three different roles that each input embedding plays during the course
of the attention process.

query

key

value

• As the current focus of attention when being compared to all of the other

preceding inputs. We’ll refer to this role as a query.

• In its role as a preceding input being compared to the current focus of atten-

tion. We’ll refer to this role as a key.

• And ﬁnally, as a value used to compute the output for the current focus of

attention.

To capture these three different roles, transformers introduce weight matrices
WQ, WK, and WV. These weights will be used to project each input vector xi into
a representation of its role as a key, query, or value.

qi = xiWQ; ki = xiWK; vi = xiWV

(10.8)

×

The inputs x and outputs y of transformers, as well as the intermediate vectors after
the various layers like the attention output vector a, all have the same dimensionality
d. We’ll have a dimension dk for the key and query vectors, and a separate
1
dimension dv for the value vectors. In the original transformer work (Vaswani et al.,
2017), d was 512, dk and dv were both 64. The shapes of the transform matrices are
then WQ

∈
Given these projections, the score between a current focus of attention, xi, and
an element in the preceding context, x j, consists of a dot product between its query
vector qi and the preceding element’s key vectors k j. This dot product has the right
dk. Let’s update
shape since both the query and the key are of dimensionality 1
our previous comparison calculation to reﬂect this, replacing Eq. 10.4 with Eq. 10.9:

dk , and WV
×

dk , WK
×

dv.
×

Rd

Rd

Rd

×

∈

∈

Verson 2:

score(xi, x j) = qi ·

k j

(10.9)

The ensuing softmax calculation resulting in αi, j remains the same, but the output
calculation for ai is now based on a weighted sum over the value vectors v.

ai =

αi jv j

(10.10)

i
(cid:88)j
≤
Again, the softmax weight αi j will likely be highest for the current focus element
i, and so the value for yi will be most inﬂuenced by vi. But the model will also pay
attention to other contextual words if they are similar to i, allowing their values to

10.1

• THE TRANSFORMER: A SELF-ATTENTION NETWORK

219

Figure 10.3 Calculating the value of a3, the third element of a sequence using causal (left-
to-right) self-attention.

also inﬂuence the ﬁnal value of v j. Context words that are not similar to i will have
their values downweighted and won’t contribute to the ﬁnal value.

There is one ﬁnal part of the self-attention model. The result of a dot product
can be an arbitrarily large (positive or negative) value. Exponentiating large values
can lead to numerical issues and to an effective loss of gradients during training. To
avoid this, we scale down the result of the dot product, by dividing it by a factor
related to the size of the embeddings. A typical approach is to divide by the square
root of the dimensionality of the query and key vectors (dk), leading us to update
our scoring function one more time, replacing Eq. 10.4 and Eq. 10.9 with Eq. 10.12.
Here’s a ﬁnal set of equations for computing self-attention for a single self-attention
output vector ai from a single input vector xi, illustrated in Fig. 10.3 for the case of
calculating the value of the third output a3 in a sequence.

qi = xiWQ; ki = xiWK; vi = xiWV

Final verson:

score(xi, x j) =

k j
qi ·
√dk

αi j = softmax(score(xi, x j))

ai =

αi jv j

i
(cid:88)j
≤

(10.11)

(10.12)

(10.13)

(10.14)

j

∀

≤

i

10.1.4 Parallelizing self-attention using a single matrix X

This description of the self-attention process has been from the perspective of com-
puting a single output at a single time step i. However, since each output, yi, is
computed independently, this entire process can be parallelized, taking advantage of

6. Sum the weighted value vectors4. Turn into weights via softmaxa31. Generate key, query, value vectors2. Compare x3’s query withthe keys for x1, x2, and x3Output of self-attentionWkWvWqx1kqvx3kqvx2kqv××WkWkWqWqWvWv5. Weigh each value vector÷dk3. Divide score by dk÷dk÷dk220 CHAPTER 10

• TRANSFORMERS AND LARGE LANGUAGE MODELS

efﬁcient matrix multiplication routines by packing the input embeddings of the N
d. That is, each row of X
tokens of the input sequence into a single matrix X
×
is the embedding of one token of the input. Transformers for large language models
can have an input length N = 1024, 2048, or 4096 tokens, so X has between 1K and
4K rows, each of the dimensionality of the embedding d.

RN

∈

We then multiply X by the key, query, and value matrices (all of dimensionality
d, containing all the
RN
d) to produce matrices Q
d
×
key, query, and value vectors:

d, and V
×

d, K
×

RN

RN

×

∈

∈

∈

Q = XWQ; K = XWK; V = XWV

(10.15)

Given these matrices we can compute all the requisite query-key comparisons simul-
taneously by multiplying Q and K(cid:124)
in a single matrix multiplication (the product is
of shape N
N; Fig. 10.4 shows a visualization). Taking this one step further, we
can scale these scores, take the softmax, and then multiply the result by V resulting
d: a vector embedding representation for each token in the
in a matrix of shape N
input. We’ve reduced the entire self-attention step for an entire sequence of N tokens
to the following computation:

×

×

A = SelfAttention(Q, K, V) = softmax

QK(cid:124)
√dk (cid:19)

V

(cid:18)

(10.16)

10.1.5 Masking out the future

The self-attention computation as we’ve described it has a problem: the calculation
in QK(cid:124)
results in a score for each query value to every key value, including those that
follow the query. This is inappropriate in the setting of language modeling: guessing
the next word is pretty simple if you already know it! To ﬁx this, the elements in the
∞), thus eliminating
upper-triangular portion of the matrix are zeroed out (set to
any knowledge of words that follow in the sequence. Fig. 10.4 shows this masked
QK(cid:124)
matrix. (we’ll see in Chapter 11 how to make use of words in the future for
tasks that need it).

−

N QK(cid:124)
Figure 10.4 The N
portion of the comparisons matrix zeroed out (set
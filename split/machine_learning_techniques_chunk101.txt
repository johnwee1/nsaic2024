g agents
capable  of  taking  actions  in  an  environment  in  a  way  that  maximizes  rewards
over  time.  There  are  many  differences  between  RL  and  regular  supervised  and
unsupervised learning. Here are a few:

• In supervised and unsupervised learning, the goal is generally to find patterns
in the data and use them to make predictions. In Reinforcement Learning, the
goal is to find a good policy.

• Unlike  in  supervised  learning,  the  agent  is  not  explicitly  given  the  “right”

answer. It must learn by trial and error.

• Unlike  in  unsupervised  learning,  there  is  a  form  of  supervision,  through
rewards.  We  do  not  tell  the  agent  how  to  perform  the  task,  but  we  do  tell  it
when it is making progress or when it is failing.

• A  Reinforcement  Learning  agent  needs  to  find  the  right  balance  between
exploring  the  environment,  looking  for  new  ways  of  getting  rewards,  and
exploiting sources of rewards that it already knows. In contrast, supervised and
unsupervised  learning  systems  generally  don’t  need  to  worry  about  explora‐
tion; they just feed on the training data they are given.

• In supervised and unsupervised learning, training instances are typically inde‐
pendent (in fact, they are generally shuffled). In Reinforcement Learning, con‐
secutive observations are generally not independent. An agent may remain in
the same region of the environment for a while before it moves on, so consecu‐
tive  observations  will  be  very  correlated.  In  some  cases  a  replay  memory
(buffer)  is  used  to  ensure  that  the  training  algorithm  gets  fairly  independent
observations.

748 

|  Appendix A: Exercise Solutions

2. Here are a few possible applications of Reinforcement Learning, other than those

mentioned in Chapter 18:

Music personalization

The environment is a user’s personalized web radio. The agent is the software
deciding what song to play next for that user. Its possible actions are to play
any song in the catalog (it must try to choose a song the user will enjoy) or to
play an advertisement (it must try to choose an ad that the user will be inter‐
ested in). It gets a small reward every time the user listens to a song, a larger
reward every time the user listens to an ad, a negative reward when the user
skips a song or an ad, and a very negative reward if the user leaves.

Marketing

The environment is your company’s marketing department. The agent is the
software that defines which customers a mailing campaign should be sent to,
given their profile and purchase history (for each customer it has two possi‐
ble actions: send or don’t send). It gets a negative reward for the cost of the
mailing  campaign,  and  a  positive  reward  for  estimated  revenue  generated
from this campaign.

Product delivery

Let  the  agent  control  a  fleet  of  delivery  trucks,  deciding  what  they  should
pick up at the depots, where they should go, what they should drop off, and
so  on.  It  will  get  positive  rewards  for  each  product  delivered  on  time,  and
negative rewards for late deliveries.

3. When estimating the value of an action, Reinforcement Learning algorithms typ‐
ically sum all the rewards that this action led to, giving more weight to immediate
rewards  and  less  weight  to  later  rewards  (considering  that  an  action  has  more
influence on the near future than on the distant future). To model this, a discount
factor is typically applied at each time step. For example, with a discount factor of
0.9, a reward of 100 that is received two time steps later is counted as only 0.92 ×
100 = 81 when you are estimating the value of the action. You can think of the
discount  factor  as  a  measure  of  how  much  the  future  is  valued  relative  to  the
present:  if  it  is  very  close  to  1,  then  the  future  is  valued  almost  as  much  as  the
present;  if  it  is  close  to  0,  then  only  immediate  rewards  matter.  Of  course,  this
impacts  the  optimal  policy  tremendously:  if  you  value  the  future,  you  may  be
willing  to  put  up  with  a  lot  of  immediate  pain  for  the  prospect  of  eventual
rewards,  while  if  you  don’t  value  the  future,  you  will  just  grab  any  immediate
reward you can find, never investing in the future.

4. To measure the performance of a Reinforcement Learning agent, you can simply
sum up the rewards it gets. In a simulated environment, you can run many epi‐
sodes  and  look  at  the  total  rewards  it  gets  on  average  (and  possibly  look  at  the
min, max, standard deviation, and so on).

Exercise Solutions 

| 

749

5. The  credit  assignment  problem  is  the  fact  that  when  a  Reinforcement  Learning
agent  receives  a  reward,  it  has  no  direct  way  of  knowing  which  of  its  previous
actions contributed to this reward. It typically occurs when there is a large delay
between an action and the resulting reward (e.g., during a game of Atari’s Pong,
there may be a few dozen time steps between the moment the agent hits the ball
and the moment it wins the point). One way to alleviate it is to provide the agent
with shorter-term rewards, when possible. This usually requires prior knowledge
about the task. For example, if we want to build an agent that will learn to play
chess, instead of giving it a reward only when it wins the game, we could give it a
reward every time it captures one of the opponent’s pieces.

6. An agent can often remain in the same region of its environment for a while, so
all of its experiences will be very similar for that period of time. This can intro‐
duce some bias in the learning algorithm. It may tune its policy for this region of
the  environment,  but  it  will  not  perform  well  as  soon  as  it  moves  out  of  this
region.  To  solve  this  problem,  you  can  use  a  replay  memory;  instead  of  using
only the most immediate experiences for learning, the agent will learn based on a
buffer  of  its  past  experiences,  recent  and  not  so  recent  (perhaps  this  is  why  we
dream at night: to replay our experiences of the day and better learn from them?).

7. An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of
discounted rewards that can be expected for each state if the agent acts optimally)
while the agent follows a different policy. Q-Learning is a good example of such
an  algorithm.  In  contrast,  an  on-policy  algorithm  learns  the  value  of  the  policy
that the agent actually executes, including both exploration and exploitation.

For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available
at https://github.com/ageron/handson-ml2.

Chapter 19: Training and Deploying TensorFlow Models
at Scale

1. A SavedModel contains a TensorFlow model, including its architecture (a com‐
putation  graph)  and  its  weights.  It  is  stored  as  a  directory  containing  a
saved_model.pb file, which defines the computation graph (represented as a seri‐
alized protocol buffer), and a variables subdirectory containing the variable val‐
ues. For models containing a large number of weights, these variable values may
be split across multiple files. A SavedModel also includes an assets subdirectory
that may contain additional data, such as vocabulary files, class names, or some
example instances for this model. To be more accurate, a SavedModel can con‐
tain  one  or  more  metagraphs.  A  metagraph  is  a  computation  graph  plus  some
function  signature  definitions  (including  their  input  and  output  names,  types,
and shapes). Each metagraph is identified by a set of tags. To inspect a SavedMo‐

750 

|  Appendix A: Exercise Solutions

del,  you  can  use  the  command-line  tool  saved_model_cli  or  just  load  it  using
tf.saved_model.load() and inspect it in Python.

2. TF  Serving  allows  you  to  deploy  multiple  TensorFlow  models  (or  multiple  ver‐
sions of the same model) and make them accessible to all your applications easily
via a REST API or a gRPC API. Using your models directly in your applications
would make it harder to deploy a new version of a model across all applications.
Implementing  your  own  microservice  to  wrap  a  TF  model  would  require  extra
work, and it would be hard to match TF Serving’s features. TF Serving has many
features:  it  can  monitor  a  directory  and  autodeploy  the  models  that  are  placed
there, and you won’t have to change or even restart any of your applications to
benefit  from  the  new  model  versions;  it’s  fast,  well  tested,  and  scales  very  well;
and it supports A/B testing of experimental models and deploying a new model
version to just a subset of your users (in this case the model is called a canary).
TF  Serving  is  also  capable  of  grouping  individual  requests  into  batches  to  run
them jointly on the GPU. To deploy TF Serving, you can install it from source,
but it is much simpler to install it using a Docker image. To deploy a cluster of TF
Serving Docker images, you can use an orchestration tool such as Kubernetes, or
use a fully hosted solution such as Google Cloud AI Platform.

3. To  deploy  a  model  across  multiple  TF  Serving  instances,  all  you  need  to  do  is
configure these TF Serving instances to monitor the same models directory, and
then export your new model as a SavedModel into a subdirectory.

4. The gRPC API is more efficient than the REST API. However, its client libraries
are not as widely available, and if you activate compression when using the REST
API, you can get almost the same performance. So, the gRPC API is most useful
when you need the highest possible performance and the clients are not limited
to the REST API.

5. To reduce a model’s size so it can run on a mobile or embedded device, TFLite

uses several techniques:

• It provides a converter which can optimize a SavedModel: it shrinks the model
and  reduces  its  latency.  To  do  this,  it  prunes  all  the  operations  that  are  not
needed to make predictions (such as training operations), and it optimizes and
fuses operations whenever possible.

• The converter can also perform post-training quantization: this technique dra‐
matically reduces the model’s size, so it’s much faster to download and store.

• It saves the optimized model using the FlatBuffer format, which can be loaded
to RAM directly, without parsing. This reduces the loading time and memory
footprint.

Exercise Solutions 

| 

751

6. Quantization-aware  training  consists  in  adding  fake  quantization  operations  to
the model during training. This allows the model to learn to ignore the quantiza‐
tion noise; the final weights will be more robust to quantization.

7. Model parallelism means chopping your model into multiple parts and running
them in parallel across multiple devices, hopefully speeding up the model during
training or inference. Data parallelism means creating multiple exact replicas of
your model and deploying them across multiple devices. At each iteration during
training, each replica is given a different batch of data, and it computes the gradi‐
ents of the loss with regard to the model parameters. In synchronous data paral‐
lelism,  the  gradients  from  all  replicas  are  then  aggregated  and  the  optimizer
performs  a  Gradient  Descent  step.  The  parameters  may  be  centralized  (e.g.,  on
parameter servers) or replicated across all replicas and kept in sync using AllRe‐
duce.  In  asynchronous  data  parallelism,  the  parameters  are  centralized  and  the
replicas  run  independently  from  each  other,  each  updating  the  central  parame‐
ters directly at the end of each training iteration, without having to wait for the
other  replicas.  To  speed  up  training,  data  parallelism  turns  out  to  work  better
than  model  parallelism,  in  general.  This  is  mostly  because  it  requires  less  com‐
munication  across  devices.  Moreover,  it  is  much  easier  to  implement,  and  it
works the same way for any model, whereas model parallelism requires analyzing
the model to determine the best way to chop it into pieces.

8. When training a model across multiple servers, you can use the following distri‐

bution strategies:

• The  MultiWorkerMirroredStrategy  performs  mirrored  data  parallelism.  The
model  is  replicated  across  all  available  servers  and  devices,  and  each  replica
gets a different batch of data at each training iteration and computes its own
gradients. The mean of the gradients is computed and shared across all replicas
using a distributed AllReduce implementation (NCCL by default), and all rep‐
licas perform the same Gradient Descent step. This strategy is the simplest to
use since all servers and devices are treated in exactly the same way, and it per‐
forms fairly well. In general, you should use this strategy. Its main limitation is
that it requires the model to fit in RAM on every replica.

• The ParameterServerStrategy performs asynchronous data parallelism. The
model  is  replicated  across  all  devices  on  all  workers,  and  the  parameters  are
sharded  across  all  parameter  servers.  Each  worker  has  its  own  training  loop,
running  asynchronously  with  the  other  workers;  at  each  training  iteration,
each  worker  gets  its  own  batch  of  data  and  fetches  the  latest  version  of  the
model parameters from the parameter servers, then it computes the gradients
of the loss with regard to these parameters, and it sends them to the parameter
servers.  Lastly,  the  parameter  servers  perform  a  Gradient  Descent  step  using
these  gradients.  This  strategy  is  generally  slower  than  the  previous  strategy,

752 

|  Appendix A: Exercise Solutions

and a bit harder to deploy, since it requires managing parameter servers. How‐
ever, it is useful to train huge models that don’t fit in GPU RAM.

For the solutions to exercises 9, 10, and 11, please see the Jupyter notebooks available
at https://github.com/ageron/handson-ml2.

Exercise Solutions 

| 

753

APPENDIX B
Machine Learning Project Checklist

This  checklist  can  guide  you  through  your  Machine  Learning  projects.  There  are
eight main steps:

1. Frame the problem and look at the big picture.

2. Get the data.

3. Explore the data to gain insights.

4. Prepare the data to better expose the underlying data patterns to Machine Learn‐

ing algorithms.

5. Explore many different models and shortlist the best ones.

6. Fine-tune your models and combine them into a great solution.

7. Present your solution.

8. Launch, monitor, and maintain your system.

Obviously, you should feel free to adapt this checklist to your needs.

Frame the Problem and Look at the Big Picture

1. Define the objective in business terms.

2. How will your solution be used?

3. What are the current solutions/workarounds (if any)?

4. How  should  you  frame  this  problem  (supervised/unsupervised,  online/offline,

etc.)?

5. How should performance be measured?

6. Is the performance measure aligned with the business objective?

755

7. What would be the minimum performance needed to reach the business objec‐

tive?

8. What are comparable problems? Can you reuse experience or tools?

9. Is human expertise available?

10. How would you solve the problem manually?

11. List the assumptions you (or others) have made so far.

12. Verify assumptions if possible.

Get the Data
No
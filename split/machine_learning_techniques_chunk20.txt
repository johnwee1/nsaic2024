

i = 1

subject to t i w⊺x i + b ≥ 1 − ζ i

and ζ i ≥ 0

for i = 1, 2, ⋯, m

Quadratic Programming
The hard margin and soft margin problems are both convex quadratic optimization
problems  with  linear  constraints.  Such  problems  are  known  as  Quadratic  Program‐
ming  (QP)  problems.  Many  off-the-shelf  solvers  are  available  to  solve  QP  problems
by using a variety of techniques that are outside the scope of this book.5

4 Zeta (ζ) is the sixth letter of the Greek alphabet.

5 To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vandenber‐
ghe’s book Convex Optimization (Cambridge University Press, 2004) or watch Richard Brown’s series of video
lectures.

Under the Hood 

| 

167

The general problem formulation is given by Equation 5-5.

Equation 5-5. Quadratic Programming problem

Minimize
p
subject to

where

p⊺Hp

+

f⊺p

1
2

Ap ≤ b
p is an np‐dimensional vector (np = number of parameters),
H is an np × np matrix,
f

is an np‐dimensional vector,

A is an nc × np matrix (nc = number of constraints),
b

is an nc‐dimensional vector.

Note that the expression A p ≤ b defines nc constraints: p⊺ a(i) ≤ b(i) for i = 1, 2, ⋯, nc,
where  a(i)  is  the  vector  containing  the  elements  of  the  ith  row  of  A  and  b(i)  is  the  ith
element of b.

You can easily verify that if you set the QP parameters in the following way, you get
the hard margin linear SVM classifier objective:

• np = n + 1, where n is the number of features (the +1 is for the bias term).
• nc = m, where m is the number of training instances.
• H is the np × np identity matrix, except with a zero in the top-left cell (to ignore

the bias term).

• f = 0, an np-dimensional vector full of 0s.
• b = –1, an nc-dimensional vector full of –1s.
• a(i) = –t(i) x˙(i), where x˙(i) is equal to x(i) with an extra bias feature x˙0 = 1.

One  way  to  train  a  hard  margin  linear  SVM  classifier  is  to  use  an  off-the-shelf  QP
solver  and  pass  it  the  preceding  parameters.  The  resulting  vector  p  will  contain  the
bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you can
use a QP solver to solve the soft margin problem (see the exercises at the end of the
chapter).

To use the kernel trick, we are going to look at a different constrained optimization
problem.

The Dual Problem
Given a constrained optimization problem, known as the primal problem, it is possi‐
ble  to  express  a  different  but  closely  related  problem,  called  its  dual  problem.  The

168 

| 

Chapter 5: Support Vector Machines

solution to the dual problem typically gives a lower bound to the solution of the pri‐
mal problem, but under some conditions it can have the same solution as the primal
problem.  Luckily,  the  SVM  problem  happens  to  meet  these  conditions,6  so  you  can
choose to solve the primal problem or the dual problem; both will have the same sol‐
ution. Equation 5-6 shows the dual form of the linear SVM objective (if you are inter‐
ested  in  knowing  how  to  derive  the  dual  problem  from  the  primal  problem,  see
Appendix C).

Equation 5-6. Dual form of the linear SVM objective

minimize
α

m

1
2 ∑

i = 1

α i α j t i t j x i ⊺

m
∑
j = 1
subject to α i ≥ 0

x j

−

m
∑
i = 1

α i

for i = 1, 2, ⋯, m

Once  you  find  the  vector  α  that  minimizes  this  equation  (using  a  QP  solver),  use
Equation 5-7 to compute w and b that minimize the primal problem.

Equation 5-7. From the dual solution to the primal solution

m
w = ∑
i = 1

α i t i x i

b =

1
ns

m
∑
i = 1
i

> 0

α

t i − w⊺x i

The dual problem is faster to solve than the primal one when the number of training
instances is smaller than the number of features. More importantly, the dual problem
makes the kernel trick possible, while the primal does not. So what is this kernel trick,
anyway?

Kernelized SVMs
Suppose  you  want  to  apply  a  second-degree  polynomial  transformation  to  a  two-
dimensional  training  set  (such  as  the  moons  training  set),  then  train  a  linear  SVM
classifier on the transformed training set. Equation 5-8 shows the second-degree pol‐
ynomial mapping function ϕ that you want to apply.

6 The objective function is convex, and the inequality constraints are continuously differentiable and convex

functions.

Under the Hood 

| 

169

Equation 5-8. Second-degree polynomial mapping

ϕ x = ϕ

x1
x2

=

2

x1
2 x1x2
2
x2

Notice that the transformed vector is 3D instead of 2D. Now let’s look at what hap‐
pens  to  a  couple  of  2D  vectors,  a  and  b,  if  we  apply  this  second-degree  polynomial
mapping and then compute the dot product7 of the transformed vectors (See Equa‐
tion 5-9).

Equation 5-9. Kernel trick for a second-degree polynomial mapping

ϕ a ⊺ϕ b

=

2

a1
2 a1a2
2

a2

⊺

2
b1
2 b1b2
2

b2

= a1

2b1

2 + 2a1b1a2b2 + a2

2b2

2

= a1b1 + a2b2

2 =

2

a1
a2

⊺ b1
b2

= a⊺b

2

How about that? The dot product of the transformed vectors is equal to the square of
the dot product of the original vectors: ϕ(a)⊺ ϕ(b) = (a⊺ b)2.

Here  is  the  key  insight:  if  you  apply  the  transformation  ϕ  to  all  training  instances,
then the dual problem (see Equation 5-6) will contain the dot product ϕ(x(i))⊺ ϕ(x(j)).
But  if  ϕ  is  the  second-degree  polynomial  transformation  defined  in  Equation  5-8,
then you can replace this dot product of transformed vectors simply by  x i ⊺
. So,
you don’t need to transform the training instances at all; just replace the dot product
by its square in Equation 5-6. The result will be strictly the same as if you had gone
through the trouble of transforming the training set then fitting a linear SVM algo‐
rithm, but this trick makes the whole process much more computationally efficient.

x j 2

The  function  K(a,  b)  =  (a⊺  b)2  is  a  second-degree  polynomial  kernel.  In  Machine
Learning,  a  kernel  is  a  function  capable  of  computing  the  dot  product  ϕ(a)⊺  ϕ(b),

7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b. However, in

Machine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the
dot product is achieved by computing a⊺b. To remain consistent with the rest of the book, we will use this
notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.

170 

| 

Chapter 5: Support Vector Machines

based  only  on  the  original  vectors  a  and  b,  without  having  to  compute  (or  even  to
know about) the transformation ϕ. Equation 5-10 lists some of the most commonly
used kernels.

Linear:

Equation 5-10. Common kernels
K a, b = a⊺b
K a, b = γa⊺b + r
K a, b = exp −γ∥ a − b ∥2
K a, b = tanh γa⊺b + r

Gaussian RBF:

Polynomial:

Sigmoid:

d

Mercer’s Theorem
According to Mercer’s theorem, if a function K(a, b) respects a few mathematical con‐
ditions  called  Mercer’s  conditions  (e.g.,  K  must  be  continuous  and  symmetric  in  its
arguments so that K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a
and b into another space (possibly with much higher dimensions) such that K(a, b) =
ϕ(a)⊺  ϕ(b).  You  can  use  K  as  a  kernel  because  you  know  ϕ  exists,  even  if  you  don’t
know what ϕ is. In the case of the Gaussian RBF kernel, it can be shown that ϕ maps
each training instance to an infinite-dimensional space, so it’s a good thing you don’t
need to actually perform the mapping!

Note that some frequently used kernels (such as the sigmoid kernel) don’t respect all
of Mercer’s conditions, yet they generally work well in practice.

There is still one loose end we must tie up. Equation 5-7 shows how to go from the
dual solution to the primal solution in the case of a linear SVM classifier. But if you
apply the kernel trick, you end up with equations that include ϕ(x(i)). In fact, w must
have the same number of dimensions as ϕ(x(i)), which may be huge or even infinite,
so you can’t compute it. But how can you make predictions without knowing w? Well,
the good news is that you can plug the formula for w from Equation 5-7 into the deci‐
sion function for a new instance x(n), and you get an equation with only dot products
between input vectors. This makes it possible to use the kernel trick (Equation 5-11).

Under the Hood 

| 

171

Equation 5-11. Making predictions with a kernelized SVM

ϕ x n

h

w, b

= w⊺ϕ x n + b = ∑

i = 1

m

⊺

α i t i ϕ x i

ϕ x n + b

m
= ∑
i = 1

α i t i ϕ x i ⊺

ϕ x n

+ b

α i t i K x i , x n + b

m
= ∑
i = 1
i

α

> 0

Note that since α(i) ≠ 0 only for support vectors, making predictions involves comput‐
ing the dot product of the new input vector x(n) with only the support vectors, not all
the training instances. Of course, you need to use the same trick to compute the bias
term b (Equation 5-12).

Equation 5-12. Using the kernel trick to compute the bias term

b =

1
ns

m
∑
i = 1
i

> 0

α

=

1
ns

m
∑
i = 1
i

> 0

α

t i − w⊺ϕ x i

=

1
ns

m
∑
i = 1
i

> 0

α

m
t i − ∑
j = 1

⊺

α j t j ϕ x j

ϕ x i

m
t i − ∑
j = 1
j

α

> 0

α j t j K x i , x j

If  you  are  starting  to  get  a  headache,  it’s  perfectly  normal:  it’s  an  unfortunate  side
effect of the kernel trick.

Online SVMs
Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall
that online learning means learning incrementally, typically as new instances arrive).

For linear SVM classifiers, one method for implementing an online SVM classifier is
to use Gradient Descent (e.g., using SGDClassifier) to minimize the cost function in
Equation  5-13,  which  is  derived  from  the  primal  problem.  Unfortunately,  Gradient
Descent converges much more slowly than the methods based on QP.

172 

| 

Chapter 5: Support Vector Machines

Equation 5-13. Linear SVM classifier cost function

J w, b =

1
2

w⊺w + C ∑

m

i = 1

max 0, 1 − t i w⊺x i + b

The first sum in the cost function will push the model to have a small weight vector
w, leading to a larger margin. The second sum computes the total of all margin viola‐
tions. An instance’s margin violation is equal to 0 if it is located off the street and on
the  correct  side,  or  else  it  is  proportional  to  the  distance  to  the  correct  side  of  the
street. Minimizing this term ensures that the model makes the margin violations as
small and as few as possible.

Hinge Loss
The function max(0, 1 – t) is called the hinge loss function (see the following image).
It is equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is
not differentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”
on page 137), you can still use Gradient Descent using any subderivative at t = 1 (i.e.,
any value between –1 and 0).

It  is  also  possible  to  implement  online  kernelized  SVMs,  as  described  in  the  papers
“Incremental and Decremental Support Vector Machine Learning”8 and “Fast Kernel
Classifiers  with  Online  and  Active  Learning”.9  These  kernelized  SVMs  are  imple‐

8 Gert Cauwenberghs and Tomaso Poggio, “Incremental and Decremental Support Vector Machine Learning,”
Proceedings of the 13th International Conference on Neural Information Processing Systems (2000): 388–394.

9 Antoine Bordes et al., “Fast Kernel Classifiers with Online and Active Learning,” Journal of Machine Learning

Research 6 (2005): 1579–1619.

Under the Hood 

| 

173

mented  in  Matlab  and  C++.  For  large-scale  nonlinear  problems,  you  may  want  to
consider using neural networks instead (see Part II).

Exercises

1. What is the fundamental idea behind Support Vector Machines?

2. What is a support vector?

3. Why is it important to scale the inputs when using SVMs?

4. Can an SVM classifier output a confidence score when it classifies an instance?

What about a probability?

5. Should you use the primal or the dual form of the SVM problem to train a model

on a training set with millions of instances and hundreds of features?

6. Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit

the training set. Should you increase or decrease γ (gamma)? What about C?

7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin

linear SVM classifier problem using an off-the-shelf QP solver?

8. Train  a  LinearSVC  on  a  linearly  separable  dataset.  Then  train  an  SVC  and  a
SGDClassifier on the same dataset. See if you can get them to produce roughly
the same model.

9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary
classifiers,  you  will  need  to  use  one-versus-the-rest  to  classify  all  10  digits.  You
may want to tune the hyperparameters using small validation sets to speed up the
process. What accuracy can you reach?

10. Train an SVM regressor on the California housing dataset.

Solutions to these exercises are available in Appendix A.

174 

| 

Chapter 5: Support Vector Machines

CHAPTER 6
Decision Trees

Like  SVMs,  Decision  Trees  are  versatile  Machine  Learning  algorithms  that  can  per‐
form  both  classification  and  regression  tasks,  and  even  multioutput  tasks.  They  are
powerful algorithms, capable of fitting complex datasets. For example, in Chapter 2
you trained a DecisionTreeRegressor model on the California housing dataset, fit‐
ting it perfectly (actually, overfitting it).

Decision Trees are also the fundamental components of Random Forests (see Chap‐
ter  7),  which  are  among  the  most  powerful  Machine  Learning  algorithms  available
today.

In this chapter we will start by discussing how to train, visualize, and make predic‐
tions  with  Decision  Trees.  Then  we  will  go  through  the  CART  training  algorithm
used  by  Scikit-Learn,  and  we  will  discuss  how  to  regularize  trees  and  use  them  for
regression tasks. Finally, we will discuss some of the limitations of Decision Trees.

Training and Visualizing a Decision Tree
To understand Decision Trees, let’s build one and take a look at how it makes predic‐
tions.  The  following  code  trains  a  DecisionTreeClassifier  on  the  iris  dataset  (see
Chapter 4):

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:, 2:] # petal length and width
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X, y)

175

You  can  visualize  the  trained  Decision  Tree  by  first  using  the  export_graphviz()
method to output a graph definition file called iris_tree.dot:

from sklearn.tree import export_graphviz

export_graphviz(
        tree_clf,
        out_file=image_path("iris_tree.dot"),
        feature_names=iris.feature_names[2:],
        class_names=iris.target_names,
        rounded=True,
        filled=True
    )

Then you can use the dot command-line tool from the Graphviz package to convert
this .dot file to a variety of formats, such as PDF or PNG.1 This command line con‐
verts the .dot file to a .png image file:

$ dot -Tpng iris_tree.dot -o iris_tree.png

Your first Decision Tree looks like Figure 6-1.

Figure 6-1. Iris Decision Tree

Making Predictions
Let’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find
an iris flower and you want to classify it. You start at the root node (depth 0, at the
top): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,
then yo
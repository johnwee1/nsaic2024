origin (where the reward
is higher). For example, if Ut = Id (the identity matrix) and Wt = Id, then
Rt = âˆ’||st||2 âˆ’ ||at||2, meaning that we want to take smooth actions (small
norm of at) to go back to the origin (small norm of st). This could model a
car trying to stay in the middle of lane without making impulsive moves...

Now that we have deï¬ned the assumptions of our LQR model, letâ€™s cover

the 2 steps of the LQR algorithm

step 1 suppose that we donâ€™t know the matrices A, B, Î£.

To esti-
mate them, we can follow the ideas outlined in the Value Ap-
proximation section of the RL notes.
First, collect transitions
from an arbitrary policy.
Then, use linear regression to ï¬nd
(cid:13)
As(i)
(cid:13)s(i)
(cid:13)

t+1 âˆ’
argminA,B
nique seen in Gaussian Discriminant Analysis to learn Î£.

. Finally, use a tech-

t + Ba(i)

(cid:80)T âˆ’1
t=0

(cid:17)(cid:13)
2
(cid:13)
(cid:13)

(cid:80)n

i=1

(cid:16)

t

step 2 assuming that the parameters of our model are known (given or esti-
mated with step 1), we can derive the optimal policy using dynamic
programming.

In other words, given

(cid:40)

st+1
R(t)(st, at) = âˆ’s(cid:62)

= Atst + Btat + wt
t Utst âˆ’ a(cid:62)

t Wtat

At, Bt, Ut, Wt, Î£t known

we want to compute V âˆ—
t .

If we go back to section 16.1, we can apply

dynamic programming, which yields

1. Initialization step

For the last time step T ,

V âˆ—
T (sT ) = max
aT âˆˆA
= max
aT âˆˆA
= âˆ’s(cid:62)

T UtsT

RT (sT , aT )

âˆ’s(cid:62)

T UT sT âˆ’ a(cid:62)

T WtaT

(maximized for aT = 0)

210

2. Recurrence step

Let t < T . Suppose we know V âˆ—
t+1.
t+1 is a quadratic function in st, then V âˆ—
Fact 1: It can be shown that if V âˆ—
t
is also a quadratic function. In other words, there exists some matrix Î¦
and some scalar Î¨ such that

if V âˆ—
then V âˆ—

t+1(st+1) = s(cid:62)
t (st) = s(cid:62)

t+1Î¦t+1st+1 + Î¨t+1
t Î¦tst + Î¨t

For time step t = T , we had Î¦t = âˆ’UT and Î¨T = 0.
Fact 2: We can show that the optimal policy is just a linear function of
the state.
Knowing V âˆ—
t+1 is equivalent to knowing Î¦t+1 and Î¨t+1, so we just need
to explain how we compute Î¦t and Î¨t from Î¦t+1 and Î¨t+1 and the other
parameters of the problem.

t (st) = s(cid:62)
V âˆ—

t Î¦tst + Î¨t
(cid:104)
R(t)(st, at) + E
(cid:2)âˆ’s(cid:62)

t Utst âˆ’ a(cid:62)

= max
at
= max
at

(cid:105)
t+1(st+1)]

[V âˆ—

st+1âˆ¼P (t)

st,at

t Vtat + Est+1âˆ¼N (Atst+Btat,Î£t)[s(cid:62)

t+1Î¦t+1st+1 + Î¨t+1](cid:3)

where the second line is just the deï¬nition of the optimal value function
and the third line is obtained by plugging in the dynamics of our model
along with the quadratic assumption. Notice that the last expression is
a quadratic function in at and can thus be (easily) optimized1. We get
the optimal action aâˆ—
t

t = (cid:2)(B(cid:62)
aâˆ—
= Lt Â· st

t Î¦t+1Bt âˆ’ Vt)âˆ’1BtÎ¦t+1At

(cid:3) Â· st

where

1Use the identity E (cid:2)w(cid:62)

t Î¦t+1wt

Lt := (cid:2)(B(cid:62)

t Î¦t+1Bt âˆ’ Wt)âˆ’1BtÎ¦t+1At
(cid:3) = Tr(Î£tÎ¦t+1) with wt âˆ¼ N (0, Î£t)

(cid:3)

211

which is an impressive result: our optimal policy is linear in st. Given
aâˆ—
t we can solve for Î¦t and Î¨t. We ï¬nally get the Discrete Ricatti
equations

(cid:16)

Î¦t = A(cid:62)
t

Î¦t+1 âˆ’ Î¦t+1Bt

(cid:0)B(cid:62)

t Î¦t+1Bt âˆ’ Wt

(cid:1)âˆ’1

BtÎ¦t+1

(cid:17)

At âˆ’ Ut

Î¨t = âˆ’ tr (Î£tÎ¦t+1) + Î¨t+1

Fact 3: we notice that Î¦t depends on neither Î¨ nor the noise Î£t! As Lt
is a function of At, Bt and Î¦t+1, it implies that the optimal policy also
does not depend on the noise! (But Î¨t does depend on Î£t, which
implies that V âˆ—

t depends on Î£t.)

Then, to summarize, the LQR algorithm works as follows

1. (if necessary) estimate parameters At, Bt, Î£t

2. initialize Î¦T := âˆ’UT and Î¨T := 0.

3. iterate from t = T âˆ’ 1 . . . 0 to update Î¦t and Î¨t using Î¦t+1 and Î¨t+1
using the discrete Ricatti equations. If there exists a policy that drives
the state towards zero, then convergence is guaranteed!

Using Fact 3, we can be even more clever and make our algorithm run
(slightly) faster! As the optimal policy does not depend on Î¨t, and the
update of Î¦t only depends on Î¦t, it is suï¬ƒcient to update only Î¦t!

16.3 From non-linear dynamics to LQR

It turns out that a lot of problems can be reduced to LQR, even if dynamics
are non-linear. While LQR is a nice formulation because we are able to come
up with a nice exact solution, it is far from being general. Letâ€™s take for
instance the case of the inverted pendulum. The transitions between states
look like

ï£¶

ï£·
ï£·
ï£¸

ï£«

ï£¬
ï£¬
ï£­

xt+1
Ë™xt+1
Î¸t+1
Ë™Î¸t+1

ï£«

ï£¬
ï£¬
ï£­

= F

ï£«

ï£¬
ï£¬
ï£­

xt
Ë™xt
Î¸t
Ë™Î¸t

ï£¶

ï£·
ï£·
ï£¸

ï£¶

ï£·
ï£·
ï£¸

, at

where the function F depends on the cos of the angle etc. Now, the

question we may ask is

Can we linearize this system?

212

16.3.1 Linearization of dynamics

Letâ€™s suppose that at time t, the system spends most of its time in some state
Â¯st and the actions we perform are around Â¯at. For the inverted pendulum, if
we reached some kind of optimal, this is true: our actions are small and we
donâ€™t deviate much from the vertical.

We are going to use Taylor expansion to linearize the dynamics. In the
simple case where the state is one-dimensional and the transition function F
does not depend on the action, we would write something like

st+1 = F (st) â‰ˆ F ( Â¯st) + F (cid:48)( Â¯st) Â· (st âˆ’ Â¯st)

In the more general setting, the formula looks the same, with gradients

instead of simple derivatives

st+1 â‰ˆ F ( Â¯st, Â¯at) + âˆ‡sF ( Â¯st, Â¯at) Â· (st âˆ’ Â¯st) + âˆ‡aF ( Â¯st, Â¯at) Â· (at âˆ’ Â¯at)

(16.3)

and now, st+1 is linear in st and at, because we can rewrite equation (16.3)

as

st+1 â‰ˆ Ast + Bst + Îº

where Îº is some constant and A, B are matrices. Now, this writing looks
awfully similar to the assumptions made for LQR. We just have to get rid
of the constant term Îº! It turns out that the constant term can be absorbed
into st by artiï¬cially increasing the dimension by one. This is the same trick
that we used at the beginning of the class for linear regression...

16.3.2 Diï¬€erential Dynamic Programming (DDP)

The previous method works well for cases where the goal is to stay around
some state sâˆ— (think about the inverted pendulum, or a car having to stay
in the middle of a lane). However, in some cases, the goal can be more
complicated.

Weâ€™ll cover a method that applies when our system has to follow some
trajectory (think about a rocket). This method is going to discretize the
trajectory into discrete time steps, and create intermediary goals around
which we will be able to use the previous technique! This method is called
Diï¬€erential Dynamic Programming. The main steps are

step 1 come up with a nominal trajectory using a naive controller, that approx-
imate the trajectory we want to follow. In other words, our controller
is able to approximate the gold trajectory with

213

step 2 linearize the dynamics around each trajectory point sâˆ—

t , in other words

0, aâˆ—
sâˆ—

0 â†’ sâˆ—

1, aâˆ—

1 â†’ . . .

st+1 â‰ˆ F (sâˆ—

t , aâˆ—

t ) + âˆ‡sF (sâˆ—

t , aâˆ—

t )(st âˆ’ sâˆ—

t ) + âˆ‡aF (sâˆ—

t , aâˆ—

t )(at âˆ’ aâˆ—
t )

where st, at would be our current state and action. Now that we have
a linear approximation around each of these points, we can use the
previous section and rewrite

st+1 = At Â· st + Bt Â· at

(notice that in that case, we use the non-stationary dynamics setting
that we mentioned at the beginning of these lecture notes)
Note We can apply a similar derivation for the reward R(t), with a
second-order Taylor expansion.

R(st, at) â‰ˆ R(sâˆ—

t , aâˆ—

t ) + âˆ‡sR(sâˆ—

t , aâˆ—

t )(st âˆ’ sâˆ—

t ) + âˆ‡aR(sâˆ—

t , aâˆ—

t )(at âˆ’ aâˆ—
t )

+

+

1
2
1
2

(st âˆ’ sâˆ—

t )(cid:62)Hss(st âˆ’ sâˆ—

t ) + (st âˆ’ sâˆ—

t )(cid:62)Hsa(at âˆ’ aâˆ—
t )

(at âˆ’ aâˆ—

t )(cid:62)Haa(at âˆ’ aâˆ—
t )

where Hxy refers to the entry of the Hessian of R with respect to x and
y evaluated in (sâˆ—
t ) (omitted for readability). This expression can be
re-written as

t , aâˆ—

Rt(st, at) = âˆ’s(cid:62)

t Utst âˆ’ a(cid:62)

t Wtat

for some matrices Ut, Wt, with the same trick of adding an extra dimen-
sion of ones. To convince yourself, notice that

(cid:0)1 x(cid:1) Â·

(cid:19)

(cid:18)a b
c
b

(cid:19)

(cid:18)1
x

Â·

= a + 2bx + cx2

214

step 3 Now, you can convince yourself that our problem is strictly re-written
in the LQR framework. Letâ€™s just use LQR to ï¬nd the optimal policy
Ï€t. As a result, our new controller will (hopefully) be better!
Note: Some problems might arise if the LQR trajectory deviates too
much from the linearized approximation of the trajectory, but that can
be ï¬xed with reward-shaping...

step 4 Now that we get a new controller (our new policy Ï€t), we use it to

produce a new trajectory

0, Ï€0(sâˆ—
sâˆ—

0) â†’ sâˆ—

1, Ï€1(sâˆ—

1) â†’ . . . â†’ sâˆ—
T

note that when we generate this new trajectory, we use the real F and
not its linear approximation to compute transitions, meaning that

sâˆ—
t+1 = F (sâˆ—

t , aâˆ—
t )

then, go back to step 2 and repeat until some stopping criterion.

16.4 Linear Quadratic Gaussian (LQG)

Often, in the real word, we donâ€™t get to observe the full state st. For example,
an autonomous car could receive an image from a camera, which is merely
an observation, and not the full state of the world. So far, we assumed
that the state was available. As this might not hold true for most of the
real-world problems, we need a new tool to model this situation: Partially
Observable MDPs.

A POMDP is an MDP with an extra observation layer. In other words,
we introduce a new variable ot, that follows some conditional distribution
given the current state st

Formally, a ï¬nite-horizon POMDP is given by a tuple

ot|st âˆ¼ O(o|s)

(S, O, A, Psa, T, R)

Within this framework, the general strategy is to maintain a belief state
(distribution over states) based on the observation o1, . . . , ot. Then, a policy
in a POMDP maps this belief states to actions.

215

In this section, weâ€™ll present a extension of LQR to this new setting.

Assume that we observe yt âˆˆ Rn with m < n such that

(cid:40)

= C Â· st + vt
yt
st+1 = A Â· st + B Â· at + wt

where C âˆˆ RnÃ—d is a compression matrix and vt is the sensor noise (also
gaussian, like wt). Note that the reward function R(t) is left unchanged, as a
function of the state (not the observation) and action. Also, as distributions
are gaussian, the belief state is also going to be gaussian. In this new frame-
work, letâ€™s give an overview of the strategy we are going to adopt to ï¬nd the
optimal policy:

step 1 ï¬rst, compute the distribution on the possible states (the belief state),
based on the observations we have. In other words, we want to compute
the mean st|t and the covariance Î£t|t of

st|y1, . . . , yt âˆ¼ N (cid:0)st|t, Î£t|t

(cid:1)

to perform the computation eï¬ƒciently over time, weâ€™ll use the Kalman
Filter algorithm (used on-board Apollo Lunar Module!).

step 2 now that we have the distribution, weâ€™ll use the mean st|t as the best

approximation for st

step 3 then set the action at := Ltst|t where Lt comes from the regular LQR

algorithm.

Intuitively, to understand why this works, notice that st|t is a noisy ap-
proximation of st (equivalent to adding more noise to LQR) but we proved
that LQR is independent of the noise!

Step 1 needs to be explicated. Weâ€™ll cover a simple case where there is
no action dependence in our dynamics (but the general case follows the same
idea). Suppose that

(cid:40)

st+1 = A Â· st + wt, wt âˆ¼ N (0, Î£s)
vt âˆ¼ N (0, Î£y)
yt

= C Â· st + vt,

As noises are Gaussians, we can easily prove that the joint distribution is

also Gaussian

216

ï£¶

ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

ï£«

ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­

s1
...
st
y1
...
yt

âˆ¼ N (Âµ, Î£)

for some Âµ, Î£

then, using the marginal formulas of gaussians (see Factor Analysis notes),

we would get

st|y1, . . . , yt âˆ¼ N (cid:0)st|t, Î£t|t

(cid:1)

However, computing the marginal distribution parameters using these
formulas would be computationally expensive! It would require manipulating
matrices of shape t Ã— t. Recall that inverting a matrix can be done in O(t3),
and it would then have to be repeated over the time steps, yielding a cost in
O(t4)!

The Kalman ï¬lter algorithm provides a much better way of computing
the mean and variance, by updating them over time in constant time in
t! The kalman ï¬lter is based on two basics steps. Assume that we know the
distribution of st|y1, . . . , yt:

predict step compute st+1|y1, . . . , yt

update step compute st+1|y1, . . . , yt+1

and iterate over time steps! The combination of the predict and update

steps updates our belief states. In other words, the process looks like

(st|y1, . . . , yt)

predict
âˆ’âˆ’âˆ’âˆ’â†’ (st+1|y1, . . . , yt)

update
âˆ’âˆ’âˆ’âˆ’â†’ (st+1|y1, . . . , yt+1)

predict
âˆ’âˆ’âˆ’âˆ’â†’ . . .

predict step Suppose that we know the distribution of

st|y1, . . . , yt âˆ¼ N (cid:0)st|t, Î£t|t

(cid:1)

then, the distribution over the next state is also a gaussian distribution

st+1|y1, . . . , yt âˆ¼ N (cid:0)st+1|t, Î£t+1|t

(cid:1)

where

217

(cid:40)

st+1|t = A Â· st|t
Î£t+1|t = A Â· Î£t|t Â· A(cid:62) + Î£s

update step given st+1|t and Î£t+1|t such that

st+1|y1, . . . , yt âˆ¼ N (cid:0)st+1|t, Î£t+1|t

(cid:1)

we can prove that

st+1|y1, . . . , yt+1 âˆ¼ N (cid:0)st+1|t+1, Î£t+1|t+1

(cid:1)

where

with

(cid:40)

st+1|t+1 = st+1|t + Kt(yt+1 âˆ’ Cst+1|t)
Î£t+1|t+1 = Î£t+1|t âˆ’ Kt Â· C Â· Î£t+1|t

Kt := Î£t+1|tC (cid:62)(CÎ£t+1|tC (cid:62) + Î£y)âˆ’1

The matrix Kt is called the Kalman gain.

Now, if we have a closer look at the formulas, we notice that we donâ€™t
need the observations prior to time step t! The update steps only depends
on the previous distribution. Putting it all together, the algorithm ï¬rst runs
a forward pass to compute the Kt, Î£t|t and st|t (sometimes referred to as
Ë†s in the literature). Then, it runs a backward pass (the LQR updates) to
compute the quantities Î¨t, Î¨t and Lt. Finally, we recover the optimal policy
with aâˆ—

t = Ltst|t.

Chapter 17

Policy Gradient
(REINFORCE)

We will present a model-free algorithm called REINFORCE that does not
require the notion of value functions and Q functions. It turns out to be more
convenient to introduce REINFORCE in the ï¬nite horizon case, which will
be assumed throughout this note: we use Ï„ = (s0, a0, . . . , sT âˆ’1, aT âˆ’1, sT ) to
denote a trajectory, where T < âˆž is the length of the trajectory. Moreover,
REINFORCE only applies to learning a randomized policy. We use Ï€Î¸(a|s)
to denote the probability of the policy Ï€Î¸ outputting the action a at state s.
The other notations will be the same as in previous lecture notes.

The advantage of applying REINFORCE is that we only need to assume
that we can sample from the transition probabilities {Psa} and can query the
reward function R(s, a) at state s and action a,1 but we do not need to know
the analytical form of the transition probabilities or the reward function.
We do not explicitly learn the transition probabilities or the reward function
either.

Let s0 be sampled from some distribution Âµ. We consider optimizing the

expected total payoï¬€ of the policy Ï€Î¸ over the parameter Î¸ deï¬ned as.

Î·(Î¸) (cid:44) E

(cid:34)T âˆ’1
(cid:88)

(cid:35)
Î³tR(st, at)

t=0

(17.1)

Recall that st âˆ¼ Pstâˆ’1atâˆ’1 and at âˆ¼ Ï€Î¸(Â·|st). Also note that 
tent; that
to use for training.
is, for each mention we have a whole cluster of legal gold antecedents to choose
from. Early work used heuristics to choose an antecedent, for example choosing the
closest antecedent as the gold antecedent and all non-antecedents in a window of
two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds
of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,
Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent
by summing over all of them, with a loss function that optimizes the likelihood of
all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the
details in Section 22.6.

Mention-ranking models can be implemented with hand-build features or with
neural representation learning (which might also incorporate some hand-built fea-
tures). we’ll explore both directions in Section 22.5 and Section 22.6.

22.4.3 Entity-based Models

Both the mention-pair and mention-ranking models make their decisions about men-
tions. By contrast, entity-based models link each mention not to a previous mention
but to a previous discourse entity (cluster of mentions).

A mention-ranking model can be turned into an entity-ranking model simply
by having the classiﬁer make its decisions over clusters of mentions rather than
individual mentions (Rahman and Ng, 2009).

For traditional feature-based models, this can be done by extracting features over
clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the
list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,
Victoria, her, the
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun, so that a cluster composed of
{
38-year-old
would have the shape P-Pr-D (Bj¨orkelund and Kuhn, 2014). An entity-
}
based model that includes a mention-pair classiﬁer can use as features aggregates of
mention-pair probabilities, for example computing the average probability of coref-
erence over all mention-pairs in the two clusters (Clark and Manning 2015).

Neural models can learn representations of clusters automatically, for example
by using an RNN over the sequence of cluster mentions to encode a state correspond-
ing to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-
resentations for pairs of clusters by pooling over learned representations of mention
pairs (Clark and Manning, 2016b).

However, although entity-based models are more expressive, the use of cluster-
level information in practice has not led to large gains in performance, so mention-
ranking models are still more commonly used.

22.5 Classiﬁers using hand-built features

Feature-based classiﬁers, use hand-designed features in logistic regression, SVM,
or random forest classiﬁers for coreference resolution. These classiﬁers don’t per-

496 CHAPTER 22

• COREFERENCE RESOLUTION AND ENTITY LINKING

form as well as neural ones. Nonetheless, they are still sometimes useful to build
lightweight systems when compute or data are sparse, and the features themselves
are useful for error analysis even in neural systems.

Given an anaphor mention and a potential antecedent mention, feature based
classiﬁers make use of three types of features: (i) features of the anaphor, (ii) features
of the candidate antecedent, and (iii) features of the relationship between the pair.
Entity-based models can make additional use of two additional classes: (iv) feature
of all mentions from the antecedent’s entity cluster, and (v) features of the relation
between the anaphor and the mentions in the antecedent entity cluster.

First (last) word
Head word
Attributes

Length
Mention type

Features of the Anaphor or Antecedent Mention
Victoria/she
Victoria/she
Sg-F-A-3-PER/
Sg-F-A-3-PER
2/1
P/Pr

First or last word (or embedding) of antecedent/anaphor
Head word (or head embedding) of antecedent/anaphor
The number, gender, animacy, person, named entity type
attributes of (antecedent/anaphor)
length in words of (antecedent/anaphor)
Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an-
tecedent/anaphor
Features of the Antecedent Entity

Entity shape

P-Pr-D

The ‘shape’ or list of types of the mentions in the
antecedent entity (cluster),
i.e., sequences of (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.

Entity attributes

Sg-F-A-3-PER The number, gender, animacy, person, named entity type

Ant. cluster size

Sentence distance
Mention distance
i-within-i
Cosine

3

1
4
F

attributes of the antecedent entity
Number of mentions in the antecedent cluster

Features of the Pair of Mentions

The number of sentences between antecedent and anaphor
The number of mentions between antecedent and anaphor
Anaphor has i-within-i relation with antecedent
Cosine between antecedent and anaphor embeddings

Features of the Pair of Entities

Head Word Match

Exact String Match F

True if the strings of any two mentions from the antecedent
and anaphor clusters are identical.
True if any mentions from antecedent cluster has same
headword as any mention in anaphor cluster
All words in anaphor cluster included in antecedent cluster
Word Inclusion
Figure 22.4 Feature-based coreference: sample feature values for anaphor “she” and potential antecedent
“Victoria Chen”.

F

F

Figure 22.4 shows a selection of commonly used features, and shows the value
that would be computed for the potential anaphor “she” and potential antecedent
“Victoria Chen” in our example sentence, repeated below:

(22.47) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3

million, as the 38-year-old also became the company’s president. It is
widely known that she came to Megabucks from rival Lotsabucks.

Features that prior work has found to be particularly useful are exact string
match, entity headword agreement, mention distance, as well as (for pronouns) exact
attribute match and i-within-i, and (for nominals and proper names) word inclusion
and cosine. For lexical features (like head words) it is common to only use words
that appear enough times (>20 times).

22.6

• A NEURAL MENTION-RANKING ALGORITHM 497

It is crucial in feature-based systems to use conjunctions of features; one exper-
iment suggested that moving from individual features in a classiﬁer to conjunctions
of multiple features increased F1 by 4 points (Lee et al., 2017a). Speciﬁc conjunc-
tions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be
conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using
decision tree or random forest classiﬁers (Ng and Cardie 2002a, Lee et al. 2017a).

Features can also be used in neural models as well. Neural systems use contex-
tual word embeddings so don’t beneﬁt from shallow features like string match or or
mention types. However features like mention length, distance between mentions,
or genre can complement neural contextual embedding models.

22.6 A neural mention-ranking algorithm

In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)
(simpliﬁed and extended a bit, drawing on Joshi et al. (2019) and others). This is
a mention-ranking algorithm that considers all possible spans of text in the docu-
ment, assigns a mention-score to each span, prunes the mentions based on this score,
then assigns coreference links to the remaining mentions.

2

More formally, given a document D with T words, the model considers all of
the T (T +1)
text spans in D (unigrams, bigrams, trigrams, 4-grams, etc; in practice
we only consider spans up a maximum length around 10). The task is to assign
to each span i an antecedent yi, a random variable ranging over the values Y (i) =
; each previous span and a special dummy token (cid:15). Choosing the
1, ..., i
{
dummy token means that i does not have an antecedent, either because i is discourse-
new and starts a new coreference chain, or because i is non-anaphoric.

1, (cid:15)

−

}

For each pair of spans i and j, the system assigns a score s(i, j) for the coref-
erence link between span i and span j. The system then learns a distribution P(yi)
over the antecedents for span i:

P(yi) =

exp(s(i, yi))
Y (i) exp(s(i, y(cid:48)))

(22.48)

y(cid:48)∈
This score s(i, j) includes three factors that we’ll deﬁne below: m(i); whether span
i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the
antecedent of i:

(cid:80)

s(i, j) = m(i) + m( j) + c(i, j)

(22.49)

For the dummy antecedent (cid:15), the score s(i, (cid:15)) is ﬁxed to 0. This way if any non-
dummy scores are positive, the model predicts the highest-scoring antecedent, but if
all the scores are negative it abstains.

22.6.1 Computing span representations

To compute the two functions m(i) and c(i, j) which score a span i or a pair of spans
(i, j), we’ll need a way to represent a span. The e2e-coref family of algorithms
represents each span by trying to capture 3 words/tokens: the ﬁrst word, the last
word, and the most important word. We ﬁrst run each paragraph or subdocument
through an encoder (like BERT) to generate embeddings hi for each token i. The
span i is then represented by a vector gi that is a concatenation of the encoder output

498 CHAPTER 22

• COREFERENCE RESOLUTION AND ENTITY LINKING

embedding for the ﬁrst (start) token of the span, the encoder output for the last (end)
token of the span, and a third vector which is an attention-based representation:

gi = [hSTART(i), hEND(i), hATT(i)]

(22.50)

The goal of the attention vector is to represent which word/token is the likely
syntactic head-word of the span; we saw in the prior section that head-words are
a useful feature; a matching head-word is a good indicator of coreference. The
attention representation is computed as usual; the system learns a weight vector wα ,
and computes its dot product with the hidden state ht transformed by a FFN:

The attention score is normalized into a distribution via a softmax:

αt = wα

·

FFNα (ht )

ai,t =

exp(αt )
END(i)
k=START(i) exp(αk)

(22.51)

(22.52)

And then the attention distribution is used to create a vector hATT(i) which is an
attention-weighted sum of the embeddings et of each of the words in span i:

(cid:80)

END(i)

hATT(i) =

et

(22.53)

ai,t ·

(cid:88)t=START(i)
Fig. 22.5 shows the computation of the span representation and the mention

score.

Figure 22.5 Computation of the span representation g (and the mention score m) in a BERT version of the
e2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of
say 10; the ﬁgure shows a small subset of the bigram and trigram spans.

22.6.2 Computing the mention and antecedent scores m and c
Now that we know how to compute the vector gi for representing span i, we can
see the details of the two scoring functions m(i) and c(i, j). Both are computed by
feedforward networks:

m(i) = wm ·
c(i, j) = wc ·

FFNm(gi)
FFNc([gi, g j, gi ◦

g j, ])

(22.54)

(22.55)

At inference time, this mention score m is used as a ﬁlter to keep only the best few
mentions.

Encodings (h)…Encoder GeneralElectricsaidthePostalServicecontactedthecompanySpan head (hATT) Span representation (g) Mention score (m)+++++General ElectricElectric said thethe Postal ServiceService contacted thethe company22.6

• A NEURAL MENTION-RANKING ALGORITHM 499

We then compute the antecedent score for high-scoring mentions. The antecedent
score c(i, j) takes as input a representation of the spans i and j, but also the element-
wise similarity of the two spans to each other gi ◦
is element-wise mul-
tiplication). Fig. 22.6 shows the computation of the score s for the three possible
antecedents of the company in the example sentence from Fig. 22.5.

g j (here

◦

Figure 22.6 The computation of the score s for the three possible antecedents of the com-
pany in the example sentence from Fig. 22.5. Figure after Lee et al. (2017b).

Given the set of mentions, the joint distribution of antecedents for each docu-
ment is computed in a forward pass, and we can then do transitive closure on the
antecedents to create a ﬁnal clustering for the document.

Fig. 22.7 shows example predictions from the model, showing the attention
weights, which Lee et al. (2017b) ﬁnd correlate with traditional semantic heads.
Note that the model gets the second example wrong, presumably because attendants
and pilot likely have nearby word embeddings.

Figure 22.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per
example, showing one correct example and one mistake. Bold, parenthesized spans are men-
tions in the predicted cluster. The amount of red color on a word indicates the head-ﬁnding
attention weight ai,t in (22.52). Figure adapted from Lee et al. (2017b).

22.6.3 Learning

For training, we don’t have a single gold antecedent for each mention; instead the
coreference labeling only gives us each entire cluster of coreferent mentions; so a
mention only has a latent antecedent. We therefore use a loss function that maxi-
mizes the sum of the coreference probability of any of the legal antecedents. For a
given mention i with possible antecedents Y (i), let GOLD(i) be the set of mentions
in the gold cluster containing i. Since the set of mentions occurring before i is Y (i),
GOLD(i). We
the set of mentions in that gold cluster that also occur before i is Y (i)

∩

500 CHAPTER 22

• COREFERENCE RESOLUTION AND ENTITY LINKING

therefore want to maximize:

If a mention i is not in a gold cluster GOLD(i) = (cid:15).

Y (i)
(cid:88)ˆy
∩
∈

GOLD(i)

P( ˆy)

(22.56)

To turn this probability into a loss function, we’ll use the cross-entropy loss
log of the probability.

function we deﬁned in Eq. 5.23 in Chapter 5, by taking the
If we then sum over all mentions, we get the ﬁnal loss function for training:

−

L =

N

(cid:88)i=2

log

−

Y (i)
(cid:88)ˆy
∩
∈

GOLD(i)

P( ˆy)

(22.57)

22.7 Entity Linking

entity linking

wikiﬁcation

anchor texts

Entity linking is the task of associating a mention in text with the representation of
some real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It
is the natural follow-on to coreference resolution; coreference resolution is the task
of associating textual mentions that corefer to the same entity. Entity linking takes
the further step of identifying who that entity is. It is especially important for any
NLP task that links to a knowledge base.

While there are all sorts of potential knowledge-bases, we’ll focus in this section
on Wikipedia, since it’s widely used as an ontology for NLP tasks. In this usage,
each unique Wikipedia page acts as the unique id for a particular entity. This task of
deciding which Wikipedia page corresponding to an individual is being referred to
by a text mention has its own name: wikiﬁcation (Mihalcea and Csomai, 2007).

Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne
and Witten 2008), entity linking is done in (roughly) two stages: mention detec-
tion and mention disambiguation. We’ll give two algorithms, one simple classic
baseline that uses anchor dictionaries and information from the Wikipedia graph
structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,
2020). We’ll focus here mainly on the application of entity linking to questions,
since a lot of the literature has been in that context.

22.7.1 Linking based on Anchor Dictionaries and Web Graph

As a simple baseline we introduce the TAGME linker (Ferra
ed control. In D. A. White and D. A. Sofge
(eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Ap-
proaches, pp. 215–232. Van Nostrand Reinhold, New York.

Kumar, P. R., Varaiya, P. (1986). Stochastic Systems: Estimation, Identiﬁca-

tion, and Adaptive Control. Prentice-Hall, Englewood Cliﬀs, NJ.

Kumar, P. R. (1985). A survey of some results in stochastic adaptive control.

SIAM Journal of Control and Optimization, 23:329–380.

324

CHAPTER 15. PROSPECTS

Kumar, V., Kanal, L. N. (1988). The CDP: A unifying formulation for heuris-
tic search, dynamic programming, and branch-and-bound. In L. N. Kanal
and V. Kumar (eds.), Search in Artiﬁcial Intelligence, pp. 1–37. Springer-
Verlag, Berlin.

Kushner, H. J., Dupuis, P. (1992). Numerical Methods for Stochastic Control

Problems in Continuous Time. Springer-Verlag, New York.

Lai, T. L., Robbins, H. (1985). Asymptotically eﬃcient adaptive allocation

rules. Advances in applied mathematics, 6(1):4–22.

Lang, K. J., Waibel, A. H., Hinton, G. E. (1990). A time-delay neural network
architecture for isolated word recognition. Neural Networks, 3:33–43.

Lin, C.-S., Kim, H. (1991). CMAC-based adaptive critic self-learning control.

IEEE Transactions on Neural Networks, 2:530–533.

Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learn-

ing, planning and teaching. Machine Learning, 8:293–321.

Lin, L.-J., Mitchell, T. (1992). Reinforcement learning with hidden states.
In Proceedings of the Second International Conference on Simulation of
Adaptive Behavior: From Animals to Animats, pp. 271–280. MIT Press,
Cambridge, MA.

Littman, M. L. (1994). Markov games as a framework for multi-agent rein-
In Proceedings of the Eleventh International Confer-
forcement learning.
ence on Machine Learning, pp. 157–163. Morgan Kaufmann, San Fran-
cisco.

Littman, M. L., Cassandra, A. R., Kaelbling, L. P. (1995). Learning policies
for partially observable environments: Scaling up.
In A. Prieditis and
S. Russell (eds.), Proceedings of the Twelfth International Conference on
Machine Learning, pp. 362–370. Morgan Kaufmann, San Francisco.

Littman, M. L., Dean, T. L., Kaelbling, L. P. (1995). On the complexity of
solving Markov decision problems. In Proceedings of the Eleventh Annual
Conference on Uncertainty in Artiﬁcial Intelligence, pp. 394–402.

Liu, J. S. (2001). Monte Carlo strategies in scientiﬁc computing. Berlin,

Springer-Verlag.

Ljung, L., S¨oderstrom, T. (1983). Theory and Practice of Recursive Identiﬁ-

cation. MIT Press, Cambridge, MA.

Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observed
Markov decision processes. Annals of Operations Research, 28:47–66.

Luce, D. (1959). Individual Choice Behavior. Wiley, New York.

15.5. OTHER FRONTIER DIMENSIONS

325

Maclin, R., Shavlik, J. W. (1994). Incorporating advice into agents that learn
In Proceedings of the Twelfth National Conference

from reinforcements.
on Artiﬁcial Intelligence, pp. 694–699. AAAI Press, Menlo Park, CA.

Mahadevan, S. (1996). Average reward reinforcement learning: Foundations,

algorithms, and empirical results. Machine Learning, 22:159–196.

Markey, K. L. (1994). Eﬃcient learning of multiple degree-of-freedom control
problems with quasi-independent Q-agents. In M. C. Mozer, P. Smolensky,
D. S. Touretzky, J. L. Elman, and A. S. Weigend (eds.), Proceedings of the
1990 Connectionist Models Summer School. Erlbaum, Hillsdale, NJ.

Mazur, J. E. (1994). Learning and Behavior, 3rd ed. Prentice-Hall, Englewood

Cliﬀs, NJ.

McCallum, A. K. (1993). Overcoming incomplete perception with utile dis-
tinction memory. In Proceedings of the Tenth International Conference on
Machine Learning, pp. 190–196. Morgan Kaufmann, San Mateo, CA.

McCallum, A. K. (1995). Reinforcement Learning with Selective Perception
and Hidden State. Ph.D. thesis, University of Rochester, Rochester, NY.

Mendel, J. M. (1966). A survey of learning control systems. ISA Transactions,

5:297–303.

Mendel, J. M., McLaren, R. W. (1970). Reinforcement learning control and
pattern recognition systems. In J. M. Mendel and K. S. Fu (eds.), Adap-
tive, Learning and Pattern Recognition Systems: Theory and Applications,
pp. 287–318. Academic Press, New York.

Michie, D. (1961). Trial and error. In S. A. Barnett and A. McLaren (eds.),

Science Survey, Part 2, pp. 129–145. Penguin, Harmondsworth.

Michie, D. (1963). Experiments on the mechanisation of game learning. 1.
characterization of the model and its parameters. Computer Journal,
1:232–263.

Michie, D. (1974). On Machine Intelligence. Edinburgh University Press,

Edinburgh.

Michie, D., Chambers, R. A. (1968). BOXES: An experiment in adaptive
control. In E. Dale and D. Michie (eds.), Machine Intelligence 2, pp. 137–
152. Oliver and Boyd, Edinburgh.

Miller, S., Williams, R. J. (1992). Learning to control a bioreactor using a neu-
ral net Dyna-Q system. In Proceedings of the Seventh Yale Workshop on
Adaptive and Learning Systems, pp. 167–172. Center for Systems Science,
Dunham Laboratory, Yale University, New Haven.

Miller, W. T., Scalera, S. M., Kim, A. (1994). Neural network control of

326

CHAPTER 15. PROSPECTS

dynamic balance for a biped walking robot. In Proceedings of the Eighth
Yale Workshop on Adaptive and Learning Systems, pp. 156–161. Center
for Systems Science, Dunham Laboratory, Yale University, New Haven.

Minsky, M. L. (1954). Theory of Neural-Analog Reinforcement Systems and
Its Application to the Brain-Model Problem. Ph.D. thesis, Princeton Uni-
versity.

Minsky, M. L. (1961). Steps toward artiﬁcial intelligence. Proceedings of the
Institute of Radio Engineers, 49:8–30. Reprinted in E. A. Feigenbaum and
J. Feldman (eds.), Computers and Thought, pp. 406–450. McGraw-Hill,
New York, 1963.

Minsky, M. L. (1967). Computation: Finite and Inﬁnite Machines. Prentice-

Hall, Englewood Cliﬀs, NJ.

Montague, P. R., Dayan, P., Sejnowski, T. J. (1996). A framework for mesen-
cephalic dopamine systems based on predictive Hebbian learning. Journal
of Neuroscience, 16:1936–1947.

Moore, A. W. (1990). Eﬃcient Memory-Based Learning for Robot Control.

Ph.D. thesis, University of Cambridge.

Moore, A. W. (1994). The parti-game algorithm for variable resolution rein-
forcement learning in multidimensional spaces. In J. D. Cohen, G. Tesauro
and J. Alspector (eds.), Advances in Neural Information Processing Sys-
tems: Proceedings of the 1993 Conference, pp. 711–718. Morgan Kauf-
mann, San Francisco.

Moore, A. W., Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement
learning with less data and less real time. Machine Learning, 13:103–130.

Moore, J. W., Desmond, J. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., and
Barto, A. G. (1986). Simulation of the classically conditioned nictitating
membrane response by a neuron-like adaptive element: I. Response to-
pography, neuronal ﬁring, and interstimulus intervals. Behavioural Brain
Research, 21:143–154.

Narendra, K. S., Thathachar, M. A. L. (1974). Learning automata—A survey.

IEEE Transactions on Systems, Man, and Cybernetics, 4:323–334.

Narendra, K. S., Thathachar, M. A. L. (1989).

Learning Automata: An

Introduction. Prentice-Hall, Englewood Cliﬀs, NJ.

Narendra, K. S., Wheeler, R. M. (1986). Decentralized learning in ﬁnite
Markov chains. IEEE Transactions on Automatic Control, AC31(6):519–
526.

Nie, J., Haykin, S. (1996). A dynamic channel assignment policy through

15.5. OTHER FRONTIER DIMENSIONS

327

Q-learning. CRL Report 334. Communications Research Laboratory,
McMaster University, Hamilton, Ontario.

Now´e, A., Vrancx, P., De Hauwere, Y. M. (2012). Game theory and multi-
agent reinforcement learning. In Reinforcement Learning (pp. 441-470).
Springer Berlin Heidelberg.

Page, C. V. (1977). Heuristics for signature table analysis as a pattern recog-
nition technique. IEEE Transactions on Systems, Man, and Cybernetics,
7:77–86.

Parr, R., Russell, S. (1995). Approximating optimal policies for partially
observable stochastic domains. In Proceedings of the Fourteenth Interna-
tional Joint Conference on Artiﬁcial Intelligence, pp. 1088–1094. Morgan
Kaufmann.

Pavlov, P. I. (1927). Conditioned Reﬂexes. Oxford University Press, London.

Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Prob-

lem Solving. Addison-Wesley, Reading, MA.

Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4),

669-688.

Balke, A., Pearl, J. (1994). Counterfactual probabilities: Computational
methods, bounds and applications.
In Proceedings of the Tenth Inter-
national Conference on Uncertainty in Artiﬁcial Intelligence (pp. 46-54).
Morgan Kaufmann.

Peng, J. (1993). Eﬃcient Dynamic Programming-Based Learning for Control.

Ph.D. thesis, Northeastern University, Boston.

Peng, J., Williams, R. J. (1993). Eﬃcient learning and planning within the

Dyna framework. Adaptive Behavior, 1(4):437–454.

Peng, J., Williams, R. J. (1994).

In
W. W. Cohen and H. Hirsh (eds.), Proceedings of the Eleventh International
Conference on Machine Learning, pp. 226–232. Morgan Kaufmann, San
Francisco.

Incremental multi-step Q-learning.

Peng, J., Williams, R. J. (1996). Incremental multi-step Q-learning. Machine

Learning, 22:283–290.

Phansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization
algorithms for generalized learning automata. Neural Computation, 7:950–
973.

Poggio, T., Girosi, F. (1989). A theory of networks for approximation and
learning. A.I. Memo 1140. Artiﬁcial Intelligence Laboratory, Massachusetts
Institute of Technology, Cambridge, MA.

328

CHAPTER 15. PROSPECTS

Poggio, T., Girosi, F. (1990). Regularization algorithms for learning that are

equivalent to multilayer networks. Science, 247:978–982.

Powell, M. J. D. (1987). Radial basis functions for multivariate interpola-
In J. C. Mason and M. G. Cox (eds.), Algorithms for

tion: A review.
Approximation, pp. 143–167. Clarendon Press, Oxford.

Powell, W. B. (2011). Approximate Dynamic Programming: Solving the Curses

of Dimensionality, Second edition. John Wiley and Sons.

Precup, D., Sutton, R. S., Dasgupta, S. (2001). Oﬀ-policy temporal-diﬀerence
learning with function approximation. In Proceedings of the 18th Interna-
tional Conference on Machine Learning.

Precup, D., Sutton, R. S., Singh, S. (2000). Eligibility traces for oﬀ-policy
policy evaluation. In Proceedings of the 17th International Conference on
Machine Learning, pp. 759–766. Morgan Kaufmann.

Puterman, M. L. (1994). Markov Decision Problems. Wiley, New York.

Puterman, M. L., Shin, M. C. (1978). Modiﬁed policy iteration algorithms
for discounted Markov decision problems. Management Science, 24:1127–
1137.

Reetz, D. (1977). Approximate solutions of a discounted Markovian decision

process. Bonner Mathematische Schriften, 98:77–92.

Ring, M. B. (1994). Continual Learning in Reinforcement Environments.

Ph.D. thesis, University of Texas, Austin.

Rivest, R. L., Schapire, R. E. (1987). Diversity-based inference of ﬁnite
In Proceedings of the Twenty-Eighth Annual Symposium on
automata.
Foundations of Computer Science, pp. 78–87. Computer Society Press of
the IEEE, Washington, DC.

Robbins, H. (1952). Some aspects of the sequential design of experiments.

Bulletin of the American Mathematical Society, 58:527–535.

Robertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon.

Inside Backgammon, 2:14–22.

Rosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the

Theory of Brain Mechanisms. Spartan Books, Washington, DC.

Ross, S. (1983). Introduction to Stochastic Dynamic Programming. Academic

Press, New York.

Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method. Wiley,

New York.

Rumelhart, D. E., Hinton, G. E., Williams, R. J. (1986). Learning internal

15.5. OTHER FRONTIER DIMENSIONS

329

representations by error propagation.
In D. E. Rumelhart and J. L. Mc-
Clelland (eds.), Parallel Distributed Processing: Explorations in the Mi-
crostructure of Cognition, vol. I, Foundations. Bradford/MIT Press, Cam-
bridge, MA.

Rummery, G. A. (1995).

Problem Solving with Reinforcement Learning.

Ph.D. thesis, Cambridge University.

Rummery, G. A., Niranjan, M. (1994). On-line Q-learning using connection-
ist systems. Technical Report CUED/F-INFENG/TR 166. Engineering
Department, Cambridge University.

Russell, S., Norvig, P. (2009). Artiﬁcial Intelligence: A Modern Approach.

Prentice-Hall, Englewood Cliﬀs, NJ.

Rust, J. (1996). Numerical dynamic programming in economics. In H. Am-
man, D. Kendrick, and J. Rust (eds.), Handbook of Computational Eco-
nomics, pp. 614–722. Elsevier, Amsterdam.

Samuel, A. L. (1959).

Some studies in machine learning using the game
of checkers.
IBM Journal on Research and Development, 3:211–229.
Reprinted in E. A. Feigenbaum and J. Feldman (eds.), Computers and
Thought, pp. 71–105. McGraw-Hill, New York, 1963.

Samuel, A. L. (1967). Some studies in machine learning using the game of
IBM Journal on Research and Develop-

checkers. II—Recent progress.
ment, 11:601–617.

Schultz, D. G., Melsa, J. L. (1967).
Systems. McGraw-Hill, New York.

State Functions and Linear Control

Schultz, W., Dayan, P., Montague, P. R. (1997). A neural substrate of pre-

diction and reward. Science, 275:1593–1598.

Schwartz, A. (1993). A reinforcement learning method for maximizing undis-
counted rewards. In Proceedings of the Tenth International Conference on
Machine Learning, pp. 298–305. Morgan Kaufmann, San Mateo, CA.

Schweitzer, P. J., Seidmann, A. (1985). Generalized polynomial approxima-
tions in Markovian decision processes. Journal of Mathematical Analysis
and Applications, 110:568–582.

Selfridge, O. J., Sutton, R. S., Barto, A. G. (1985). Training and tracking in
robotics.
In A. Joshi (ed.), Proceedings of the Ninth International Joint
Conference on Artiﬁcial Intelligence, pp. 670–672. Morgan Kaufmann, San
Mateo, CA.

Shannon, C. E. (1950). Programming a computer for playing chess. Philo-

sophical Magazine, 41:256–275.

330

CHAPTER 15. PROSPECTS

Shelton, C. R. (2001). Importance Sampling for Reinforcement Learning with
Multiple Objectives. PhD thesis, Massachusetts Institute of Technology.

Shewchuk, J., Dean, T. (1990). Towards learning time-varying functions with
high input dimensionality. In Proceedings of the Fifth IEEE International
Symposium on Intelligent Control, pp. 383–388. IEEE Computer Society
Press, Los Alamitos, CA.

Si, J., Barto, A., Powell, W., Wunsch, D. (Eds.). (2004). Handbook of learning

and approximate dynamic programming. John Wiley and Sons.

Singh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract
In Proceedings of the Tenth National Conference on Artiﬁcial

models.
Intelligence, pp. 202–207. AAAI/MIT Press, Menlo Park, CA.

Singh, S. P. (1992b). Scaling reinforcement learning algorithms by learning
variable temporal resolution models.
In Proceedings of the Ninth Inter-
national Machine Learning Conference, pp. 406–415. Morgan Kaufmann,
San Mateo, CA.

Singh, S. P. (1993). Learning to Solve Markovian Decision Processes. Ph.D. the-
sis, University of Massachusetts, Amherst. Appeared as CMPSCI Techni-
cal Report 93-77.

Singh, S. P., Bertsekas, D. (1997). Reinforcement learning for dynamic channel
allocation in cellular telephone systems. In Advances in Neural Information
Processing Systems: Proceedings of the 1996 Conference, pp. 974–980. MIT
Press, Cambridge, MA.

Singh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-

Vk(s′) is called the TD target.
• δk(s, r, s′) is called the TD error.

A more concise way of writing the first form of this equation is to use the notation
a
b, which means ak+1 ← (1 – α) · ak + α ·bk. So, the first line of Equation 18-4 can

α

be rewritten like this: V s

r + γ · V s′ .

α

TD  Learning  has  many  similarities  with  Stochastic  Gradient
Descent, in particular the fact that it handles one sample at a time.
Moreover, just like Stochastic GD, it can only truly converge if you
gradually reduce the learning rate (otherwise it will keep bouncing
around the optimum Q-Values).

For each state s, this algorithm simply keeps track of a running average of the imme‐
diate rewards the agent gets upon leaving that state, plus the rewards it expects to get
later (assuming it acts optimally).

Q-Learning
Similarly,  the  Q-Learning  algorithm  is  an  adaptation  of  the  Q-Value  Iteration  algo‐
rithm to the situation where the transition probabilities and the rewards are initially
unknown  (see  Equation  18-5).  Q-Learning  works  by  watching  an  agent  play  (e.g.,
randomly)  and  gradually  improving  its  estimates  of  the  Q-Values.  Once  it  has

630 

| 

Chapter 18: Reinforcement Learning

accurate Q-Value estimates (or close enough), then the optimal policy is choosing the
action that has the highest Q-Value (i.e., the greedy policy).

Equation 18-5. Q-Learning algorithm

Q s, a

α

r + γ · max

a′

  Q s′, a′

For each state-action pair (s, a), this algorithm keeps track of a running average of the
rewards r the agent gets upon leaving the state s with action a, plus the sum of dis‐
counted future rewards it expects to get. To estimate this sum, we take the maximum
of the Q-Value estimates for the next state s′, since we assume that the target policy
would act optimally from then on.

Let’s  implement  the  Q-Learning  algorithm.  First,  we  will  need  to  make  an  agent
explore the environment. For this, we need a step function so that the agent can exe‐
cute one action and get the resulting state and reward:

def step(state, action):
    probas = transition_probabilities[state][action]
    next_state = np.random.choice([0, 1, 2], p=probas)
    reward = rewards[state][action][next_state]
    return next_state, reward

Now  let’s  implement  the  agent’s  exploration  policy.  Since  the  state  space  is  pretty
small,  a  simple  random  policy  will  be  sufficient.  If  we  run  the  algorithm  for  long
enough, the agent will visit every state many times, and it will also try every possible
action many times:

def exploration_policy(state):
    return np.random.choice(possible_actions[state])

Next,  after  we  initialize  the  Q-Values  just  like  earlier,  we  are  ready  to  run  the  Q-
Learning algorithm with learning rate decay (using power scheduling, introduced in
Chapter 11):

alpha0 = 0.05 # initial learning rate
decay = 0.005 # learning rate decay
gamma = 0.90 # discount factor
state = 0 # initial state

for iteration in range(10000):
    action = exploration_policy(state)
    next_state, reward = step(state, action)
    next_value = np.max(Q_values[next_state])
    alpha = alpha0 / (1 + iteration * decay)
    Q_values[state, action] *= 1 - alpha
    Q_values[state, action] += alpha * (reward + gamma * next_value)
    state = next_state

Q-Learning 

| 

631

This algorithm will converge to the optimal Q-Values, but it will take many iterations,
and possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9, the
Q-Value Iteration algorithm (left) converges very quickly, in fewer than 20 iterations,
while  the  Q-Learning  algorithm  (right)  takes  about  8,000  iterations  to  converge.
Obviously, not knowing the transition probabilities or the rewards makes finding the
optimal policy significantly harder!

Figure 18-9. The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm
(right)

The Q-Learning algorithm is called an off-policy algorithm because the policy being
trained is not necessarily the one being executed: in the previous code example, the
policy being executed (the exploration policy) is completely random, while the policy
being  trained  will  always  choose  the  actions  with  the  highest  Q-Values.  Conversely,
the Policy Gradients algorithm is an on-policy algorithm: it explores the world using
the  policy  being  trained.  It  is  somewhat  surprising  that  Q-Learning  is  capable  of
learning the optimal policy by just watching an agent act randomly (imagine learning
to play golf when your teacher is a drunk monkey). Can we do better?

Exploration Policies
Of  course,  Q-Learning  can  work  only  if  the  exploration  policy  explores  the  MDP
thoroughly  enough.  Although  a  purely  random  policy  is  guaranteed  to  eventually
visit every state and every transition many times, it may take an extremely long time
to do so. Therefore, a better option is to use the ε-greedy policy (ε is epsilon): at each
step it acts randomly with probability ε, or greedily with probability 1–ε (i.e., choos‐
ing the action with the highest Q-Value). The advantage of the ε-greedy policy (com‐
pared  to  a  completely  random  policy)  is  that  it  will  spend  more  and  more  time
exploring the interesting parts of the environment, as the Q-Value estimates get better
and better, while still spending some time visiting unknown regions of the MDP. It is
quite common to start with a high value for ε (e.g., 1.0) and then gradually reduce it
(e.g., down to 0.05).

632 

| 

Chapter 18: Reinforcement Learning

Alternatively, rather than relying only on chance for exploration, another approach is
to encourage the exploration policy to try actions that it has not tried much before.
This  can  be  implemented  as  a  bonus  added  to  the  Q-Value  estimates,  as  shown  in
Equation 18-6.

Equation 18-6. Q-Learning using an exploration function

Q s, a

α

r + γ · max

a′

  f Q s′, a′ , N s′, a′

In this equation:

• N(s′, a′) counts the number of times the action a′ was chosen in state s′.

• f(Q, N) is an exploration function, such as f(Q, N) = Q + κ/(1 + N), where κ is a
curiosity  hyperparameter  that  measures  how  much  the  agent  is  attracted  to  the
unknown.

Approximate Q-Learning and Deep Q-Learning
The  main  problem  with  Q-Learning  is  that  it  does  not  scale  well  to  large  (or  even
medium) MDPs with many states and actions. For example, suppose you wanted to
use  Q-Learning  to  train  an  agent  to  play  Ms.  Pac-Man  (see  Figure  18-1).  There  are
about 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent
(i.e., already eaten). So, the number of possible states is greater than 2150 ≈ 1045. And if
you  add  all  the  possible  combinations  of  positions  for  all  the  ghosts  and  Ms.  Pac-
Man, the number of possible states becomes larger than the number of atoms in our
planet, so there’s absolutely no way you can keep track of an estimate for every single
Q-Value.

The solution is to find a function Qθ(s, a) that approximates the Q-Value of any state-
action pair (s, a) using a manageable number of parameters (given by the parameter
vector  θ).  This  is  called  Approximate  Q-Learning.  For  years  it  was  recommended  to
use  linear  combinations  of  handcrafted  features  extracted  from  the  state  (e.g.,  dis‐
tance of the closest ghosts, their directions, and so on) to estimate Q-Values, but in
2013,  DeepMind  showed  that  using  deep  neural  networks  can  work  much  better,
especially for complex problems, and it does not require any feature engineering. A
DNN  used  to  estimate  Q-Values  is  called  a  Deep  Q-Network  (DQN),  and  using  a
DQN for Approximate Q-Learning is called Deep Q-Learning.

Now, how can we train a DQN? Well, consider the approximate Q-Value computed
by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want
this approximate Q-Value to be as close as possible to the reward r that we actually
observe after playing action a in state s, plus the discounted value of playing optimally

Q-Learning 

| 

633

from then on. To estimate this sum of future discounted rewards, we can simply exe‐
cute the DQN on the next state s′ and for all possible actions a′. We get an approxi‐
mate  future  Q-Value  for  each  possible  action.  We  then  pick  the  highest  (since  we
assume we will be playing optimally) and discount it, and this gives us an estimate of
the sum of future discounted rewards. By summing the reward r and the future dis‐
counted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a),
as shown in Equation 18-7.

Equation 18-7. Target Q-Value

Qtarget s, a = r + γ · max
a′

  Qθ s′, a′

With this target Q-Value, we can run a training step using any Gradient Descent algo‐
rithm. Specifically, we generally try to minimize the squared error between the esti‐
mated  Q-Value  Q(s,  a)  and  the  target  Q-Value  (or  the  Huber  loss  to  reduce  the
algorithm’s  sensitivity  to  large  errors).  And  that’s  all  for  the  basic  Deep  Q-Learning
algorithm! Let’s see how to implement it to solve the CartPole environment.

Implementing Deep Q-Learning
The first thing we need is a Deep Q-Network. In theory, you need a neural net that
takes  a  state-action  pair  and  outputs  an  approximate  Q-Value,  but  in  practice  it’s
much more efficient to use a neural net that takes a state and outputs one approxi‐
mate Q-Value for each possible action. To solve the CartPole environment, we do not
need a very complicated neural net; a couple of hidden layers will do:

env = gym.make("CartPole-v0")
input_shape = [4] # == env.observation_space.shape
n_outputs = 2 # == env.action_space.n

model = keras.models.Sequential([
    keras.layers.Dense(32, activation="elu", input_shape=input_shape),
    keras.layers.Dense(32, activation="elu"),
    keras.layers.Dense(n_outputs)
])

To select an action using this DQN, we pick the action with the largest predicted Q-
Value.  To  ensure  that  the  agent  explores  the  environment,  we  will  use  an  ε-greedy
policy (i.e., we will choose a random action with probability ε):

def epsilon_greedy_policy(state, epsilon=0):
    if np.random.rand() < epsilon:
        return np.random.randint(2)
    else:
        Q_values = model.predict(state[np.newaxis])
        return np.argmax(Q_values[0])

634 

| 

Chapter 18: Reinforcement Learning

Instead  of  training  the  DQN  based  only  on  the  latest  experiences,  we  will  store  all
experiences in a replay buffer (or replay memory), and we will sample a random train‐
ing  batch  from  it  at  each  training  iteration.  This  helps  reduce  the  correlations
between the experiences in a training batch, which tremendously helps training. For
this, we will just use a deque list:

from collections import deque

replay_buffer = deque(maxlen=2000)

A deque is a linked list, where each element points to the next one
and to the previous one. It makes inserting and deleting items very
fast, but the longer the deque is, the slower random access will be.
If you need a very large replay buffer, use a circular buffer; see the
“Deque  vs  Rotating  List”  section  of  the  notebook  for  an
implementation.

Each experience will be composed of five elements: a state, the action the agent took,
the  resulting  reward,  the  next  state  it  reached,  and  finally  a  Boolean  indicating
whether the episode ended at that point (done). We will need a small function to sam‐
ple a random batch of experiences from the replay buffer. It will return five NumPy
arrays corresponding to the five experience elements:

def sample_experiences(batch_size):
    indices = np.random.randint(len(replay_buffer), size=batch_size)
    batch = [replay_buffer[index] for index in indices]
    states, actions, rewards, next_states, dones = [
        np.array([experience[field_index] for experience in batch])
        for field_index in range(5)]
    return states, actions, rewards, next_states, dones

Let’s also create a function that will play a single step using the ε-greedy policy, then
store the resulting experience in the replay buffer:

def play_one_step(env, state, epsilon):
    action = epsilon_greedy_policy(state, epsilon)
    next_state, reward, done, info = env.step(action)
    replay_buffer.append((state, action, reward, next_state, done))
    return next_state, reward, done, info

Finally, let’s create one last function that will sample a batch of experiences from the
replay buffer and train the DQN by performing a single Gradient Descent step on this
batch:

batch_size = 32
discount_factor = 0.95
optimizer = keras.optimizers.Adam(lr=1e-3)
loss_fn = keras.losses.mean_squared_error

Implementing Deep Q-Learning 

| 

635

def training_step(batch_size):
    experiences = sample_experiences(batch_size)
    states, actions, rewards, next_states, dones = experiences
    next_Q_values = model.predict(next_states)
    max_next_Q_values = np.max(next_Q_values, axis=1)
    target_Q_values = (rewards +
                       (1 - dones) * discount_factor * max_next_Q_values)
    mask = tf.one_hot(actions, n_outputs)
    with tf.GradientTape() as tape:
        all_Q_values = model(states)
        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)
        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

Let’s go through this code:

• First we define some hyperparameters, and we create the optimizer and the loss

function.

• Then  we  create  the  training_step()  function.  It  starts  by  sampling  a  batch  of
experiences, then it uses the DQN to predict the Q-Value for each possible action
in  each  experience’s  next  state.  Since  we  assume  that  the  agent  will  be  playing
optimally, we only keep the maximum Q-Value for each next state. Next, we use
Equation  18-7  to  compute  the  target  Q-Value  for  each  experience’s  state-action
pair.

• Next,  we  want  to  use  the  DQN  to  compute  the  Q-Value  for  each  experienced
state-action pair. However, the DQN will also output the Q-Values for the other
possible actions, not just for the action that was actually chosen by the agent. So
we need to mask out all the Q-Values we do not need. The tf.one_hot() func‐
tion  makes  it  easy  to  convert  an  array  of  action  indices  into  such  a  mask.  For
example,  if  the  first  three  experiences  contain  actions  1,  1,  0,  respectively,  then
the mask will start with [[0, 1], [0, 1], [1, 0],...]. We can then multiply
the DQN’s output with this mask, and this will zero out all the Q-Values we do
not want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-
Values  of  the  experienced  state-action  pairs.  This  gives  us  the  Q_values  tensor,
containing one predicted Q-Value for each experience in the batch.

• Then we compute the loss: it is the mean squared error between the target and

predicted Q-Values for the experienced state-action pairs.

• Finally, we perform a Gradient Descent step to minimize the loss with regard to

the model’s trainable variables.

This was the hardest part. Now training the model is straightforward:

636 

| 

Chapter 18: Reinforcement Learning

for episode in range(600):
    obs = env.reset()
    for step in ran
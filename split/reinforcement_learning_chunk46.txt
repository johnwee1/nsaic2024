
estimation in partially observable Markovian decision problems. In W. W. Co-
hen and H. Hirsch (eds.), Proceedings of the Eleventh International Con-
ference on Machine Learning, pp. 284–292. Morgan Kaufmann, San Fran-
cisco.

Singh, S. P., Jaakkola, T., Jordan, M. I. (1995). Reinforcement learing with
soft state aggregation.
In G. Tesauro, D. S. Touretzky, T. Leen (eds.),
Advances in Neural Information Processing Systems: Proceedings of the
1994 Conference, pp. 359–368. MIT Press, Cambridge, MA.

Singh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing

eligibility traces. Machine Learning, 22:123–158.

Sivarajan, K. N., McEliece, R. J., Ketchum, J. W. (1990). Dynamic channel
assignment in cellular radio. In Proceedings of the 40th Vehicular Technol-
ogy Conference, pp. 631–637.

Skinner, B. F. (1938). The Behavior of Organisms: An Experimental Analysis.

15.5. OTHER FRONTIER DIMENSIONS

331

Appleton-Century, New York.

Sofge, D. A., White, D. A. (1992). Applied learning: Optimal control for
manufacturing.
In D. A. White and D. A. Sofge (eds.), Handbook of
Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 259–281.
Van Nostrand Reinhold, New York.

Spong, M. W. (1994). Swing up control of the acrobot. In Proceedings of the
1994 IEEE Conference on Robotics and Automation, pp. 2356-2361. IEEE
Computer Society Press, Los Alamitos, CA.

Staddon, J. E. R. (1983). Adaptive Behavior and Learning. Cambridge

University Press, Cambridge.

Sutton, R. S. (1978a). Learning theory support for a single channel theory of

the brain. Unpublished report.

Sutton, R. S. (1978b). Single channel theory: A neuronal theory of learn-
ing. Brain Theory Newsletter, 4:72–75. Center for Systems Neuroscience,
University of Massachusetts, Amherst, MA.

Sutton, R. S. (1978c). A uniﬁed theory of expectation in classical and instru-

mental conditioning. Bachelors thesis, Stanford University.

Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning.

Ph.D. thesis, University of Massachusetts, Amherst.

Sutton, R. S. (1988). Learning to predict by the method of temporal diﬀer-

ences. Machine Learning, 3:9–44.

Sutton, R. S. (1990).

Integrated architectures for learning, planning, and
In Proceedings
reacting based on approximating dynamic programming.
of the Seventh International Conference on Machine Learning, pp. 216–
224. Morgan Kaufmann, San Mateo, CA.

Sutton, R. S. (1991a). Dyna, an integrated architecture for learning, planning,

and reacting. SIGART Bulletin, 2:160–163. ACM Press.

Sutton, R. S. (1991b). Planning by incremental dynamic programming.

In
L. A. Birnbaum and G. C. Collins (eds.), Proceedings of the Eighth Interna-
tional Workshop on Machine Learning, pp. 353–357. Morgan Kaufmann,
San Mateo, CA.

Sutton, R. S. (1995). TD models: Modeling the world at a mixture of time
scales. In A. Prieditis and S. Russell (eds.), Proceedings of the Twelfth In-
ternational Conference on Machine Learning, pp. 531–539. Morgan Kauf-
mann, San Francisco.

Sutton, R. S. (1996). Generalization in reinforcement learning: Successful
In D. S. Touretzky, M. C. Mozer

examples using sparse coarse coding.

332

CHAPTER 15. PROSPECTS

and M. E. Hasselmo (eds.), Advances in Neural Information Processing
Systems: Proceedings of the 1995 Conference, pp. 1038–1044. MIT Press,
Cambridge, MA.

Sutton, R. S. (ed.). (1992). Special issue of Machine Learning on reinforce-
ment learning, 8. Also published as Reinforcement Learning. Kluwer
Academic, Boston, 1992.

Sutton, R. S., Barto, A. G. (1981a). Toward a modern theory of adaptive
networks: Expectation and prediction. Psychological Review, 88:135–170.

Sutton, R. S., Barto, A. G. (1981b). An adaptive network that constructs
and uses an internal model of its world. Cognition and Brain Theory,
3:217–246.

Sutton, R. S., Barto, A. G. (1987). A temporal-diﬀerence model of classi-
In Proceedings of the Ninth Annual Conference of the

cal conditioning.
Cognitive Science Society, pp. 355-378. Erlbaum, Hillsdale, NJ.

Sutton, R. S., Barto, A. G. (1990). Time-derivative models of Pavlovian
reinforcement.
In M. Gabriel and J. Moore (eds.), Learning and Com-
putational Neuroscience: Foundations of Adaptive Networks, pp. 497–537.
MIT Press, Cambridge, MA.

Sutton, R. S., Mahmood, A. R., Precup, D., van Hasselt, H. (2014). A new
Q(λ) with interim forward view and Monte Carlo equivalence. In Proceed-
ings of the 31st International Conference on Machine Learning, Beijing,
China.

Sutton, R. S., Pinette, B. (1985). The learning of world models by connec-
tionist networks. In Proceedings of the Seventh Annual Conference of the
Cognitive Science Society, pp. 54–64.

Sutton, R. S., Singh, S. (1994). On bias and step size in temporal-diﬀerence
learning.
In Proceedings of the Eighth Yale Workshop on Adaptive and
Learning Systems, pp. 91–96. Center for Systems Science, Dunham Labo-
ratory, Yale University, New Haven.

Sutton, R. S., Whitehead, D. S. (1993). Online learning with random repre-
In Proceedings of the Tenth International Machine Learning

sentations.
Conference, pp. 314–321. Morgan Kaufmann, San Mateo, CA.

Szepesvri, C. (2010). Algorithms for reinforcement learning. Synthesis Lec-

tures on Artiﬁcial Intelligence and Machine Learning 4(1), 1–103.

Szita, I. (2012). Reinforcement learning in games. In Reinforcement Learning

(pp. 539-577). Springer Berlin Heidelberg.

Tadepalli, P., Ok, D. (1994). H-learning: A reinforcement learning method to

15.5. OTHER FRONTIER DIMENSIONS

333

optimize undiscounted average reward. Technical Report 94-30-01. Oregon
State University, Computer Science Department, Corvallis.

Tan, M. (1991). Learning a cost-sensitive internal representation for reinforce-
ment learning.
In L. A. Birnbaum and G. C. Collins (eds.), Proceedings
of the Eighth International Workshop on Machine Learning, pp. 358–362.
Morgan Kaufmann, San Mateo, CA.

Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. coop-
In Proceedings of the Tenth International Conference on

erative agents.
Machine Learning, pp. 330–337. Morgan Kaufmann, San Mateo, CA.

Tesauro, G. J. (1986). Simple neural models of classical conditioning. Biolog-

ical Cybernetics, 55:187–200.

Tesauro, G. J. (1992). Practical issues in temporal diﬀerence learning. Ma-

chine Learning, 8:257–277.

Tesauro, G. J. (1994). TD-Gammon, a self-teaching backgammon program,

achieves master-level play. Neural Computation, 6(2):215–219.

Tesauro, G. J. (1995). Temporal diﬀerence learning and TD-Gammon. Com-

munications of the ACM, 38:58–68.

Tesauro, G. J., Galperin, G. R. (1997). On-line policy improvement using
Monte-Carlo search.
In Advances in Neural Information Processing Sys-
tems: Proceedings of the 1996 Conference, pp. 1068–1074. MIT Press,
Cambridge, MA.

Tham, C. K. (1994). Modular On-Line Function Approximation for Scaling

up Reinforcement Learning. PhD thesis, Cambridge University.

Thathachar, M. A. L. and Sastry, P. S. (1985). A new approach to the design
IEEE Transactions on

of reinforcement schemes for learning automata.
Systems, Man, and Cybernetics, 15:168–175.

Thompson, W. R. (1933). On the likelihood that one unknown probability
exceeds another in view of the evidence of two samples. Biometrika,
25:285–294.

Thompson, W. R. (1934). On the theory of apportionment. American Journal

of Mathematics, 57:450–457.

Thorndike, E. L. (1911). Animal Intelligence. Hafner, Darien, CT.

Thorp, E. O. (1966). Beat the Dealer: A Winning Strategy for the Game of

Twenty-One. Random House, New York.

Tolman, E. C. (1932). Purposive Behavior in Animals and Men. Century,

New York.

334

CHAPTER 15. PROSPECTS

Tsetlin, M. L. (1973). Automaton Theory and Modeling of Biological Systems.

Academic Press, New York.

Tsitsiklis, J. N. (1994).

Asynchronous stochastic approximation and Q-

learning. Machine Learning, 16:185–202.

Tsitsiklis, J. N. (2002). On the convergence of optimistic policy iteration.

Journal of Machine Learning Research, 3:59–72.

Tsitsiklis, J. N. and Van Roy, B. (1996). Feature-based methods for large

scale dynamic programming. Machine Learning, 22:59–94.

Tsitsiklis, J. N., Van Roy, B. (1997). An analysis of temporal-diﬀerence
learning with function approximation. IEEE Transactions on Automatic
Control, 42:674–690.

Tsitsiklis, J. N., Van Roy, B. (1999). Average cost temporal-diﬀerence learn-
ing. Automatica, 35:1799–1808. Also: Technical Report LIDS-P-2390.
Laboratory for Information and Decision Systems, Massachusetts Institute
of Technology, Cambridge, MA, 1997.

Turing, A. M. (1950). Computing machinery and intelligence. Mind 433–460.

Turing, A. M. (1948). Intelligent Machinery, A Heretical Theory. The Turing

Test: Verbal Behavior as the Hallmark of Intelligence, 105.

Ungar, L. H. (1990). A bioreactor benchmark for adaptive network-based
In W. T. Miller, R. S. Sutton, and P. J. Werbos (eds.),

process control.
Neural Networks for Control, pp. 387–402. MIT Press, Cambridge, MA.

Urbanowicz, R. J., Moore, J. H. (2009). Learning classiﬁer systems: A com-
plete introduction, review, and roadmap. Journal of Artiﬁcial Evolution
and Applications.

Waltz, M. D., Fu, K. S. (1965). A heuristic approach to reinforcement learning
control systems. IEEE Transactions on Automatic Control, 10:390–398.

Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis,

Cambridge University.

Watkins, C. J. C. H., Dayan, P. (1992). Q-learning. Machine Learning,

8:279–292.

Wiering, M., Van Otterlo, M. (2012). Reinforcement Learning. Springer Berlin

Heidelberg.

Werbos, P. J. (1977). Advanced forecasting methods for global crisis warning

and models of intelligence. General Systems Yearbook, 22:25–38.

Werbos, P. J. (1982). Applications of advances in nonlinear sensitivity analy-
sis. In R. F. Drenick and F. Kozin (eds.), System Modeling and Optimiza-

15.5. OTHER FRONTIER DIMENSIONS

335

tion, pp. 762–770. Springer-Verlag, Berlin.

Werbos, P. J. (1987). Building and understanding adaptive systems: A statis-
tical/numerical approach to factory automation and brain research. IEEE
Transactions on Systems, Man, and Cybernetics, 17:7–20.

Werbos, P. J. (1988). Generalization of back propagation with applications to

a recurrent gas market model. Neural Networks, 1:339–356.

Werbos, P. J. (1989). Neural networks for control and system identiﬁcation. In
Proceedings of the 28th Conference on Decision and Control, pp. 260–265.
IEEE Control Systems Society.

Werbos, P. J. (1990). Consistency of HDP applied to a simple reinforcement

learning problem. Neural Networks, 3:179–189.

Werbos, P. J. (1992). Approximate dynamic programming for real-time con-
trol and neural modeling. In D. A. White and D. A. Sofge (eds.), Handbook
of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 493–
525. Van Nostrand Reinhold, New York.

White, D. J. (1969). Dynamic Programming. Holden-Day, San Francisco.

White, D. J. (1985). Real applications of Markov decision processes. Inter-

faces, 15:73–83.

White, D. J. (1988). Further real applications of Markov decision processes.

Interfaces, 18:55–61.

White, D. J. (1993). A survey of applications of Markov decision processes.

Journal of the Operational Research Society, 44:1073–1096.

Whitehead, S. D., Ballard, D. H. (1991). Learning to perceive and act by trial

and error. Machine Learning, 7:45–83.

Whitt, W. (1978). Approximations of dynamic programs I. Mathematics of

Operations Research, 3:231–243.

Whittle, P. (1982). Optimization over Time, vol. 1. Wiley, New York.

Whittle, P. (1983). Optimization over Time, vol. 2. Wiley, New York.

Widrow, B., Gupta, N. K., Maitra, S. (1973). Punish/reward: Learning with
IEEE Transactions on Systems,

a critic in adaptive threshold systems.
Man, and Cybernetics, 3:455–465.

Widrow, B., Hoﬀ, M. E. (1960). Adaptive switching circuits.

In 1960
WESCON Convention Record Part IV, pp. 96–104.
Institute of Radio
Engineers, New York. Reprinted in J. A. Anderson and E. Rosenfeld,
Neurocomputing: Foundations of Research, pp. 126–134. MIT Press, Cam-
bridge, MA, 1988.

336

CHAPTER 15. PROSPECTS

Widrow, B., Smith, F. W. (1964). Pattern-recognizing control systems.

In
J. T. Tou and R. H. Wilcox (eds.), Computer and Information Sciences,
pp. 288–317. Spartan, Washington, DC.

Widrow, B., Stearns, S. D. (1985). Adaptive Signal Processing. Prentice-Hall,

Englewood Cliﬀs, NJ.

Williams, R. J. (1986). Reinforcement learning in connectionist networks: A
mathematical analysis. Technical Report ICS 8605. Institute for Cognitive
Science, University of California at San Diego, La Jolla.

Williams, R. J. (1987). Reinforcement-learning connectionist systems. Tech-
nical Report NU-CCS-87-3. College of Computer Science, Northeastern
University, Boston.

Williams, R. J. (1988). On the use of backpropagation in associative rein-
forcement learning. In Proceedings of the IEEE International Conference
IEEE San Diego section and IEEE
on Neural Networks, pp. I263–I270.
TAB Neural Network Committee.

Williams, R. J. (1992). Simple statistical gradient-following algorithms for

connectionist reinforcement learning. Machine Learning, 8:229–256.

Williams, R. J., Baird, L. C. (1990). A mathematical analysis of actor-critic
architectures for learning optimal controls through incremental dynamic
In Proceedings of the Sixth Yale Workshop on Adaptive
programming.
and Learning Systems, pp. 96–101. Center for Systems Science, Dunham
Laboratory, Yale University, New Haven.

Wilson, S. W. (1994). ZCS: A zeroth order classiﬁer system. Evolutionary

Computation, 2:1–18.

Witten, I. H. (1976). The apparent conﬂict between estimation and control—
Journal of the Franklin Institute,

A survey of the two-armed problem.
301:161–189.

Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov

environments. Information and Control, 34:286–295.

Witten, I. H., Corbin, M. J. (1973). Human operators and automatic adaptive
Interna-

controllers: A comparative study on a particular control task.
tional Journal of Man–Machine Studies, 5:75–104.

Woodworth, R. S., Schlosberg, H. (1938). Experimental psychology. New York:

Henry Holt and Company.

Yee, R. C., Saxena, S., Utgoﬀ, P. E., Barto, A. G. (1990). Explaining temporal
diﬀerences to create useful concepts for evaluating states. In Proceedings
of the Eighth National Conference on Artiﬁcial Intelligence, pp. 882–888.

15.5. OTHER FRONTIER DIMENSIONS

337

AAAI Press, Menlo Park, CA.

Young, P. (1984). Recursive Estimation and Time-Series Analysis. Springer-

Verlag, Berlin.

Zhang, M., Yum, T. P. (1989). Comparisons of channel-assignment strategies
IEEE Transactions on Vehicular

in cellular mobile telephone systems.
Technology, 38:211-215.

Zhang, W. (1996). Reinforcement Learning for Job-shop Scheduling. Ph.D.

thesis, Oregon State University. Technical Report CS-96-30-1.

Zhang, W., Dietterich, T. G. (1995). A reinforcement learning approach to
job-shop scheduling. In Proceedings of the Fourteenth International Joint
Conference on Artiﬁcial Intelligence, pp. 1114–1120. Morgan Kaufmann.

Zhang, W., Dietterich, T. G. (1996). High-performance job-shop scheduling
with a time–delay TD(λ) network.
In D. S. Touretzky, M. C. Mozer,
M. E. Hasselmo (eds.), Advances in Neural Information Processing Sys-
tems: Proceedings of the 1995 Conference, pp. 1024–1030. MIT Press,
Cambridge, MA.

Zweben, M., Daun, B., Deale, M. (1994). Scheduling and rescheduling with
iterative repair.
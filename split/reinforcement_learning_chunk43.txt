c-
tions and one-step models of the dynamics of the environment. But much of
what people learn does not seem to fall exactly into either of these categories.
For example, consider what we know about tying our shoes, making a phone
call, or traveling to London. Having learned how to do such things, we are
then able to choose among them and plan as if they were primitive actions.
What we have learned in order to do this are not conventional value functions
or one-step models. We are able to plan and learn at a variety of levels and
ﬂexibly interrelate them. Much of our learning appears not to be about learn-
ing values directly, but about preparing us to quickly estimate values later in
response to new situations or new information. Considerable reinforcement
learning research has been directed at capturing such abilities (e.g., Watkins,
1989; Dayan and Hinton, 1993; Singh, 1992a, 1992b; Ring, 1994, Kaelbling,
1993b; Sutton, 1995).

Researchers have also explored ways of using the structure of particular
tasks to advantage. For example, many problems have state representations
that are naturally lists of variables, like the readings of multiple sensors or ac-
tions that are lists of component actions. The independence or near indepen-
dence of some variables from others can sometimes be exploited to obtain more
eﬃcient special forms of reinforcement learning algorithms. It is sometimes
even possible to decompose a problem into several independent subproblems
that can be solved by separate learning agents. A reinforcement learning prob-
lem can usually be structured in many diﬀerent ways, some reﬂecting natural
aspects of the problem, such as the existence of physical sensors, and others
being the result of explicit attempts to decompose the problem into simpler
subproblems. Possibilities for exploiting structure in reinforcement learning
and related planning problems have been studied by many researchers (e.g.,
Boutilier, Dearden, and Goldszmidt, 1995; Dean and Lin, 1995). There are
also related studies of multiagent or distributed reinforcement learning (e.g.,
Littman, 1994; Markey, 1994; Crites and Barto, 1996; Tan, 1993).

Finally, we want to emphasize that reinforcement learning is meant to be a
general approach to learning from interaction. It is general enough not to re-
quire special-purpose teachers and domain knowledge, but also general enough
to utilize such things if they are available. For example, it is often possible
to accelerate reinforcement learning by giving advice or hints to the agent

15.5. OTHER FRONTIER DIMENSIONS

309

(Clouse and Utgoﬀ, 1992; Maclin and Shavlik, 1994) or by demonstrating in-
structive behavioral trajectories (Lin, 1992). Another way to make learning
easier, related to “shaping” in psychology, is to give the learning agent a series
of relatively easy problems building up to the harder problem of ultimate inter-
est (e.g., Selfridge, Sutton, and Barto, 1985). These methods, and others not
yet developed, have the potential to give the machine-learning terms training
and teaching new meanings that are closer to their meanings for animal and
human learning.

310

CHAPTER 15. PROSPECTS

References

Agrawal, R. (1995). Sample mean based index policies with O(logn) regret
for the multi-armed bandit problem. Advances in Applied Probability,
27:1054–1078.

Agre, P. E. (1988). The Dynamic Structure of Everyday Life. Ph.D. the-
sis, Massachusetts Institute of Technology. AI-TR 1085, MIT Artiﬁcial
Intelligence Laboratory.

Agre, P. E., Chapman, D. (1990). What are plans for? Robotics and Au-

tonomous Systems, 6:17–34.

Albus, J. S. (1971). A theory of cerebellar function. Mathematical Biosciences,

10:25–61.

Albus, J. S. (1981). Brain, Behavior, and Robotics. Byte Books, Peterbor-

ough, NH.

Anderson, C. W. (1986). Learning and Problem Solving with Multilayer Con-
nectionist Systems. Ph.D. thesis, University of Massachusetts, Amherst.

Anderson, C. W. (1987).

Strategy learning with multilayer connectionist
representations. Proceedings of the Fourth International Workshop on
Machine Learning, pp. 103–114. Morgan Kaufmann, San Mateo, CA.

Anderson, J. A., Silverstein, J. W., Ritz, S. A., Jones, R. S. (1977). Dis-
tinctive features, categorical perception, and probability learning: Some
applications of a neural model. Psychological Review, 84:413–451.

Andreae, J. H. (1963).

In
Proceedings of the 2nd IFAC Congress, Basle, pp. 497–502. Butterworths,
London.

STELLA: A scheme for a learning machine.

Andreae, J. H. (1969a). A learning machine with monologue. International

Journal of Man–Machine Studies, 1:1–20.

Andreae, J. H. (1969b). Learning machines—a uniﬁed view. In A. R. Meetham
and R. A. Hudson (eds.), Encyclopedia of Information, Linguistics, and
Control, pp. 261–270. Pergamon, Oxford.

311

312

CHAPTER 15. PROSPECTS

Andreae, J. H. (1977). Thinking with the Teachable Machine. Academic

Press, London.

Arthur, W. B. (1991). Designing economic agents that act like human agents:
A behavioral approach to bounded rationality. The American Economic
Review 81 (2):353-359.

Auer, P., Cesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the

multiarmed bandit problem. Machine learning, 47(2-3):235–256.

Baird, L. C. (1995). Residual algorithms: Reinforcement learning with func-
tion approximation. In Proceedings of the Twelfth International Conference
on Machine Learning, pp. 30–37. Morgan Kaufmann, San Francisco.

Bao, G., Cassandras, C. G., Djaferis, T. E., Gandhi, A. D., Looze, D. P.
(1994). Elevator dispatchers for down peak traﬃc. Technical report. ECE
Department, University of Massachusetts, Amherst.

Barnard, E. (1993). Temporal-diﬀerence methods and Markov models. IEEE

Transactions on Systems, Man, and Cybernetics, 23:357–365.

Barto, A. G. (1985). Learning by statistical cooperation of self-interested

neuron-like computing elements. Human Neurobiology, 4:229–256.

Barto, A. G. (1986). Game-theoretic cooperativity in networks of self-interested
In J. S. Denker (ed.), Neural Networks for Computing, pp. 41–46.

units.
American Institute of Physics, New York.

Barto, A. G. (1990). Connectionist learning for control: An overview.

In
T. Miller, R. S. Sutton, and P. J. Werbos (eds.), Neural Networks for
Control, pp. 5–58. MIT Press, Cambridge, MA.

Barto, A. G. (1991). Some learning tasks from a control perspective.

In
L. Nadel and D. L. Stein (eds.), 1990 Lectures in Complex Systems, pp. 195–
223. Addison-Wesley, Redwood City, CA.

Barto, A. G. (1992). Reinforcement learning and adaptive critic methods. In
D. A. White and D. A. Sofge (eds.), Handbook of Intelligent Control: Neu-
ral, Fuzzy, and Adaptive Approaches, pp. 469–491. Van Nostrand Reinhold,
New York.

Barto, A. G. (1995a). Adaptive critics and the basal ganglia. In J. C. Houk,
J. L. Davis, and D. G. Beiser (eds.), Models of Information Processing in
the Basal Ganglia, pp. 215–232. MIT Press, Cambridge, MA.

Barto, A. G. (1995b). Reinforcement learning. In M. A. Arbib (ed.), Hand-
book of Brain Theory and Neural Networks, pp. 804–809. MIT Press, Cam-
bridge, MA.

Barto, A. G., Anandan, P. (1985). Pattern recognizing stochastic learning

15.5. OTHER FRONTIER DIMENSIONS

313

automata. IEEE Transactions on Systems, Man, and Cybernetics, 15:360–
375.

Barto, A. G., Anderson, C. W. (1985). Structural learning in connectionist
In Program of the Seventh Annual Conference of the Cognitive

systems.
Science Society, pp. 43–54.

Barto, A. G., Anderson, C. W., Sutton, R. S. (1982). Synthesis of nonlin-
ear control surfaces by a layered associative search network. Biological
Cybernetics, 43:175–185.

Barto, A. G., Bradtke, S. J., Singh, S. P. (1991). Real-time learning and
control using asynchronous dynamic programming. Technical Report 91-
57. Department of Computer and Information Science, University of Mas-
sachusetts, Amherst.

Barto, A. G., Bradtke, S. J., Singh, S. P. (1995).

Learning to act using

real-time dynamic programming. Artiﬁcial Intelligence, 72:81–138.

Barto, A. G., Duﬀ, M. (1994). Monte Carlo matrix inversion and reinforce-
ment learning.
In J. D. Cohen, G. Tesauro, and J. Alspector (eds.), Ad-
vances in Neural Information Processing Systems: Proceedings of the 1993
Conference, pp. 687–694. Morgan Kaufmann, San Francisco.

Barto, A. G., Jordan, M. I. (1987). Gradient following without back-propagation
in layered networks. In M. Caudill and C. Butler (eds.), Proceedings of the
IEEE First Annual Conference on Neural Networks, pp. II629–II636. SOS
Printing, San Diego, CA.

Barto, A. G., Sutton, R. S. (1981a). Goal seeking components for adaptive
intelligence: An initial assessment. Technical Report AFWAL-TR-81-
1070. Air Force Wright Aeronautical Laboratories/Avionics Laboratory,
Wright-Patterson AFB, OH.

Barto, A. G., Sutton, R. S. (1981b). Landmark learning: An illustration of

associative search. Biological Cybernetics, 42:1–8.

Barto, A. G., Sutton, R. S. (1982).

Simulation of anticipatory responses
in classical conditioning by a neuron-like adaptive element. Behavioural
Brain Research, 4:221–235.

Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements
that can solve diﬃcult learning control problems. IEEE Transactions on
Systems, Man, and Cybernetics, 13:835–846. Reprinted in J. A. Ander-
son and E. Rosenfeld (eds.), Neurocomputing: Foundations of Research,
pp. 535–549. MIT Press, Cambridge, MA, 1988.

Barto, A. G., Sutton, R. S., Brouwer, P. S. (1981). Associative search net-

314

CHAPTER 15. PROSPECTS

work: A reinforcement learning associative memory. Biological Cybernet-
ics, 40:201–211.

Bellman, R. E. (1956). A problem in the sequential design of experiments.

Sankhya, 16:221–229.

Bellman, R. E. (1957a). Dynamic Programming. Princeton University Press,

Princeton.

Bellman, R. E. (1957b). A Markov decision process. Journal of Mathematical

Mechanics, 6:679–684.

Bellman, R. E., Dreyfus, S. E. (1959). Functional approximations and dy-
namic programming. Mathematical Tables and Other Aids to Computa-
tion, 13:247–251.

Bellman, R. E., Kalaba, R., Kotkin, B. (1973). Polynomial approximation—A
new computational technique in dynamic programming: Allocation pro-
cesses. Mathematical Computation, 17:155–161.

Berry, D. A., Fristedt, B. (1985). Bandit Problems. Chapman and Hall,

London.

Bertsekas, D. P. (1982). Distributed dynamic programming. IEEE Transac-

tions on Automatic Control, 27:610–616.

Bertsekas, D. P. (1983). Distributed asynchronous computation of ﬁxed points.

Mathematical Programming, 27:107–120.

Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic

Models. Prentice-Hall, Englewood Cliﬀs, NJ.

Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena

Scientiﬁc, Belmont, MA.

Bertsekas, D. P., Tsitsiklis, J. N. (1989). Parallel and Distributed Computa-

tion: Numerical Methods. Prentice-Hall, Englewood Cliﬀs, NJ.

Bertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming.

Athena Scientiﬁc, Belmont, MA.

Biermann, A. W., Fairﬁeld, J. R. C., Beres, T. R. (1982). Signature table sys-
tems and learning. IEEE Transactions on Systems, Man, and Cybernetics,
12:635–648.

Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Clarendon,

Oxford.

Booker, L. B. (1982).

Intelligent Behavior as an Adaptation to the Task

Environment. Ph.D. thesis, University of Michigan, Ann Arbor.

15.5. OTHER FRONTIER DIMENSIONS

315

Boone, G. (1997). Minimum-time control of the acrobot.

In 1997 Inter-
national Conference on Robotics and Automation, pp. 3281–3287. IEEE
Robotics and Automation Society.

Boutilier, C., Dearden, R., Goldszmidt, M. (1995). Exploiting structure in
In Proceedings of the Fourteenth International Joint
policy construction.
Conference on Artiﬁcial Intelligence, pp. 1104–1111. Morgan Kaufmann.

Boyan, J. A., Moore, A. W. (1995). Generalization in reinforcement learning:
Safely approximating the value function. In G. Tesauro, D. S. Touretzky,
and T. Leen (eds.), Advances in Neural Information Processing Systems:
Proceedings of the 1994 Conference, pp. 369–376. MIT Press, Cambridge,
MA.

Boyan, J. A., Moore, A. W., Sutton, R. S. (eds.). (1995). Proceedings of
the Workshop on Value Function Approximation. Machine Learning Con-
ference 1995. Technical Report CMU-CS-95-206. School of Computer
Science, Carnegie Mellon University, Pittsburgh, PA.

Bradtke, S. J. (1993). Reinforcement learning applied to linear quadratic
regulation.
In S. J. Hanson, J. D. Cowan, and C. L. Giles (eds.), Ad-
vances in Neural Information Processing Systems: Proceedings of the 1992
Conference, pp. 295–302. Morgan Kaufmann, San Mateo, CA.

Bradtke, S. J. (1994). Incremental Dynamic Programming for On-Line Adap-
tive Optimal Control. Ph.D. thesis, University of Massachusetts, Amherst.
Appeared as CMPSCI Technical Report 94-62.

Bradtke, S. J., Barto, A. G. (1996).

Linear least–squares algorithms for

temporal diﬀerence learning. Machine Learning, 22:33–57.

Bradtke, S. J., Ydstie, B. E., Barto, A. G. (1994). Adaptive linear quadratic
control using policy iteration. In Proceedings of the American Control Con-
ference, pp. 3475–3479. American Automatic Control Council, Evanston,
IL.

Bradtke, S. J., Duﬀ, M. O. (1995). Reinforcement learning methods for
continuous-time Markov decision problems. In G. Tesauro, D. Touretzky,
and T. Leen (eds.), Advances in Neural Information Processing Systems:
Proceedings of the 1994 Conference, pp. 393–400. MIT Press, Cambridge,
MA.

Bridle, J. S. (1990). Training stochastic model recognition algorithms as
networks can lead to maximum mutual information estimates of parame-
ters. In D. S. Touretzky (ed.), Advances in Neural Information Processing
Systems: Proceedings of the 1989 Conference, pp. 211–217. Morgan Kauf-
mann, San Mateo, CA.

316

CHAPTER 15. PROSPECTS

Broomhead, D. S., Lowe, D. (1988). Multivariable functional interpolation

and adaptive networks. Complex Systems, 2:321–355.

Bryson, A. E., Jr. (1996). Optimal control—1950 to 1985.

IEEE Control

Systems, 13(3):26–33.

Bush, R. R., Mosteller, F. (1955). Stochastic Models for Learning. Wiley,

New York.

Byrne, J. H., Gingrich, K. J., Baxter, D. A. (1990). Computational capa-
bilities of single neurons: Relationship to simple forms of associative and
nonassociative learning in aplysia.
In R. D. Hawkins and G. H. Bower
(eds.), Computational Models of Learning, pp. 31–63. Academic Press,
New York.

Camerer, C. (2003). Behavioral game theory: Experiments in strategic inter-

action. Princeton University Press.

Campbell, D. T. (1960). Blind variation and selective survival as a general
strategy in knowledge-processes. In M. C. Yovits and S. Cameron (eds.),
Self-Organizing Systems, pp. 205–231. Pergamon, New York.

Carlstr¨om, J., Nordstr¨om, E. (1997). Control of self-similar ATM call traﬃc
by reinforcement learning.
In Proceedings of the International Workshop
on Applications of Neural Networks to Telecommunications 3, pp. 54–62.
Erlbaum, Hillsdale, NJ.

Chapman, D., Kaelbling, L. P. (1991).

Input generalization in delayed rein-
forcement learning: An algorithm and performance comparisons. In Pro-
ceedings of the Twelfth International Conference on Artiﬁcial Intelligence,
pp. 726–731. Morgan Kaufmann, San Mateo, CA.

Chow, C.-S., Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithm
IEEE Transactions on Automatic

for discrete-time stochastic control.
Control, 36:898–914.

Chrisman, L. (1992). Reinforcement learning 
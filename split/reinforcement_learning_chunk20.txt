ansquareerror(average over100 runs)0245.5. OFF-POLICY PREDICTION VIA IMPORTANCE SAMPLING 131

Figure 5.8: Ordinary importance sampling produces surprisingly unstable esti-
mates on the one-state MDP shown inset (Example 5.5). The correct estimate
here is 1, and, even though this is the expected value of a sample return (after
importance sampling), the variance of the samples is inﬁnite, and the estimates
do not convergence to this value. These results are for oﬀ-policy ﬁrst-visit MC.

Example 5.5: Inﬁnite Variance
The estimates of ordinary importance sampling will typically have inﬁnite
variance, and thus unsatisfactory convergence properties, whenever the scaled
returns have inﬁnite variance—and this can easily happen in oﬀ-policy learning
when trajectories contain loops. A simple example is shown inset in Figure 5.8.
There is only one nonterminal state s and two actions, end and back. The
end action causes a deterministic transition to termination, whereas the back
action transitions, with probability 0.9, back to s or, with probability 0.1, on
to termination. The rewards are +1 on the latter transition and otherwise
zero. Consider the target policy that always selects back. All episodes under
this policy consist of some number (possibly zero) of transitions back to s
followed by termination with a reward and return of +1. Thus the value of
s under the target policy is thus 1. Suppose we are estimating this value
from oﬀ-policy data using the behavior policy that selects end and back with
equal probability. The lower part of Figure 5.8 shows ten independent runs
of the ﬁrst-visit MC algorithm using ordinary importance sampling. Even
after millions of episodes, the estimates fail to converge to the correct value
of 1. In contrast, the weighted importance-sampling algorithm would give an
estimate of exactly 1 everafter the ﬁrst episode that was consistent with the
target policy (i.e., that ended with the back action). This is clear because

1100,0001,000,00010,000,000100,000,00020.10.9R=+1s⇡(back|s)=1µ(back|s)=12backendv⇡(s)Monte-Carlo estimate of           with ordinaryimportance sampling(ten runs)Episodes (log scale)110100100010,0000132

CHAPTER 5. MONTE CARLO METHODS

that algorithm produces a weighted average of the returns consistent with the
target policy, all of which would be exactly 1.

We can verify that the variance of the importance-sampling-scaled returns
is inﬁnite in this example by a simple calculation. The variance of any random
variable X is the expected value of the deviation from its mean ¯X, which can
be written

Var[X] = E

X

2

¯X

= E

X 2

2X ¯X + ¯X 2

= E

X 2

¯X 2.

−

(cid:104)(cid:0)

−
Thus, if the mean is ﬁnite, as it is in our case, the variance is inﬁnite if and
only if the expectation of the square of the random variable is inﬁnite. Thus,
we need only show that the expected square of the importance-sampling-scaled
return is inﬁnite:

−

(cid:105)

(cid:1)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

T

1

−

t=0
(cid:89)

E



(cid:32)



π(At|
µ(At|

St)
St)

2

G0

.



(cid:33)



To compute this expectation, we break it down into cases based on episode
length and termination. First note that, for any episode ending with the
end action, the importance sampling ratio is zero, because the target policy
would never take this action; these episodes thus contribute nothing to the
expectation (the quantity in parenthesis will be zero) and can be ignored. We
need only consider episodes that involve some number (possibly zero) of back
actions that transition back to the nonterminal state, followed by a back action
transitioning to termination. All of these episodes have a return of 1, so the
G0 factor can be ignored. To get the expected square we need only consider
each length of episode, multiplying the probability of the episode’s occurrence
by the square of its importance-sampling ratio, and add these up:

(the length 1 episode)

(the length 2 episode)

(the length 3 episode)

1
0.5

1
0.5

2

(cid:19)

2

1
0.5

1
0.5

(cid:18)
1
2 ·

·

(cid:19)

1
0.5

0.1

(cid:18)

=

1
2 ·

0.1

1
0.5

2

(cid:18)

0.9

0.9

(cid:19)

1
2 ·

0.1

1
2 ·

0.9

·

·

1
2 ·

1
2 ·

+

+

+

0.9k

2k

·

2

·

1.8k

· · ·
∞

= 0.1

(cid:88)k=0
∞

(cid:88)k=0

= 0.2

=

.
∞

5.6.

INCREMENTAL IMPLEMENTATION

133

5.6 Incremental Implementation

Monte Carlo prediction methods can be implemented incrementally, on an
episode-by-episode basis, using extensions of the techniques described in Chap-
ter 2. Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods
we average returns. In all other respects exactly the same methods as used
in Chapter 2 can be used for on-policy Monte Carlo methods. For oﬀ-policy
Monte Carlo methods, we need to separately consider those that use ordinary
importance sampling and those that use weighted importance sampling.

t

In ordinary importance sampling, the returns are scaled by the importance
sampling ratio ρT (t)
(5.3), then simply averaged. For these methods we can
again use the incremental methods of Chapter 2, but using the scaled returns in
place of the rewards of that chapter. This leaves the case of oﬀ-policy methods
using weighted importance sampling. Here we have to form a weighted average
of the returns, and a slightly diﬀerent incremental algorithm is required.

Suppose we have a sequence of returns G1, G2, . . . , Gn

same state and each with a corresponding random weight Wi (e.g., Wi = ρT (t)
We wish to form the estimate

t

−

1, all starting in the
).

Vn =

1

n
k=1 WkGk
−
1
n
k=1 Wk
−

,

(cid:80)

2,

n

≥

(5.6)

(cid:80)

and keep it up-to-date as we obtain a single additional return Gn. In addition
to keeping track of Vn, we must maintain for each state the cumulative sum
Cn of the weights given to the ﬁrst n returns. The update rule for Vn is

Vn+1 = Vn +

Wn
Cn

Gn −

Vn

,

(cid:105)

(cid:104)

1,

n

≥

and

Cn+1 = Cn + Wn+1,

(5.7)

where C0 = 0 (and V1 is arbitrary and thus need not be speciﬁed). Figure 5.9
gives a complete episode-by-episode incremental algorithm for Monte Carlo
policy evaluation. The algorithm is nominally for the oﬀ-policy case, using
weighted importance sampling, but applies as well to the on-policy case just
by choosing the target and behavior policies as the same.

134

CHAPTER 5. MONTE CARLO METHODS

Initialize, for all s

S, a

A(s):

Q(s, a)
C(s, a)
s)
µ(a
|
s)
π(a
|

←
←
←
←

∈

∈
arbitrary
0
an arbitrary soft behavior policy
an arbitrary target policy

Repeat forever:

Generate an episode using µ:

S0, A0, R1, . . . , ST

1, AT

1, RT , ST

−

−

2, . . . downto 0:

0
1

1, T
−
−
γG + Rt+1

G
←
W
←
For t = T
G
←
C(St, At) + W
C(St, At)
Q(St, At) + W
Q(St, At)
St)
W
|
St)
|
If W = 0 then ExitForLoop

←
←
W π(At
µ(At

←

C(St,At) [G

Q(St, At)]

−

Figure 5.9: An incremental every-visit MC policy-evaluation algorithm, using
weighted importance sampling. The approximation Q converges to qπ (for all
encountered state–action pairs) even though all actions are selected according
to a potentially diﬀerent policy, µ. In the on-policy case (π = µ), W is always
1.

5.7. OFF-POLICY MONTE CARLO CONTROL

135

5.7 Oﬀ-Policy Monte Carlo Control

We are now ready to present an example of the second class of learning control
methods we consider in this book: oﬀ-policy methods. Recall that the distin-
guishing feature of on-policy methods is that they estimate the value of a
policy while using it for control. In oﬀ-policy methods these two functions are
separated. The policy used to generate behavior, called the behavior policy,
may in fact be unrelated to the policy that is evaluated and improved, called
the target policy. An advantage of this separation is that the target policy
may be deterministic (e.g., greedy), while the behavior policy can continue to
sample all possible actions.

Oﬀ-policy Monte Carlo control methods use one of the techniques presented
in the preceding two sections. They follow the behavior policy while learning
about and improving the target policy. These techniques requires that the
behavior policy has a nonzero probability of selecting all actions that might
be selected by the target policy (coverage). To explore all possibilities, we
require that the behavior policy be soft (i.e., that it select all actions in all
states with nonzero probability).

Figure 5.10 shows an oﬀ-policy Monte Carlo method, based on GPI and
weighted importance sampling, for estimating q
. The target policy π is the
greedy policy with respect to Q, which is an estimate of qπ. The behavior
policy µ can be anything, but in order to assure convergence of π to the
optimal policy, an inﬁnite number of returns must be obtained for each pair
of state and action. This can be assured by choosing µ to be ε-soft.

∗

A potential problem is that this method learns only from the tails of
episodes, after the last nongreedy action. If nongreedy actions are frequent,
then learning will be slow, particularly for states appearing in the early por-
tions of long episodes. Potentially, this could greatly slow learning. There
has been insuﬃcient experience with oﬀ-policy Monte Carlo methods to assess
how serious this problem is. If it is serious, the most important way to address
it is probably by incorporating temporal-diﬀerence learning, the algorithmic
idea developed in the next chapter. Alternatively, if γ is less than 1, then the
idea developed in the next section may also help signiﬁcantly.

136

CHAPTER 5. MONTE CARLO METHODS

Initialize, for all s

S, a

A(s):

∈

∈
arbitrary
0

Q(s, a)
C(s, a)
π(s)

←

←
←
a deterministic policy that is greedy with respect to Q

Repeat forever:

Generate an episode using any soft policy µ:

S0, A0, R1, . . . , ST

1, AT

1, RT , ST

−

−

2, . . . downto 0:

0
1

1, T
−
−
γG + Rt+1

G
←
W
←
For t = T
G
←
C(St, At)
Q(St, At)
π(St)
←
W 1
W
St)
µ(At
|
If W = 0 then ExitForLoop

←
←
argmaxa Q(St, a)

C(St, At) + W
Q(St, At) + W

←

C(St,At) [G

Q(St, At)]
(with ties broken arbitrarily)

−

Figure 5.10: An oﬀ-policy every-visit MC control algorithm, using weighted
importance sampling. The policy π converges to optimal at all encountered
states even though actions are selected according to a diﬀerent soft policy µ,
which may change between or even within episodes.

Importance Sampling on Truncated Re-

∗5.8
turns

So far our oﬀ-policy methods have formed importance-sampling ratios for
returns considered as unitary wholes. This is clearly the right thing for a
Monte Carlo method to do in the absence of discounting (i.e., if γ = 1),
but if γ < 1 then there may be something better. Consider the case where
episodes are long and γ is signiﬁcantly less than 1. For concreteness, say that
episodes last 100 steps and that γ = 0. The return from time 0 will then be
G0 = R1, and its importance sampling ratio will be a product of 100 factors,
S99)
π(A0
S99) . In ordinary importance sampling, the return will
|
µ(A0
|
be scaled by the entire product, but it is really only necessary to scale by the
ﬁrst factor, by π(A0
S99)
S99) are irrelevant
|
µ(A0
|
because after the ﬁrst reward the return has already been determined. These
later factors are all independent of the return and of expected value 1; they
do not change the expected update, but they add enormously to its variance.
In some cases they could even make the variance inﬁnite. Let us now consider
an idea for avoiding this large extraneous variance.

S0) . The other 99 factors π(A1

S1)
|
S1) · · ·
|

S1)
|
S1) · · ·
|

π(A99
µ(A99

π(A99
µ(A99

S0)
|
S0)
|

π(A1
µ(A1

µ(A1

S0)

|
|

∗5.8. IMPORTANCE SAMPLING ON TRUNCATED RETURNS

137

∈

The essence of the idea is to think of discounting as determining a proba-
bility of termination or, equivalently, a degree of partial termination. For any
[0, 1), we can think of the return G0 as partly terminating in one step,
γ
γ, producing a return of just the ﬁrst reward, R1, and as
to the degree 1
γ)γ, producing a return
partly terminating after two steps, to the degree (1
of R1 + R2, and so on. The latter degree corresponds to terminating on the
γ, and not having already terminated on the ﬁrst step, γ. The
second step, 1
γ)γ2, with the γ2 reﬂecting
degree of termination on the third step is thus (1
that termination did not occur on either of the ﬁrst two steps. The partial
returns here are called ﬂat partial returns:

−

−

−

−

¯Gh

t = Rt+1 + Rt+2 +

+ Rh,

· · ·

0

≤

t < h

T,

≤

where “ﬂat” denotes the absence of discounting, and “partial” denotes that
these returns do not extend all the way to termination but instead stop at
h, called the horizon (and T is the time of termination of the episode). The
conventional full return Gt can be viewed as a sum of ﬂat partial returns as
suggested above as follows:

= (1

−
+ (1
+ (1
...
+ (1
+ γT

Gt = Rt+1 + γRt+2 + γ2Rt+3 +

+ γT

t
−
−

1RT

· · ·

γ)Rt+1

γ)γ (Rt+1 + Rt+2)
γ)γ2 (Rt+1 + Rt+2 + Rt+3)

−
−

2 (Rt+1 + Rt+2 +
t
−

−

γ)γT
−
1 (Rt+1 + Rt+2 +
t
−
−
1

T

· · ·

· · ·
+ RT )

+ RT

1)

−

= γT

t
−
−

1 ¯GT

t + (1

−

γ)

−

t
γh
−
−

1 ¯Gh
t

(cid:88)h=t+1

Now we need to scale the ﬂat partial returns by an importance sampling
ratio that is similarly truncated. As Gh
t only involves rewards up to a horizon
h, we only need the ratio of the probabilities up to h. We deﬁne an ordinary
importance-sampling estimator, analogous to (5.4), as

γT (t)

t
−
−

1ρT (t)
t

V (s) =

T(s)

t
∈

(cid:16)

(cid:80)

¯GT (t)

t + (1
T(s)
|
|

γ)

−

T (t)
1
h=t+1 γh
−
−

1ρh
t
−
t

¯Gh
t

(cid:80)

, (5.8)

(cid:17)

and a weighted importance-sampling estimator, analogous to (5.5), as

V (s) =

(cid:80)

T(s)

t
∈

(cid:16)
T(s)
t
∈

(cid:16)

(cid:80)

γT (t)

t
−
−

1ρT (t)
t

¯GT (t)

t + (1

γT (t)

t
−
−

1ρT (t)

t + (1

γ)

−

γ)

−

T (t)
1
h=t+1 γh
−
−
T (t)
1
(cid:80)
t
h=t+1 γh
−
−
−

1ρh
t
−
t

¯Gh
t

. (5.9)

(cid:17)

1ρh
t

(cid:17)

(cid:80)

138

CHAPTER 5. MONTE CARLO METHODS

5.9 Summary

The Monte Carlo methods presented in this chapter learn value functions and
optimal policies from experience in the form of sample episodes. This gives
them at least three kinds of advantages over DP methods. First, they can be
used to learn optimal behavior directly from interaction with the environment,
with no model of the environment’s dynamics. Second, they can be used with
simulation or sample models. For surprisingly many applications it is easy to
simulate sample episodes even though it is diﬃcult to construct the kind of
explicit model of transition probabilities required by DP methods. Third, it
is easy and eﬃcient to focus Monte Carlo methods on a small subset of the
states. A region of special interest can be accurately evaluated without going
to the expense of accurately evaluating the rest of the state set (we explore
this further in Chapter 8).

A fourth advantage of Monte Carlo methods, which we discuss later in the
book, is that they may be less harmed by violations of the Markov property.
This is because they do not update their value estimates on the basis of the
value estimates of successor states. In other words, it is because they do not
bootstrap.

In designing Monte Carlo control methods we have followed the overall
schema of generalized policy iteration (GPI) introduced in Chapter 4. GPI
involves interacting processes of policy evaluation and policy improvement.
Monte Carlo methods provide an alternative policy evaluation process. Rather
than use a model to compute the value of each state, they simply average many
returns that start in the 
u move down to the root’s left child node (depth 1, left). In this case, it is a leaf

1 Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/.

176 

| 

Chapter 6: Decision Trees

node (i.e., it does not have any child nodes), so it does not ask any questions: simply
look  at  the  predicted  class  for  that  node,  and  the  Decision  Tree  predicts  that  your
flower is an Iris setosa (class=setosa).

Now suppose you find another flower, and this time the petal length is greater than
2.45 cm. You must move down to the root’s right child node (depth 1, right), which is
not  a  leaf  node,  so  the  node  asks  another  question:  is  the  petal  width  smaller  than
1.75 cm? If it is, then your flower is most likely an Iris versicolor (depth 2, left). If not,
it is likely an Iris virginica (depth 2, right). It’s really that simple.

One  of  the  many  qualities  of  Decision  Trees  is  that  they  require
very little data preparation. In fact, they don’t require feature scal‐
ing or centering at all.

A  node’s  samples  attribute  counts  how  many  training  instances  it  applies  to.  For
example,  100  training  instances  have  a  petal  length  greater  than  2.45  cm  (depth  1,
right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left). A
node’s value attribute tells you how many training instances of each class this node
applies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor,
and 45 Iris virginica. Finally, a node’s gini attribute measures its impurity: a node is
“pure”  (gini=0)  if  all  training  instances  it  applies  to  belong  to  the  same  class.  For
example, since the depth-1 left node applies only to Iris setosa training instances, it is
pure  and  its  gini  score  is  0.  Equation  6-1  shows  how  the  training  algorithm  com‐
putes the gini score Gi of the ith node. The depth-2 left node has a gini score equal to
1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168.

Equation 6-1. Gini impurity

n
Gi = 1 − ∑
k = 1

2

pi, k

In this equation:

• pi,k is the ratio of class k instances among the training instances in the ith node.

Scikit-Learn uses the CART algorithm, which produces only binary
trees:  nonleaf  nodes  always  have  two  children  (i.e.,  questions  only
have yes/no answers). However, other algorithms such as ID3 can
produce  Decision  Trees  with  nodes  that  have  more  than  two
children.

Making Predictions 

| 

177

Figure 6-2 shows this Decision Tree’s decision boundaries. The thick vertical line rep‐
resents  the  decision  boundary  of  the  root  node  (depth  0):  petal  length  =  2.45  cm.
Since the lefthand area is pure (only Iris setosa), it cannot be split any further. How‐
ever, the righthand area is impure, so the depth-1 right node splits it at petal width =
1.75 cm (represented by the dashed line). Since max_depth was set to 2, the Decision
Tree stops right there. If you set max_depth to 3, then the two depth-2 nodes would
each add another decision boundary (represented by the dotted lines).

Figure 6-2. Decision Tree decision boundaries

Model Interpretation: White Box Versus Black Box
Decision Trees are intuitive, and their decisions are easy to interpret. Such models are
often called white box models. In contrast, as we will see, Random Forests or neural
networks  are  generally  considered  black  box  models.  They  make  great  predictions,
and you can easily check the calculations that they performed to make these predic‐
tions; nevertheless, it is usually hard to explain in simple terms why the predictions
were made. For example, if a neural network says that a particular person appears on
a picture, it is hard to know what contributed to this prediction: did the model recog‐
nize  that  person’s  eyes?  Their  mouth?  Their  nose?  Their  shoes?  Or  even  the  couch
that they were sitting on? Conversely, Decision Trees provide nice, simple classifica‐
tion rules that can even be applied manually if need be (e.g., for flower classification).

Estimating Class Probabilities
A Decision Tree can also estimate the probability that an instance belongs to a partic‐
ular class k. First it traverses the tree to find the leaf node for this instance, and then it
returns  the  ratio  of  training  instances  of  class  k  in  this  node.  For  example,  suppose
you  have  found  a  flower  whose  petals  are  5  cm  long  and  1.5  cm  wide.  The

178 

| 

Chapter 6: Decision Trees

corresponding leaf node is the depth-2 left node, so the Decision Tree should output
the following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54),
and 9.3% for Iris virginica (5/54). And if you ask it to predict the class, it should out‐
put Iris versicolor (class 1) because it has the highest probability. Let’s check this:

>>> tree_clf.predict_proba([[5, 1.5]])
array([[0.        , 0.90740741, 0.09259259]])
>>> tree_clf.predict([[5, 1.5]])
array([1])

Perfect!  Notice  that  the  estimated  probabilities  would  be  identical  anywhere  else  in
the bottom-right rectangle of Figure 6-2—for example, if the petals were 6 cm long
and 1.5 cm wide (even though it seems obvious that it would most likely be an Iris
virginica in this case).

The CART Training Algorithm
Scikit-Learn  uses  the  Classification  and  Regression  Tree  (CART)  algorithm  to  train
Decision Trees (also called “growing” trees). The algorithm works by first splitting the
training  set  into  two  subsets  using  a  single  feature  k  and  a  threshold  tk  (e.g.,  “petal
length ≤ 2.45 cm”). How does it choose k and tk? It searches for the pair (k, tk) that
produces the purest subsets (weighted by their size). Equation 6-2 gives the cost func‐
tion that the algorithm tries to minimize.

Equation 6-2. CART cost function for classification

J k, tk =

mleft
m

Gleft +

mright
m

Gright

where

Gleft/right measures the impurity of the left/right subset,
mleft/right is the number of instances in the left/right subset.

Once  the  CART  algorithm  has  successfully  split  the  training  set  in  two,  it  splits  the
subsets  using  the  same  logic,  then  the  sub-subsets,  and  so  on,  recursively.  It  stops
recursing once it reaches the maximum depth (defined by the max_depth hyperpara‐
meter), or if it cannot find a split that will reduce impurity. A few other hyperparame‐
conditions
ters 
in 
(described 
(min_samples_split, 
and
max_leaf_nodes).

a  moment) 
min_samples_leaf, 

min_weight_fraction_leaf, 

additional 

stopping 

control 

The CART Training Algorithm 

| 

179

As you can see, the CART algorithm is a greedy algorithm: it greed‐
ily searches for an optimum split at the top level, then repeats the
process at each subsequent level. It does not check whether or not
the  split  will  lead  to  the  lowest  possible  impurity  several  levels
down. A greedy algorithm often produces a solution that’s reasona‐
bly good but not guaranteed to be optimal.

Unfortunately,  finding  the  optimal  tree  is  known  to  be  an  NP-
Complete  problem:2  it  requires  O(exp(m))  time,  making  the  prob‐
lem  intractable  even  for  small  training  sets.  This  is  why  we  must
settle for a “reasonably good” solution.

Computational Complexity
Making  predictions  requires  traversing  the  Decision  Tree  from  the  root  to  a  leaf.
Decision Trees generally are approximately balanced, so traversing the Decision Tree
requires  going  through  roughly  O(log2(m))  nodes.3  Since  each  node  only  requires
checking  the  value  of  one  feature,  the  overall  prediction  complexity  is  O(log2(m)),
independent of the number of features. So predictions are very fast, even when deal‐
ing with large training sets.

The  training  algorithm  compares  all  features  (or  less  if  max_features  is  set)  on  all
samples at each node. Comparing all features on all samples at each node results in a
training complexity of O(n × m log2(m)). For small training sets (less than a few thou‐
sand  instances),  Scikit-Learn  can  speed  up  training  by  presorting  the  data  (set  pre
sort=True), but doing that slows down training considerably for larger training sets.

Gini Impurity or Entropy?
By default, the Gini impurity measure is used, but you can select the entropy impurity
measure instead by setting the criterion hyperparameter to "entropy". The concept
of  entropy  originated  in  thermodynamics  as  a  measure  of  molecular  disorder:
entropy  approaches  zero  when  molecules  are  still  and  well  ordered.  Entropy  later
spread to a wide variety of domains, including Shannon’s information theory, where it
measures  the  average  information  content  of  a  message:4  entropy  is  zero  when  all
messages  are  identical.  In  Machine  Learning,  entropy  is  frequently  used  as  an

2 P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can
be verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced
in polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐
tion is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be
found for any NP-Complete problem (except perhaps on a quantum computer).

3 log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).
4 A reduction of entropy is often called an information gain.

180 

| 

Chapter 6: Decision Trees

impurity measure: a set’s entropy is zero when it contains instances of only one class.
Equation  6-3  shows  the  definition  of  the  entropy  of  the  ith  node.  For  example,  the
depth-2 left node in Figure 6-1 has an entropy equal to –(49/54) log2 (49/54) – (5/54)
log2 (5/54) ≈ 0.445.

Equation 6-3. Entropy

n
Hi = − ∑
k = 1

p
i, k

≠ 0

pi, k log2 pi, k

So, should you use Gini impurity or entropy? The truth is, most of the time it does
not make a big difference: they lead to similar trees. Gini impurity is slightly faster to
compute,  so  it  is  a  good  default.  However,  when  they  differ,  Gini  impurity  tends  to
isolate the most frequent class in its own branch of the tree, while entropy tends to
produce slightly more balanced trees.5

Regularization Hyperparameters
Decision Trees make very few assumptions about the training data (as opposed to lin‐
ear models, which assume that the data is linear, for example). If left unconstrained,
the tree structure will adapt itself to the training data, fitting it very closely—indeed,
most  likely  overfitting  it.  Such  a  model  is  often  called  a  nonparametric  model,  not
because it does not have any parameters (it often has a lot) but because the number of
parameters is not determined prior to training, so the model structure is free to stick
closely to the data. In contrast, a parametric model, such as a linear model, has a pre‐
determined number of parameters, so its degree of freedom is limited, reducing the
risk of overfitting (but increasing the risk of underfitting).

To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom
during training. As you know by now, this is called regularization. The regularization
hyperparameters depend on the algorithm used, but generally you can at least restrict
the  maximum  depth  of  the  Decision  Tree.  In  Scikit-Learn,  this  is  controlled  by  the
max_depth  hyperparameter  (the  default  value  is  None,  which  means  unlimited).
Reducing max_depth will regularize the model and thus reduce the risk of overfitting.

The DecisionTreeClassifier class has a few other parameters that similarly restrict
the shape of the Decision Tree: min_samples_split (the minimum number of sam‐
ples a node must have before it can be split), min_samples_leaf (the minimum num‐
ber  of  samples  a  leaf  node  must  have),  min_weight_fraction_leaf  (same  as

5 See Sebastian Raschka’s interesting analysis for more details.

Regularization Hyperparameters 

| 

181

min_samples_leaf  but  expressed  as  a  fraction  of  the  total  number  of  weighted
instances), max_leaf_nodes (the maximum number of leaf nodes), and max_features
(the  maximum  number  of  features  that  are  evaluated  for  splitting  at  each  node).
Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize
the model.

Other algorithms work by first training the Decision Tree without
restrictions,  then  pruning  (deleting)  unnecessary  nodes.  A  node
whose  children  are  all  leaf  nodes  is  considered  unnecessary  if  the
purity improvement it provides is not statistically significant. Stan‐
dard statistical tests, such as the χ2 test (chi-squared test), are used
to  estimate  the  probability  that  the  improvement  is  purely  the
result of chance (which is called the null hypothesis). If this proba‐
bility, called the p-value, is higher than a given threshold (typically
5%, controlled by a hyperparameter), then the node is considered
unnecessary  and  its  children  are  deleted.  The  pruning  continues
until all unnecessary nodes have been pruned.

Figure  6-3  shows  two  Decision  Trees  trained  on  the  moons  dataset  (introduced  in
Chapter 5). On the left the Decision Tree is trained with the default hyperparameters
(i.e.,  no  restrictions),  and  on  the  right  it’s  trained  with  min_samples_leaf=4.  It  is
quite obvious that the model on the left is overfitting, and the model on the right will
probably generalize better.

Figure 6-3. Regularization using min_samples_leaf

182 

| 

Chapter 6: Decision Trees

Regression
Decision  Trees  are  also  capable  of  performing  regression  tasks.  Let’s  build  a  regres‐
sion  tree  using  Scikit-Learn’s  DecisionTreeRegressor  class,  training  it  on  a  noisy
quadratic dataset with max_depth=2:

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=2)
tree_reg.fit(X, y)

The resulting tree is represented in Figure 6-4.

Figure 6-4. A Decision Tree for regression

This tree looks very similar to the classification tree you built earlier. The main differ‐
ence is that instead of predicting a class in each node, it predicts a value. For example,
suppose you want to make a prediction for a new instance with x1 = 0.6. You traverse
the  tree  starting  at  the  root,  and  you  eventually  reach  the  leaf  node  that  predicts
value=0.111. This prediction is the average target value of the 110 training instances
associated with this leaf node, and it results in a mean squared error equal to 0.015
over these 110 instances.

This  model’s  predictions  are  represented  on  the  left  in  Figure  6-5.  If  you  set
max_depth=3, you get the predictions represented on the right. Notice how the pre‐
dicted value for each region is always the average target value of the instances in that
region. The algorithm splits each region in a way that makes most training instances
as close as possible to that predicted value.

Regression 

| 

183

Figure 6-5. Predictions of two Decision Tree regression models

The CART algorithm works mostly the same way as earlier, except that instead of try‐
ing to split the training set in a way that minimizes impurity, it now tries to split the
training set in a way that minimizes the MSE. Equation 6-4 shows the cost function
that the algorithm tries to minimize.

Equatio
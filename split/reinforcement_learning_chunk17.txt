 iteration, which includes policy it-
eration and value iteration as special cases. An analysis showing how
value iteration can be made to ﬁnd an optimal policy in ﬁnite time is
given by Bertsekas (1987).

Iterative policy evaluation is an example of a classical successive ap-
proximation algorithm for solving a system of linear equations. The
version of the algorithm that uses two arrays, one holding the old val-
ues while the other is updated, is often called a Jacobi-style algorithm,
after Jacobi’s classical use of this method. It is also sometimes called
a synchronous algorithm because it can be performed in parallel, with
separate processors simultaneously updating the values of individual
states using input from other processors. The second array is needed
to simulate this parallel computation sequentially. The in-place version
of the algorithm is often called a Gauss–Seidel-style algorithm after
the classical Gauss–Seidel algorithm for solving systems of linear equa-
tions. In addition to iterative policy evaluation, other DP algorithms
can be implemented in these diﬀerent versions. Bertsekas and Tsit-
siklis (1989) provide excellent coverage of these variations and their
performance diﬀerences.

4.5

Asynchronous DP algorithms are due to Bertsekas (1982, 1983), who
also called them distributed DP algorithms. The original motivation for
asynchronous DP was its implementation on a multiprocessor system
with communication delays between processors and no global synchro-
nizing clock. These algorithms are extensively discussed by Bertsekas
and Tsitsiklis (1989). Jacobi-style and Gauss–Seidel-style DP algo-
rithms are special cases of the asynchronous version. Williams and
Baird (1990) presented DP algorithms that are asynchronous at a ﬁner
grain than the ones we have discussed: the backup operations them-
selves are broken into steps that can be performed asynchronously.

4.7

This section, written with the help of Michael Littman, is based on
Littman, Dean, and Kaelbling (1995).

110

CHAPTER 4. DYNAMIC PROGRAMMING

Exercises

Exercise 4.1 If π is the equiprobable random policy, what is qπ(11, down)?
What is qπ(7, down)?

Exercise 4.2 Suppose a new state 15 is added to the gridworld just below
state 13, and its actions, left, up, right, and down, take the agent to states
12, 13, 14, and 15, respectively. Assume that the transitions from the original
states are unchanged. What, then, is vπ(15) for the equiprobable random
policy? Now suppose the dynamics of state 13 are also changed, such that
action down from state 13 takes the agent to the new state 15. What is vπ(15)
for the equiprobable random policy in this case?

Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5) for
the action-value function qπ and its successive approximation by a sequence of
functions q0, q1, q2, . . . ?

Exercise 4.4 In some undiscounted episodic tasks there may be policies
for which eventual termination is not guaranteed. For example, in the grid
problem above it is possible to go back and forth between two states forever.
In a task that is otherwise perfectly sensible, vπ(s) may be negative inﬁnity
for some policies and states, in which case the algorithm for iterative policy
evaluation given in Figure 4.1 will not terminate. As a purely practical matter,
how might we amend this algorithm to assure termination even in this case?
Assume that eventual termination is guaranteed under the optimal policy.

In addition, Jack has limited parking space at each location.

Exercise 4.5 (programming) Write a program for policy iteration and
re-solve Jack’s car rental problem with the following changes. One of Jack’s
employees at the ﬁrst location rides a bus home each night and lives near
the second location. She is happy to shuttle one car to the second location
for free. Each additional car still costs $2, as do all cars moved in the other
direction.
If
more than 10 cars are kept overnight at a location (after any moving of cars),
then an additional cost of $4 must be incurred to use a second parking lot
(independent of how many cars are kept there). These sorts of nonlinearities
and arbitrary dynamics often occur in real problems and cannot easily be
handled by optimization methods other than dynamic programming. To check
your program, ﬁrst replicate the results given for the original problem. If your
computer is too slow for the full problem, cut all the numbers of cars in half.

Exercise 4.6 How would policy iteration be deﬁned for action values? Give
a complete algorithm for computing q
, analogous to Figure 4.3 for computing
v
. Please pay special attention to this exercise, because the ideas involved
∗
will be used throughout the rest of the book.

∗

4.8. SUMMARY

111

Exercise 4.7 Suppose you are restricted to considering only policies that are
(cid:15)-soft, meaning that the probability of selecting each action in each state, s,
A(s)
is at least (cid:15)/
. Describe qualitatively the changes that would be required
|
|
in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm
for v
∗

(Figure 4.3).

Exercise 4.8 Why does the optimal policy for the gambler’s problem have
such a curious form? In particular, for capital of 50 it bets it all on one ﬂip,
but for capital of 51 it does not. Why is this a good policy?

Exercise 4.9 (programming) Implement value iteration for the gambler’s
problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may
ﬁnd it convenient to introduce two dummy states corresponding to termination
with capital of 0 and 100, giving them values of 0 and 1 respectively. Show
your results graphically, as in Figure 4.6. Are your results stable as θ

0?

→

Exercise 4.10 What is the analog of the value iteration backup (4.10) for
action values, qk+1(s, a)?

112

CHAPTER 4. DYNAMIC PROGRAMMING

Chapter 5

Monte Carlo Methods

In this chapter we consider our ﬁrst learning methods for estimating value func-
tions and discovering optimal policies. Unlike the previous chapter, here we
do not assume complete knowledge of the environment. Monte Carlo methods
require only experience—sample sequences of states, actions, and rewards from
actual or simulated interaction with an environment. Learning from actual ex-
perience is striking because it requires no prior knowledge of the environment’s
dynamics, yet can still attain optimal behavior. Learning from simulated ex-
perience is also powerful. Although a model is required, the model need only
generate sample transitions, not the complete probability distributions of all
possible transitions that is required for dynamic programming (DP). In sur-
prisingly many cases it is easy to generate experience sampled according to
the desired probability distributions, but infeasible to obtain the distributions
in explicit form.

Monte Carlo methods are ways of solving the reinforcement learning prob-
lem based on averaging sample returns. To ensure that well-deﬁned returns are
available, here we deﬁne Monte Carlo methods only for episodic tasks. That is,
we assume experience is divided into episodes, and that all episodes eventually
terminate no matter what actions are selected. Only on the completion of an
episode are value estimates and policies changed. Monte Carlo methods can
thus be incremental in an episode-by-episode sense, but not in a step-by-step
(online) sense. The term “Monte Carlo” is often used more broadly for any
estimation method whose operation involves a signiﬁcant random component.
Here we use it speciﬁcally for methods based on averaging complete returns
(as opposed to methods that learn from partial returns, considered in the next
chapter).

Monte Carlo methods sample and average returns for each state–action pair
much like the bandit methods we explored in Chapter 2 sample and average

113

114

CHAPTER 5. MONTE CARLO METHODS

rewards for each action. The main diﬀerence is that now there are multiple
states, each acting like a diﬀerent bandit problem (like an associative-search
or contextual bandit) and that the diﬀerent bandit problems are interrelated.
That is, the return after taking an action in one state depends on the actions
taken in later states in the same episode. Because all the action selections
are undergoing learning, the problem becomes nonstationary from the point
of view of the earlier state.

To handle the nonstationarity, we adapt the idea of general policy iteration
(GPI) developed in Chapter 4 for DP. Whereas there we computed value func-
tions from knowledge of the MDP, here we learn value functions from sample
returns with the MDP. The value functions and corresponding policies still
interact to attain optimality in essentially the same way (GPI). As in the DP
chapter, ﬁrst we consider the prediction problem (the computation of vπ and
qπ for a ﬁxed arbitrary policy π) then policy improvement, and, ﬁnally, the
control problem and its solution by GPI. Each of these ideas taken from DP is
extended to the Monte Carlo case in which only sample experience is available.

5.1 Monte Carlo Prediction

We begin by considering Monte Carlo methods for learning the state-value
function for a given policy. Recall that the value of a state is the expected
return—expected cumulative future discounted reward—starting from that
state. An obvious way to estimate it from experience, then, is simply to
average the returns observed after visits to that state. As more returns are
observed, the average should converge to the expected value. This idea under-
lies all Monte Carlo methods.

In particular, suppose we wish to estimate vπ(s), the value of a state s
under policy π, given a set of episodes obtained by following π and passing
through s. Each occurrence of state s in an episode is called a visit to s. Of
course, s may be visited multiple times in the same episode; let us call the ﬁrst
time it is visited in an episode the ﬁrst visit to s. The ﬁrst-visit MC method
estimates vπ(s) as the average of the returns following ﬁrst visits to s, whereas
the every-visit MC method averages the returns following all visits to s. These
two Monte Carlo (MC) methods are very similar but have slightly diﬀerent
theoretical properties. First-visit MC has been most widely studied, dating
back to the 1940s, and is the one we focus on in this chapter. Every-visit
MC extends more naturally to function approximation and eligibility traces,
as discussed in Chapters 9 and 7. First-visit MC is shown in procedural form
in Figure 5.1.

5.1. MONTE CARLO PREDICTION

115

Initialize:
π
V
Returns(s)

←
←

policy to be evaluated
an arbitrary state-value function
an empty list, for all s

←

S

∈

Repeat forever:

Generate an episode using π
For each state s appearing in the episode:

return following the ﬁrst occurrence of s

←

G
Append G to Returns(s)
V (s)

average(Returns(s))

←

Figure 5.1: The ﬁrst-visit MC method for estimating vπ. Note that we use a
capital letter V for the approximate value function because, after initialization,
it soon becomes a random variable.

Both ﬁrst-visit MC and every-visit MC converge to vπ(s) as the number
of visits (or ﬁrst visits) to s goes to inﬁnity. This is easy to see for the
case of ﬁrst-visit MC. In this case each return is an independent, identically
distributed estimate of vπ(s) with ﬁnite variance. By the law of large numbers
the sequence of averages of these estimates converges to their expected value.
Each average is itself an unbiased estimate, and the standard deviation of its
error falls as 1/√n, where n is the number of returns averaged. Every-visit
MC is less straightforward, but its estimates also converge asymptotically to
vπ(s) (Singh and Sutton, 1996).

The use of Monte Carlo methods is best illustrated through an example.

Example 5.1: Blackjack The object of the popular casino card game of
blackjack is to obtain cards the sum of whose numerical values is as great as
possible without exceeding 21. All face cards count as 10, and an ace can count
either as 1 or as 11. We consider the version in which each player competes
independently against the dealer. The game begins with two cards dealt to
both dealer and player. One of the dealer’s cards is face up and the other is
face down. If the player has 21 immediately (an ace and a 10-card), it is called
a natural. He then wins unless the dealer also has a natural, in which case the
game is a draw. If the player does not have a natural, then he can request
additional cards, one by one (hits), until he either stops (sticks) or exceeds 21
(goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer’s
turn. The dealer hits or sticks according to a ﬁxed strategy without choice:
he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes
bust, then the player wins; otherwise, the outcome—win, lose, or draw—is
determined by whose ﬁnal sum is closer to 21.

116

CHAPTER 5. MONTE CARLO METHODS

Figure 5.2: Approximate state-value functions for the blackjack policy that
sticks only on 20 or 21, computed by Monte Carlo policy evaluation.

−

Playing blackjack is naturally formulated as an episodic ﬁnite MDP. Each
game of blackjack is an episode. Rewards of +1,
1, and 0 are given for
winning, losing, and drawing, respectively. All rewards within a game are
zero, and we do not discount (γ = 1); therefore these terminal rewards are
also the returns. The player’s actions are to hit or to stick. The states depend
on the player’s cards and the dealer’s showing card. We assume that cards
are dealt from an inﬁnite deck (i.e., with replacement) so that there is no
advantage to keeping track of the cards already dealt. If the player holds an
ace that he could count as 11 without going bust, then the ace is said to be
usable. In this case it is always counted as 11 because counting it as 1 would
make the sum 11 or less, in which case there is no decision to be made because,
obviously, the player should always hit. Thus, the player makes decisions on
the basis of three variables: his current sum (12–21), the dealer’s one showing
card (ace–10), and whether or not he holds a usable ace. This makes for a
total of 200 states.

Consider the policy that sticks if the player’s sum is 20 or 21, and other-
wise hits. To ﬁnd the state-value function for this policy by a Monte Carlo
approach, one simulates many blackjack games using the policy and averages
the returns following each state. Note that in this task the same state never
recurs within one episode, so there is no diﬀerence between ﬁrst-visit and
every-visit MC methods. In this way, we obtained the estimates of the state-
value function shown in Figure 5.2. The estimates for states with a usable ace
are less certain and less regular because these states are less common. In any
event, after 500,000 games the value function is very well approximated.

+1!1ADealer showing1012Player sum21After 500,000 episodesAfter 10,000 episodesUsableaceNousableace5.1. MONTE CARLO PREDICTION

117

Although we have complete knowledge of the environment in this task, it
would not be easy to apply DP methods to compute the value function. DP
methods require the distribution of next events—in particular, they require
the quantities p(s(cid:48), r
s, a)—and it is not easy to determine these for blackjack.
|
For example, suppose the player’s sum is 14 and he chooses to stick. What is
his expected reward as a function of the dealer’s showing card? All of these
expected rewards and t
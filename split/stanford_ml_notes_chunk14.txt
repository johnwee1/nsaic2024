t,â€ which is a framework and set of
assumptions under which numerous results on learning theory were proved. Of these, the
assumption of training and testing on the same distribution, and the assumption of the
independently drawn training examples, were the most important.

128

focus on in these notes. (Algorithms such as logistic regression can also be
viewed as approximations to empirical risk minimization.)

In our study of learning theory, it will be useful to abstract away from
the speciï¬c parameterization of hypotheses and from issues such as whether
weâ€™re using a linear classiï¬er. We deï¬ne the hypothesis class H used by a
learning algorithm to be the set of all classiï¬ers considered by it. For linear
classiï¬cation, H = {hÎ¸ : hÎ¸(x) = 1{Î¸T x â‰¥ 0}, Î¸ âˆˆ Rd+1} is thus the set of
all classiï¬ers over X (the domain of the inputs) where the decision boundary
is linear. More broadly, if we were studying, say, neural networks, then we
could let H be the set of all classiï¬ers representable by some neural network
architecture.

Empirical risk minimization can now be thought of as a minimization over
the class of functions H, in which the learning algorithm picks the hypothesis:

Ë†h = arg min
hâˆˆH

Ë†Îµ(h)

8.3.2 The case of ï¬nite H

Letâ€™s start by considering a learning problem in which we have a ï¬nite hy-
pothesis class H = {h1, . . . , hk} consisting of k hypotheses. Thus, H is just a
set of k functions mapping from X to {0, 1}, and empirical risk minimization
selects Ë†h to be whichever of these k functions has the smallest training error.
We would like to give guarantees on the generalization error of Ë†h. Our
strategy for doing so will be in two parts: First, we will show that Ë†Îµ(h) is a
reliable estimate of Îµ(h) for all h. Second, we will show that this implies an
upper-bound on the generalization error of Ë†h.

Take any one, ï¬xed, hi âˆˆ H. Consider a Bernoulli random variable Z
whose distribution is deï¬ned as follows. Weâ€™re going to sample (x, y) âˆ¼ D.
I.e., weâ€™re going to draw one example,
Then, we set Z = 1{hi(x) (cid:54)= y}.
and let Z indicate whether hi misclassiï¬es it. Similarly, we also deï¬ne Zj =
1{hi(x(j)) (cid:54)= y(j)}. Since our training set was drawn iid from D, Z and the
Zjâ€™s have the same distribution.

We see that the misclassiï¬cation probability on a randomly drawn
exampleâ€”that is, Îµ(h)â€”is exactly the expected value of Z (and Zj). More-
over, the training error can be written

Ë†Îµ(hi) =

1
n

n
(cid:88)

j=1

Zj.

Thus, Ë†Îµ(hi) is exactly the mean of the n random variables Zj that are drawn
iid from a Bernoulli distribution with mean Îµ(hi). Hence, we can apply the

129

Hoeï¬€ding inequality, and obtain

P (|Îµ(hi) âˆ’ Ë†Îµ(hi)| > Î³) â‰¤ 2 exp(âˆ’2Î³2n).

This shows that, for our particular hi, training error will be close to
generalization error with high probability, assuming n is large. But we donâ€™t
just want to guarantee that Îµ(hi) will be close to Ë†Îµ(hi) (with high probability)
for just only one particular hi. We want to prove that this will be true
simultaneously for all h âˆˆ H. To do so, let Ai denote the event that |Îµ(hi) âˆ’
Ë†Îµ(hi)| > Î³. Weâ€™ve already shown that, for any particular Ai, it holds true
that P (Ai) â‰¤ 2 exp(âˆ’2Î³2n). Thus, using the union bound, we have that

P (âˆƒ h âˆˆ H.|Îµ(hi) âˆ’ Ë†Îµ(hi)| > Î³) = P (A1 âˆª Â· Â· Â· âˆª Ak)

â‰¤

â‰¤

k
(cid:88)

i=1
k
(cid:88)

i=1

P (Ai)

2 exp(âˆ’2Î³2n)

= 2k exp(âˆ’2Î³2n)

If we subtract both sides from 1, we ï¬nd that

P (Â¬âˆƒ h âˆˆ H.|Îµ(hi) âˆ’ Ë†Îµ(hi)| > Î³) = P (âˆ€h âˆˆ H.|Îµ(hi) âˆ’ Ë†Îµ(hi)| â‰¤ Î³)

â‰¥ 1 âˆ’ 2k exp(âˆ’2Î³2n)

(The â€œÂ¬â€ symbol means â€œnot.â€)
So, with probability at least 1 âˆ’
2k exp(âˆ’2Î³2n), we have that Îµ(h) will be within Î³ of Ë†Îµ(h) for all h âˆˆ H.
This is called a uniform convergence result, because this is a bound that
holds simultaneously for all (as opposed to just one) h âˆˆ H.

In the discussion above, what we did was, for particular values of n and
Î³, give a bound on the probability that for some h âˆˆ H, |Îµ(h) âˆ’ Ë†Îµ(h)| > Î³.
There are three quantities of interest here: n, Î³, and the probability of error;
we can bound either one in terms of the other two.

For instance, we can ask the following question: Given Î³ and some Î´ > 0,
how large must n be before we can guarantee that with probability at least
1 âˆ’ Î´, training error will be within Î³ of generalization error? By setting
Î´ = 2k exp(âˆ’2Î³2n) and solving for n, [you should convince yourself this is
the right thing to do!], we ï¬nd that if

n â‰¥

1
2Î³2 log

2k
Î´

,

130

then with probability at least 1 âˆ’ Î´, we have that |Îµ(h) âˆ’ Ë†Îµ(h)| â‰¤ Î³ for all
h âˆˆ H. (Equivalently, this shows that the probability that |Îµ(h) âˆ’ Ë†Îµ(h)| > Î³
for some h âˆˆ H is at most Î´.) This bound tells us how many training
examples we need in order make a guarantee. The training set size n that
a certain method or algorithm requires in order to achieve a certain level of
performance is also called the algorithmâ€™s sample complexity.

The key property of the bound above is that the number of training
examples needed to make this guarantee is only logarithmic in k, the number
of hypotheses in H. This will be important later.

Similarly, we can also hold n and Î´ ï¬xed and solve for Î³ in the previous
equation, and show [again, convince yourself that this is right!] that with
probability 1 âˆ’ Î´, we have that for all h âˆˆ H,

|Ë†Îµ(h) âˆ’ Îµ(h)| â‰¤

(cid:114)

1
2n

log

2k
Î´

.

Now, letâ€™s assume that uniform convergence holds, i.e., that |Îµ(h)âˆ’Ë†Îµ(h)| â‰¤
Î³ for all h âˆˆ H. What can we prove about the generalization of our learning
algorithm that picked Ë†h = arg minhâˆˆH Ë†Îµ(h)?

Deï¬ne hâˆ— = arg minhâˆˆH Îµ(h) to be the best possible hypothesis in H. Note
that hâˆ— is the best that we could possibly do given that we are using H, so
it makes sense to compare our performance to that of hâˆ—. We have:

Îµ(Ë†h) â‰¤ Ë†Îµ(Ë†h) + Î³
â‰¤ Ë†Îµ(hâˆ—) + Î³
â‰¤ Îµ(hâˆ—) + 2Î³

The ï¬rst line used the fact that |Îµ(Ë†h)âˆ’ Ë†Îµ(Ë†h)| â‰¤ Î³ (by our uniform convergence
assumption). The second used the fact that Ë†h was chosen to minimize Ë†Îµ(h),
and hence Ë†Îµ(Ë†h) â‰¤ Ë†Îµ(h) for all h, and in particular Ë†Îµ(Ë†h) â‰¤ Ë†Îµ(hâˆ—). The third
line used the uniform convergence assumption again, to show that Ë†Îµ(hâˆ—) â‰¤
Îµ(hâˆ—) + Î³. So, what weâ€™ve shown is the following: If uniform convergence
occurs, then the generalization error of Ë†h is at most 2Î³ worse than the best
possible hypothesis in H!

Letâ€™s put all this together into a theorem.

Theorem. Let |H| = k, and let any n, Î´ be ï¬xed. Then with probability at
least 1 âˆ’ Î´, we have that

(cid:18)

Îµ(Ë†h) â‰¤

(cid:19)

(cid:114)

min
hâˆˆH

Îµ(h)

+ 2

1
2n

log

2k
Î´

.

131

This is proved by letting Î³ equal the

Â· term, using our previous argu-
ment that uniform convergence occurs with probability at least 1 âˆ’ Î´, and
then noting that uniform convergence implies Îµ(h) is at most 2Î³ higher than
Îµ(hâˆ—) = minhâˆˆH Îµ(h) (as we showed previously).

âˆš

This also quantiï¬es what we were saying previously saying about the
bias/variance tradeoï¬€ in model selection. Speciï¬cally, suppose we have some
hypothesis class H, and are considering switching to some much larger hy-
pothesis class H(cid:48) âŠ‡ H.
If we switch to H(cid:48), then the ï¬rst term minh Îµ(h)
can only decrease (since weâ€™d then be taking a min over a larger set of func-
tions). Hence, by learning using a larger hypothesis class, our â€œbiasâ€ can
Â· term would also
only decrease. However, if k increases, then the second 2
increase. This increase corresponds to our â€œvarianceâ€ increasing when we use
a larger hypothesis class.

âˆš

By holding Î³ and Î´ ï¬xed and solving for n like we did before, we can also

obtain the following sample complexity bound:
Corollary. Let |H| = k, and let any Î´, Î³ be ï¬xed. Then for Îµ(Ë†h) â‰¤
minhâˆˆH Îµ(h) + 2Î³ to hold with probability at least 1 âˆ’ Î´, it suï¬ƒces that

n â‰¥

2k
Î´

1
2Î³2 log
(cid:18) 1
Î³2 log

(cid:19)

,

k
Î´

= O

8.3.3 The case of inï¬nite H

We have proved some useful theorems for the case of ï¬nite hypothesis classes.
But many hypothesis classes, including any parameterized by real numbers
(as in linear classiï¬cation) actually contain an inï¬nite number of functions.
Can we prove similar results for this setting?

Letâ€™s start by going through something that is not the â€œrightâ€ argument.
Better and more general arguments exist, but this will be useful for honing
our intuitions about the domain.

Suppose we have an H that is parameterized by d real numbers. Since we
are using a computer to represent real numbers, and IEEE double-precision
ï¬‚oating point (doubleâ€™s in C) uses 64 bits to represent a ï¬‚oating point num-
ber, this means that our learning algorithm, assuming weâ€™re using double-
precision ï¬‚oating point, is parameterized by 64d bits. Thus, our hypothesis
class really consists of at most k = 264d diï¬€erent hypotheses. From the Corol-
lary at the end of the previous section, we therefore ï¬nd that, to guarantee

132

(cid:17)

(cid:16) 1
Î³2 log 264d

Îµ(Ë†h) â‰¤ Îµ(hâˆ—) + 2Î³, with to hold with probability at least 1 âˆ’ Î´, it suï¬ƒces that
= OÎ³,Î´(d). (The Î³, Î´ subscripts indicate
n â‰¥ O
that the last big-O is hiding constants that may depend on Î³ and Î´.) Thus,
the number of training examples needed is at most linear in the parameters
of the model.

(cid:16) d
Î³2 log 1

= O

(cid:17)

Î´

Î´

The fact that we relied on 64-bit ï¬‚oating point makes this argument not
entirely satisfying, but the conclusion is nonetheless roughly correct: If what
we try to do is minimize training error, then in order to learn â€œwellâ€ using a
hypothesis class that has d parameters, generally weâ€™re going to need on the
order of a linear number of training examples in d.

(At this point, itâ€™s worth noting that these results were proved for an al-
gorithm that uses empirical risk minimization. Thus, while the linear depen-
dence of sample complexity on d does generally hold for most discriminative
learning algorithms that try to minimize training error or some approxima-
tion to training error, these conclusions do not always apply as readily to
discriminative learning algorithms. Giving good theoretical guarantees on
many non-ERM learning algorithms is still an area of active research.)

The other part of our previous argument thatâ€™s slightly unsatisfying is
that it relies on the parameterization of H. Intuitively, this doesnâ€™t seem like
it should matter: We had written the class of linear classiï¬ers as hÎ¸(x) =
1{Î¸0 + Î¸1x1 + Â· Â· Â· Î¸dxd â‰¥ 0}, with n + 1 parameters Î¸0, . . . , Î¸d. But it could
also be written hu,v(x) = 1{(u2
d)xd â‰¥ 0}
with 2d + 2 parameters ui, vi. Yet, both of these are just deï¬ning the same
H: The set of linear classiï¬ers in d dimensions.

1)x1 + Â· Â· Â· (u2

0) + (u2

d âˆ’ v2

0 âˆ’ v2

1 âˆ’ v2

To derive a more satisfying argument, letâ€™s deï¬ne a few more things.
Given a set S = {x(i), . . . , x(D)} (no relation to the training set) of points
x(i) âˆˆ X , we say that H shatters S if H can realize any labeling on S.
I.e., if for any set of labels {y(1), . . . , y(D)}, there exists some h âˆˆ H so that
h(x(i)) = y(i) for all i = 1, . . . D.

Given a hypothesis class H, we then deï¬ne its Vapnik-Chervonenkis
dimension, written VC(H), to be the size of the largest set that is shattered
by H. (If H can shatter arbitrarily large sets, then VC(H) = âˆž.)

For instance, consider the following set of three points:

133

Can the set H of linear classiï¬ers in two dimensions (h(x) = 1{Î¸0 +Î¸1x1 +
Î¸2x2 â‰¥ 0}) can shatter the set above? The answer is yes. Speciï¬cally, we
see that, for any of the eight possible labelings of these points, we can ï¬nd a
linear classiï¬er that obtains â€œzero training errorâ€ on them:

Moreover, it is possible to show that there is no set of 4 points that this
hypothesis class can shatter. Thus, the largest set that H can shatter is of
size 3, and hence VC(H) = 3.

Note that the VC dimension of H here is 3 even though there may be
sets of size 3 that it cannot shatter. For instance, if we had a set of three
points lying in a straight line (left ï¬gure), then there is no way to ï¬nd a linear
separator for the labeling of the three points shown below (right ï¬gure):

(cid:0)(cid:1)(cid:0)(cid:1)(cid:0)(cid:1)xx12xx12xx12xx12xx12xx12xx12xx12xx12134

In order words, under the deï¬nition of the VC dimension, in order to
prove that VC(H) is at least D, we need to show only that thereâ€™s at least
one set of size D that H can shatter.

The following theorem, due to Vapnik, can then be shown. (This is, many

would argue, the most important theorem in all of learning theory.)
Theorem. Let H be given, and let D = VC(H). Then with probability at
least 1 âˆ’ Î´, we have that for all h âˆˆ H,
(cid:32)(cid:114)

(cid:33)

|Îµ(h) âˆ’ Ë†Îµ(h)| â‰¤ O

log

+

log

.

D
n

n
D

1
n

1
Î´

Thus, with probability at least 1 âˆ’ Î´, we also have that:

Îµ(Ë†h) â‰¤ Îµ(hâˆ—) + O

(cid:32)(cid:114)

D
n

log

n
D

+

1
n

log

(cid:33)

.

1
Î´

In other words, if a hypothesis class has ï¬nite VC dimension, then uniform
convergence occurs as n becomes large. As before, this allows us to give a
bound on Îµ(h) in terms of Îµ(hâˆ—). We also have the following corollary:
Corollary. For |Îµ(h) âˆ’ Ë†Îµ(h)| â‰¤ Î³ to hold for all h âˆˆ H (and hence Îµ(Ë†h) â‰¤
Îµ(hâˆ—) + 2Î³) with probability at least 1 âˆ’ Î´, it suï¬ƒces that n = OÎ³,Î´(D).

In other words, the number of training examples needed to learn â€œwellâ€
using H is linear in the VC dimension of H. It turns out that, for â€œmostâ€
hypothesis classes, the VC dimension (assuming a â€œreasonableâ€ parameter-
ization) is also roughly linear in the number of parameters. Putting these
together, we conclude that for a given hypothesis class H (and for an algo-
rithm that tries to minimize training error), the number of training examples
needed to achieve generalization error close to that of the optimal classiï¬er
is usually roughly linear in the number of parameters of H.

xx12(cid:0)(cid:1)(cid:0)(cid:1)(cid:0)(cid:1)xx12Chapter 9

Regularization and model
selection

9.1 Regularization

Recall that as discussed in Section 8.1, overftting is typically a result of using
too complex models, and we need to choose a proper model complexity to
achieve the optimal bias-variance tradeoï¬€. When the model complexity is
measured by the number of parameters, we can vary the size of the model
(e.g., the width of a neural net). However, the correct, informative complex-
ity measure of the models can be a function of the parameters (e.g., (cid:96)2 norm
of the parameters), which may not necessarily depend on the number of pa-
rameters. In such cases, we will use regularization, an important technique
in machine learning, control the model complexity and prevent overï¬tting.

Regularization typically involves adding an additional term, called a reg-

ularizer and denoted by R(Î¸) here, to the training loss/cost function:

JÎ»(Î¸) = J(Î¸) + Î»R(Î¸)

(9.1)

Here JÎ» is often called the regularized loss, and Î» â‰¥ 0 is called the regular-
ization parameter. The regularizer R(Î¸) is a nonnegative function (in almost
all cases). In classical methods, R(Î¸) is purely a function of the parameter Î¸,
but some modern approach allows R(Î¸) to depend on the training dataset.1
The regula
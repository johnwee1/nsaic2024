lue, then any set of orthogonal vectors lying in their span
are also eigenvectors with that eigenvalue, and we could equivalently choose a Q
using those eigenvectors instead. By convention, we usually sort the entries of Î›
in descending order. Under this convention, the eigendecomposition is unique only
if all of the eigenvalues are unique.
The eigendecomposition of a matrix tells us many useful facts about the
matrix. The matrix is singular if and only if any of the eigenvalues are zero.
The eigendecomposition of a real symmetric matrix can also be used to optimize
quadratic expressions of the form f( x) = xî€¾ Ax subject to ||x||2 = 1. Whenever x
is equal to an eigenvector of A, f takes on the value of the corresponding eigenvalue.
The maximum value of f within the constraint region is the maximum eigenvalue
and its minimum value within the constraint region is the minimum eigenvalue.
A matrix whose eigenvalues are all positive is called positive deï¬?nite. A
matrix whose eigenvalues are all positive or zero-valued is called positive semideï¬?nite. Likewise, if all eigenvalues are negative, the matrix is negative deï¬?nite, and
if all eigenvalues are negative or zero-valued, it is negative semideï¬?nite. Positive
semideï¬?nite matrices are interesting because they guarantee that âˆ€x, x î€¾Ax â‰¥ 0.
Positive deï¬?nite matrices additionally guarantee that xî€¾Ax = 0 â‡’ x = 0.

2.8

Singular Value Decomposition

In section 2.7, we saw how to decompose a matrix into eigenvectors and eigenvalues.
The singular value decomposition (SVD) provides another way to factorize
a matrix, into singular vectors and singular values. The SVD allows us to
discover some of the same kind of information as the eigendecomposition. However,
44

CHAPTER 2. LINEAR ALGEBRA

the SVD is more generally applicable. Every real matrix has a singular value
decomposition, but the same is not true of the eigenvalue decomposition. For
example, if a matrix is not square, the eigendecomposition is not deï¬?ned, and we
must use a singular value decomposition instead.
Recall that the eigendecomposition involves analyzing a matrix A to discover
a matrix V of eigenvectors and a vector of eigenvalues Î» such that we can rewrite
A as
A = V diag(Î»)V âˆ’1 .
(2.42)
The singular value decomposition is similar, except this time we will write A
as a product of three matrices:
A = U DV î€¾ .

(2.43)

Suppose that A is an m Ã— n matrix. Then U is deï¬?ned to be an m Ã— m matrix,
D to be an m Ã— n matrix, and V to be an n Ã— n matrix.
Each of these matrices is deï¬?ned to have a special structure. The matrices U
and V are both deï¬?ned to be orthogonal matrices. The matrix D is deï¬?ned to be
a diagonal matrix. Note that D is not necessarily square.

The elements along the diagonal of D are known as the singular values of
the matrix A. The columns of U are known as the left-singular vectors. The
columns of V are known as as the right-singular vectors.
We can actually interpret the singular value decomposition of A in terms of
the eigendecomposition of functions of A. The left-singular vectors of A are the
eigenvectors of AAî€¾ . The right-singular vectors of A are the eigenvectors of Aî€¾ A.
The non-zero singular values of A are the square roots of the eigenvalues of Aî€¾ A.
The same is true for AA î€¾ .
Perhaps the most useful feature of the SVD is that we can use it to partially
generalize matrix inversion to non-square matrices, as we will see in the next
section.

2.9

The Moore-Penrose Pseudoinverse

Matrix inversion is not deï¬?ned for matrices that are not square. Suppose we want
to make a left-inverse B of a matrix A, so that we can solve a linear equation
Ax = y
45

(2.44)

CHAPTER 2. LINEAR ALGEBRA

by left-multiplying each side to obtain
x = By .

(2.45)

Depending on the structure of the problem, it may not be possible to design a
unique mapping from A to B .
If A is taller than it is wide, then it is possible for this equation to have
no solution. If A is wider than it is tall, then there could be multiple possible
solutions.
The Moore-Penrose pseudoinverse allows us to make some headway in
these cases. The pseudoinverse of A is deï¬?ned as a matrix
A+ = lim(A î€¾A + Î±I ) âˆ’1 Aî€¾ .
Î±î€¦0

(2.46)

Practical algorithms for computing the pseudoinverse are not based on this deï¬?nition, but rather the formula
A + = V D +U î€¾,

(2.47)

where U, D and V are the singular value decomposition of A, and the pseudoinverse
D+ of a diagonal matrix D is obtained by taking the reciprocal of its non-zero
elements then taking the transpose of the resulting matrix.
When A has more columns than rows, then solving a linear equation using the
pseudoinverse provides one of the many possible solutions. Speciï¬?cally, it provides
the solution x = A+ y with minimal Euclidean norm ||x||2 among all possible
solutions.
When A has more rows than columns, it is possible for there to be no solution.
In this case, using the pseudoinverse gives us the x for which Ax is as close as
possible to y in terms of Euclidean norm ||Ax âˆ’ y||2.

2.10

The Trace Operator

The trace operator gives the sum of all of the diagonal entries of a matrix:
î?˜
Tr(A) =
Ai,i .
(2.48)
i

The trace operator is useful for a variety of reasons. Some operations that are
diï¬ƒcult to specify without resorting to summation notation can be speciï¬?ed using
46

CHAPTER 2. LINEAR ALGEBRA

matrix products and the trace operator. For example, the trace operator provides
an alternative way of writing the Frobenius norm of a matrix:
î?±
(2.49)
||A|| F = Tr(AAî€¾ ).
Writing an expression in terms of the trace operator opens up opportunities to
manipulate the expression using many useful identities. For example, the trace
operator is invariant to the transpose operator:
Tr(A) = Tr(Aî€¾).

(2.50)

The trace of a square matrix composed of many factors is also invariant to
moving the last factor into the ï¬?rst position, if the shapes of the corresponding
matrices allow the resulting product to be deï¬?ned:
Tr(ABC ) = Tr(CAB ) = Tr(BCA)

(2.51)

or more generally,
Tr(

n
î?™

F

(i)

) = Tr(F

i=1

(n )

nâˆ’1
î?™

F (i) ).

(2.52)

i=1

This invariance to cyclic permutation holds even if the resulting product has a
diï¬€erent shape. For example, for A âˆˆ RmÃ—n and B âˆˆ R nÃ—m, we have
Tr(AB ) = Tr(BA)

(2.53)

even though AB âˆˆ RmÃ—m and BA âˆˆ RnÃ—n.

Another useful fact to keep in mind is that a scalar is its own trace: a = Tr(a).

2.11

The Determinant

The determinant of a square matrix, denoted det(A), is a function mapping
matrices to real scalars. The determinant is equal to the product of all the
eigenvalues of the matrix. The absolute value of the determinant can be thought
of as a measure of how much multiplication by the matrix expands or contracts
space. If the determinant is 0, then space is contracted completely along at least
one dimension, causing it to lose all of its volume. If the determinant is 1, then
the transformation preserves volume.

47

CHAPTER 2. LINEAR ALGEBRA

2.12

Example: Principal Components Analysis

One simple machine learning algorithm, principal components analysis or PCA
can be derived using only knowledge of basic linear algebra.
Suppose we have a collection of m points {x(1) , . . . , x(m) } in Rn. Suppose we
would like to apply lossy compression to these points. Lossy compression means
storing the points in a way that requires less memory but may lose some precision.
We would like to lose as little precision as possible.
One way we can encode these points is to represent a lower-dimensional version
of them. For each point x(i) âˆˆ R n we will ï¬?nd a corresponding code vector c (i) âˆˆ R l.
If l is smaller than n, it will take less memory to store the code points than the
original data. We will want to ï¬?nd some encoding function that produces the code
for an input, f(x) = c, and a decoding function that produces the reconstructed
input given its code, x â‰ˆ g(f (x)).
PCA is deï¬?ned by our choice of the decoding function. Speciï¬?cally, to make the
decoder very simple, we choose to use matrix multiplication to map the code back
into Rn. Let g(c) = Dc, where D âˆˆ RnÃ—l is the matrix deï¬?ning the decoding.
Computing the optimal code for this decoder could be a diï¬ƒcult problem. To
keep the encoding problem easy, PCA constrains the columns of D to be orthogonal
to each other. (Note that D is still not technically â€œan orthogonal matrixâ€? unless
l = n)

With the problem as described so far, many solutions are possible, because we
can increase the scale of D:,i if we decrease c i proportionally for all points. To give
the problem a unique solution, we constrain all of the columns of D to have unit
norm.
In order to turn this basic idea into an algorithm we can implement, the ï¬?rst
thing we need to do is ï¬?gure out how to generate the optimal code point câˆ— for
each input point x. One way to do this is to minimize the distance between the
input point x and its reconstruction, g(c âˆ—). We can measure this distance using a
norm. In the principal components algorithm, we use the L2 norm:
c âˆ— = arg min ||x âˆ’ g(c)||2.

(2.54)

c

We can switch to the squared L 2 norm instead of the L2 norm itself, because
both are minimized by the same value of c. Both are minimized by the same
value of c because the L2 norm is non-negative and the squaring operation is

48

CHAPTER 2. LINEAR ALGEBRA

monotonically increasing for non-negative arguments.
c âˆ— = arg min ||x âˆ’ g(c)||22.

(2.55)

c

The function being minimized simpliï¬?es to
(x âˆ’ g(c))î€¾ (x âˆ’ g(c))

(2.56)

(by the deï¬?nition of the L2 norm, equation 2.30)
= xî€¾ x âˆ’ xî€¾g(c) âˆ’ g (c)î€¾ x + g(c)î€¾g(c)

(2.57)

(by the distributive property)
= xî€¾ x âˆ’ 2xî€¾ g(c) + g (c)î€¾g(c)

(2.58)

(because the scalar g(c)î€¾ x is equal to the transpose of itself).
We can now change the function being minimized again, to omit the ï¬?rst term,
since this term does not depend on c:
câˆ— = arg min âˆ’2x î€¾g(c) + g (c)î€¾g(c).

(2.59)

c

To make further progress, we must substitute in the deï¬?nition of g(c):
câˆ— = arg min âˆ’2xî€¾Dc + cî€¾ Dî€¾ Dc

(2.60)

= arg min âˆ’2xî€¾ Dc + cî€¾Il c

(2.61)

c

c

(by the orthogonality and unit norm constraints on D)
= arg min âˆ’2x î€¾Dc + cî€¾ c

(2.62)

c

We can solve this optimization problem using vector calculus (see section 4.3 if
you do not know how to do this):
âˆ‡c(âˆ’2xî€¾ Dc + cî€¾ c) = 0

(2.63)

âˆ’ 2D î€¾x + 2c = 0

(2.64)

c = D î€¾ x.
49

(2.65)

CHAPTER 2. LINEAR ALGEBRA

This makes the algorithm eï¬ƒcient: we can optimally encode x just using a
matrix-vector operation. To encode a vector, we apply the encoder function
f (x) = D î€¾x.

(2.66)

Using a further matrix multiplication, we can also deï¬?ne the PCA reconstruction
operation:
r(x) = g (f (x)) = DD î€¾x.
(2.67)
Next, we need to choose the encoding matrix D. To do so, we revisit the idea
of minimizing the L 2 distance between inputs and reconstructions. Since we will
use the same matrix D to decode all of the points, we can no longer consider the
points in isolation. Instead, we must minimize the Frobenius norm of the matrix
of errors computed over all dimensions and all points:
î?³
î€‘2
î?˜ î€? ( i)
D âˆ— = arg min
xj âˆ’ r(x (i)) j subject to Dî€¾D = Il
D

(2.68)

i,j

To derive the algorithm for ï¬?nding Dâˆ— , we will start by considering the case
where l = 1. In this case, D is just a single vector, d. Substituting equation 2.67
into equation 2.68 and simplifying D into d, the problem reduces to
dâˆ— = arg min
d

î?˜
i

||x(i) âˆ’ ddî€¾x(i)|| 22 subject to ||d||2 = 1.

(2.69)

The above formulation is the most direct way of performing the substitution,
but is not the most stylistically pleasing way to write the equation. It places the
scalar value dî€¾x (i) on the right of the vector d. It is more conventional to write
scalar coeï¬ƒcients on the left of vector they operate on. We therefore usually write
such a formula as
î?˜
dâˆ— = arg min
(2.70)
||x(i) âˆ’ dî€¾x (i)d||22 subject to ||d||2 = 1,
d

i

or, exploiting the fact that a scalar is its own transpose, as
î?˜
âˆ—
d = arg min
||x(i) âˆ’ x(i)î€¾ dd||22 subject to ||d||2 = 1.
d

(2.71)

i

The reader should aim to become familiar with such cosmetic rearrangements.
50

CHAPTER 2. LINEAR ALGEBRA

At this point, it can be helpful to rewrite the problem in terms of a single
design matrix of examples, rather than as a sum over separate example vectors.
This will allow us to use more compact notation. Let X âˆˆ RmÃ—n be the matrix
î€¾
deï¬?ned by stacking all of the vectors describing the points, such that Xi,: = x (i) .
We can now rewrite the problem as
dâˆ— = arg min ||X âˆ’ Xddî€¾||2F subject to dî€¾ d = 1.

(2.72)

d

Disregarding the constraint for the moment, we can simplify the Frobenius norm
portion as follows:
arg min ||X âˆ’ Xddî€¾ || 2F
(2.73)
d

= arg min Tr
d

(by equation 2.49)

î€’î€?

î€‘î€¾ î€?

î€¾

X âˆ’ Xdd

î€¾

X âˆ’ Xdd

î€‘î€“

(2.74)

= arg min Tr(X î€¾X âˆ’ X î€¾Xddî€¾ âˆ’ ddî€¾ X î€¾X + dd î€¾X î€¾Xdd î€¾)

(2.75)

d

= arg min Tr(X î€¾ X) âˆ’ Tr(X î€¾Xddî€¾ ) âˆ’ Tr(ddî€¾X î€¾X) + Tr(ddî€¾X î€¾ Xddî€¾ )
d

î€¾

î€¾

î€¾

î€¾

î€¾

î€¾

î€¾

= arg min âˆ’ Tr(X Xdd ) âˆ’ Tr(dd X X) + Tr(dd X Xdd )

(2.76)
(2.77)

d

(because terms not involving d do not aï¬€ect the arg min)
= arg min âˆ’2 Tr(X î€¾ Xddî€¾) + Tr(ddî€¾ X î€¾ Xddî€¾ )

(2.78)

d

(because we can cycle the order of the matrices inside a trace, equation 2.52)
= arg min âˆ’2 Tr(X î€¾ Xddî€¾) + Tr(X î€¾ Xddî€¾ddî€¾ )

(2.79)

d

(using the same property again)
At this point, we re-introduce the constraint:
arg min âˆ’2 Tr(Xî€¾Xddî€¾ ) + Tr(Xî€¾Xddî€¾ddî€¾) subject to d î€¾d = 1

(2.80)

= arg min âˆ’2 Tr(Xî€¾ Xddî€¾ ) + Tr(X î€¾Xdd î€¾) subject to dî€¾d = 1

(2.81)

d

d

(due to the constraint)
= arg min âˆ’ Tr(X î€¾Xddî€¾ ) subject to dî€¾d = 1
d

51

(2.82)

CHAPTER 2. LINEAR ALGEBRA

= arg max Tr(X î€¾Xdd î€¾ ) subject to dî€¾ d = 1

(2.83)

d

= arg max Tr(dî€¾ Xî€¾ Xd) subject to dî€¾ d = 1

(2.84)

d

This optimization problem may be solved using eigendecomposition. Speciï¬?cally,
the optimal d is given by the eigenvector of X î€¾X corresponding to the largest
eigenvalue.
This derivation is speciï¬?c to the case of l = 1 and recovers only the ï¬?rst
principal component. More generally, when we wish to recover a basis of principal
components, the matrix D is given by the l eigenvectors corresponding to the
largest eigenvalues. This may be shown using proof by induction. We recommend
writing this proof as an exercise.
Linear algebra is one of the fundamental mathematical disciplines that is
necessary to understand deep learning. Another key area of mathematics that is
ubiquitous in machine learning is probability theory, presented next.

52

Chapter 3

Probability and Information
Theory
In this chapter, we describe probability theory and information theory.
Probability theory is a mathematical framework for representing uncertain
statements. It provides a means of quantifying uncertainty and axioms for deriving
new uncertain statements. In artiï¬?cial intelligence applications, we use probability
theory in two major ways. First, the laws of probability tell us how AI systems
should reason, so we design our algorithms to compute or approximate various
expressions derived using probability theory. Second, we can use probability and
statistics to theoretically analyze the behavior of proposed AI systems.
Probability theory is a fundamental tool of m
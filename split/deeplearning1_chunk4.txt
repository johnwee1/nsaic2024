n et al., 1998b; Bengio et al., 2001). The Canadian Institute
for Advanced Research (CIFAR) helped to keep neural networks research alive
via its Neural Computation and Adaptive Perception (NCAP) research initiative.
This program united machine learning research groups led by Geoï¬€rey Hinton
at University of Toronto, Yoshua Bengio at University of Montreal, and Yann
LeCun at New York University. The CIFAR NCAP research initiative had a
multi-disciplinary nature that also included neuroscientists and experts in human
and computer vision.
At this point in time, deep networks were generally believed to be very diï¬ƒcult
to train. We now know that algorithms that have existed since the 1980s work
quite well, but this was not apparent circa 2006. The issue is perhaps simply that
these algorithms were too computationally costly to allow much experimentation
with the hardware available at the time.
The third wave of neural networks research began with a breakthrough in
18

CHAPTER 1. INTRODUCTION

2006. Geoï¬€rey Hinton showed that a kind of neural network called a deep belief
network could be eï¬ƒciently trained using a strategy called greedy layer-wise pretraining (Hinton et al., 2006), which will be described in more detail in section 15.1.
The other CIFAR-aï¬ƒliated research groups quickly showed that the same strategy
could be used to train many other kinds of deep networks (Bengio et al., 2007;
Ranzato et al., 2007a) and systematically helped to improve generalization on
test examples. This wave of neural networks research popularized the use of the
term â€œdeep learningâ€? to emphasize that researchers were now able to train deeper
neural networks than had been possible before, and to focus attention on the
theoretical importance of depth (Bengio and LeCun, 2007; Delalleau and Bengio,
2011; Pascanu et al., 2014a; Montufar et al., 2014). At this time, deep neural
networks outperformed competing AI systems based on other machine learning
technologies as well as hand-designed functionality. This third wave of popularity
of neural networks continues to the time of this writing, though the focus of deep
learning research has changed dramatically within the time of this wave. The
third wave began with a focus on new unsupervised learning techniques and the
ability of deep models to generalize well from small datasets, but today there is
more interest in much older supervised learning algorithms and the ability of deep
models to leverage large labeled datasets.

1.2.2

Increasing Dataset Sizes

One may wonder why deep learning has only recently become recognized as a
crucial technology though the ï¬?rst experiments with artiï¬?cial neural networks were
conducted in the 1950s. Deep learning has been successfully used in commercial
applications since the 1990s, but was often regarded as being more of an art than
a technology and something that only an expert could use, until recently. It is true
that some skill is required to get good performance from a deep learning algorithm.
Fortunately, the amount of skill required reduces as the amount of training data
increases. The learning algorithms reaching human performance on complex tasks
today are nearly identical to the learning algorithms that struggled to solve toy
problems in the 1980s, though the models we train with these algorithms have
undergone changes that simplify the training of very deep architectures. The most
important new development is that today we can provide these algorithms with
the resources they need to succeed. Figure 1.8 shows how the size of benchmark
datasets has increased remarkably over time. This trend is driven by the increasing
digitization of society. As more and more of our activities take place on computers,
more and more of what we do is recorded. As our computers are increasingly
networked together, it becomes easier to centralize these records and curate them
19

CHAPTER 1. INTRODUCTION

into a dataset appropriate for machine learning applications. The age of â€œBig
Dataâ€? has made machine learning much easier because the key burden of statistical
estimationâ€”generalizing well to new data after observing only a small amount
of dataâ€”has been considerably lightened. As of 2016, a rough rule of thumb
is that a supervised deep learning algorithm will generally achieve acceptable
performance with around 5,000 labeled examples per category, and will match or
exceed human performance when trained with a dataset containing at least 10
million labeled examples. Working successfully with datasets smaller than this is
an important research area, focusing in particular on how we can take advantage
of large quantities of unlabeled examples, with unsupervised or semi-supervised
learning.

1.2.3

Increasing Model Sizes

Another key reason that neural networks are wildly successful today after enjoying
comparatively little success since the 1980s is that we have the computational
resources to run much larger models today. One of the main insights of connectionism is that animals become intelligent when many of their neurons work together.
An individual neuron or small collection of neurons is not particularly useful.
Biological neurons are not especially densely connected. As seen in ï¬?gure 1.10,
our machine learning models have had a number of connections per neuron that
was within an order of magnitude of even mammalian brains for decades.
In terms of the total number of neurons, neural networks have been astonishingly
small until quite recently, as shown in ï¬?gure 1.11. Since the introduction of hidden
units, artiï¬?cial neural networks have doubled in size roughly every 2.4 years. This
growth is driven by faster computers with larger memory and by the availability
of larger datasets. Larger networks are able to achieve higher accuracy on more
complex tasks. This trend looks set to continue for decades. Unless new technologies
allow faster scaling, artiï¬?cial neural networks will not have the same number of
neurons as the human brain until at least the 2050s. Biological neurons may
represent more complicated functions than current artiï¬?cial neurons, so biological
neural networks may be even larger than this plot portrays.
In retrospect, it is not particularly surprising that neural networks with fewer
neurons than a leech were unable to solve sophisticated artiï¬?cial intelligence problems. Even todayâ€™s networks, which we consider quite large from a computational
systems point of view, are smaller than the nervous system of even relatively
primitive vertebrate animals like frogs.
The increase in model size over time, due to the availability of faster CPUs,
20

Dataset size (number examples)

CHAPTER 1. INTRODUCTION

10 9
Canadian Hansard

10 8

WMT Sports-1M
ImageNet10k

10 7
10 6
10 5

Public SVHN
Criminals

ImageNet

10 4

MNIST

10 3
10 2
10

1

10 0

T vs. G vs. F

ILSVRC 2014
CIFAR-10

Rotated T vs. C

Iris
1900

1950

1985 2000 2015

Y

Figure 1.8: Dataset sizes have increased greatly over time. In the early 1900s, statisticians
studied datasets using hundreds or thousands of manually compiled measurements (Garson,
1900; Gosset, 1908; Anderson, 1935; Fisher, 1936). In the 1950s through 1980s, the pioneers
of biologically inspired machine learning often worked with small, synthetic datasets, such
as low-resolution bitmaps of letters, that were designed to incur low computational cost and
demonstrate that neural networks were able to learn speciï¬?c kinds of functions (Widrow
and Hoï¬€, 1960; Rumelhart et al., 1986b). In the 1980s and 1990s, machine learning
became more statistical in nature and began to leverage larger datasets containing tens
of thousands of examples such as the MNIST dataset (shown in ï¬?gure 1.9) of scans
of handwritten numbers (LeCun et al., 1998b). In the ï¬?rst decade of the 2000s, more
sophisticated datasets of this same size, such as the CIFAR-10 dataset (Krizhevsky and
Hinton, 2009) continued to be produced. Toward the end of that decade and throughout
the ï¬?rst half of the 2010s, signiï¬?cantly larger datasets, containing hundreds of thousands
to tens of millions of examples, completely changed what was possible with deep learning.
These datasets included the public Street View House Numbers dataset (Netzer et al.,
2011), various versions of the ImageNet dataset (Deng et al., 2009, 2010a; Russakovsky
et al., 2014a), and the Sports-1M dataset (Karpathy et al., 2014). At the top of the
graph, we see that datasets of translated sentences, such as IBMâ€™s dataset constructed
from the Canadian Hansard (Brown et al., 1990) and the WMT 2014 English to French
dataset (Schwenk, 2014) are typically far ahead of other dataset sizes.

21

CHAPTER 1. INTRODUCTION

Figure 1.9: Example inputs from the MNIST dataset. The â€œNISTâ€? stands for National
Institute of Standards and Technology, the agency that originally collected this data.
The â€œMâ€? stands for â€œmodiï¬?ed,â€? since the data has been preprocessed for easier use with
machine learning algorithms. The MNIST dataset consists of scans of handwritten digits
and associated labels describing which digit 0â€“9 is contained in each image. This simple
classiï¬?cation problem is one of the simplest and most widely used tests in deep learning
research. It remains popular despite being quite easy for modern techniques to solve.
Geoï¬€rey Hinton has described it as â€œthe drosophila of machine learning,â€? meaning that
it allows machine learning researchers to study their algorithms in controlled laboratory
conditions, much as biologists often study fruit ï¬‚ies.

22

CHAPTER 1. INTRODUCTION

the advent of general purpose GPUs (described in section 12.1.2), faster network
connectivity and better software infrastructure for distributed computing, is one of
the most important trends in the history of deep learning. This trend is generally
expected to continue well into the future.

1.2.4

Increasing Accuracy, Complexity and Real-World Impact

Since the 1980s, deep learning has consistently improved in its ability to provide
accurate recognition or prediction. Moreover, deep learning has consistently been
applied with success to broader and broader sets of applications.
The earliest deep models were used to recognize individual objects in tightly
cropped, extremely small images (Rumelhart et al., 1986a). Since then there has
been a gradual increase in the size of images neural networks could process. Modern
object recognition networks process rich high-resolution photographs and do not
have a requirement that the photo be cropped near the object to be recognized
(Krizhevsky et al., 2012). Similarly, the earliest networks could only recognize
two kinds of objects (or in some cases, the absence or presence of a single kind of
object), while these modern networks typically recognize at least 1,000 diï¬€erent
categories of objects. The largest contest in object recognition is the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) held each year. A dramatic
moment in the meteoric rise of deep learning came when a convolutional network
won this challenge for the ï¬?rst time and by a wide margin, bringing down the
state-of-the-art top-5 error rate from 26.1% to 15.3% (Krizhevsky et al., 2012),
meaning that the convolutional network produces a ranked list of possible categories
for each image and the correct category appeared in the ï¬?rst ï¬?ve entries of this
list for all but 15.3% of the test examples. Since then, these competitions are
consistently won by deep convolutional nets, and as of this writing, advances in
deep learning have brought the latest top-5 error rate in this contest down to 3.6%,
as shown in ï¬?gure 1.12.
Deep learning has also had a dramatic impact on speech recognition. After
improving throughout the 1990s, the error rates for speech recognition stagnated
starting in about 2000. The introduction of deep learning (Dahl et al., 2010; Deng
et al., 2010b; Seide et al., 2011; Hinton et al., 2012a) to speech recognition resulted
in a sudden drop of error rates, with some error rates cut in half. We will explore
this history in more detail in section 12.3.
Deep networks have also had spectacular successes for pedestrian detection and
image segmentation (Sermanet et al., 2013; Farabet et al., 2013; Couprie et al.,
2013) and yielded superhuman performance in traï¬ƒc sign classiï¬?cation (Ciresan
23

CHAPTER 1. INTRODUCTION

Connections per neuron

10 4

Human
6

9

4

10 3

7

Cat
Mouse

2
10

5
8

10 2

Fruit ï¬‚y

3
1

10 1

1950

1985

2000

2015

Figure 1.10: Initially, the number of connections between neurons in artiï¬?cial neural
networks was limited by hardware capabilities. Today, the number of connections between
neurons is mostly a design consideration. Some artiï¬?cial neural networks have nearly as
many connections per neuron as a cat, and it is quite common for other neural networks
to have as many connections per neuron as smaller mammals like mice. Even the human
brain does not have an exorbitant amount of connections per neuron. Biological neural
network sizes from Wikipedia (2015).
1. Adaptive linear element (Widrow and Hoï¬€, 1960)
2. Neocognitron (Fukushima, 1980)
3. GPU-accelerated convolutional network (Chellapilla et al., 2006)
4. Deep Boltzmann machine (Salakhutdinov and Hinton, 2009a)
5. Unsupervised convolutional network (Jarrett et al., 2009)
6. GPU-accelerated multilayer perceptron (Ciresan et al., 2010)
7. Distributed autoencoder (Le et al., 2012)
8. Multi-GPU convolutional network (Krizhevsky et al., 2012)
9. COTS HPC unsupervised convolutional network (Coates et al., 2013)
10. GoogLeNet (Szegedy et al., 2014a)

24

CHAPTER 1. INTRODUCTION

et al., 2012).
At the same time that the scale and accuracy of deep networks has increased,
so has the complexity of the tasks that they can solve. Goodfellow et al. (2014d)
showed that neural networks could learn to output an entire sequence of characters
transcribed from an image, rather than just identifying a single object. Previously,
it was widely believed that this kind of learning required labeling of the individual
elements of the sequence (GÃ¼lÃ§ehre and Bengio, 2013). Recurrent neural networks,
such as the LSTM sequence model mentioned above, are now used to model
relationships between sequences and other sequences rather than just ï¬?xed inputs.
This sequence-to-sequence learning seems to be on the cusp of revolutionizing
another application: machine translation (Sutskever et al., 2014; Bahdanau et al.,
2015).
This trend of increasing complexity has been pushed to its logical conclusion
with the introduction of neural Turing machines (Graves et al., 2014a) that learn
to read from memory cells and write arbitrary content to memory cells. Such
neural networks can learn simple programs from examples of desired behavior. For
example, they can learn to sort lists of numbers given examples of scrambled and
sorted sequences. This self-programming technology is in its infancy, but in the
future could in principle be applied to nearly any task.
Another crowning achievement of deep learning is its extension to the domain of
reinforcement learning. In the context of reinforcement learning, an autonomous
agent must learn to perform a task by trial and error, without any guidance from
the human operator. DeepMind demonstrated that a reinforcement learning system
based on deep learning is capable of learning to play Atari video games, reaching
human-level performance on many tasks (M
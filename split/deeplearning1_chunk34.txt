eries. Alternately, we could imagine that this is a truncated Taylor
series approximating the cost function of a more sophisticated model. The gradient
in this setting is given by
Ë† w) = H (w âˆ’ w âˆ—),
âˆ‡w J(

(7.21)

where, again, H is the Hessian matrix of J with respect to w evaluated at wâˆ—.
Because the L1 penalty does not admit clean algebraic expressions in the case
of a fully general Hessian, we will also make the further simplifying assumption
that the Hessian is diagonal, H = diag([H1,1 , . . . , Hn,n ]), where each Hi,i > 0.
This assumption holds if the data for the linear regression problem has been
preprocessed to remove all correlation between the input features, which may be
accomplished using PCA.
Our quadratic approximation of the L1 regularized objective function decomposes into a sum over the parameters:
î€•
î?˜ î€”1
JÌ‚ (w; X , y) = J (wâˆ—; X , y) +
H i,i(wi âˆ’ wâˆ—i )2 + Î±|wi| .
(7.22)
2
i

The problem of minimizing this approximate cost function has an analytical solution
(for each dimension i), with the following form:
î€š
î€›
Î±
w i = sign(wiâˆ— ) max |w âˆ—i | âˆ’
,0 .
(7.23)
Hi,i
235

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

Consider the situation where wâˆ—i > 0 for all i. There are two possible outcomes:
1. The case where w âˆ—i â‰¤ HÎ±i,i . Here the optimal value of wi under the regularized
objective is simply w i = 0. This occurs because the contribution of J (w; X , y)
to the regularized objective JËœ(w; X , y) is overwhelmedâ€”in direction iâ€”by
the L 1 regularization which pushes the value of wi to zero.
2. The case where w âˆ—i > HÎ±i,i . In this case, the regularization does not move the
optimal value of wi to zero but instead it just shifts it in that direction by a
distance equal to HÎ±i,i .
A similar process happens when wiâˆ— < 0, but with the L1 penalty making wi less
negative by HÎ±i,i , or 0.
In comparison to L 2 regularization, L1 regularization results in a solution that
is more sparse. Sparsity in this context refers to the fact that some parameters
have an optimal value of zero. The sparsity of L1 regularization is a qualitatively
diï¬€erent behavior than arises with L 2 regularization. Equation 7.13 gave the
solution wÌƒ for L2 regularization. If we revisit that equation using the assumption
of a diagonal and positive deï¬?nite Hessian H that we introduced for our analysis of
H
L1 regularization, we ï¬?nd that wÌƒi = H i,ii,i+Î± wâˆ—i . If wâˆ—i was nonzero, then wÌƒi remains
nonzero. This demonstrates that L2 regularization does not cause the parameters
to become sparse, while L1 regularization may do so for large enough Î±.

The sparsity property induced by L 1 regularization has been used extensively
as a feature selection mechanism. Feature selection simpliï¬?es a machine learning
problem by choosing which subset of the available features should be used. In
particular, the well known LASSO (Tibshirani, 1995) (least absolute shrinkage and
selection operator) model integrates an L1 penalty with a linear model and a least
squares cost function. The L 1 penalty causes a subset of the weights to become
zero, suggesting that the corresponding features may safely be discarded.
In section 5.6.1, we saw that many regularization strategies can be interpreted
as MAP Bayesian inference, and that in particular, L2 regularization is equivalent
to MAP Bayesian inference with a Gaussian prior on the weights. For L 1 reguî??
larization, the penalty Î±â„¦(w) = Î± i |wi | used to regularize a cost function is
equivalent to the log-prior term that is maximized by MAP Bayesian inference
when the prior is an isotropic Laplace distribution (equation 3.26) over w âˆˆ R n:
log p(w) =

î?˜
i

log Laplace(w i; 0,

1
) = âˆ’Î±||w||1 + n log Î± âˆ’ n log 2.
Î±
236

(7.24)

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

From the point of view of learning via maximization with respect to w, we can
ignore the log Î± âˆ’ log 2 terms because they do not depend on w.

7.2

Norm Penalties as Constrained Optimization

Consider the cost function regularized by a parameter norm penalty:
JËœ(Î¸; X , y) = J (Î¸; X , y) + Î±â„¦(Î¸).

(7.25)

Recall from section 4.4 that we can minimize a function subject to constraints
by constructing a generalized Lagrange function, consisting of the original objective
function plus a set of penalties. Each penalty is a product between a coeï¬ƒcient,
called a Karushâ€“Kuhnâ€“Tucker (KKT) multiplier, and a function representing
whether the constraint is satisï¬?ed. If we wanted to constrain â„¦(Î¸) to be less than
some constant k, we could construct a generalized Lagrange function
L(Î¸, Î±; X , y) = J (Î¸; X , y) + Î±(â„¦(Î¸) âˆ’ k).

(7.26)

The solution to the constrained problem is given by
Î¸âˆ— = arg min max L(Î¸, Î±).
Î¸

Î±,Î±â‰¥0

(7.27)

As described in section 4.4, solving this problem requires modifying both Î¸
and Î± . Section 4.5 provides a worked example of linear regression with an L 2
constraint. Many diï¬€erent procedures are possibleâ€”some may use gradient descent,
while others may use analytical solutions for where the gradient is zeroâ€”but in all
procedures Î± must increase whenever â„¦(Î¸) > k and decrease whenever â„¦(Î¸) < k.
All positive Î± encourage â„¦(Î¸) to shrink. The optimal value Î± âˆ— will encourage â„¦(Î¸)
to shrink, but not so strongly to make â„¦(Î¸) become less than k.
To gain some insight into the eï¬€ect of the constraint, we can ï¬?x Î± âˆ— and view
the problem as just a function of Î¸ :
Î¸ âˆ— = arg min L(Î¸, Î±âˆ—) = arg min J (Î¸; X , y) + Î± âˆ—â„¦(Î¸).
Î¸

(7.28)

Î¸

Ëœ
This is exactly the same as the regularized training problem of minimizing J.
We can thus think of a parameter norm penalty as imposing a constraint on the
weights. If â„¦ is the L2 norm, then the weights are constrained to lie in an L 2
ball. If â„¦ is the L1 norm, then the weights are constrained to lie in a region of
237

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

limited L1 norm. Usually we do not know the size of the constraint region that we
impose by using weight decay with coeï¬ƒcient Î±âˆ— because the value of Î±âˆ— does not
directly tell us the value of k. In principle, one can solve for k, but the relationship
between k and Î±âˆ— depends on the form of J. While we do not know the exact size
of the constraint region, we can control it roughly by increasing or decreasing Î±
in order to grow or shrink the constraint region. Larger Î± will result in a smaller
constraint region. Smaller Î± will result in a larger constraint region.
Sometimes we may wish to use explicit constraints rather than penalties. As
described in section 4.4, we can modify algorithms such as stochastic gradient
descent to take a step downhill on J (Î¸) and then project Î¸ back to the nearest
point that satisï¬?es â„¦(Î¸) < k. This can be useful if we have an idea of what value
of k is appropriate and do not want to spend time searching for the value of Î± that
corresponds to this k.
Another reason to use explicit constraints and reprojection rather than enforcing
constraints with penalties is that penalties can cause non-convex optimization
procedures to get stuck in local minima corresponding to small Î¸. When training
neural networks, this usually manifests as neural networks that train with several
â€œdead units.â€? These are units that do not contribute much to the behavior of the
function learned by the network because the weights going into or out of them are
all very small. When training with a penalty on the norm of the weights, these
conï¬?gurations can be locally optimal, even if it is possible to signiï¬?cantly reduce
J by making the weights larger. Explicit constraints implemented by re-projection
can work much better in these cases because they do not encourage the weights
to approach the origin. Explicit constraints implemented by re-projection only
have an eï¬€ect when the weights become large and attempt to leave the constraint
region.
Finally, explicit constraints with reprojection can be useful because they impose
some stability on the optimization procedure. When using high learning rates, it
is possible to encounter a positive feedback loop in which large weights induce
large gradients which then induce a large update to the weights. If these updates
consistently increase the size of the weights, then Î¸ rapidly moves away from
the origin until numerical overï¬‚ow occurs. Explicit constraints with reprojection
prevent this feedback loop from continuing to increase the magnitude of the weights
without bound. Hinton et al. (2012c) recommend using constraints combined with
a high learning rate to allow rapid exploration of parameter space while maintaining
some stability.
In particular, Hinton et al. (2012c) recommend a strategy introduced by Srebro
and Shraibman (2005): constraining the norm of each column of the weight matrix
238

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

of a neural net layer, rather than constraining the Frobenius norm of the entire
weight matrix. Constraining the norm of each column separately prevents any one
hidden unit from having very large weights. If we converted this constraint into a
penalty in a Lagrange function, it would be similar to L2 weight decay but with a
separate KKT multiplier for the weights of each hidden unit. Each of these KKT
multipliers would be dynamically updated separately to make each hidden unit
obey the constraint. In practice, column norm limitation is always implemented as
an explicit constraint with reprojection.

7.3

Regularization and Under-Constrained Problems

In some cases, regularization is necessary for machine learning problems to be properly deï¬?ned. Many linear models in machine learning, including linear regression
and PCA, depend on inverting the matrix X î€¾X. This is not possible whenever
Xî€¾ X is singular. This matrix can be singular whenever the data generating distribution truly has no variance in some direction, or when no variance is observed in
some direction because there are fewer examples (rows of X) than input features
(columns of X ). In this case, many forms of regularization correspond to inverting
Xî€¾ X + Î±I instead. This regularized matrix is guaranteed to be invertible.
These linear problems have closed form solutions when the relevant matrix
is invertible. It is also possible for a problem with no closed form solution to be
underdetermined. An example is logistic regression applied to a problem where
the classes are linearly separable. If a weight vector w is able to achieve perfect
classiï¬?cation, then 2w will also achieve perfect classiï¬?cation and higher likelihood.
An iterative optimization procedure like stochastic gradient descent will continually
increase the magnitude of w and, in theory, will never halt. In practice, a numerical
implementation of gradient descent will eventually reach suï¬ƒciently large weights
to cause numerical overï¬‚ow, at which point its behavior will depend on how the
programmer has decided to handle values that are not real numbers.
Most forms of regularization are able to guarantee the convergence of iterative
methods applied to underdetermined problems. For example, weight decay will
cause gradient descent to quit increasing the magnitude of the weights when the
slope of the likelihood is equal to the weight decay coeï¬ƒcient.
The idea of using regularization to solve underdetermined problems extends
beyond machine learning. The same idea is useful for several basic linear algebra
problems.
As we saw in section 2.9, we can solve underdetermined linear equations using
239

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

the Moore-Penrose pseudoinverse. Recall that one deï¬?nition of the pseudoinverse
X+ of a matrix X is
X+ = lim (X î€¾ X + Î±I )âˆ’1 Xî€¾ .
Î±î€¦0

(7.29)

We can now recognize equation 7.29 as performing linear regression with weight
decay. Speciï¬?cally, equation 7.29 is the limit of equation 7.17 as the regularization
coeï¬ƒcient shrinks to zero. We can thus interpret the pseudoinverse as stabilizing
underdetermined problems using regularization.

7.4

Dataset Augmentation

The best way to make a machine learning model generalize better is to train it on
more data. Of course, in practice, the amount of data we have is limited. One way
to get around this problem is to create fake data and add it to the training set.
For some machine learning tasks, it is reasonably straightforward to create new
fake data.
This approach is easiest for classiï¬?cation. A classiï¬?er needs to take a complicated, high dimensional input x and summarize it with a single category identity y.
This means that the main task facing a classiï¬?er is to be invariant to a wide variety
of transformations. We can generate new (x, y) pairs easily just by transforming
the x inputs in our training set.
This approach is not as readily applicable to many other tasks. For example, it
is diï¬ƒcult to generate new fake data for a density estimation task unless we have
already solved the density estimation problem.
Dataset augmentation has been a particularly eï¬€ective technique for a speciï¬?c
classiï¬?cation problem: object recognition. Images are high dimensional and include
an enormous variety of factors of variation, many of which can be easily simulated.
Operations like translating the training images a few pixels in each direction can
often greatly improve generalization, even if the model has already been designed to
be partially translation invariant by using the convolution and pooling techniques
described in chapter 9. Many other operations such as rotating the image or scaling
the image have also proven quite eï¬€ective.
One must be careful not to apply transformations that would change the correct
class. For example, optical character recognition tasks require recognizing the
diï¬€erence between â€˜bâ€™ and â€˜dâ€™ and the diï¬€erence between â€˜6â€™ and â€˜9â€™, so horizontal
ï¬‚ips and 180â—¦ rotations are not appropriate ways of augmenting datasets for these
tasks.
240

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

There are also transformations that we would like our classiï¬?ers to be invariant
to, but which are not easy to perform. For example, out-of-plane rotation can not
be implemented as a simple geometric operation on the input pixels.
Dataset augmentation is eï¬€ective for speech recognition tasks as well (Jaitly
and Hinton, 2013).
Injecting noise in the input to a neural network (Sietsma and Dow, 1991)
can also be seen as a form of data augmentation. For many classiï¬?cation and
even some regression tasks, the task should still be possible to solve even if small
random noise is added to the input. Neural networks prove not to be very robust
to noise, however (Tang and Eliasmith, 2010). One way to improve the robustness
of neural networks is simply to train them with random noise applied to their
inputs. Input noise injection is part of some unsupervised learning algorithms such
as the denoising autoencoder (Vincent et al., 2008). Noise injection also works
when the noise is applied to the hidden units, which can be seen as doing dataset
augmentation at multiple levels of abstraction. Poole et al. (2014) recently showed
that this approach can be highly eï¬€ective provided that the magnitude of the
noise is carefully tuned. Dropout, a powerful regularization strategy that will be
described in section 7.12, can be seen as a process of constructing new inputs by
multiplying by noise.
When comparing machine learning ben
del outperformed the state of the art on six text classifica‐
tion tasks by a large margin (reducing the error rate by 18–24% in most cases).
Moreover,  they  showed  that  by  fine-tuning  the  pretrained  model  on  just  100
labeled  examples,  they  could  achieve  the  same  performance  as  a  model  trained
from scratch on 10,000 examples.

• The  GPT  paper26  by  Alec  Radford  and  other  OpenAI  researchers  also  demon‐
strated  the  effectiveness  of  unsupervised  pretraining,  but  this  time  using  a

24 Matthew Peters et al., “Deep Contextualized Word Representations,” Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1
(2018): 2227–2237.

25 Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification,” Pro‐
ceedings of the 56th Annual Meeting of the Association for Computational Linguistics 1 (2018): 328–339.

26 Alec Radford et al., “Improving Language Understanding by Generative Pre-Training” (2018).

Recent Innovations in Language Models 

| 

563

Transformer-like  architecture.  The  authors  pretrained  a  large  but  fairly  simple
architecture composed of a stack of 12 Transformer modules (using only Masked
Multi-Head  Attention  layers)  on  a  large  dataset,  once  again  trained  using  self-
supervised  learning.  Then  they  fine-tuned  it  on  various  language  tasks,  using
only minor adaptations for each task. The tasks were quite diverse: they included
text classification, entailment (whether sentence A entails sentence B),27 similarity
(e.g., “Nice weather today” is very similar to “It is sunny”), and question answer‐
ing (given a few paragraphs of text giving some context, the model must answer
some multiple-choice questions). Just a few months later, in February 2019, Alec
Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper,28
which proposed a very similar architecture, but larger still (with over 1.5 billion
parameters!) and they showed that it could achieve good performance on many
tasks without any fine-tuning. This is called zero-shot learning (ZSL). A smaller
version  of  the  GPT-2  model  (with  “just”  117  million  parameters)  is  available  at
https://github.com/openai/gpt-2, along with its pretrained weights.

• The  BERT  paper29  by  Jacob  Devlin  and  other  Google  researchers  also  demon‐
strates the effectiveness of self-supervised pretraining on a large corpus, using a
similar architecture to GPT but non-masked Multi-Head Attention layers (like in
the Transformer’s encoder). This means that the model is naturally bidirectional;
hence the B in BERT (Bidirectional Encoder Representations from Transformers).
Most importantly, the authors proposed two pretraining tasks that explain most
of the model’s strength:

Masked language model (MLM)

Each  word  in  a  sentence  has  a  15%  probability  of  being  masked,  and  the
model  is  trained  to  predict  the  masked  words.  For  example,  if  the  original
sentence is “She had fun at the birthday party,” then the model may be given
the sentence “She <mask> fun at the <mask> party” and it must predict the
words “had” and “birthday” (the other outputs will be ignored). To be more
precise,  each  selected  word  has  an  80%  chance  of  being  masked,  a  10%
chance  of  being  replaced  by  a  random  word  (to  reduce  the  discrepancy
between  pretraining  and  fine-tuning,  since  the  model  will  not  see  <mask>
tokens during fine-tuning), and a 10% chance of being left alone (to bias the
model toward the correct answer).

27 For example, the sentence “Jane had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party,”

but it is contradicted by “Everyone hated the party” and it is unrelated to “The Earth is flat.”

28 Alec Radford et al., “Language Models Are Unsupervised Multitask Learners” (2019).

29 Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,”
Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin‐
guistics: Human Language Technologies 1 (2019).

564 

| 

Chapter 16: Natural Language Processing with RNNs and Attention

Next sentence prediction (NSP)

The  model  is  trained  to  predict  whether  two  sentences  are  consecutive  or
not.  For  example,  it  should  predict  that  “The  dog  sleeps”  and  “It  snores
loudly”  are  consecutive  sentences,  while  “The  dog  sleeps”  and  “The  Earth
orbits the Sun” are not consecutive. This is a challenging task, and it signifi‐
cantly improves the performance of the model when it is fine-tuned on tasks
such as question answering or entailment.

As  you  can  see,  the  main  innovations  in  2018  and  2019  have  been  better  subword
tokenization,  shifting  from  LSTMs  to  Transformers,  and  pretraining  universal  lan‐
guage  models  using  self-supervised  learning,  then  fine-tuning  them  with  very  few
architectural  changes  (or  none  at  all).  Things  are  moving  fast;  no  one  can  say  what
architectures will prevail next year. Today, it’s clearly Transformers, but tomorrow it
might  be  CNNs  (e.g.,  check  out  the  2018  paper30  by  Maha  Elbayad  et  al.,  where  the
researchers use masked 2D convolutional layers for sequence-to-sequence tasks). Or
it  might  even  be  RNNs,  if  they  make  a  surprise  comeback  (e.g.,  check  out  the  2018
paper31  by  Shuai  Li  et  al.  that  shows  that  by  making  neurons  independent  of  each
other in a given RNN layer, it is possible to train much deeper RNNs capable of learn‐
ing much longer sequences).

In the next chapter we will discuss how to learn deep representations in an unsuper‐
vised  way  using  autoencoders,  and  we  will  use  generative  adversarial  networks
(GANs) to produce images and more!

Exercises

1. What are the pros and cons of using a stateful RNN versus a stateless RNN?

2. Why  do  people  use  Encoder–Decoder  RNNs  rather  than  plain  sequence-to-

sequence RNNs for automatic translation?

3. How  can  you  deal  with  variable-length  input  sequences?  What  about  variable-

length output sequences?

4. What is beam search and why would you use it? What tool can you use to imple‐

ment it?

5. What is an attention mechanism? How does it help?

30 Maha Elbayad et al., “Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Pre‐

diction,” arXiv preprint arXiv:1808.03867 (2018).

31 Shuai Li et al., “Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN,”

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018): 5457–5466.

Exercises 

| 

565

6. What  is  the  most  important  layer  in  the  Transformer  architecture?  What  is  its

purpose?

7. When would you need to use sampled softmax?

8. Embedded  Reber  grammars  were  used  by  Hochreiter  and  Schmidhuber  in  their
paper  about  LSTMs.  They  are  artificial  grammars  that  produce  strings  such  as
“BPBTSXXVPSEPE.”  Check  out  Jenny  Orr’s  nice  introduction  to  this  topic.
Choose a particular embedded Reber grammar (such as the one represented on
Jenny  Orr’s  page),  then  train  an  RNN  to  identify  whether  a  string  respects  that
grammar  or  not.  You  will  first  need  to  write  a  function  capable  of  generating  a
training batch containing about 50% strings that respect the grammar, and 50%
that don’t.

9. Train an Encoder–Decoder model that can convert a date string from one format

to another (e.g., from “April 22, 2019” to “2019-04-22”).

10. Go through TensorFlow’s Neural Machine Translation with Attention tutorial.

11. Use one of the recent language models (e.g., BERT) to generate more convincing

Shakespearean text.

Solutions to these exercises are available in Appendix A.

566 

| 

Chapter 16: Natural Language Processing with RNNs and Attention

CHAPTER 17
Representation Learning and Generative
Learning Using Autoencoders and GANs

Autoencoders are artificial neural networks capable of learning dense representations
of the input data, called latent representations or codings, without any supervision (i.e.,
the training set is unlabeled). These codings typically have a much lower dimension‐
ality  than  the  input  data,  making  autoencoders  useful  for  dimensionality  reduction
(see Chapter 8), especially for visualization purposes. Autoencoders also act as feature
detectors, and they can be used for unsupervised pretraining of deep neural networks
(as we discussed in Chapter 11). Lastly, some autoencoders are generative models: they
are capable of randomly generating new data that looks very similar to the training
data. For example, you could train an autoencoder on pictures of faces, and it would
then be able to generate new faces. However, the generated images are usually fuzzy
and not entirely realistic.

In  contrast,  faces  generated  by  generative  adversarial  networks  (GANs)  are  now  so
convincing that it is hard to believe that the people they represent do not exist. You
can judge so for yourself by visiting https://thispersondoesnotexist.com/, a website that
shows  faces  generated  by  a  recent  GAN  architecture  called  StyleGAN  (you  can  also
check  out  https://thisrentaldoesnotexist.com/  to  see  some  generated  Airbnb  bed‐
rooms). GANs are now widely used for super resolution (increasing the resolution of
an image), colorization, powerful image editing (e.g., replacing photo bombers with
realistic background), turning a simple sketch into a photorealistic image, predicting
the next frames in a video, augmenting a dataset (to train other models), generating
other types of data (such as text, audio, and time series), identifying the weaknesses in
other models and strengthening them, and more.

567

Autoencoders  and  GANs  are  both  unsupervised,  they  both  learn  dense  representa‐
tions, they can both be used as generative models, and they have many similar appli‐
cations. However, they work very differently:

• Autoencoders simply learn to copy their inputs to their outputs. This may sound
like a trivial task, but we will see that constraining the network in various ways
can make it rather difficult. For example, you can limit the size of the latent rep‐
resentations, or you can add noise to the inputs and train the network to recover
the  original  inputs.  These  constraints  prevent  the  autoencoder  from  trivially
copying the inputs directly to the outputs, which forces it to learn efficient ways
of representing the data. In short, the codings are byproducts of the autoencoder
learning the identity function under some constraints.

• GANs  are  composed  of  two  neural  networks:  a  generator  that  tries  to  generate
data  that  looks  similar  to  the  training  data,  and  a  discriminator  that  tries  to  tell
real  data  from  fake  data.  This  architecture  is  very  original  in  Deep  Learning  in
that  the  generator  and  the  discriminator  compete  against  each  other  during
training:  the  generator  is  often  compared  to  a  criminal  trying  to  make  realistic
counterfeit money, while the discriminator is like the police investigator trying to
tell  real  money  from  fake.  Adversarial  training  (training  competing  neural  net‐
works) is widely considered as one of the most important ideas in recent years. In
2016, Yann LeCun even said that it was “the most interesting idea in the last 10
years in Machine Learning.”

In this chapter we will start by exploring in more depth how autoencoders work and
how to use them for dimensionality reduction, feature extraction, unsupervised pre‐
training, or as generative models. This will naturally lead us to GANs. We will start by
building a simple GAN to generate fake images, but we will see that training is often
quite difficult. We will discuss the main difficulties you will encounter with adversa‐
rial training, as well as some of the main techniques to work around these difficulties.
Let’s start with autoencoders!

568 

| 

Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs

Efficient Data Representations
Which of the following number sequences do you find the easiest to memorize?

• 40, 27, 25, 36, 81, 57, 10, 73, 19, 68

• 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14

At first glance, it would seem that the first sequence should be easier, since it is much
shorter. However, if you look carefully at the second sequence, you will notice that it
is just the list of even numbers from 50 down to 14. Once you notice this pattern, the
second sequence becomes much easier to memorize than the first because you only
need  to  remember  the  pattern  (i.e.,  decreasing  even  numbers)  and  the  starting  and
ending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize
very long sequences, you would not care much about the existence of a pattern in the
second  sequence.  You  would  just  learn  every  number  by  heart,  and  that  would  be
that. The fact that it is hard to memorize long sequences is what makes it useful to
recognize patterns, and hopefully this clarifies why constraining an autoencoder dur‐
ing training pushes it to discover and exploit patterns in the data.

The  relationship  between  memory,  perception,  and  pattern  matching  was  famously
studied by William Chase and Herbert Simon in the early 1970s.1 They observed that
expert chess players were able to memorize the positions of all the pieces in a game by
looking at the board for just five seconds, a task that most people would find impossi‐
ble. However, this was only the case when the pieces were placed in realistic positions
(from actual games), not when the pieces were placed randomly. Chess experts don’t
have a much better memory than you and I; they just see chess patterns more easily,
thanks to their experience with the game. Noticing patterns helps them store infor‐
mation efficiently.

Just  like  the  chess  players  in  this  memory  experiment,  an  autoencoder  looks  at  the
inputs, converts them to an efficient latent representation, and then spits out some‐
thing that (hopefully) looks very close to the inputs. An autoencoder is always com‐
posed of two parts: an encoder (or recognition network) that converts the inputs to a
latent representation, followed by a decoder (or generative network) that converts the
internal representation to the outputs (see Figure 17-1).

1 William G. Chase and Herbert A. Simon, “Perception in Chess,” Cognitive Psychology 4, no. 1 (1973): 55–81.

Efficient Data Representations 

| 

569

Figure 17-1. The chess memory experiment (left) and a simple autoencoder (right)

As you can see, an autoencoder typically has the same architecture as a Multi-Layer
Perceptron (MLP; see Chapter 10), except that the number of neurons in the output
layer must be equal to the number of inputs. In this example, there is just one hidden
layer  composed  of  two  neurons  (the  encoder),  and  one  output  layer  composed  of
three neurons (the decoder). The outputs are often called the reconstructions because
the  autoencoder  tries  to  reconstruct  the  inputs,  and  the  cost  function  contains  a
reconstruction  loss  that  penalizes  the  model  when  the  reconstructions  a
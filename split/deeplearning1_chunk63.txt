as a budget
for training time, memory or recognition time). It is therefore possible, in principle,
to develop hyperparameter optimization algorithms that wrap a learning
algorithm and choose its hyperparameters, thus hiding the hyperparameters of the
learning algorithm from the user. Unfortunately, hyperparameter optimization
algorithms often have their own hyperparameters, such as the range of values that
should be explored for each of the learning algorithmâ€™s hyperparameters. However,
these secondary hyperparameters are usually easier to choose, in the sense that
acceptable performance may be achieved on a wide range of tasks using the same
secondary hyperparameters for all tasks.

11.4.3

Grid Search

When there are three or fewer hyperparameters, the common practice is to perform
grid search. For each hyperparameter, the user selects a small ï¬?nite set of
values to explore. The grid search algorithm then trains a model for every joint
speciï¬?cation of hyperparameter values in the Cartesian product of the set of values
for each individual hyperparameter. The experiment that yields the best validation
432

CHAPTER 11. PRACTICAL METHODOLOGY

Grid

Random

Figure 11.2: Comparison of grid search and random search. For illustration purposes we
display two hyperparameters but we are typically interested in having many more. (Left)To
perform grid search, we provide a set of values for each hyperparameter. The search
algorithm runs training for every joint hyperparameter setting in the cross product of these
sets. (Right)To perform random search, we provide a probability distribution over joint
hyperparameter conï¬?gurations. Usually most of these hyperparameters are independent
from each other. Common choices for the distribution over a single hyperparameter include
uniform and log-uniform (to sample from a log-uniform distribution, take the exp of a
sample from a uniform distribution). The search algorithm then randomly samples joint
hyperparameter conï¬?gurations and runs training with each of them. Both grid search
and random search evaluate the validation set error and return the best conï¬?guration.
The ï¬?gure illustrates the typical case where only some hyperparameters have a signiï¬?cant
inï¬‚uence on the result. In this illustration, only the hyperparameter on the horizontal axis
has a signiï¬?cant eï¬€ect. Grid search wastes an amount of computation that is exponential
in the number of non-inï¬‚uential hyperparameters, while random search tests a unique
value of every inï¬‚uential hyperparameter on nearly every trial. Figure reproduced with
permission from Bergstra and Bengio (2012).

433

CHAPTER 11. PRACTICAL METHODOLOGY

set error is then chosen as having found the best hyperparameters. See the left of
ï¬?gure 11.2 for an illustration of a grid of hyperparameter values.
How should the lists of values to search over be chosen? In the case of numerical
(ordered) hyperparameters, the smallest and largest element of each list is chosen
conservatively, based on prior experience with similar experiments, to make sure
that the optimal value is very likely to be in the selected range. Typically, a grid
search involves picking values approximately on a logarithmic scale, e.g., a learning
rate taken within the set {. 1, .01, 10âˆ’3 , 10âˆ’4 , 10âˆ’5 } , or a number of hidden units
taken with the set {50, 100, 200, 500, 1000, 2000}.

Grid search usually performs best when it is performed repeatedly. For example,
suppose that we ran a grid search over a hyperparameter Î± using values of {âˆ’1, 0, 1}.
If the best value found is 1, then we underestimated the range in which the best Î±
lies and we should shift the grid and run another search with Î± in, for example,
{ 1, 2, 3}. If we ï¬?nd that the best value of Î± is 0, then we may wish to reï¬?ne our
estimate by zooming in and running a grid search over {âˆ’.1, 0, .1}.

The obvious problem with grid search is that its computational cost grows
exponentially with the number of hyperparameters. If there are m hyperparameters,
each taking at most n values, then the number of training and evaluation trials
required grows as O(nm ). The trials may be run in parallel and exploit loose
parallelism (with almost no need for communication between diï¬€erent machines
carrying out the search) Unfortunately, due to the exponential cost of grid search,
even parallelization may not provide a satisfactory size of search.

11.4.4

Random Search

Fortunately, there is an alternative to grid search that is as simple to program, more
convenient to use, and converges much faster to good values of the hyperparameters:
random search (Bergstra and Bengio, 2012).
A random search proceeds as follows. First we deï¬?ne a marginal distribution
for each hyperparameter, e.g., a Bernoulli or multinoulli for binary or discrete
hyperparameters, or a uniform distribution on a log-scale for positive real-valued
hyperparameters. For example,
log_learning_rate âˆ¼ u(âˆ’1, âˆ’5)

learning_rate = 10log_learning_rate.

(11.2)
(11.3)

where u(a, b) indicates a sample of the uniform distribution in the interval (a, b).
Similarly the log_number_of_hidden_units may be sampled from u(log(50),
log(2000)).
434

CHAPTER 11. PRACTICAL METHODOLOGY

Unlike in the case of a grid search, one should not discretize or bin the values
of the hyperparameters. This allows one to explore a larger set of values, and does
not incur additional computational cost. In fact, as illustrated in ï¬?gure 11.2, a
random search can be exponentially more eï¬ƒcient than a grid search, when there
are several hyperparameters that do not strongly aï¬€ect the performance measure.
This is studied at length in Bergstra and Bengio (2012), who found that random
search reduces the validation set error much faster than grid search, in terms of
the number of trials run by each method.
As with grid search, one may often want to run repeated versions of random
search, to reï¬?ne the search based on the results of the ï¬?rst run.
The main reason why random search ï¬?nds good solutions faster than grid search
is that there are no wasted experimental runs, unlike in the case of grid search,
when two values of a hyperparameter (given values of the other hyperparameters)
would give the same result. In the case of grid search, the other hyperparameters
would have the same values for these two runs, whereas with random search, they
would usually have diï¬€erent values. Hence if the change between these two values
does not marginally make much diï¬€erence in terms of validation set error, grid
search will unnecessarily repeat two equivalent experiments while random search
will still give two independent explorations of the other hyperparameters.

11.4.5

Model-Based Hyperparameter Optimization

The search for good hyperparameters can be cast as an optimization problem.
The decision variables are the hyperparameters. The cost to be optimized is the
validation set error that results from training using these hyperparameters. In
simpliï¬?ed settings where it is feasible to compute the gradient of some diï¬€erentiable
error measure on the validation set with respect to the hyperparameters, we can
simply follow this gradient (Bengio et al., 1999; Bengio, 2000; Maclaurin et al.,
2015). Unfortunately, in most practical settings, this gradient is unavailable, either
due to its high computation and memory cost, or due to hyperparameters having
intrinsically non-diï¬€erentiable interactions with the validation set error, as in the
case of discrete-valued hyperparameters.
To compensate for this lack of a gradient, we can build a model of the validation
set error, then propose new hyperparameter guesses by performing optimization
within this model. Most model-based algorithms for hyperparameter search use a
Bayesian regression model to estimate both the expected value of the validation set
error for each hyperparameter and the uncertainty around this expectation. Optimization thus involves a tradeoï¬€ between exploration (proposing hyperparameters
435

CHAPTER 11. PRACTICAL METHODOLOGY

for which there is high uncertainty, which may lead to a large improvement but may
also perform poorly) and exploitation (proposing hyperparameters which the model
is conï¬?dent will perform as well as any hyperparameters it has seen so farâ€”usually
hyperparameters that are very similar to ones it has seen before). Contemporary
approaches to hyperparameter optimization include Spearmint (Snoek et al., 2012),
TPE (Bergstra et al., 2011) and SMAC (Hutter et al., 2011).
Currently, we cannot unambiguously recommend Bayesian hyperparameter
optimization as an established tool for achieving better deep learning results or
for obtaining those results with less eï¬€ort. Bayesian hyperparameter optimization
sometimes performs comparably to human experts, sometimes better, but fails
catastrophically on other problems. It may be worth trying to see if it works on
a particular problem but is not yet suï¬ƒciently mature or reliable. That being
said, hyperparameter optimization is an important ï¬?eld of research that, while
often driven primarily by the needs of deep learning, holds the potential to beneï¬?t
not only the entire ï¬?eld of machine learning but the discipline of engineering in
general.
One drawback common to most hyperparameter optimization algorithms with
more sophistication than random search is that they require for a training experiment to run to completion before they are able to extract any information
from the experiment. This is much less eï¬ƒcient, in the sense of how much information can be gleaned early in an experiment, than manual search by a human
practitioner, since one can usually tell early on if some set of hyperparameters is
completely pathological. Swersky et al. (2014) have introduced an early version
of an algorithm that maintains a set of multiple experiments. At various time
points, the hyperparameter optimization algorithm can choose to begin a new
experiment, to â€œfreezeâ€? a running experiment that is not promising, or to â€œthawâ€?
and resume an experiment that was earlier frozen but now appears promising given
more information.

11.5

Debugging Strategies

When a machine learning system performs poorly, it is usually diï¬ƒcult to tell
whether the poor performance is intrinsic to the algorithm itself or whether there
is a bug in the implementation of the algorithm. Machine learning systems are
diï¬ƒcult to debug for a variety of reasons.
In most cases, we do not know a priori what the intended behavior of the
algorithm is. In fact, the entire point of using machine learning is that it will
discover useful behavior that we were not able to specify ourselves. If we train a
436

CHAPTER 11. PRACTICAL METHODOLOGY

neural network on a new classiï¬?cation task and it achieves 5% test error, we have
no straightforward way of knowing if this is the expected behavior or sub-optimal
behavior.
A further diï¬ƒculty is that most machine learning models have multiple parts
that are each adaptive. If one part is broken, the other parts can adapt and still
achieve roughly acceptable performance. For example, suppose that we are training
a neural net with several layers parametrized by weights W and biases b. Suppose
further that we have manually implemented the gradient descent rule for each
parameter separately, and we made an error in the update for the biases:
bâ†? bâˆ’Î±

(11.4)

where Î± is the learning rate. This erroneous update does not use the gradient at
all. It causes the biases to constantly become negative throughout learning, which
is clearly not a correct implementation of any reasonable learning algorithm. The
bug may not be apparent just from examining the output of the model though.
Depending on the distribution of the input, the weights may be able to adapt to
compensate for the negative biases.
Most debugging strategies for neural nets are designed to get around one or
both of these two diï¬ƒculties. Either we design a case that is so simple that the
correct behavior actually can be predicted, or we design a test that exercises one
part of the neural net implementation in isolation.
Some important debugging tests include:
Visualize the model in action : When training a model to detect objects in
images, view some images with the detections proposed by the model displayed
superimposed on the image. When training a generative model of speech, listen to
some of the speech samples it produces. This may seem obvious, but it is easy to
fall into the practice of only looking at quantitative performance measurements
like accuracy or log-likelihood. Directly observing the machine learning model
performing its task will help to determine whether the quantitative performance
numbers it achieves seem reasonable. Evaluation bugs can be some of the most
devastating bugs because they can mislead you into believing your system is
performing well when it is not.
Visualize the worst mistakes : Most models are able to output some sort of
conï¬?dence measure for the task they perform. For example, classiï¬?ers based on a
softmax output layer assign a probability to each class. The probability assigned
to the most likely class thus gives an estimate of the conï¬?dence the model has in
its classiï¬?cation decision. Typically, maximum likelihood training results in these
values being overestimates rather than accurate probabilities of correct prediction,
437

CHAPTER 11. PRACTICAL METHODOLOGY

but they are somewhat useful in the sense that examples that are actually less
likely to be correctly labeled receive smaller probabilities under the model. By
viewing the training set examples that are the hardest to model correctly, one can
often discover problems with the way the data has been preprocessed or labeled.
For example, the Street View transcription system originally had a problem where
the address number detection system would crop the image too tightly and omit
some of the digits. The transcription network then assigned very low probability
to the correct answer on these images. Sorting the images to identify the most
conï¬?dent mistakes showed that there was a systematic problem with the cropping.
Modifying the detection system to crop much wider images resulted in much better
performance of the overall system, even though the transcription network needed
to be able to process greater variation in the position and scale of the address
numbers.
Reasoning about software using train and test error: It is often diï¬ƒcult to
determine whether the underlying software is correctly implemented. Some clues
can be obtained from the train and test error. If training error is low but test error
is high, then it is likely that that the training procedure works correctly, and the
model is overï¬?tting for fundamental algorithmic reasons. An alternative possibility
is that the test error is measured incorrectly due to a problem with saving the
model after training then reloading it for test set evaluation, or if the test data
was prepared diï¬€erently from the training data. If both train and test error are
high, then it is diï¬ƒcult to determine whether there is a software defect or whether
the model is underï¬?tting due to fundamental algorithmic reasons. This scenario
requires further tests, described next.
Fit a tiny dataset: If you have high error on the training set, determine whether
it is due to genuine underï¬?tting or due to a software defect. Usually even small
mod
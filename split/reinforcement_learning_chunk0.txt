
SUMMARY OF NOTATION

xiii

Summary of Notation

Capital letters are used for random variables and major algorithm variables.
Lower case letters are used for the values of random variables and for scalar
functions. Quantities that are required to be real-valued vectors are written
in bold and in lower case (even if random variables).

s
a
S
S+
A(s)
R

t
T
St
At
Rt
Gt
G(n)
t
Gλ
t

state
action
set of all nonterminal states
set of all states, including the terminal state
set of actions possible in state s
set of possible rewards

discrete time step
ﬁnal time step of an episode
state at t
action at t
reward at t, dependent, like St, on At
1
return (cumulative discounted reward) following t
n-step return (Section 7.1)
λ-return (Section 7.2)

1 and St

−

−

π
π(s)
s)
π(a
|
p(s(cid:48), r

policy, decision-making rule
action taken in state s under deterministic policy π
probability of taking action a in state s under stochastic policy π

s, a) probability of transitioning to state s(cid:48), with reward r, from s, a
|

vπ(s)
v
(s)
∗
qπ(s, a)
q
(s, a)
∗
Vt(s)
Qt(s, a)

value of state s under policy π (expected return)
value of state s under the optimal policy
value of taking action a in state s under policy π
value of taking action a in state s under the optimal policy
estimate (a random variable) of vπ(s) or v
∗
estimate (a random variable) of qπ(s, a) or q

(s, a)

(s)

∗

ˆv(s,w)
ˆq(s, a,w)
w, wt
x(s)
w(cid:62)x

approximate value of state s given a vector of weights w
approximate value of state–action pair s, a given weights w
vector of (possibly learned) weights underlying an approximate value function
vector of features visible when in state s
inner product of vectors, w(cid:62)x =

i wixi; e.g., ˆv(s,w) = w(cid:62)x(s)

(cid:80)

xiv

δt
Et(s)
Et(s, a)
et

γ
ε
α, β
λ

SUMMARY OF NOTATION

temporal-diﬀerence error at t (a random variable, even though not upper case)
eligibility trace for state s at t
eligibility trace for a state–action pair
eligibility trace vector at t

discount-rate parameter
probability of random action in ε-greedy policy
step-size parameters
decay-rate parameter for eligibility traces

Chapter 1

The Reinforcement Learning
Problem

The idea that we learn by interacting with our environment is probably the
ﬁrst to occur to us when we think about the nature of learning. When an
infant plays, waves its arms, or looks about, it has no explicit teacher, but it
does have a direct sensorimotor connection to its environment. Exercising this
connection produces a wealth of information about cause and eﬀect, about
the consequences of actions, and about what to do in order to achieve goals.
Throughout our lives, such interactions are undoubtedly a major source of
knowledge about our environment and ourselves. Whether we are learning to
drive a car or to hold a conversation, we are acutely aware of how our environ-
ment responds to what we do, and we seek to inﬂuence what happens through
our behavior. Learning from interaction is a foundational idea underlying
nearly all theories of learning and intelligence.

In this book we explore a computational approach to learning from inter-
action. Rather than directly theorizing about how people or animals learn, we
explore idealized learning situations and evaluate the eﬀectiveness of various
learning methods. That is, we adopt the perspective of an artiﬁcial intelligence
researcher or engineer. We explore designs for machines that are eﬀective in
solving learning problems of scientiﬁc or economic interest, evaluating the
designs through mathematical analysis or computational experiments. The
approach we explore, called reinforcement learning, is much more focused on
goal-directed learning from interaction than are other approaches to machine
learning.

1

2

CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM

1.1 Reinforcement Learning

Reinforcement learning is like many topics with names ending in -ing, such
as machine learning, planning, and mountaineering, in that it is simultane-
ously a problem, a class of solution methods that work well on the class of
problems, and the ﬁeld that studies these problems and their solution meth-
ods. Reinforcement learning problems involve learning what to do—how to
map situations to actions—so as to maximize a numerical reward signal. In
an essential way they are closed-loop problems because the learning system’s
actions inﬂuence its later inputs. Moreover, the learner is not told which ac-
tions to take, as in many forms of machine learning, but instead must discover
which actions yield the most reward by trying them out. In the most interest-
ing and challenging cases, actions may aﬀect not only the immediate reward
but also the next situation and, through that, all subsequent rewards. These
three characteristics—being closed-loop in an essential way, not having direct
instructions as to what actions to take, and where the consequences of actions,
including reward signals, play out over extended time periods—are the three
most important distinguishing features of reinforcement learning problems.

A full speciﬁcation of reinforcement learning problems in terms of optimal
control of Markov decision processes must wait until Chapter 3, but the basic
idea is simply to capture the most important aspects of the real problem facing
a learning agent interacting with its environment to achieve a goal. Clearly,
such an agent must be able to sense the state of the environment to some extent
and must be able to take actions that aﬀect the state. The agent also must
have a goal or goals relating to the state of the environment. The formulation
is intended to include just these three aspects—sensation, action, and goal—in
their simplest possible forms without trivializing any of them.

Any method that is well suited to solving this kind of problem we consider
to be a reinforcement learning method. Reinforcement learning is diﬀerent
from supervised learning, the kind of learning studied in most current research
in ﬁeld of machine learning. Supervised learning is learning from a train-
ing set of labeled examples provided by a knowledgable external supervisor.
Each example is a description of a situation together with a speciﬁcation—the
label—of the correct action the system should take to that situation, which is
often to identify a category to which the situation belongs. The object of this
kind of learning is for the system to extrapolate, or generalize, its responses
so that it acts correctly in situations not present in the training set. This is
an important kind of learning, but alone it is not adequate for learning from
interaction. In interactive problems it is often impractical to obtain examples
of desired behavior that are both correct and representative of all the situa-
tions in which the agent has to act. In uncharted territory—where one would

1.1. REINFORCEMENT LEARNING

3

expect learning to be most beneﬁcial—an agent must be able to learn from its
own experience.

Reinforcement learning is also diﬀerent from what machine learning re-
searchers call unsupervised learning, which is typically about ﬁnding struc-
ture hidden in collections of unlabeled data. The terms supervised learning
and unsupervised learning appear to exhaustively classify machine learning
paradigms, but they do not. Although one might be tempted to think of rein-
forcement learning as a kind of unsupervised learning because it does not rely
on examples of correct behavior, reinforcement learning is trying to maximize
a reward signal instead of trying to ﬁnd hidden structure. Uncovering struc-
ture in an agent’s experience can certainly be useful in reinforcement learning,
but by itself does not address the reinforcement learning agent’s problem of
maximizing a reward signal. We therefore consider reinforcement learning to
be a third machine learning paradigm, alongside of supervised learning, unsu-
pervised learning, and perhaps other paradigms as well.

One of the challenges that arise in reinforcement learning, and not in other
kinds of learning, is the trade-oﬀ between exploration and exploitation. To
obtain a lot of reward, a reinforcement learning agent must prefer actions
that it has tried in the past and found to be eﬀective in producing reward.
But to discover such actions, it has to try actions that it has not selected
before. The agent has to exploit what it already knows in order to obtain
reward, but it also has to explore in order to make better action selections in
the future. The dilemma is that neither exploration nor exploitation can be
pursued exclusively without failing at the task. The agent must try a variety of
actions and progressively favor those that appear to be best. On a stochastic
task, each action must be tried many times to gain a reliable estimate its
expected reward. The exploration–exploitation dilemma has been intensively
studied by mathematicians for many decades (see Chapter 2). For now, we
simply note that the entire issue of balancing exploration and exploitation
does not even arise in supervised and unsupervised learning, at least in their
purist forms.

Another key feature of reinforcement learning is that it explicitly considers
the whole problem of a goal-directed agent interacting with an uncertain envi-
ronment. This is in contrast with many approaches that consider subproblems
without addressing how they might ﬁt into a larger picture. For example,
we have mentioned that much of machine learning research is concerned with
supervised learning without explicitly specifying how such an ability would
ﬁnally be useful. Other researchers have developed theories of planning with
general goals, but without considering planning’s role in real-time decision-
making, or the question of where the predictive models necessary for planning
would come from. Although these approaches have yielded many useful results,

4

CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM

their focus on isolated subproblems is a signiﬁcant limitation.

Reinforcement learning takes the opposite tack, starting with a complete,
interactive, goal-seeking agent. All reinforcement learning agents have explicit
goals, can sense aspects of their environments, and can choose actions to inﬂu-
ence their environments. Moreover, it is usually assumed from the beginning
that the agent has to operate despite signiﬁcant uncertainty about the environ-
ment it faces. When reinforcement learning involves planning, it has to address
the interplay between planning and real-time action selection, as well as the
question of how environment models are acquired and improved. When rein-
forcement learning involves supervised learning, it does so for speciﬁc reasons
that determine which capabilities are critical and which are not. For learning
research to make progress, important subproblems have to be isolated and
studied, but they should be subproblems that play clear roles in complete,
interactive, goal-seeking agents, even if all the details of the complete agent
cannot yet be ﬁlled in.

One of the most exciting aspects of modern reinforcement learning is its
substantive and fruitful interactions with other engineering and scientiﬁc disci-
plines. Reinforcement learning is part of a decades-long trend within artiﬁcial
intelligence and machine learning toward greater integration with statistics,
optimization, and other mathematical subjects. For example, the ability of
some reinforcement learning methods to learn with parameterized approxima-
tors addresses the classical “curse of dimensionality” in operations research
and control theory. More distinctively, reinforcement learning has also in-
teracted strongly with psychology and neuroscience, with substantial beneﬁts
going both ways. Of all the forms of machine learning, reinforcement learn-
ing is the closest to the kind of learning that humans and other animals do,
and many of the core algorithms of reinforcement learning were originally in-
spired by biological learning systems. And reinforcement learning has also
given back, both through a psychological model of animal learning that better
matches some of the empirical data, and through an inﬂuential model of parts
of the brain’s reward system. The body of this book develops the ideas of
reinforcement learning that pertain to engineering and artiﬁcial intelligence,
with connections to psychology and neuroscience summarized in Chapters ??
and ??.

Finally, reinforcement learning is also part of a larger trend in artiﬁcial
intelligence back toward simple general principles. Since the late 1960’s, many
artiﬁcial intelligence researchers presumed that there are no general principles
to be discovered, that intelligence is instead due to the possession of vast num-
bers of special purpose tricks, procedures, and heuristics. It was sometimes
said that if we could just get enough relevant facts into a machine, say one mil-
lion, or one billion, then it would become intelligent. Methods based on general

1.2. EXAMPLES

5

principles, such as search or learning, were characterized as “weak methods,”
whereas those based on speciﬁc knowledge were called “strong methods.” This
view is still common today, but much less dominant. From our point of view,
it was simply premature: too little eﬀort had been put into the search for
general principles to conclude that there were none. Modern AI now includes
much research looking for general principles of learning, search, and decision-
making, as well as trying to incorporate vast amounts of domain knowledge. It
is not clear how far back the pendulum will swing, but reinforcement learning
research is certainly part of the swing back toward simpler and fewer general
principles of artiﬁcial intelligence.

1.2 Examples

A good way to understand reinforcement learning is to consider some of the
examples and possible applications that have guided its development.

•

•

•

•

•

A master chess player makes a move. The choice is informed both by
planning—anticipating possible replies and counterreplies—and by im-
mediate, intuitive judgments of the desirability of particular positions
and moves.

An adaptive controller adjusts parameters of a petroleum reﬁnery’s op-
eration in real time. The controller optimizes the yield/cost/quality
trade-oﬀ on the basis of speciﬁed marginal costs without sticking strictly
to the set points originally suggested by engineers.

A gazelle calf struggles to its feet minutes after being born. Half an hour
later it is running at 20 miles per hour.

A mobile robot decides whether it should enter a new room in search of
more trash to collect or start trying to ﬁnd its way back to its battery
It makes its decision based on the current charge
recharging station.
level of its battery and how quickly and easily it has been able to ﬁnd
the recharger in the past.

Phil prepares his breakfast. Closely examined, even this apparently mun-
dane activity reveals a complex web of conditional behavior and inter-
locking goal–subgoal relationships: walking to the cupboard, opening it,
selecting a cereal box, then reaching for, grasping, and retrieving the
box. Other complex, tuned, interactive sequences of behavior are re-
quired to obtain a bowl, spoon, and milk jug. Each step involves a series

6

CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM

of eye movements to obtain information and to guide reaching and loco-
motion. Rapid judgments are continually made about how to carry the
objects or whether it is better
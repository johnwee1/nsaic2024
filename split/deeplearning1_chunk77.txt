se f (x) provides a good initial value of h and the
cost function constrains h to remain near f (x) anyway. Simple gradient descent
can obtain reasonable values of h in as few as ten steps.
The training procedure used by PSD is diï¬€erent from ï¬?rst training a sparse
coding model and then training f(x) to predict the values of the sparse coding
features. The PSD training procedure regularizes the decoder to use parameters
for which f (x) can infer good code values.
Predictive sparse coding is an example of learned approximate inference.
In section 19.5, this topic is developed further. The tools presented in chapter 19
make it clear that PSD can be interpreted as training a directed sparse coding
probabilistic model by maximizing a lower bound on the log-likelihood of the
model.
In practical applications of PSD, the iterative optimization is only used during
training. The parametric encoder f is used to compute the learned features when
the model is deployed. Evaluating f is computationally inexpensive compared to
inferring h via gradient descent. Because f is a diï¬€erentiable parametric function,
PSD models may be stacked and used to initialize a deep network to be trained
with another criterion.

14.9

Applications of Autoencoders

Autoencoders have been successfully applied to dimensionality reduction and information retrieval tasks. Dimensionality reduction was one of the ï¬?rst applications
of representation learning and deep learning. It was one of the early motivations
for studying autoencoders. For example, Hinton and Salakhutdinov (2006) trained
a stack of RBMs and then used their weights to initialize a deep autoencoder
with gradually smaller hidden layers, culminating in a bottleneck of 30 units. The
resulting code yielded less reconstruction error than PCA into 30 dimensions and
the learned representation was qualitatively easier to interpret and relate to the
underlying categories, with these categories manifesting as well-separated clusters.
Lower-dimensional representations can improve performance on many tasks,
such as classiï¬?cation. Models of smaller spaces consume less memory and runtime.
524

CHAPTER 14. AUTOENCODERS

Many forms of dimensionality reduction place semantically related examples near
each other, as observed by Salakhutdinov and Hinton (2007b) and Torralba et al.
(2008). The hints provided by the mapping to the lower-dimensional space aid
generalization.
One task that beneï¬?ts even more than usual from dimensionality reduction is
information retrieval, the task of ï¬?nding entries in a database that resemble a
query entry. This task derives the usual beneï¬?ts from dimensionality reduction
that other tasks do, but also derives the additional beneï¬?t that search can become
extremely eï¬ƒcient in certain kinds of low dimensional spaces. Speciï¬?cally, if
we train the dimensionality reduction algorithm to produce a code that is lowdimensional and binary, then we can store all database entries in a hash table
mapping binary code vectors to entries. This hash table allows us to perform
information retrieval by returning all database entries that have the same binary
code as the query. We can also search over slightly less similar entries very
eï¬ƒciently, just by ï¬‚ipping individual bits from the encoding of the query. This
approach to information retrieval via dimensionality reduction and binarization
is called semantic hashing (Salakhutdinov and Hinton, 2007b, 2009b), and has
been applied to both textual input (Salakhutdinov and Hinton, 2007b, 2009b) and
images (Torralba et al., 2008; Weiss et al., 2008; Krizhevsky and Hinton, 2011).
To produce binary codes for semantic hashing, one typically uses an encoding
function with sigmoids on the ï¬?nal layer. The sigmoid units must be trained to be
saturated to nearly 0 or nearly 1 for all input values. One trick that can accomplish
this is simply to inject additive noise just before the sigmoid nonlinearity during
training. The magnitude of the noise should increase over time. To ï¬?ght that
noise and preserve as much information as possible, the network must increase the
magnitude of the inputs to the sigmoid function, until saturation occurs.
The idea of learning a hashing function has been further explored in several
directions, including the idea of training the representations so as to optimize
a loss more directly linked to the task of ï¬?nding nearby examples in the hash
table (Norouzi and Fleet, 2011).

525

Chapter 15

Representation Learning
In this chapter, we ï¬?rst discuss what it means to learn representations and how
the notion of representation can be useful to design deep architectures. We discuss
how learning algorithms share statistical strength across diï¬€erent tasks, including
using information from unsupervised tasks to perform supervised tasks. Shared
representations are useful to handle multiple modalities or domains, or to transfer
learned knowledge to tasks for which few or no examples are given but a task
representation exists. Finally, we step back and argue about the reasons for the
success of representation learning, starting with the theoretical advantages of
distributed representations (Hinton et al., 1986) and deep representations and
ending with the more general idea of underlying assumptions about the data
generating process, in particular about underlying causes of the observed data.
Many information processing tasks can be very easy or very diï¬ƒcult depending
on how the information is represented. This is a general principle applicable to
daily life, computer science in general, and to machine learning. For example, it
is straightforward for a person to divide 210 by 6 using long division. The task
becomes considerably less straightforward if it is instead posed using the Roman
numeral representation of the numbers. Most modern people asked to divide CCX
by VI would begin by converting the numbers to the Arabic numeral representation,
permitting long division procedures that make use of the place value system. More
concretely, we can quantify the asymptotic runtime of various operations using
appropriate or inappropriate representations. For example, inserting a number
into the correct position in a sorted list of numbers is an O(n) operation if the
list is represented as a linked list, but only O(log n) if the list is represented as a
red-black tree.
In the context of machine learning, what makes one representation better than
526

CHAPTER 15. REPRESENTATION LEARNING

another? Generally speaking, a good representation is one that makes a subsequent
learning task easier. The choice of representation will usually depend on the choice
of the subsequent learning task.
We can think of feedforward networks trained by supervised learning as performing a kind of representation learning. Speciï¬?cally, the last layer of the network
is typically a linear classiï¬?er, such as a softmax regression classiï¬?er. The rest of
the network learns to provide a representation to this classiï¬?er. Training with a
supervised criterion naturally leads to the representation at every hidden layer (but
more so near the top hidden layer) taking on properties that make the classiï¬?cation
task easier. For example, classes that were not linearly separable in the input
features may become linearly separable in the last hidden layer. In principle, the
last layer could be another kind of model, such as a nearest neighbor classiï¬?er
(Salakhutdinov and Hinton, 2007a). The features in the penultimate layer should
learn diï¬€erent properties depending on the type of the last layer.
Supervised training of feedforward networks does not involve explicitly imposing
any condition on the learned intermediate features. Other kinds of representation
learning algorithms are often explicitly designed to shape the representation in
some particular way. For example, suppose we want to learn a representation that
makes density estimation easier. Distributions with more independences are easier
to model, so we could design an objective function that encourages the elements
of the representation vector h to be independent. Just like supervised networks,
unsupervised deep learning algorithms have a main training objective but also
learn a representation as a side eï¬€ect. Regardless of how a representation was
obtained, it can be used for another task. Alternatively, multiple tasks (some
supervised, some unsupervised) can be learned together with some shared internal
representation.
Most representation learning problems face a tradeoï¬€ between preserving as
much information about the input as possible and attaining nice properties (such
as independence).
Representation learning is particularly interesting because it provides one
way to perform unsupervised and semi-supervised learning. We often have very
large amounts of unlabeled training data and relatively little labeled training
data. Training with supervised learning techniques on the labeled subset often
results in severe overï¬?tting. Semi-supervised learning oï¬€ers the chance to resolve
this overï¬?tting problem by also learning from the unlabeled data. Speciï¬?cally,
we can learn good representations for the unlabeled data, and then use these
representations to solve the supervised learning task.
Humans and animals are able to learn from very few labeled examples. We do
527

CHAPTER 15. REPRESENTATION LEARNING

not yet know how this is possible. Many factors could explain improved human
performanceâ€”for example, the brain may use very large ensembles of classiï¬?ers
or Bayesian inference techniques. One popular hypothesis is that the brain is
able to leverage unsupervised or semi-supervised learning. There are many ways
to leverage unlabeled data. In this chapter, we focus on the hypothesis that the
unlabeled data can be used to learn a good representation.

15.1

Greedy Layer-Wise Unsupervised Pretraining

Unsupervised learning played a key historical role in the revival of deep neural
networks, enabling researchers for the ï¬?rst time to train a deep supervised network
without requiring architectural specializations like convolution or recurrence. We
call this procedure unsupervised pretraining, or more precisely, greedy layerwise unsupervised pretraining. This procedure is a canonical example of how
a representation learned for one task (unsupervised learning, trying to capture
the shape of the input distribution) can sometimes be useful for another task
(supervised learning with the same input domain).
Greedy layer-wise unsupervised pretraining relies on a single-layer representation learning algorithm such as an RBM, a single-layer autoencoder, a sparse
coding model, or another model that learns latent representations. Each layer is
pretrained using unsupervised learning, taking the output of the previous layer
and producing as output a new representation of the data, whose distribution (or
its relation to other variables such as categories to predict) is hopefully simpler.
See algorithm 15.1 for a formal description.
Greedy layer-wise training procedures based on unsupervised criteria have long
been used to sidestep the diï¬ƒculty of jointly training the layers of a deep neural net
for a supervised task. This approach dates back at least as far as the Neocognitron
(Fukushima, 1975). The deep learning renaissance of 2006 began with the discovery
that this greedy learning procedure could be used to ï¬?nd a good initialization for
a joint learning procedure over all the layers, and that this approach could be used
to successfully train even fully connected architectures (Hinton et al., 2006; Hinton
and Salakhutdinov, 2006; Hinton, 2006; Bengio et al., 2007; Ranzato et al., 2007a).
Prior to this discovery, only convolutional deep networks or networks whose depth
resulted from recurrence were regarded as feasible to train. Today, we now know
that greedy layer-wise pretraining is not required to train fully connected deep
architectures, but the unsupervised pretraining approach was the ï¬?rst method to
succeed.
Greedy layer-wise pretraining is called greedy because it is a greedy algo528

CHAPTER 15. REPRESENTATION LEARNING

rithm, meaning that it optimizes each piece of the solution independently, one
piece at a time, rather than jointly optimizing all pieces. It is called layer-wise
because these independent pieces are the layers of the network. Speciï¬?cally, greedy
layer-wise pretraining proceeds one layer at a time, training the k-th layer while
keeping the previous ones ï¬?xed. In particular, the lower layers (which are trained
ï¬?rst) are not adapted after the upper layers are introduced. It is called unsupervised because each layer is trained with an unsupervised representation learning
algorithm. However it is also called pretraining, because it is supposed to be
only a ï¬?rst step before a joint training algorithm is applied to ï¬?ne-tune all the
layers together. In the context of a supervised learning task, it can be viewed
as a regularizer (in some experiments, pretraining decreases test error without
decreasing training error) and a form of parameter initialization.
It is common to use the word â€œpretrainingâ€? to refer not only to the pretraining
stage itself but to the entire two phase protocol that combines the pretraining
phase and a supervised learning phase. The supervised learning phase may involve
training a simple classiï¬?er on top of the features learned in the pretraining phase,
or it may involve supervised ï¬?ne-tuning of the entire network learned in the
pretraining phase. No matter what kind of unsupervised learning algorithm or
what model type is employed, in the vast majority of cases, the overall training
scheme is nearly the same. While the choice of unsupervised learning algorithm
will obviously impact the details, most applications of unsupervised pretraining
follow this basic protocol.
Greedy layer-wise unsupervised pretraining can also be used as initialization
for other unsupervised learning algorithms, such as deep autoencoders (Hinton
and Salakhutdinov, 2006) and probabilistic models with many layers of latent
variables. Such models include deep belief networks (Hinton et al., 2006) and deep
Boltzmann machines (Salakhutdinov and Hinton, 2009a). These deep generative
models will be described in chapter 20.
As discussed in section 8.7.4, it is also possible to have greedy layer-wise
supervised pretraining. This builds on the premise that training a shallow network
is easier than training a deep one, which seems to have been validated in several
contexts (Erhan et al., 2010).

15.1.1

When and Why Does Unsupervised Pretraining Work?

On many tasks, greedy layer-wise unsupervised pretraining can yield substantial
improvements in test error for classiï¬?cation tasks. This observation was responsible
for the renewed interested in deep neural networks starting in 2006 (Hinton et al.,
529

CHAPTER 15. REPRESENTATION LEARNING

Algorithm 15.1 Greedy layer-wise unsupervised pretraining protocol.
Given the following: Unsupervised feature learning algorithm L, which takes a
training set of examples and returns an encoder or feature function f . The raw
input data is X, with one row per example and f (1) (X) is the output of the ï¬?rst
stage encoder on X. In the case where ï¬?ne-tuning is performed, we use a learner
T which takes an initial function f, input examples X (and in the supervised
ï¬?ne-tuning case, associated targets Y ), and returns a 
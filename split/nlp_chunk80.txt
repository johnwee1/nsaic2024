ure vector thus represents the ﬁrst NP in our example (recall
that most observations will have the value NONE rather than, for example, ARG0,
since most constituents in the parse tree will not bear a semantic role):

S
ARG0: [issued, NP, Examiner, NNP, NP
↓
↑
ORG, The, Examiner]

VBD, active, before, VP
VP
↓

→

NP PP,

Other features are often used in addition, such as sets of n-grams inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).

It’s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.

20.6.2 A Neural Algorithm for Semantic Role Labeling

A simple neural approach to SRL is to treat it as a sequence labeling task like named-
entity recognition, using the BIO approach. Let’s assume that we are given the
predicate and the task is just detecting and labeling spans. Recall that with BIO
tagging, we have a begin and end tag for each possible role (B-ARG0, I-ARG0; B-
ARG1, I-ARG1, and so on), plus an outside tag O.

Figure 20.6 A simple neural approach to semantic role labeling. The input sentence is
followed by [SEP] and an extra input for the predicate, in this case love. The encoder outputs
are concatenated to an indicator variable which is 1 for the predicate and 0 for all other words
After He et al. (2017) and Shi and Lin (2019).

As with all the taggers, the goal is to compute the highest probability tag se-

quence ˆy, given the input sequence of words w:

ˆy = argmax

P(y

y

T

∈

w)
|

Fig. 20.6 shows a sketch of a standard algorithm from He et al. (2017). Here each
input word is mapped to pretrained embeddings, and then each token is concatenated
with the predicate embedding and then passed through a feedforward network with
a softmax which outputs a distribution over each SRL label. For decoding, a CRF
layer can be used instead of the MLP layer on top of the biLSTM output to do global
inference, but in practice this doesn’t seem to provide much beneﬁt.

ENCODER[CLS]thecatslovehats[SEP]love[SEP]FFNB-ARG0I-ARG0B-PREDconcatenate with predicateB-ARG1FFNSoftmaxFFNFFNFFN452 CHAPTER 20

• SEMANTIC ROLE LABELING

20.6.3 Evaluation of Semantic Role Labeling

The standard evaluation for semantic role labeling is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and F-measure. Identiﬁcation and classiﬁcation can
also be evaluated separately. Two common datasets used for evaluation are CoNLL-
2005 (Carreras and M`arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).

20.7 Selectional Restrictions

selectional
restriction

We turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. A selectional restriction is a semantic type con-
straint that a verb imposes on the kind of concepts that are allowed to ﬁll its argument
roles. Consider the two meanings associated with the following example:

(20.29) I want to eat someplace nearby.

There are two possible parses and semantic interpretations for this sentence.
In
the sensible interpretation, eat is intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. In the nonsensical speaker-as-
Godzilla interpretation, eat is transitive and the phrase someplace nearby is the direct
object and the THEME of the eating, like the NP Malaysian food in the following
sentences:

(20.30)

I want to eat Malaysian food.

How do we know that someplace nearby isn’t the direct object in this sentence?
One useful cue is the semantic fact that the THEME of EATING events tends to be
something that is edible. This restriction placed by the verb eat on the ﬁller of its
THEME argument is a selectional restriction.

Selectional restrictions are associated with senses, not entire lexemes. We can

see this in the following examples of the lexeme serve:

(20.31) The restaurant serves green-lipped mussels.
(20.32) Which airlines serve Denver?
Example (20.31) illustrates the offering-food sense of serve, which ordinarily re-
stricts its THEME to be some kind of food Example (20.32) illustrates the provides a
commercial service to sense of serve, which constrains its THEME to be some type
of appropriate location.

Selectional restrictions vary widely in their speciﬁcity. The verb imagine, for
example, imposes strict requirements on its AGENT role (restricting it to humans
and other animate entities) but places very few semantic requirements on its THEME
role. A verb like diagonalize, on the other hand, places a very speciﬁc constraint
on the ﬁller of its THEME role: it has to be a matrix, while the arguments of the
adjective odorless are restricted to concepts that could possess an odor:
(20.33) In rehearsal, I often ask the musicians to imagine a tennis game.
(20.34) Radon is an odorless gas that can’t be detected by human senses.
(20.35) To diagonalize a matrix is to ﬁnd its eigenvalues.

These examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
This distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.

20.7

• SELECTIONAL RESTRICTIONS

453

20.7.1 Representing Selectional Restrictions

One way to capture the semantics of selectional restrictions is to use and extend the
event representation of Chapter 19. Recall that the neo-Davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. Ignoring the issue of
the λ -structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eat might look like the following:

e, x, y Eating(e)

∃

Agent(e, x)

∧

∧

T heme(e, y)

With this representation, all we know about y, the ﬁller of the THEME role, is that
it is associated with an Eating event through the Theme relation. To stipulate the
selectional restriction that y must be something edible, we simply add a new term to
that effect:

e, x, y Eating(e)

∃

Agent(e, x)

∧

∧

T heme(e, y)

∧

EdibleT hing(y)

When a phrase like ate a hamburger is encountered, a semantic analyzer can form
the following kind of representation:

e, x, y Eating(e)

∃

Eater(e, x)

∧

∧

T heme(e, y)

∧

EdibleT hing(y)

∧

Hamburger(y)

This representation is perfectly reasonable since the membership of y in the category
Hamburger is consistent with its membership in the category EdibleThing, assuming
a reasonable set of facts in the knowledge base. Correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as Takeoff would be inconsistent with membership in the
category EdibleThing.

While this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. First, using FOL to perform the simple
task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms
can do the job with far less computational cost. The second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. Unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.

A more practical approach is to state selectional restrictions in terms of WordNet
synsets rather than as logical concepts. Each predicate simply speciﬁes a WordNet
synset as the selectional restriction on each of its arguments. A meaning representa-
tion is well-formed if the role ﬁller word is a hyponym (subordinate) of this synset.
For our ate a hamburger example, for instance, we could set the selectional
restriction on the THEME role of the verb eat to the synset
, glossed
as any substance that can be metabolized by an animal to give energy and build
tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 20.7 reveals
that hamburgers are indeed food. Again, the ﬁller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
We can apply this approach to the THEME roles of the verbs imagine, lift, and di-
agonalize, discussed earlier. Let us restrict imagine’s THEME to the synset
,
entity
}
lift’s THEME to
. This arrangement
matrix
}
correctly permits imagine a hamburger and lift a hamburger, while also correctly
ruling out diagonalize a hamburger.

physical entity
{

, and diagonalize to

food, nutrient

{

}

{

}

{

454 CHAPTER 20

• SEMANTIC ROLE LABELING

Sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich

=> snack food
=> dish

=> nutriment, nourishment, nutrition...

=> food, nutrient
=> substance
=> matter

=> physical entity

=> entity

Figure 20.7 Evidence from WordNet that hamburgers are edible.

20.7.2 Selectional Preferences

In the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (Katz and Fodor 1963,
Hirst 1987). For example, the verb eat might require that its THEME argument be
[+FOOD]. Early word sense disambiguation systems used this idea to rule out senses
that violated the selectional restrictions of their governing predicates.

Very quickly, however, it became clear that these selectional restrictions were
better represented as preferences rather than strict constraints (Wilks 1975b, Wilks
1975a). For example, selectional restriction violations (like inedible arguments of
eat) often occur in well-formed sentences, for example because they are negated
(20.36), or because selectional restrictions are overstated (20.37):

(20.36) But it fell apart in 1931, perhaps because people realized you can’t eat

gold for lunch if you’re hungry.

(20.37) In his two championship trials, Mr. Kulkarni ate glass on an empty

stomach, accompanied only by water and tea.

Modern systems for selectional preferences therefore specify the relation be-

tween a predicate and its possible arguments with soft constraints of some kind.

Selectional Association

One of the most inﬂuential has been the selectional association model of Resnik
(1993). Resnik deﬁnes the idea of selectional preference strength as the general
amount of information that a predicate tells us about the semantic class of its argu-
ments. For example, the verb eat tells us a lot about the semantic class of its direct
objects, since they tend to be edible. The verb be, by contrast, tells us less about
its direct objects. The selectional preference strength can be deﬁned by the differ-
ence in information between two distributions: the distribution of expected semantic
classes P(c) (how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb P(c
v) (how likely is
|
it that the direct object of the speciﬁc verb v will fall into semantic class c). The
greater the difference between these distributions, the more information the verb
is giving us about possible objects. The difference between these two distributions
can be quantiﬁed by relative entropy, or the Kullback-Leibler divergence (Kullback
Q) expresses the
and Leibler, 1951). The Kullback-Leibler or KL divergence D(P

||

selectional
preference
strength

relative entropy

KL divergence

20.7

• SELECTIONAL RESTRICTIONS

455

difference between two probability distributions P and Q

D(P

Q) =

P(x) log

||

P(x)
Q(x)

(20.38)

x
(cid:88)
The selectional preference SR(v) uses the KL divergence to express how much in-
formation, in bits, the verb v expresses about the possible semantic class of its argu-
ment.

v)
SR(v) = D(P(c
||
|
v) log
P(c
|

=

P(c))

c
(cid:88)

v)
P(c
|
P(c)

(20.39)

selectional
association

Resnik then deﬁnes the selectional association of a particular class and verb as the
relative contribution of that class to the general selectional preference of the verb:

AR(v, c) =

1
SR(v)

v) log
P(c
|

P(c
v)
|
P(c)

(20.40)

The selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
Resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the WordNet concepts containing the
word. The following table from Resnik (1996) shows some sample high and low
selectional associations for verbs and some WordNet semantic classes of their direct
objects.

Verb
read
write
see

Direct Object
Semantic Class Assoc
WRITING
WRITING
ENTITY

6.80
7.26
5.79

Direct Object
Semantic Class Assoc
ACTIVITY
COMMERCE
METHOD

-.20
0
-0.01

Selectional Preference via Conditional Probability

An alternative to using selectional association between a verb and the WordNet class
of its arguments is to use the conditional probability of an argument word given a
predicate verb, directly modeling the strength of association of one verb (predicate)
with one noun (argument).

The conditional probability model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. The conditional probability of an
argument noun given a verb for a particular relation P(n
v, r) can then be used as a
|
selectional preference metric for that pair of words (Brockmann and Lapata 2003,
Keller and Lapata 2003):

P(n

v, r) =
|

(cid:40)

C(n,v,r)
C(v,r)

if C(n, v, r) > 0

0 otherwise

The inverse probability P(v
n, r) was found to have better performance in some cases
|
(Brockmann and Lapata, 2003):

n, r) =
P(v
|

(cid:40)

C(n,v,r)
C(n,r)

if C(n, v, r) > 0

0 otherwise

456 CHAPTER 20

• SEMANTIC ROLE LABELING

pseudowords

An even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument log count(v, n, r) instead of conditional probability;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (Brockmann and Lapata, 2003).

Evaluating Selectional Preferences

One way to evaluate models of selectional preferences is to use pseudowords (Gale
et al. 1992b, Sch¨utze 1992a). A pseudoword is an artiﬁcial word created by concate-
nating a test word in some context (say banana) with a confounder word (say door)
to create banana-door). The task of the system is to identify which of the two words
is the original word. To evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. For each verb token (say drive) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor, the noun with the
frequency closest to the original (say house), to make car/house). We then use the
selectional preference model to choose which of car and house are more preferred
objects of drive, and compute how often the model chooses the correct original ob-
ject (e.g., car) (Chambe
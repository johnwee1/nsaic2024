fanout. It took over a decade for researchers to realize how important
this trick is. Using Glorot initialization can speed up training considerably, and it is
one of the tricks that led to the success of Deep Learning.

Some  papers3  have  provided  similar  strategies  for  different  activation  functions.
These strategies differ only by the scale of the variance and whether they use fanavg or
fanin, as shown in Table 11-1 (for the uniform distribution, just compute r = 3σ2).
The initialization strategy for the ReLU activation function (and its variants, includ‐
ing the ELU activation described shortly) is sometimes called He initialization, after
the  paper’s  first  author.  The  SELU  activation  function  will  be  explained  later  in  this
chapter. It should be used with LeCun initialization (preferably with a normal distri‐
bution, as we will see).

Table 11-1. Initialization parameters for each type of activation function

Initialization Activation functions
Glorot

None, tanh, logistic, softmax

He

LeCun

ReLU and variants

SELU

σ² (Normal)
1 / fanavg
2 / fanin
1 / fanin

By default, Keras uses Glorot initialization with a uniform distribution. When creat‐
ing  a  layer,  you  can  change  this  to  He  initialization  by  setting  kernel_initial
izer="he_uniform" or kernel_initializer="he_normal" like this:

keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal")

If  you  want  He  initialization  with  a  uniform  distribution  but  based  on  fanavg  rather
than fanin, you can use the VarianceScaling initializer like this:

3 E.g., Kaiming He et al., “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet

Classification,” Proceedings of the 2015 IEEE International Conference on Computer Vision (2015): 1026–1034.

334 

| 

Chapter 11: Training Deep Neural Networks

he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',
                                                 distribution='uniform')
keras.layers.Dense(10, activation="sigmoid", kernel_initializer=he_avg_init)

Nonsaturating Activation Functions
One  of  the  insights  in  the  2010  paper  by  Glorot  and  Bengio  was  that  the  problems
with unstable gradients were in part due to a poor choice of activation function. Until
then most people had assumed that if Mother Nature had chosen to use roughly sig‐
moid activation functions in biological neurons, they must be an excellent choice. But
it  turns  out  that  other  activation  functions  behave  much  better  in  deep  neural  net‐
works—in particular, the ReLU activation function, mostly because it does not satu‐
rate for positive values (and because it is fast to compute).

Unfortunately, the ReLU activation function is not perfect. It suffers from a problem
known as the dying ReLUs: during training, some neurons effectively “die,” meaning
they stop outputting anything other than 0. In some cases, you may find that half of
your network’s neurons are dead, especially if you used a large learning rate. A neu‐
ron  dies  when  its  weights  get  tweaked  in  such  a  way  that  the  weighted  sum  of  its
inputs  are  negative  for  all  instances  in  the  training  set.  When  this  happens,  it  just
keeps outputting zeros, and Gradient Descent does not affect it anymore because the
gradient of the ReLU function is zero when its input is negative.4

To solve this problem, you may want to use a variant of the ReLU function, such as
the  leaky  ReLU.  This  function  is  defined  as  LeakyReLUα(z)  =  max(αz,  z)  (see
Figure 11-2). The hyperparameter α defines how much the function “leaks”: it is the
slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that
leaky ReLUs never die; they can go into a long coma, but they have a chance to even‐
tually wake up. A 2015 paper5 compared several variants of the ReLU activation func‐
tion, and one of its conclusions was that the leaky variants always outperformed the
strict ReLU activation function. In fact, setting α = 0.2 (a huge leak) seemed to result
in  better  performance  than  α  =  0.01  (a  small  leak).  The  paper  also  evaluated  the
randomized leaky ReLU (RReLU), where α is picked randomly in a given range during
training and is fixed to an average value during testing. RReLU also performed fairly
well and seemed to act as a regularizer (reducing the risk of overfitting the training
set).  Finally,  the  paper  evaluated  the  parametric  leaky  ReLU  (PReLU),  where  α  is
authorized  to  be  learned  during  training  (instead  of  being  a  hyperparameter,  it
becomes a parameter that can be modified by backpropagation like any other param‐

4 Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: Gradient Descent
may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s
inputs is positive again.

5 Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network,” arXiv preprint

arXiv:1505.00853 (2015).

The Vanishing/Exploding Gradients Problems 

| 

335

eter). PReLU was reported to strongly outperform ReLU on large image datasets, but
on smaller datasets it runs the risk of overfitting the training set.

Figure 11-2. Leaky ReLU: like ReLU, but with a small slope for negative values

Last but not least, a 2015 paper by Djork-Arné Clevert et al.6 proposed a new activa‐
tion function called the exponential linear unit (ELU) that outperformed all the ReLU
variants in the authors’ experiments: training time was reduced, and the neural net‐
work performed better on the test set. Figure 11-3 graphs the function, and Equation
11-2 shows its definition.

Equation 11-2. ELU activation function

ELUα z =

α exp z − 1 if z < 0
if z ≥ 0
z

Figure 11-3. ELU activation function

6 Djork-Arné Clevert et al., “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),”

Proceedings of the International Conference on Learning Representations (2016).

336 

| 

Chapter 11: Training Deep Neural Networks

The  ELU  activation  function  looks  a  lot  like  the  ReLU  function,  with  a  few  major
differences:

• It takes on negative values when z < 0, which allows the unit to have an average
output  closer  to  0  and  helps  alleviate  the  vanishing  gradients  problem.  The
hyperparameter α defines the value that the ELU function approaches when z is a
large negative number. It is usually set to 1, but you can tweak it like any other
hyperparameter.

• It has a nonzero gradient for z < 0, which avoids the dead neurons problem.

• If α is equal to 1 then the function is smooth everywhere, including around z = 0,
which helps speed up Gradient Descent since it does not bounce as much to the
left and right of z = 0.

The  main  drawback  of  the  ELU  activation  function  is  that  it  is  slower  to  compute
than the ReLU function and its variants (due to the use of the exponential function).
Its  faster  convergence  rate  during  training  compensates  for  that  slow  computation,
but still, at test time an ELU network will be slower than a ReLU network.

Then, a 2017 paper7 by Günter Klambauer et al. introduced the Scaled ELU (SELU)
activation function: as its name suggests, it is a scaled variant of the ELU activation
function.  The  authors  showed  that  if  you  build  a  neural  network  composed  exclu‐
sively of a stack of dense layers, and if all hidden layers use the SELU activation func‐
tion,  then  the  network  will  self-normalize:  the  output  of  each  layer  will  tend  to
preserve  a  mean  of  0  and  standard  deviation  of  1  during  training,  which  solves  the
vanishing/exploding  gradients  problem.  As  a  result,  the  SELU  activation  function
often significantly outperforms other activation functions for such neural nets (espe‐
cially deep ones). There are, however, a few conditions for self-normalization to hap‐
pen (see the paper for the mathematical justification):

• The input features must be standardized (mean 0 and standard deviation 1).

• Every hidden layer’s weights must be initialized with LeCun normal initialization.

In Keras, this means setting kernel_initializer="lecun_normal".

• The  network’s  architecture  must  be  sequential.  Unfortunately,  if  you  try  to  use
SELU  in  nonsequential  architectures,  such  as  recurrent  networks  (see  Chap‐
ter 15) or networks with skip connections (i.e., connections that skip layers, such
as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will
not necessarily outperform other activation functions.

7 Günter Klambauer et al., “Self-Normalizing Neural Networks,” Proceedings of the 31st International Conference

on Neural Information Processing Systems (2017): 972–981.

The Vanishing/Exploding Gradients Problems 

| 

337

• The  paper  only  guarantees  self-normalization  if  all  layers  are  dense,  but  some
researchers  have  noted  that  the  SELU  activation  function  can  improve  perfor‐
mance in convolutional neural nets as well (see Chapter 14).

So, which activation function should you use for the hidden layers
of your deep neural networks? Although your mileage will vary, in
general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh
>  logistic.  If  the  network’s  architecture  prevents  it  from  self-
normalizing, then ELU may perform better than SELU (since SELU
is not smooth at z = 0). If you care a lot about runtime latency, then
you may prefer leaky ReLU. If you don’t want to tweak yet another
hyperparameter,  you  may  use  the  default  α  values  used  by  Keras
(e.g.,  0.3  for  leaky  ReLU).  If  you  have  spare  time  and  computing
power,  you  can  use  cross-validation  to  evaluate  other  activation
functions, such as RReLU if your network is overfitting or PReLU
if you have a huge training set. That said, because ReLU is the most
used  activation  function  (by  far),  many  libraries  and  hardware
accelerators  provide  ReLU-specific  optimizations;  therefore,  if
speed is your priority, ReLU might still be the best choice.

To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your
model just after the layer you want to apply it to:

model = keras.models.Sequential([
    [...]
    keras.layers.Dense(10, kernel_initializer="he_normal"),
    keras.layers.LeakyReLU(alpha=0.2),
    [...]
])

For PReLU, replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no offi‐
cial implementation of RReLU in Keras, but you can fairly easily implement your own
(to learn how to do that, see the exercises at the end of Chapter 12).

For SELU activation, set activation="selu" and kernel_initializer="lecun_nor
mal" when creating a layer:

layer = keras.layers.Dense(10, activation="selu",
                           kernel_initializer="lecun_normal")

Batch Normalization
Although using He initialization along with ELU (or any variant of ReLU) can signifi‐
cantly reduce the danger of the vanishing/exploding gradients problems at the begin‐
ning of training, it doesn’t guarantee that they won’t come back during training.

338 

| 

Chapter 11: Training Deep Neural Networks

In  a  2015  paper,8  Sergey  Ioffe  and  Christian  Szegedy  proposed  a  technique  called
Batch  Normalization  (BN)  that  addresses  these  problems.  The  technique  consists  of
adding an operation in the model just before or after the activation function of each
hidden  layer.  This  operation  simply  zero-centers  and  normalizes  each  input,  then
scales and shifts the result using two new parameter vectors per layer: one for scaling,
the other for shifting. In other words, the operation lets the model learn the optimal
scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as
the very first layer of your neural network, you do not need to standardize your train‐
ing set (e.g., using a StandardScaler); the BN layer will do it for you (well, approxi‐
mately, since it only looks at one batch at a time, and it can also rescale and shift each
input feature).

In  order  to  zero-center  and  normalize  the  inputs,  the  algorithm  needs  to  estimate
each input’s mean and standard deviation. It does so by evaluating the mean and stan‐
dard deviation of the input over the current mini-batch (hence the name “Batch Nor‐
malization”). The whole operation is summarized step by step in Equation 11-3.

Equation 11-3. Batch Normalization algorithm

μB =

m
B
∑
i = 1

1
mB

x i

2

x i − μB

σB

2 =

m
B
1
∑
mB
i = 1
x i − μB
2 + ε
σB
z i = γ ⊗ x i + β

x i =

1 .

2 .

3 .

4 .

In this algorithm:

• μB is the vector of input means, evaluated over the whole mini-batch B (it con‐

tains one mean per input).

• σB is the vector of input standard deviations, also evaluated over the whole mini-

batch (it contains one standard deviation per input).

• mB is the number of instances in the mini-batch.
• x(i) is the vector of zero-centered and normalized inputs for instance i.

8 Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift,” Proceedings of the 32nd International Conference on Machine Learning (2015): 448–
456.

The Vanishing/Exploding Gradients Problems 

| 

339

• γ is the output scale parameter vector for the layer (it contains one scale parame‐

ter per input).

• ⊗ represents element-wise multiplication (each input is multiplied by its corre‐

sponding output scale parameter).

• β is the output shift (offset) parameter vector for the layer (it contains one offset
parameter per input). Each input is offset by its corresponding shift parameter.
• ε  is  a  tiny  number  that  avoids  division  by  zero  (typically  10–5).  This  is  called  a

smoothing term.

• z(i)  is  the  output  of  the  BN  operation.  It  is  a  rescaled  and  shifted  version  of  the

inputs.

So during training, BN standardizes its inputs, then rescales and offsets them. Good!
What about at test time? Well, it’s not that simple. Indeed, we may need to make pre‐
dictions for individual instances rather than for batches of instances: in this case, we
will  have  no  way  to  compute  each  input’s  mean  and  standard  deviation.  Moreover,
even if we do have a batch of instances, it may be too small, or the instances may not
be  independent  and  identically  distributed,  so  computing  statistics  over  the  batch
instances would be unreliable. One solution could be to wait until the end of training,
then run the whole training set through the neural network and compute the mean
and standard deviation of each input of the BN layer. These “final” input means and
standard  deviations  could  then  be  used  instead  of  the  batch  input  means  and  stan‐
dard deviations when making predictions. However, most implementations of Batch
Normalization estimate these final statistics during training by using a moving aver‐
age of the layer’s input means and standard deviations. This is what Keras does auto‐
matically  when  you  use  the  BatchNormalization  layer.  To  sum  up,  four  parameter
vectors are learned in each batch-normalized layer: γ (the output scale vector) and β
(the  output  offset  vector)  are  learned  through  regular  backpropagation,  and  μ  (the
final input mean vector) and σ (the final input standard deviation vector) are estima‐
ted  using  an  exponential  movi
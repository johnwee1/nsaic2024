ir advantages and disadvantages. Neither approach is clearly superior and universally preferred. Instead,
we should choose which language to use for each task. This choice will partially
depend on which probability distribution we wish to describe. We may choose to
use either directed modeling or undirected modeling based on which approach can
capture the most independences in the probability distribution or which approach
uses the fewest edges to describe the distribution. There are other factors that
can aï¬€ect the decision of which language to use. Even while working with a single
probability distribution, we may sometimes switch between diï¬€erent modeling
languages. Sometimes a diï¬€erent language becomes more appropriate if we observe
a certain subset of variables, or if we wish to perform a diï¬€erent computational
task. For example, the directed model description often provides a straightforward
approach to eï¬ƒciently draw samples from the model (described in section 16.3)
while the undirected model formulation is often useful for deriving approximate
inference procedures (as we will see in chapter 19, where the role of undirected
models is highlighted in equation 19.56).
Every probability distribution can be represented by either a directed model
or by an undirected model. In the worst case, one can always represent any
distribution by using a â€œcomplete graph.â€? In the case of a directed model, the
complete graph is any directed acyclic graph where we impose some ordering on
the random variables, and each variable has all other variables that precede it in
the ordering as its ancestors in the graph. For an undirected model, the complete
graph is simply a graph containing a single clique encompassing all of the variables.
See ï¬?gure 16.10 for an example.
Of course, the utility of a graphical model is that the graph implies that some
variables do not interact directly. The complete graph is not very useful because it
does not imply any independences.
When we represent a probability distribution with a graph, we want to choose
a graph that implies as many independences as possible, without implying any
independences that do not actually exist.
From this point of view, some distributions can be represented more eï¬ƒciently
576

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Figure 16.10: Examples of complete graphs, which can describe any probability distribution.
Here we show examples with four random variables. (Left)The complete undirected graph.
In the undirected case, the complete graph is unique. (Right)A complete directed graph.
In the directed case, there is not a unique complete graph. We choose an ordering of the
variables and draw an arc from each variable to every variable that comes after it in the
ordering. There are thus a factorial number of complete graphs for every set of random
variables. In this example we order the variables from left to right, top to bottom.

using directed models, while other distributions can be represented more eï¬ƒciently
using undirected models. In other words, directed models can encode some
independences that undirected models cannot encode, and vice versa.
Directed models are able to use one speciï¬?c kind of substructure that undirected
models cannot represent perfectly. This substructure is called an immorality.
The structure occurs when two random variables a and b are both parents of a
third random variable c, and there is no edge directly connecting a and b in either
direction. (The name â€œimmoralityâ€? may seem strange; it was coined in the graphical
models literature as a joke about unmarried parents.) To convert a directed model
with graph D into an undirected model, we need to create a new graph U. For
every pair of variables x and y, we add an undirected edge connecting x and y to
U if there is a directed edge (in either direction) connecting x and y in D or if x
and y are both parents in D of a third variable z. The resulting U is known as a
moralized graph. See ï¬?gure 16.11 for examples of converting directed models to
undirected models via moralization.
Likewise, undirected models can include substructures that no directed model
can represent perfectly. Speciï¬?cally, a directed graph D cannot capture all of the
conditional independences implied by an undirected graph U if U contains a loop
of length greater than three, unless that loop also contains a chord . A loop is
a sequence of variables connected by undirected edges, with the last variable in
the sequence connected back to the ï¬?rst variable in the sequence. A chord is a
connection between any two non-consecutive variables in the sequence deï¬?ning a
loop. If U has loops of length four or greater and does not have chords for these
loops, we must add the chords before we can convert it to a directed model. Adding
577

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a
a

b

h1

h2

h3

v1

v2

v3

h1

h2

h3

v1

v2

v3

b

c
c

a
a

b

b

c
c

Figure 16.11: Examples of converting directed models (top row) to undirected models
(bottom row) by constructing moralized graphs. (Left)This simple chain can be converted
to a moralized graph merely by replacing its directed edges with undirected edges. The
resulting undirected model implies exactly the same set of independences and conditional
independences. (Center)This graph is the simplest directed model that cannot be converted
to an undirected model without losing some independences. This graph consists entirely
of a single immorality. Because a and b are parents of c, they are connected by an active
path when c is observed. To capture this dependence, the undirected model must include
a clique encompassing all three variables. This clique fails to encode the fact thataâŠ¥b.
(Right)In general, moralization may add many edges to the graph, thus losing many
implied independences. For example, this sparse coding graph requires adding moralizing
edges between every pair of hidden units, thus introducing a quadratic number of new
direct dependences.

578

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a

b

a

b

a

b

d

c

d

c

d

c

Figure 16.12: Converting an undirected model to a directed model. (Left)This undirected
model cannot be converted directed to a directed model because it has a loop of length four
with no chords. Speciï¬?cally, the undirected model encodes two diï¬€erent independences that
no directed model can capture simultaneously: aâŠ¥c | {b, d} and bâŠ¥d | {a, c}. (Center)To
convert the undirected model to a directed model, we must triangulate the graph, by
ensuring that all loops of greater than length three have a chord. To do so, we can either
add an edge connecting a and c or we can add an edge connecting b and d. In this
example, we choose to add the edge connectinga and c. (Right)To ï¬?nish the conversion
process, we must assign a direction to each edge. When doing so, we must not create any
directed cycles. One way to avoid directed cycles is to impose an ordering over the nodes,
and always point each edge from the node that comes earlier in the ordering to the node
that comes later in the ordering. In this example, we use the variable names to impose
alphabetical order.

these chords discards some of the independence information that was encoded in U.
The graph formed by adding chords to U is known as a chordal or triangulated
graph, because all the loops can now be described in terms of smaller, triangular
loops. To build a directed graph D from the chordal graph, we need to also assign
directions to the edges. When doing so, we must not create a directed cycle in
D, or the result does not deï¬?ne a valid directed probabilistic model. One way
to assign directions to the edges in D is to impose an ordering on the random
variables, then point each edge from the node that comes earlier in the ordering to
the node that comes later in the ordering. See ï¬?gure 16.12 for a demonstration.

16.2.7

Factor Graphs

Factor graphs are another way of drawing undirected models that resolve an
ambiguity in the graphical representation of standard undirected model syntax. In
an undirected model, the scope of every Ï† function must be a subset of some clique
in the graph. Ambiguity arises because it is not clear if each clique actually has
a corresponding factor whose scope encompasses the entire cliqueâ€”for example,
a clique containing three nodes may correspond to a factor over all three nodes,
or may correspond to three factors that each contain only a pair of the nodes.
579

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Factor graphs resolve this ambiguity by explicitly representing the scope of each Ï†
function. Speciï¬?cally, a factor graph is a graphical representation of an undirected
model that consists of a bipartite undirected graph. Some of the nodes are drawn
as circles. These nodes correspond to random variables as in a standard undirected
model. The rest of the nodes are drawn as squares. These nodes correspond to
the factors Ï† of the unnormalized probability distribution. Variables and factors
may be connected with undirected edges. A variable and a factor are connected
in the graph if and only if the variable is one of the arguments to the factor in
the unnormalized probability distribution. No factor may be connected to another
factor in the graph, nor can a variable be connected to a variable. See ï¬?gure 16.13
for an example of how factor graphs can resolve ambiguity in the interpretation of
undirected networks.
a

a

b

b

a

f2

f1
f1

c

b

c

f3

c

Figure 16.13: An example of how a factor graph can resolve ambiguity in the interpretation
of undirected networks. (Left)An undirected network with a clique involving three variables:
a, b and c. (Center)A factor graph corresponding to the same undirected model. This
factor graph has one factor over all three variables. (Right)Another valid factor graph
for the same undirected model. This factor graph has three factors, each over only two
variables. Representation, inference, and learning are all asymptotically cheaper in this
factor graph than in the factor graph depicted in the center, even though both require the
same undirected graph to represent.

16.3

Sampling from Graphical Models

Graphical models also facilitate the task of drawing samples from a model.
One advantage of directed graphical models is that a simple and eï¬ƒcient procedure called ancestral sampling can produce a sample from the joint distribution
represented by the model.
The basic idea is to sort the variables xi in the graph into a topological ordering,
so that for all i and j, j is greater than i if xi is a parent of xj . The variables
580

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

can then be sampled in this order. In other words, we ï¬?rst sample x1 âˆ¼ P (x1),
then sample P (x2 | P aG (x2 )), and so on, until ï¬?nally we sample P (xn | P aG (xn )).
So long as each conditional distribution p( xi | P aG (xi )) is easy to sample from,
then the whole model is easy to sample from. The topological sorting operation
guarantees that we can read the conditional distributions in equation 16.1 and
sample from them in order. Without the topological sorting, we might attempt to
sample a variable before its parents are available.
For some graphs, more than one topological ordering is possible. Ancestral
sampling may be used with any of these topological orderings.
Ancestral sampling is generally very fast (assuming sampling from each conditional is easy) and convenient.
One drawback to ancestral sampling is that it only applies to directed graphical
models. Another drawback is that it does not support every conditional sampling
operation. When we wish to sample from a subset of the variables in a directed
graphical model, given some other variables, we often require that all the conditioning variables come earlier than the variables to be sampled in the ordered graph.
In this case, we can sample from the local conditional probability distributions
speciï¬?ed by the model distribution. Otherwise, the conditional distributions we
need to sample from are the posterior distributions given the observed variables.
These posterior distributions are usually not explicitly speciï¬?ed and parametrized
in the model. Inferring these posterior distributions can be costly. In models where
this is the case, ancestral sampling is no longer eï¬ƒcient.
Unfortunately, ancestral sampling is applicable only to directed models. We
can sample from undirected models by converting them to directed models, but this
often requires solving intractable inference problems (to determine the marginal
distribution over the root nodes of the new directed graph) or requires introducing
so many edges that the resulting directed model becomes intractable. Sampling
from an undirected model without ï¬?rst converting it to a directed model seems to
require resolving cyclical dependencies. Every variable interacts with every other
variable, so there is no clear beginning point for the sampling process. Unfortunately,
drawing samples from an undirected graphical model is an expensive, multi-pass
process. The conceptually simplest approach is Gibbs sampling. Suppose we
have a graphical model over an n-dimensional vector of random variables x. We
iteratively visit each variable xi and draw a sample conditioned on all of the other
variables, from p(xi | xâˆ’i ). Due to the separation properties of the graphical
model, we can equivalently condition on only the neighbors of xi. Unfortunately,
after we have made one pass through the graphical model and sampled all n
variables, we still do not have a fair sample from p(x). Instead, we must repeat the
581

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

process and resample all n variables using the updated values of their neighbors.
Asymptotically, after many repetitions, this process converges to sampling from
the correct distribution. It can be diï¬ƒcult to determine when the samples have
reached a suï¬ƒciently accurate approximation of the desired distribution. Sampling
techniques for undirected models are an advanced topic, covered in more detail in
chapter 17.

16.4

Advantages of Structured Modeling

The primary advantage of using structured probabilistic models is that they allow
us to dramatically reduce the cost of representing probability distributions as well
as learning and inference. Sampling is also accelerated in the case of directed
models, while the situation can be complicated with undirected models. The
primary mechanism that allows all of these operations to use less runtime and
memory is choosing to not model certain interactions. Graphical models convey
information by leaving edges out. Anywhere there is not an edge, the model
speciï¬?es the assumption that we do not need to model a direct interaction.
A less quantiï¬?able beneï¬?t of using structured probabilistic models is that
they allow us to explicitly separate representation of knowledge from learning of
knowledge or inference given existing knowledge. This makes our models easier to
develop and debug. We can design, analyze, and evaluate learning algorithms and
inference algorithms that are applicable to broad classes of graphs. Independently,
we can design models that capture the relationships we believe are important in our
data. We can then combine these diï¬€erent algorithms and structures and obtain
a 
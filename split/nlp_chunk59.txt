ers, investigating similar systems, and studying related human-human dialogues.

Wizard-of-Oz
system

2. Build simulations and prototypes: A crucial tool in building dialogue systems
is the Wizard-of-Oz system. In wizard systems, the users interact with what they

332 CHAPTER 15

• CHATBOTS & DIALOGUE SYSTEMS

think is a program but is in fact a human “wizard” disguised by a software interface
(Gould et al. 1983, Good et al. 1984, Fraser and
Gilbert 1991). The name comes from the chil-
dren’s book The Wizard of Oz (Baum, 1900),
in which the wizard turned out to be a simu-
lation controlled by a man behind a curtain or
screen. A wizard system can be used to test out
an architecture before implementation; only the
interface software and databases need to be in
place. The wizard gets input from the user, uses
a database interface to run queries based on the
user utterance, and then outputs sentences, ei-
ther by typing them or speaking them.

Wizard-of-Oz systems are not a perfect
simulation, since the wizard doesn’t exactly
simulate the errors or limitations of a real sys-
tem; but wizard studies can still provide a useful ﬁrst idea of the domain issues.

3. Iteratively test the design on users: An iterative design cycle with embedded
user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich
et al. 1995, Landauer 1995). For example in a well-known incident, an early dia-
logue system required the user to press a key to interrupt the system (Stifelman et al.,
1993). But user testing showed users barged in (interrupted, talking over the sys-
tem), which led to a redesign of the system to recognize overlapped speech. It’s also
important to incorporate value sensitive design, in which we carefully consider dur-
ing the design process the beneﬁts, harms and possible stakeholders of the resulting
system (Friedman et al. 2017, Friedman and Hendry 2019).

barged in

value sensitive
design

15.5.1 Ethical Issues in Dialogue System Design

Ethical issues have been key to how we think about designing artiﬁcial agents since
well before we had dialogue systems. Mary Shelley (depicted below) centered her
novel Frankenstein around the problem of creating artiﬁcial agents without consider-
ing
ethical and humanistic concerns. One issue is the
If users seek information from di-
safety of users.
alogue systems in safety-critical situations like ask-
ing medical advice, or in emergency situations, or
when indicating the intentions of self-harm, incorrect
advice can be dangerous and even life-threatening.
For example (Bickmore et al., 2018) gave participants
medical problems to pose to three commercial di-
alogue systems (Siri, Alexa, Google Assistant) and
asked them to determine an action to take based on
the system responses; many of the proposed actions,
if actually taken, would have led to harm or death.

A system can also harm users by verbally attacking them, or creating represen-
tational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes
that demean particular groups of people. Both abuse and stereotypes can cause psy-
chological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken
ofﬂine 16 hours after it went live, when it began posting messages with racial slurs,

Tay

15.6

• SUMMARY

333

conspiracy theories, and personal attacks on its users. Tay had learned these biases
and actions from its training data, including from users who seemed to be purposely
teaching the system to repeat this kind of language (Neff and Nagy 2016). Hender-
son et al. (2017) examined dialogue datasets used to train corpus-based chatbots and
found toxic and abusive language, especially in social media corpora like Twitter
and Reddit, and indeed such language then appears in the text generated by lan-
guage models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which
can even amplify the bias from the training data (Dinan et al., 2020). Liu et al.
(2020) developed another method for investigating bias, testing how neural dialogue
systems responded to pairs of simulated user turns that are identical except for men-
tioning different genders or race. They found, for example, that simple changes like
using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more
offensively and with more negative sentiment.

Another important ethical issue is privacy. Already in the ﬁrst days of ELIZA,
Weizenbaum pointed out the privacy implications of people’s revelations to the chat-
bot. The ubiquity of in-home dialogue systems means they may often overhear
private information (Henderson et al., 2017). If a chatbot is human-like, users are
also more likely to disclose private information, and less likely to worry about the
harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained
on transcripts of human-human or human-machine conversation must anonymize
personally identiﬁable information.

Finally, chatbots raise important issues of gender equality in addition to textual
bias. Current chatbots are overwhelmingly given female names, likely perpetuating
the stereotype of a subservient female servant (Paolino, 2017). And when users
use sexually harassing language, most commercial chatbots evade or give positive
responses rather than responding in clear negative ways (Fessler, 2017).

These ethical issues are an important area of investigation, including ﬁnding
ways to mitigate problems of abuse and toxicity, like detecting and responding ap-
propriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020).
Value sensitive design, carefully considering possible harms in advance (Friedman
et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give
a number of suggestions for best practices in dialogue system design. For exam-
ple getting informed consent from participants, whether they are used for training,
or whether they are interacting with a deployed system is important. Because di-
alogue systems by deﬁnition involve human participants, researchers also work on
these issues with the Institutional Review Boards (IRB) at their institutions, who
help protect the safety of experimental subjects.

IRB

15.6 Summary

Chatbots and dialogue systems are crucial speech and language processing appli-
cations that are already widely used commercially.

• In human dialogue, speaking is a kind of action; these acts are referred to
as speech acts or dialogue acts. Speakers also attempt to achieve common
ground by acknowledging that they have understand each other. Conversation
also is characterized by turn structure and dialogue structure.

• Chatbots are conversational systems designed to mimic the appearance of in-
formal human conversation. Rule-based chatbots like ELIZA and its modern

334 CHAPTER 15

• CHATBOTS & DIALOGUE SYSTEMS

descendants use rules to map user sentences into system responses. Corpus-
based chatbots mine logs of human conversation to learn to automatically map
user sentences into system responses.

• For task-based dialogue, most commercial dialogue systems use the GUS or
frame-based architecture, in which the designer speciﬁes frames consisting of
slots that the system must ﬁll by asking the user.

• The dialogue-state architecture augments the GUS frame-and-slot architec-
ture with richer representations and more sophisticated algorithms for keeping
track of user’s dialogue acts, policies for generating its own dialogue acts, and
a natural language component.

• Dialogue systems are a kind of human-computer interaction, and general HCI
principles apply in their design, including the role of the user, simulations such
as Wizard-of-Oz systems, and the importance of iterative design and testing
on real users.

Bibliographical and Historical Notes

conversation
analysis

The linguistic, philosophical, and psychological literature on dialogue is quite ex-
tensive. For example the idea that utterances in a conversation are a kind of action
being performed by the speaker was due originally to the philosopher Wittgenstein
(1953) but worked out more fully by Austin (1962) and his student John Searle.
Various sets of speech acts have been deﬁned over the years, and a rich linguistic
and philosophical literature developed, especially focused on explaining the use of
indirect speech acts. The idea of dialogue acts draws also from a number of other
sources, including the ideas of adjacency pairs, pre-sequences, and other aspects of
the interactional properties of human conversation developed in the ﬁeld of conver-
sation analysis (see Levinson (1983) for an introduction to the ﬁeld). This idea that
acts set up strong local dialogue expectations was also preﬁgured by Firth (1935, p.
70), in a famous quotation:

Most of the give-and-take of conversation in our everyday life is stereotyped
and very narrowly conditioned by our particular type of culture. It is a sort
of roughly prescribed social ritual, in which you generally say what the other
fellow expects you, one way or the other, to say.

Another important research thread modeled dialogue as a kind of collaborative
behavior, including the ideas of common ground (Clark and Marshall, 1981), ref-
erence as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention
(Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).

The earliest conversational systems were simple pattern-action chatbots like ELIZA

(Weizenbaum, 1966). ELIZA had a widespread inﬂuence on popular perceptions of
artiﬁcial intelligence, and brought up some of the ﬁrst ethical questions in natural
language processing —such as the issues of privacy we discussed above as well the
role of algorithms in decision-making— leading its creator Joseph Weizenbaum to
ﬁght for social responsibility in AI and computer science in general.

Computational-implemented theories of dialogue blossomed in the 1970. That
period saw the very inﬂuential GUS system (Bobrow et al., 1977), which in the late
1970s established the frame-based paradigm that became the dominant industrial
paradigm for dialogue systems for over 30 years.

BDI

BIBLIOGRAPHICAL AND HISTORICAL NOTES

335

Another inﬂuential line of research from that decade focused on modeling the hi-
erarchical structure of dialogue. Grosz’s pioneering 1977b dissertation ﬁrst showed
that “task-oriented dialogues have a structure that closely parallels the structure of
the task being performed” (p. 27), leading to her work with Sidner and others show-
ing how to use similar notions of intention and plans to model discourse structure
and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the
role of intentional structure in dialogue.

Yet a third line, ﬁrst suggested by Bruce (1975), suggested that since speech acts
are actions, they should be planned like other actions, and drew on the AI planning
literature (Fikes and Nilsson, 1971). A system seeking to ﬁnd out some information
can come up with the plan of asking the interlocutor for the information. A system
hearing an utterance can interpret a speech act by running the planner “in reverse”,
using inference rules to infer from what the interlocutor said what the plan might
have been. Plan-based models of dialogue are referred to as BDI models because
such planners model the beliefs, desires, and intentions (BDI) of the system and in-
terlocutor. BDI models of dialogue were ﬁrst introduced by Allen, Cohen, Perrault,
and their colleagues in a number of inﬂuential papers showing how speech acts could
be generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980,
Allen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based
models of understanding as part of the task of interpreting stories.

In the 1990s, machine learning models that had ﬁrst been applied to natural
language processing began to be applied to dialogue tasks like slot ﬁlling (Miller
et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the
linguistic properties of dialogue acts and on machine-learning-based methods for
their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and
Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke
et al. 2000, Gravano et al. 2012. This work strongly informed the development
of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking
quickly became an important problem for task-oriented dialogue, and there has been
an inﬂuential annual evaluation of state-tracking algorithms (Williams et al., 2016).
The turn of the century saw a line of work on applying reinforcement learning
to dialogue, which ﬁrst came out of AT&T and Bell Laboratories with work on
MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along
with work on cue phrases, prosody, and rejection and conﬁrmation. Reinforcement
learning research turned quickly to the more sophisticated POMDP models (Roy
et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-
ﬁlling dialogue tasks. Neural reinforcement learning models have been used both for
chatbot systems, for example simulating dialogues between two dialogue systems,
rewarding good conversational properties like coherence and ease of answering (Li
et al., 2016a), and for task-oriented dialogue (Williams et al., 2017).

By around 2010 the GUS architecture ﬁnally began to be widely used commer-
cially in dialogue systems on phones like Apple’s SIRI (Bellegarda, 2013) and other
digital assistants.

The rise of the web gave rise to corpus-based chatbot architectures around the
turn of the century, ﬁrst using information retrieval models and then in the 2010s,
after the rise of deep learning, with sequence-to-sequence models.

[TBD: Modern history of neural chatbots]
Other important dialogue areas include the study of affect in dialogue (Rashkin
et al. 2019, Lin et al. 2019) and conversational interface design (Cohen et al. 2004,
Harris 2005, Pearl 2017, Deibel and Evanhoe 2021).

336 CHAPTER 15

• CHATBOTS & DIALOGUE SYSTEMS

Exercises

dispreferred
response

15.1 Write a ﬁnite-state automaton for a dialogue manager for checking your bank

balance and withdrawing money at an automated teller machine.

15.2 A dispreferred response is a response that has the potential to make a person
uncomfortable or embarrassed in the conversational context; the most com-
mon example dispreferred responses is turning down a request. People signal
their discomfort with having to say no with surface cues (like the word well),
or via signiﬁcant silence. Try to notice the next time you or someone else
utters a dispreferred response, and write down the utterance. What are some
other cues in the response that a system might use to detect a dispreferred
response? Consider non-verbal cues like eye gaze and body gestures.

15.3 When asked a question to which they aren’t sure they know the answer, peo-
ple display their lack of conﬁdence by cues that resemble other dispreferred
responses. Try to notice some unsure answers to questions. What are some
of the cues? If you have trouble doing this, read Smith and Clark (1993) and
listen speciﬁcally for the cues they mention.

15.4 Implement a small air-travel help system based on text input. Your system
should get constraints from users about a particular ﬂight that they want to
take, expressed in natural language, and display possible ﬂights on a screen.
â€”and itâ€™s our job to come up
with statistical procedures (such as maximum likelihood) to try to estimate
this parameter.

An alternative way to approach our parameter estimation problems is to
take the Bayesian view of the world, and think of Î¸ as being a random
variable whose value is unknown.
In this approach, we would specify a
prior distribution p(Î¸) on Î¸ that expresses our â€œprior beliefsâ€ about the
parameters. Given a training set S = {(x(i), y(i))}n
i=1, when we are asked to
make a prediction on a new value of x, we can then compute the posterior
distribution on the parameters

p(Î¸|S) =

=

p(S|Î¸)p(Î¸)
p(S)
(cid:0)(cid:81)n
Î¸ ((cid:81)n

(cid:82)

i=1 p(y(i)|x(i), Î¸)(cid:1) p(Î¸)
i=1 p(y(i)|x(i), Î¸)p(Î¸)) dÎ¸

(9.3)

In the equation above, p(y(i)|x(i), Î¸) comes from whatever model youâ€™re using

143

for your learning problem. For example, if you are using Bayesian logistic re-
gression, then you might choose p(y(i)|x(i), Î¸) = hÎ¸(x(i))y(i)(1âˆ’hÎ¸(x(i)))(1âˆ’y(i)),
where hÎ¸(x(i)) = 1/(1 + exp(âˆ’Î¸T x(i))).7

When we are given a new test example x and asked to make it prediction
on it, we can compute our posterior distribution on the class label using the
posterior distribution on Î¸:

p(y|x, S) =

(cid:90)

Î¸

p(y|x, Î¸)p(Î¸|S)dÎ¸

(9.4)

In the equation above, p(Î¸|S) comes from Equation (9.3). Thus, for example,
if the goal is to the predict the expected value of y given x, then we would
output8

(cid:90)

E[y|x, S] =

yp(y|x, S)dy

y

The procedure that weâ€™ve outlined here can be thought of as doing â€œfully
Bayesianâ€ prediction, where our prediction is computed by taking an average
with respect to the posterior p(Î¸|S) over Î¸. Unfortunately, in general it is
computationally very diï¬ƒcult to compute this posterior distribution. This is
because it requires taking integrals over the (usually high-dimensional) Î¸ as
in Equation (9.3), and this typically cannot be done in closed-form.

Thus, in practice we will instead approximate the posterior distribution
for Î¸. One common approximation is to replace our posterior distribution for
Î¸ (as in Equation 9.4) with a single point estimate. The MAP (maximum
a posteriori) estimate for Î¸ is given by

Î¸MAP = arg max

Î¸

n
(cid:89)

i=1

p(y(i)|x(i), Î¸)p(Î¸).

(9.5)

Note that this is the same formulas as for the MLE (maximum likelihood)
estimate for Î¸, except for the prior p(Î¸) term at the end.

In practical applications, a common choice for the prior p(Î¸) is to assume
that Î¸ âˆ¼ N (0, Ï„ 2I). Using this choice of prior, the ï¬tted parameters Î¸MAP will
have smaller norm than that selected by maximum likelihood. In practice,
this causes the Bayesian MAP estimate to be less susceptible to overï¬tting
than the ML estimate of the parameters. For example, Bayesian logistic
regression turns out to be an eï¬€ective algorithm for text classiï¬cation, even
though in text classiï¬cation we usually have d (cid:29) n.

7Since we are now viewing Î¸ as a random variable, it is okay to condition on it value,

and write â€œp(y|x, Î¸)â€ instead of â€œp(y|x; Î¸).â€

8The integral below would be replaced by a summation if y is discrete-valued.

Part IV

Unsupervised learning

144

Chapter 10

Clustering and the k-means
algorithm

In the clustering problem, we are given a training set {x(1), . . . , x(n)}, and
want to group the data into a few cohesive â€œclusters.â€ Here, x(i) âˆˆ Rd
as usual; but no labels y(i) are given. So, this is an unsupervised learning
problem.

The k-means clustering algorithm is as follows:

1. Initialize cluster centroids Âµ1, Âµ2, . . . , Âµk âˆˆ Rd randomly.

2. Repeat until convergence: {

For every i, set

For each j, set

}

c(i) := arg min

j

||x(i) âˆ’ Âµj||2.

Âµj :=

(cid:80)n

i=1 1{c(i) = j}x(i)
(cid:80)n
i=1 1{c(i) = j}

.

In the algorithm above, k (a parameter of the algorithm) is the number
of clusters we want to ï¬nd; and the cluster centroids Âµj represent our current
guesses for the positions of the centers of the clusters. To initialize the cluster
centroids (in step 1 of the algorithm above), we could choose k training
examples randomly, and set the cluster centroids to be equal to the values of
these k examples. (Other initialization methods are also possible.)

The inner-loop of the algorithm repeatedly carries out two steps: (i)
â€œAssigningâ€ each training example x(i) to the closest cluster centroid Âµj, and

145

146

Figure 10.1: K-means algorithm. Training examples are shown as dots, and
cluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-
tial cluster centroids (in this instance, not chosen to be equal to two training
examples). (c-f) Illustration of running two iterations of k-means. In each
iteration, we assign each training example to the closest cluster centroid
(shown by â€œpaintingâ€ the training examples the same color as the cluster
centroid to which is assigned); then we move each cluster centroid to the
mean of the points assigned to it. (Best viewed in color.) Images courtesy
Michael Jordan.

(ii) Moving each cluster centroid Âµj to the mean of the points assigned to it.
Figure 10.1 shows an illustration of running k-means.

Is the k-means algorithm guaranteed to converge? Yes it is, in a certain

sense. In particular, let us deï¬ne the distortion function to be:

J(c, Âµ) =

n
(cid:88)

i=1

||x(i) âˆ’ Âµc(i)||2

Thus, J measures the sum of squared distances between each training exam-
ple x(i) and the cluster centroid Âµc(i) to which it has been assigned. It can
be shown that k-means is exactly coordinate descent on J. Speciï¬cally, the
inner-loop of k-means repeatedly minimizes J with respect to c while holding
Âµ ï¬xed, and then minimizes J with respect to Âµ while holding c ï¬xed. Thus,

147

J must monotonically decrease, and the value of J must converge. (Usu-
ally, this implies that c and Âµ will converge too. In theory, it is possible for
k-means to oscillate between a few diï¬€erent clusteringsâ€”i.e., a few diï¬€erent
values for c and/or Âµâ€”that have exactly the same value of J, but this almost
never happens in practice.)

The distortion function J is a non-convex function, and so coordinate
descent on J is not guaranteed to converge to the global minimum. In other
words, k-means can be susceptible to local optima. Very often k-means will
work ï¬ne and come up with very good clusterings despite this. But if you
are worried about getting stuck in bad local minima, one common thing to
do is run k-means many times (using diï¬€erent random initial values for the
cluster centroids Âµj). Then, out of all the diï¬€erent clusterings found, pick
the one that gives the lowest distortion J(c, Âµ).

Chapter 11

EM algorithms

In this set of notes, we discuss the EM (Expectation-Maximization) algorithm
for density estimation.

11.1 EM for mixture of Gaussians

Suppose that we are given a training set {x(1), . . . , x(n)} as usual. Since we
are in the unsupervised learning setting, these points do not come with any
labels.

We wish to model the data by specifying a joint distribution p(x(i), z(i)) =
p(x(i)|z(i))p(z(i)). Here, z(i) âˆ¼ Multinomial(Ï†) (where Ï†j â‰¥ 0, (cid:80)k
j=1 Ï†j = 1,
and the parameter Ï†j gives p(z(i) = j)), and x(i)|z(i) = j âˆ¼ N (Âµj, Î£j). We
let k denote the number of values that the z(i)â€™s can take on. Thus, our
model posits that each x(i) was generated by randomly choosing z(i) from
{1, . . . , k}, and then x(i) was drawn from one of k Gaussians depending on
z(i). This is called the mixture of Gaussians model. Also, note that the
z(i)â€™s are latent random variables, meaning that theyâ€™re hidden/unobserved.
This is what will make our estimation problem diï¬ƒcult.

The parameters of our model are thus Ï†, Âµ and Î£. To estimate them, we

can write down the likelihood of our data:

(cid:96)(Ï†, Âµ, Î£) =

=

n
(cid:88)

i=1
n
(cid:88)

i=1

log p(x(i); Ï†, Âµ, Î£)

log

k
(cid:88)

z(i)=1

p(x(i)|z(i); Âµ, Î£)p(z(i); Ï†).

However, if we set to zero the derivatives of this formula with respect to

148

149

the parameters and try to solve, weâ€™ll ï¬nd that it is not possible to ï¬nd the
maximum likelihood estimates of the parameters in closed form. (Try this
yourself at home.)

The random variables z(i) indicate which of the k Gaussians each x(i)
had come from. Note that if we knew what the z(i)â€™s were, the maximum
likelihood problem would have been easy. Speciï¬cally, we could then write
down the likelihood as

(cid:96)(Ï†, Âµ, Î£) =

n
(cid:88)

i=1

log p(x(i)|z(i); Âµ, Î£) + log p(z(i); Ï†).

Maximizing this with respect to Ï†, Âµ and Î£ gives the parameters:

Ï†j =

Âµj =

Î£j =

n
(cid:88)

1
n
(cid:80)n

(cid:80)n

1{z(i) = j},

i=1
i=1 1{z(i) = j}x(i)
(cid:80)n
i=1 1{z(i) = j}

,

i=1 1{z(i) = j}(x(i) âˆ’ Âµj)(x(i) âˆ’ Âµj)T
i=1 1{z(i) = j}

(cid:80)n

.

Indeed, we see that if the z(i)â€™s were known, then maximum likelihood
estimation becomes nearly identical to what we had when estimating the
parameters of the Gaussian discriminant analysis model, except that here
the z(i)â€™s playing the role of the class labels.1

However, in our density estimation problem, the z(i)â€™s are not known.

What can we do?

The EM algorithm is an iterative algorithm that has two main steps.
Applied to our problem, in the E-step, it tries to â€œguessâ€ the values of the
z(i)â€™s. In the M-step, it updates the parameters of our model based on our
guesses. Since in the M-step we are pretending that the guesses in the ï¬rst
part were correct, the maximization becomes easy. Hereâ€™s the algorithm:

Repeat until convergence: {

(E-step) For each i, j, set

w(i)
j

:= p(z(i) = j|x(i); Ï†, Âµ, Î£)

1There are other minor diï¬€erences in the formulas here from what weâ€™d obtained in
PS1 with Gaussian discriminant analysis, ï¬rst because weâ€™ve generalized the z(i)â€™s to be
multinomial rather than Bernoulli, and second because here we are using a diï¬€erent Î£j
for each Gaussian.

(M-step) Update the parameters:

150

Ï†j

:=

Âµj

:=

Î£j

:=

1
n

n
(cid:88)

i=1

w(i)
j

,

(cid:80)n

,

i=1 w(i)
j x(i)
(cid:80)n
i=1 w(i)
i=1 w(i)

j

(cid:80)n

j (x(i) âˆ’ Âµj)(x(i) âˆ’ Âµj)T

(cid:80)n

i=1 w(i)

j

}

In the E-step, we calculate the posterior probability of our parameters
the z(i)â€™s, given the x(i) and using the current setting of our parameters. I.e.,
using Bayes rule, we obtain:

p(z(i) = j|x(i); Ï†, Âµ, Î£) =

p(x(i)|z(i) = j; Âµ, Î£)p(z(i) = j; Ï†)
l=1 p(x(i)|z(i) = l; Âµ, Î£)p(z(i) = l; Ï†)

(cid:80)k

Here, p(x(i)|z(i) = j; Âµ, Î£) is given by evaluating the density of a Gaussian
with mean Âµj and covariance Î£j at x(i); p(z(i) = j; Ï†) is given by Ï†j, and so
on. The values w(i)
j calculated in the E-step represent our â€œsoftâ€ guesses2 for
the values of z(i).

Also, you should contrast the updates in the M-step with the formulas we
had when the z(i)â€™s were known exactly. They are identical, except that in-
stead of the indicator functions â€œ1{z(i) = j}â€ indicating from which Gaussian
each datapoint had come, we now instead have the w(i)
j

â€™s.

The EM-algorithm is also reminiscent of the K-means clustering algo-
rithm, except that instead of the â€œhardâ€ cluster assignments c(i), we instead
have the â€œsoftâ€ assignments w(i)
. Similar to K-means, it is also susceptible
j
to local optima, so reinitializing at several diï¬€erent initial parameters may
be a good idea.

Itâ€™s clear that the EM algorithm has a very natural interpretation of
repeatedly trying to guess the unknown z(i)â€™s; but how did it come about,
and can we make any guarantees about it, such as regarding its convergence?
In the next set of notes, we will describe a more general view of EM, one

2The term â€œsoftâ€ refers to our guesses being probabilities and taking values in [0, 1]; in
contrast, a â€œhardâ€ guess is one that represents a single best guess (such as taking values
in {0, 1} or {1, . . . , k}).

151

that will allow us to easily apply it to other estimation problems in which
there are also latent variables, and which will allow us to give a convergence
guarantee.

11.2 Jensenâ€™s inequality

We begin our discussion with a very useful result called Jensenâ€™s inequality
Let f be a function whose domain is the set of real numbers. Recall that
f is a convex function if f (cid:48)(cid:48)(x) â‰¥ 0 (for all x âˆˆ R). In the case of f taking
vector-valued inputs, this is generalized to the condition that its hessian H
is positive semi-deï¬nite (H â‰¥ 0). If f (cid:48)(cid:48)(x) > 0 for all x, then we say f is
strictly convex (in the vector-valued case, the corresponding statement is
that H must be positive deï¬nite, written H > 0). Jensenâ€™s inequality can
then be stated as follows:

Theorem. Let f be a convex function, and let X be a random variable.
Then:

E[f (X)] â‰¥ f (EX).

Moreover, if f is strictly convex, then E[f (X)] = f (EX) holds true if and
only if X = E[X] with probability 1 (i.e., if X is a constant).

Recall our convention of occasionally dropping the parentheses when writ-

ing expectations, so in the theorem above, f (EX) = f (E[X]).

For an interpretation of the theorem, consider the ï¬gure below.

Here, f is a convex function shown by the solid line. Also, X is a random
variable that has a 0.5 chance of taking the value a, and a 0.5 chance of

aE[X]bf(a)f(b)f(EX)E[f(X)]f152

taking the value b (indicated on the x-axis). Thus, the expected value of X
is given by the midpoint between a and b.

We also see the values f (a), f (b) and f (E[X]) indicated on the y-axis.
Moreover, the value E[f (X)] is now the midpoint on the y-axis between f (a)
and f (b). From our example, we see that because f is convex, it must be the
case that E[f (X)] â‰¥ f (EX).

Incidentally, quite a lot of people have trouble remembering which way
the inequality goes, and remembering a picture like this is a good way to
quickly ï¬gure out the answer.
Remark. Recall that f is [strictly] concave if and only if âˆ’f is [strictly]
convex (i.e., f (cid:48)(cid:48)(x) â‰¤ 0 or H â‰¤ 0). Jensenâ€™s inequality also holds for concave
functions f , but with the direction of all the inequalities reversed (E[f (X)] â‰¤
f (EX), etc.).

11.3 General EM algorithms

Suppose we have an estimation problem in which we have a training set
{x(1), . . . , x(n)} consisting of n independent examples. We have a latent vari-
able model p(x, z; Î¸) with z being the latent variable (which for simplicity is
assumed to take ï¬nite number of values). The density for x can be obtained
by marginalized over the latent variable z:

p(x; Î¸) =

p(x, z; Î¸)

(cid:88)

z

(11.1)

We wish to ï¬t the parameters Î¸ by maximizing the log-likelihood of the

data, deï¬ned by

(cid:96)(Î¸) =

n
(cid:88)

i=1

log p(x(i); Î¸)

(11.2)

We can rewrite the objective in terms of the joint density p(x, z; Î¸) by

(cid:96)(Î¸) =

=

n
(cid:88)

i=1
n
(cid:88)

i=1

log p(x(i); Î¸)

(cid:88)

log

p(x(i), z(i); Î¸).

z(i)

(11.3)

(11.4)

But, explicitly ï¬nding the maximum likelihood estimates of the parameters
Î¸ may be hard since it will result in diï¬ƒcult non-convex optimization prob-

lems.3 Here, the z(i)â€™s are the latent random variables; and it is often the case
that if the z(i)â€™s were observed, then maximum likelihood estimation would
be easy.

153

In such a setting, the EM algorithm gives an eï¬ƒcient method for max-
imum likelihood estimation. Maximizing (cid:96)(Î¸) explicitly might be diï¬ƒcult,
and our strategy will be to instead repeatedly construct a lower-bound on (cid:96)
(E-step), and t
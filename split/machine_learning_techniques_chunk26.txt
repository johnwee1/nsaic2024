 obtain a reduced dataset Xd-proj of
dimensionality d, compute the matrix multiplication of the training set matrix X by
the matrix Wd, defined as the matrix containing the first d columns of V, as shown in
Equation 8-2.

Equation 8-2. Projecting the training set down to d dimensions

Xd‐proj = XWd

The following Python code projects the training set onto the plane defined by the first
two principal components:

W2 = Vt.T[:, :2]
X2D = X_centered.dot(W2)

There  you  have  it!  You  now  know  how  to  reduce  the  dimensionality  of  any  dataset
down to any number of dimensions, while preserving as much variance as possible.

Using Scikit-Learn
Scikit-Learn’s PCA class uses SVD decomposition to implement PCA, just like we did
earlier in this chapter. The following code applies PCA to reduce the dimensionality
of the dataset down to two dimensions (note that it automatically takes care of center‐
ing the data):

from sklearn.decomposition import PCA

pca = PCA(n_components = 2)
X2D = pca.fit_transform(X)

After  fitting  the  PCA  transformer  to  the  dataset,  its  components_  attribute  holds  the
transpose  of  Wd  (e.g.,  the  unit  vector  that  defines  the  first  principal  component  is
equal to pca.components_.T[:, 0]).

Explained Variance Ratio
Another useful piece of information is the explained variance ratio of each principal
component,  available  via  the  explained_variance_ratio_  variable.  The  ratio  indi‐
cates  the  proportion  of  the  dataset’s  variance  that  lies  along  each  principal  compo‐
nent.  For  example,  let’s  look  at  the  explained  variance  ratios  of  the  first  two
components of the 3D dataset represented in Figure 8-2:

>>> pca.explained_variance_ratio_
array([0.84248607, 0.14631839])

222 

| 

Chapter 8: Dimensionality Reduction

This output tells you that 84.2% of the dataset’s variance lies along the first PC, and
14.6% lies along the second PC. This leaves less than 1.2% for the third PC, so it is
reasonable to assume that the third PC probably carries little information.

Choosing the Right Number of Dimensions
Instead  of  arbitrarily  choosing  the  number  of  dimensions  to  reduce  down  to,  it  is
simpler to choose the number of dimensions that add up to a sufficiently large por‐
tion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for
data visualization—in that case you will want to reduce the dimensionality down to 2
or 3.

The following code performs PCA without reducing dimensionality, then computes
the  minimum  number  of  dimensions  required  to  preserve  95%  of  the  training  set’s
variance:

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1

You could then set n_components=d and run PCA again. But there is a much better
option: instead of specifying the number of principal components you want to pre‐
serve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio
of variance you wish to preserve:

pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)

Yet  another  option  is  to  plot  the  explained  variance  as  a  function  of  the  number  of
dimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the
curve, where the explained variance stops growing fast. In this case, you can see that
reducing the dimensionality down to about 100 dimensions wouldn’t lose too much
explained variance.

PCA 

| 

223

Figure 8-8. Explained variance as a function of the number of dimensions

PCA for Compression
After  dimensionality  reduction,  the  training  set  takes  up  much  less  space.  As  an
example,  try  applying  PCA  to  the  MNIST  dataset  while  preserving  95%  of  its  var‐
iance. You should find that each instance will have just over 150 features, instead of
the  original  784  features.  So,  while  most  of  the  variance  is  preserved,  the  dataset  is
now less than 20% of its original size! This is a reasonable compression ratio, and you
can  see  how  this  size  reduction  can  speed  up  a  classification  algorithm  (such  as  an
SVM classifier) tremendously.

It  is  also  possible  to  decompress  the  reduced  dataset  back  to  784  dimensions  by
applying the inverse transformation of the PCA projection. This won’t give you back
the  original  data,  since  the  projection  lost  a  bit  of  information  (within  the  5%  var‐
iance  that  was  dropped),  but  it  will  likely  be  close  to  the  original  data.  The  mean
squared  distance  between  the  original  data  and  the  reconstructed  data  (compressed
and then decompressed) is called the reconstruction error.

The  following  code  compresses  the  MNIST  dataset  down  to  154  dimensions,  then
uses the inverse_transform() method to decompress it back to 784 dimensions:

pca = PCA(n_components = 154)
X_reduced = pca.fit_transform(X_train)
X_recovered = pca.inverse_transform(X_reduced)

Figure 8-9 shows a few digits from the original training set (on the left), and the cor‐
responding digits after compression and decompression. You can see that there is a
slight image quality loss, but the digits are still mostly intact.

224 

| 

Chapter 8: Dimensionality Reduction

Figure 8-9. MNIST compression that preserves 95% of the variance

The equation of the inverse transformation is shown in Equation 8-3.

Equation 8-3. PCA inverse transformation, back to the original number of
dimensions

Xrecovered = Xd‐projWd

⊺

Randomized PCA
If you set the svd_solver hyperparameter to "randomized", Scikit-Learn uses a sto‐
chastic algorithm called Randomized PCA that quickly finds an approximation of the
first  d  principal  components.  Its  computational  complexity  is  O(m  ×  d2)  +  O(d3),
instead  of  O(m  ×  n2)  +  O(n3)  for  the  full  SVD  approach,  so  it  is  dramatically  faster
than full SVD when d is much smaller than n:

rnd_pca = PCA(n_components=154, svd_solver="randomized")
X_reduced = rnd_pca.fit_transform(X_train)

By default, svd_solver is actually set to "auto": Scikit-Learn automatically uses the
randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m
or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full
SVD, you can set the svd_solver hyperparameter to "full".

Incremental PCA
One  problem  with  the  preceding  implementations  of  PCA  is  that  they  require  the
whole  training  set  to  fit  in  memory  in  order  for  the  algorithm  to  run.  Fortunately,
Incremental PCA (IPCA) algorithms have been developed. They allow you to split the
training set into mini-batches and feed an IPCA algorithm one mini-batch at a time.

PCA 

| 

225

This is useful for large training sets and for applying PCA online (i.e., on the fly, as
new instances arrive).

The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s
array_split()  function)  and  feeds  them  to  Scikit-Learn’s  IncrementalPCA  class5  to
reduce  the  dimensionality  of  the  MNIST  dataset  down  to  154  dimensions  (just  like
before).  Note  that  you  must  call  the  partial_fit()  method  with  each  mini-batch,
rather than the fit() method with the whole training set:

from sklearn.decomposition import IncrementalPCA

n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)

X_reduced = inc_pca.transform(X_train)

Alternatively,  you  can  use  NumPy’s  memmap  class,  which  allows  you  to  manipulate  a
large array stored in a binary file on disk as if it were entirely in memory; the class
loads only the data it needs in memory, when it needs it. Since the IncrementalPCA
class uses only a small part of the array at any given time, the memory usage remains
under control. This makes it possible to call the usual fit() method, as you can see
in the following code:

X_mm = np.memmap(filename, dtype="float32", mode="readonly", shape=(m, n))

batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)

Kernel PCA
In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly
maps instances into a very high-dimensional space (called the feature space), enabling
nonlinear  classification  and  regression  with  Support  Vector  Machines.  Recall  that  a
linear  decision  boundary  in  the  high-dimensional  feature  space  corresponds  to  a
complex nonlinear decision boundary in the original space.

It turns out that the same trick can be applied to PCA, making it possible to perform
complex  nonlinear  projections  for  dimensionality  reduction.  This  is  called  Kernel

5 Scikit-Learn uses the algorithm described in David A. Ross et al., “Incremental Learning for Robust Visual

Tracking,” International Journal of Computer Vision 77, no. 1–3 (2008): 125–141.

226 

| 

Chapter 8: Dimensionality Reduction

PCA (kPCA).6 It is often good at preserving clusters of instances after projection, or
sometimes even unrolling datasets that lie close to a twisted manifold.

The following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF
kernel (see Chapter 5 for more details about the RBF kernel and other kernels):

from sklearn.decomposition import KernelPCA

rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)

Figure  8-10  shows  the  Swiss  roll,  reduced  to  two  dimensions  using  a  linear  kernel
(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel.

Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels

Selecting a Kernel and Tuning Hyperparameters
As  kPCA  is  an  unsupervised  learning  algorithm,  there  is  no  obvious  performance
measure  to  help  you  select  the  best  kernel  and  hyperparameter  values.  That  said,
dimensionality  reduction  is  often  a  preparation  step  for  a  supervised  learning  task
(e.g., classification), so you can use grid search to select the kernel and hyperparame‐
ters that lead to the best performance on that task. The following code creates a two-
step  pipeline,  first  reducing  dimensionality  to  two  dimensions  using  kPCA,  then
applying Logistic Regression for classification. Then it uses GridSearchCV to find the
best kernel and gamma value for kPCA in order to get the best classification accuracy
at the end of the pipeline:

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

6 Bernhard Schölkopf et al., “Kernel Principal Component Analysis,” in Lecture Notes in Computer Science 1327

(Berlin: Springer, 1997): 583–588.

Kernel PCA 

| 

227

clf = Pipeline([
        ("kpca", KernelPCA(n_components=2)),
        ("log_reg", LogisticRegression())
    ])

param_grid = [{
        "kpca__gamma": np.linspace(0.03, 0.05, 10),
        "kpca__kernel": ["rbf", "sigmoid"]
    }]

grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)

The  best  kernel  and  hyperparameters  are  then  available  through  the  best_params_
variable:

>>> print(grid_search.best_params_)
{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}

Another approach, this time entirely unsupervised, is to select the kernel and hyper‐
parameters that yield the lowest reconstruction error. Note that reconstruction is not
as easy as with linear PCA. Here’s why. Figure 8-11 shows the original Swiss roll 3D
dataset (top left) and the resulting 2D dataset after kPCA is applied using an RBF ker‐
nel  (top  right).  Thanks  to  the  kernel  trick,  this  transformation  is  mathematically
equivalent  to  using  the  feature  map  φ  to  map  the  training  set  to  an  infinite-
dimensional  feature  space  (bottom  right),  then  projecting  the  transformed  training
set down to 2D using linear PCA.

Notice that if we could invert the linear PCA step for a given instance in the reduced
space,  the  reconstructed  point  would  lie  in  feature  space,  not  in  the  original  space
(e.g.,  like  the  one  represented  by  an  X  in  the  diagram).  Since  the  feature  space  is
infinite-dimensional,  we  cannot  compute  the  reconstructed  point,  and  therefore  we
cannot  compute  the  true  reconstruction  error.  Fortunately,  it  is  possible  to  find  a
point  in  the  original  space  that  would  map  close  to  the  reconstructed  point.  This
point is called the reconstruction pre-image. Once you have this pre-image, you can
measure its squared distance to the original instance. You can then select the kernel
and hyperparameters that minimize this reconstruction pre-image error.

228 

| 

Chapter 8: Dimensionality Reduction

Figure 8-11. Kernel PCA and the reconstruction pre-image error

You may be wondering how to perform this reconstruction. One solution is to train a
supervised regression model, with the projected instances as the training set and the
original  instances  as  the  targets.  Scikit-Learn  will  do  this  automatically  if  you  set
fit_inverse_transform=True, as shown in the following code:7

rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433,
                    fit_inverse_transform=True)
X_reduced = rbf_pca.fit_transform(X)
X_preimage = rbf_pca.inverse_transform(X_reduced)

By default,  fit_inverse_transform=False and  KernelPCA has no
inverse_transform()  method.  This  method  only  gets  created
when you set fit_inverse_transform=True.

7 If you set fit_inverse_transform=True, Scikit-Learn will use the algorithm (based on Kernel Ridge Regres‐
sion) described in Gokhan H. Bakır et al., “Learning to Find Pre-Images”, Proceedings of the 16th International
Conference on Neural Information Processing Systems (2004): 449–456.

Kernel PCA 

| 

229

You can then compute the reconstruction pre-image error:

>>> from sklearn.metrics import mean_squared_error
>>> mean_squared_error(X, X_preimage)
32.786308795766132

Now you can use grid search with cross-validation to find the kernel and hyperpara‐
meters that minimize this error.

LLE
Locally Linear Embedding (LLE)8 is another powerful nonlinear dimensionality reduc‐
tion  (NLDR)  technique.  It  is  a  Manifold  Learning  technique  that  does  not  rely  on
projections, like the previous algorithms do. In a nutshell, LLE works by first measur‐
ing how each training instance linearly relates to its closest neighbors (c.n.), and then
looking  for  a  low-dimensional  representation  of  the  training  set  where  these  local
relationships are best preserved (more details shortly). This approach makes it partic‐
ularly  good  at  unrolling  twisted  manifolds,  especially  when  there  is  not  too  much
noise.

The  following  code  uses  Scikit-Learn’s  LocallyLinearEmbedding  class  to  unroll  the
Swiss roll:

from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_reduced = lle.fit_transform(X)

The  resulting  2D  dataset  is  shown  in  Figure  8-12.  As  you  can  see,  the  Swiss  roll  is
completely unrolled, and the distances between instances are locally well preserved.
However,  distances  are  not  preserved  on  a  larger  scale:  the  left  part  of  the  unrolled
Swiss roll is stretched, while the right part is squeezed. Nevertheless, LLE did a pretty
good job at 
  an  “elbow”  at  k  =  4.  So,  if  we  did  not  know  better,  4
would be a good choice: any lower value would be dramatic, while any higher value
would not help much, and we might just be splitting perfectly good clusters in half for
no good reason.

This technique for choosing the best value for the number of clusters is rather coarse.
A  more  precise  approach  (but  also  more  computationally  expensive)  is  to  use  the
silhouette  score,  which  is  the  mean  silhouette  coefficient  over  all  the  instances.  An

246 

| 

Chapter 9: Unsupervised Learning Techniques

instance’s  silhouette  coefficient  is  equal  to  (b  –  a)  /  max(a,  b),  where  a  is  the  mean
distance  to  the  other  instances  in  the  same  cluster  (i.e.,  the  mean  intra-cluster  dis‐
tance)  and  b  is  the  mean  nearest-cluster  distance  (i.e.,  the  mean  distance  to  the
instances  of  the  next  closest  cluster,  defined  as  the  one  that  minimizes  b,  excluding
the instance’s own cluster). The silhouette coefficient can vary between –1 and +1. A
coefficient close to +1 means that the instance is well inside its own cluster and far
from  other  clusters,  while  a  coefficient  close  to  0  means  that  it  is  close  to  a  cluster
boundary, and finally a coefficient close to –1 means that the instance may have been
assigned to the wrong cluster.

To  compute  the  silhouette  score,  you  can  use  Scikit-Learn’s  silhouette_score()
function, giving it all the instances in the dataset and the labels they were assigned:

>>> from sklearn.metrics import silhouette_score
>>> silhouette_score(X, kmeans.labels_)
0.655517642572828

Let’s compare the silhouette scores for different numbers of clusters (see Figure 9-9).

Figure 9-9. Selecting the number of clusters k using the silhouette score

As  you  can  see,  this  visualization  is  much  richer  than  the  previous  one:  although  it
confirms that k = 4 is a very good choice, it also underlines the fact that k = 5 is quite
good as well, and much better than k = 6 or 7. This was not visible when comparing
inertias.

An  even  more  informative  visualization  is  obtained  when  you  plot  every  instance’s
silhouette coefficient, sorted by the cluster they are assigned to and by the value of the
coefficient.  This  is  called  a  silhouette  diagram  (see  Figure  9-10).  Each  diagram  con‐
tains one knife shape per cluster. The shape’s height indicates the number of instances
the cluster contains, and its width represents the sorted silhouette coefficients of the
instances in the cluster (wider is better). The dashed line indicates the mean silhou‐
ette coefficient.

Clustering 

| 

247

Figure 9-10. Analyzing the silhouette diagrams for various values of k

The vertical dashed lines represent the silhouette score for each number of clusters.
When most of the instances in a cluster have a lower coefficient than this score (i.e., if
many of the instances stop short of the dashed line, ending to the left of it), then the
cluster is rather bad since this means its instances are much too close to other clus‐
ters. We can see that when k = 3 and when k = 6, we get bad clusters. But when k = 4
or k = 5, the clusters look pretty good: most instances extend beyond the dashed line,
to the right and closer to 1.0. When k = 4, the cluster at index 1 (the third from the
top)  is  rather  big.  When  k  =  5,  all  clusters  have  similar  sizes.  So,  even  though  the
overall  silhouette  score  from  k  =  4  is  slightly  greater  than  for  k  =  5,  it  seems  like  a
good idea to use k = 5 to get clusters of similar sizes.

Limits of K-Means
Despite its many merits, most notably being fast and scalable, K-Means is not perfect.
As we saw, it is necessary to run the algorithm several times to avoid suboptimal solu‐
tions,  plus  you  need  to  specify  the  number  of  clusters,  which  can  be  quite  a  hassle.
Moreover, K-Means does not behave very well when the clusters have varying sizes,

248 

| 

Chapter 9: Unsupervised Learning Techniques

different densities, or nonspherical shapes. For example, Figure 9-11 shows how K-
Means clusters a dataset containing three ellipsoidal clusters of different dimensions,
densities, and orientations.

Figure 9-11. K-Means fails to cluster these ellipsoidal blobs properly

As you can see, neither of these solutions is any good. The solution on the left is bet‐
ter, but it still chops off 25% of the middle cluster and assigns it to the cluster on the
right.  The  solution  on  the  right  is  just  terrible,  even  though  its  inertia  is  lower.  So,
depending on the data, different clustering algorithms may perform better. On these
types of elliptical clusters, Gaussian mixture models work great.

It is important to scale the input features before you run K-Means,
or  the  clusters  may  be  very  stretched  and  K-Means  will  perform
poorly. Scaling the features does not guarantee that all the clusters
will be nice and spherical, but it generally improves things.

Now let’s look at a few ways we can benefit from clustering. We will use K-Means, but
feel free to experiment with other clustering algorithms.

Using Clustering for Image Segmentation
Image  segmentation  is  the  task  of  partitioning  an  image  into  multiple  segments.  In
semantic segmentation, all pixels that are part of the same object type get assigned to
the same segment. For example, in a self-driving car’s vision system, all pixels that are
part  of  a  pedestrian’s  image  might  be  assigned  to  the  “pedestrian”  segment  (there
would  be  one  segment  containing  all  the  pedestrians).  In  instance  segmentation,  all
pixels that are part of the same individual object are assigned to the same segment. In
this case there would be a different segment for each pedestrian. The state of the art
in semantic or instance segmentation today is achieved using complex architectures
based on convolutional neural networks (see Chapter 14). Here, we are going to do
something much simpler: color segmentation. We will simply assign pixels to the same
segment if they have a similar color. In some applications, this may be sufficient. For

Clustering 

| 

249

example,  if  you  want  to  analyze  satellite  images  to  measure  how  much  total  forest
area there is in a region, color segmentation may be just fine.

First, use Matplotlib’s imread() function to load the image (see the upper-left image
in Figure 9-12):

>>> from matplotlib.image import imread  # or `from imageio import imread`
>>> image = imread(os.path.join("images","unsupervised_learning","ladybug.png"))
>>> image.shape
(533, 800, 3)

The  image  is  represented  as  a  3D  array.  The  first  dimension’s  size  is  the  height;  the
second is the width; and the third is the number of color channels, in this case red,
green, and blue (RGB). In other words, for each pixel there is a 3D vector containing
the  intensities  of  red,  green,  and  blue,  each  between  0.0  and  1.0  (or  between  0  and
255, if you use  imageio.imread()). Some images may have fewer channels, such as
grayscale images (one channel). And some images may have more channels, such as
images  with  an  additional  alpha  channel  for  transparency  or  satellite  images,  which
often contain channels for many light frequencies (e.g., infrared). The following code
reshapes the array to get a long list of RGB colors, then it clusters these colors using
K-Means:

X = image.reshape(-1, 3)
kmeans = KMeans(n_clusters=8).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)

For  example,  it  may  identify  a  color  cluster  for  all  shades  of  green.  Next,  for  each
color  (e.g.,  dark  green),  it  looks  for  the  mean  color  of  the  pixel’s  color  cluster.  For
example, all shades of green may be replaced with the same light green color (assum‐
ing the mean color of the green cluster is light green). Finally, it reshapes this long list
of colors to get the same shape as the original image. And we’re done!

This outputs the image shown in the upper right of Figure 9-12. You can experiment
with various numbers of clusters, as shown in the figure. When you use fewer than
eight clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own:
it  gets  merged  with  colors  from  the  environment.  This  is  because  K-Means  prefers
clusters  of  similar  sizes.  The  ladybug  is  small—much  smaller  than  the  rest  of  the
image—so even though its color is flashy, K-Means fails to dedicate a cluster to it.

250 

| 

Chapter 9: Unsupervised Learning Techniques

Figure 9-12. Image segmentation using K-Means with various numbers of color clusters

That wasn’t too hard, was it? Now let’s look at another application of clustering: pre‐
processing.

Using Clustering for Preprocessing
Clustering can be an efficient approach to dimensionality reduction, in particular as a
preprocessing  step  before  a  supervised  learning  algorithm.  As  an  example  of  using
clustering for dimensionality reduction, let’s tackle the digits dataset, which is a sim‐
ple MNIST-like dataset containing 1,797 grayscale 8 × 8 images representing the dig‐
its 0 to 9. First, load the dataset:

from sklearn.datasets import load_digits

X_digits, y_digits = load_digits(return_X_y=True)

Now, split it into a training set and a test set:

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)

Next, fit a Logistic Regression model:

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

Let’s evaluate its accuracy on the test set:

>>> log_reg.score(X_test, y_test)
0.9688888888888889

Clustering 

| 

251

OK, that’s our baseline: 96.9% accuracy. Let’s see if we can do better by using K-Means
as a preprocessing step. We will create a pipeline that will first cluster the training set
into 50 clusters and replace the images with their distances to these 50 clusters, then
apply a Logistic Regression model:

from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ("kmeans", KMeans(n_clusters=50)),
    ("log_reg", LogisticRegression()),
])
pipeline.fit(X_train, y_train)

Since there are 10 different digits, it is tempting to set the number
of clusters to 10. However, each digit can be written several differ‐
ent ways, so it is preferable to use a larger number of clusters, such
as 50.

Now let’s evaluate this classification pipeline:

>>> pipeline.score(X_test, y_test)
0.9777777777777777

How about that? We reduced the error rate by almost 30% (from about 3.1% to about
2.2%)!

But we chose the number of clusters k arbitrarily; we can surely do better. Since K-
Means is just a preprocessing step in a classification pipeline, finding a good value for
k is much simpler than earlier. There’s no need to perform silhouette analysis or mini‐
mize the inertia; the best value of k is simply the one that results in the best classifica‐
tion  performance  during  cross-validation.  We  can  use  GridSearchCV  to  find  the
optimal number of clusters:

from sklearn.model_selection import GridSearchCV

param_grid = dict(kmeans__n_clusters=range(2, 100))
grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
grid_clf.fit(X_train, y_train)

Let’s look at the best value for k and the performance of the resulting pipeline:

>>> grid_clf.best_params_
{'kmeans__n_clusters': 99}
>>> grid_clf.score(X_test, y_test)
0.9822222222222222

With k = 99 clusters, we get a significant accuracy boost, reaching 98.22% accuracy
on  the  test  set.  Cool!  You  may  want  to  keep  exploring  higher  values  for  k,  since  99
was the largest value in the range we explored.

252 

| 

Chapter 9: Unsupervised Learning Techniques

Using Clustering for Semi-Supervised Learning
Another use case for clustering is in semi-supervised learning, when we have plenty
of unlabeled instances and very few labeled instances. Let’s train a Logistic Regression
model on a sample of 50 labeled instances from the digits dataset:

n_labeled = 50
log_reg = LogisticRegression()
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])

What is the performance of this model on the test set?

>>> log_reg.score(X_test, y_test)
0.8333333333333334

The accuracy is just 83.3%. It should come as no surprise that this is much lower than
earlier, when we trained the model on the full training set. Let’s see how we can do
better.  First,  let’s  cluster  the  training  set  into  50  clusters.  Then  for  each  cluster,  let’s
find  the  image  closest  to  the  centroid.  We  will  call  these  images  the  representative
images:

k = 50
kmeans = KMeans(n_clusters=k)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]

Figure 9-13 shows these 50 representative images.

Figure 9-13. Fifty representative digit images (one per cluster)

Let’s look at each image and manually label it:

y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])

Now  we  have  a  dataset  with  just  50  labeled  instances,  but  instead  of  being  random
instances, each of them is a representative image of its cluster. Let’s see if the perfor‐
mance is any better:

>>> log_reg = LogisticRegression()
>>> log_reg.fit(X_representative_digits, y_representative_digits)
>>> log_reg.score(X_test, y_test)
0.9222222222222223

Clustering 

| 

253

Wow! We jumped from 83.3% accuracy to 92.2%, although we are still only training
the model on 50 instances. Since it is often costly and painful to label instances, espe‐
cially when it has to be done manually by experts, it is a good idea to label representa‐
tive instances rather than just random instances.

But  perhaps  we  can  go  one  step  further:  what  if  we  propagated  the  labels  to  all  the
other instances in the same cluster? This is called label propagation:

y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]

Now let’s train the model again and look at its performance:

>>> log_reg = LogisticRegression()
>>> log_reg.fit(X_train, y_train_propagated)
>>> log_reg.score(X_test, y_test)
0.9333333333333333

We got a reasonable accuracy boost, but nothing absolutely astounding. The problem
is  that  we  propagated  each  representative  instance’s  label  to  all  the  instances  in  the
same  cluster,  including  the  instances  located  close  to  the  cluster  boundaries,  which
are  more  likely  to  be  mislabeled.  Let’s  see  what  happens  if  we  only  propagate  the
labels to the 20% of the instances that are closest to the centroids:

percentile_closest = 20

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster]
    cutoff_distance = np.percentile(cluster_dist, percentile_closest)
    above_cutoff = (X_cluster_dist > cutoff_distance)
    X_cluster_dist[in_cluster & above_cutoff] = -1

partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]

Now let’s train the
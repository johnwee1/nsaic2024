 of this chapter deal with standard concepts in machine learning.
If you are already familiar with these concepts, feel free to skip the relevant
sections. However, most of this chapter is concerned with the extension of these
basic concepts to the particular case of neural networks.
In section 5.2.2, we deï¬?ned regularization as â€œany modiï¬?cation we make to
a learning algorithm that is intended to reduce its generalization error but not
its training error.â€? There are many regularization strategies. Some put extra
constraints on a machine learning model, such as adding restrictions on the
parameter values. Some add extra terms in the objective function that can be
thought of as corresponding to a soft constraint on the parameter values. If chosen
carefully, these extra constraints and penalties can lead to improved performance
228

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

on the test set. Sometimes these constraints and penalties are designed to encode
speciï¬?c kinds of prior knowledge. Other times, these constraints and penalties
are designed to express a generic preference for a simpler model class in order to
promote generalization. Sometimes penalties and constraints are necessary to make
an underdetermined problem determined. Other forms of regularization, known as
ensemble methods, combine multiple hypotheses that explain the training data.
In the context of deep learning, most regularization strategies are based on
regularizing estimators. Regularization of an estimator works by trading increased
bias for reduced variance. An eï¬€ective regularizer is one that makes a proï¬?table
trade, reducing variance signiï¬?cantly while not overly increasing the bias. When we
discussed generalization and overï¬?tting in chapter 5, we focused on three situations,
where the model family being trained either (1) excluded the true data generating
processâ€”corresponding to underï¬?tting and inducing bias, or (2) matched the true
data generating process, or (3) included the generating process but also many
other possible generating processesâ€”the overï¬?tting regime where variance rather
than bias dominates the estimation error. The goal of regularization is to take a
model from the third regime into the second regime.
In practice, an overly complex model family does not necessarily include the
target function or the true data generating process, or even a close approximation
of either. We almost never have access to the true data generating process so
we can never know for sure if the model family being estimated includes the
generating process or not. However, most applications of deep learning algorithms
are to domains where the true data generating process is almost certainly outside
the model family. Deep learning algorithms are typically applied to extremely
complicated domains such as images, audio sequences and text, for which the true
generation process essentially involves simulating the entire universe. To some
extent, we are always trying to ï¬?t a square peg (the data generating process) into
a round hole (our model family).
What this means is that controlling the complexity of the model is not a
simple matter of ï¬?nding the model of the right size, with the right number of
parameters. Instead, we might ï¬?ndâ€”and indeed in practical deep learning scenarios,
we almost always do ï¬?ndâ€”that the best ï¬?tting model (in the sense of minimizing
generalization error) is a large model that has been regularized appropriately.
We now review several strategies for how to create such a large, deep, regularized
model.

229

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

7.1

Parameter Norm Penalties

Regularization has been used for decades prior to the advent of deep learning. Linear
models such as linear regression and logistic regression allow simple, straightforward,
and eï¬€ective regularization strategies.
Many regularization approaches are based on limiting the capacity of models,
such as neural networks, linear regression, or logistic regression, by adding a parameter norm penalty â„¦(Î¸) to the objective function J. We denote the regularized
Ëœ
objective function by J:
JËœ(Î¸; X , y) = J (Î¸; X , y) + Î±â„¦(Î¸)

(7.1)

where Î± âˆˆ [0, âˆž) is a hyperparameter that weights the relative contribution of the
norm penalty term, â„¦, relative to the standard objective function J . Setting Î± to 0
results in no regularization. Larger values of Î± correspond to more regularization.
When our training algorithm minimizes the regularized objective function JËœ it
will decrease both the original objective J on the training data and some measure
of the size of the parameters Î¸ (or some subset of the parameters). Diï¬€erent
choices for the parameter norm â„¦ can result in diï¬€erent solutions being preferred.
In this section, we discuss the eï¬€ects of the various norms when used as penalties
on the model parameters.
Before delving into the regularization behavior of diï¬€erent norms, we note that
for neural networks, we typically choose to use a parameter norm penalty â„¦ that
penalizes only the weights of the aï¬ƒne transformation at each layer and leaves
the biases unregularized. The biases typically require less data to ï¬?t accurately
than the weights. Each weight speciï¬?es how two variables interact. Fitting the
weight well requires observing both variables in a variety of conditions. Each
bias controls only a single variable. This means that we do not induce too much
variance by leaving the biases unregularized. Also, regularizing the bias parameters
can introduce a signiï¬?cant amount of underï¬?tting. We therefore use the vector w
to indicate all of the weights that should be aï¬€ected by a norm penalty, while the
vector Î¸ denotes all of the parameters, including both w and the unregularized
parameters.
In the context of neural networks, it is sometimes desirable to use a separate
penalty with a diï¬€erent Î± coeï¬ƒcient for each layer of the network. Because it can
be expensive to search for the correct value of multiple hyperparameters, it is still
reasonable to use the same weight decay at all layers just to reduce the size of
search space.
230

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

7.1.1

L2 Parameter Regularization

We have already seen, in section 5.2.2, one of the simplest and most common kinds
of parameter norm penalty: the L2 parameter norm penalty commonly known as
weight decay. This regularization strategy drives the weights closer to the origin1
by adding a regularization term â„¦(Î¸) = 12 î?«wî?«22 to the objective function. In other
academic communities, L2 regularization is also known as ridge regression or
Tikhonov regularization.
We can gain some insight into the behavior of weight decay regularization
by studying the gradient of the regularized objective function. To simplify the
presentation, we assume no bias parameter, so Î¸ is just w. Such a model has the
following total objective function:
Î±
JËœ(w ; X , y) = wî€¾ w + J (w; X , y ),
2

(7.2)

with the corresponding parameter gradient
âˆ‡w JËœ(w ; X , y) = Î±w + âˆ‡w J (w; X , y).

(7.3)

To take a single gradient step to update the weights, we perform this update:
w â†? w âˆ’ î€? (Î±w + âˆ‡w J (w; X , y)) .

(7.4)

Written another way, the update is:
w â†? (1 âˆ’ î€?Î±)w âˆ’ î€?âˆ‡ w J (w; X , y).

(7.5)

We can see that the addition of the weight decay term has modiï¬?ed the learning
rule to multiplicatively shrink the weight vector by a constant factor on each step,
just before performing the usual gradient update. This describes what happens in
a single step. But what happens over the entire course of training?
We will further simplify the analysis by making a quadratic approximation
to the objective function in the neighborhood of the value of the weights that
obtains minimal unregularized training cost, wâˆ— = arg minw J(w). If the objective
function is truly quadratic, as in the case of ï¬?tting a linear regression model with
1

More generally, we could regularize the parameters to be near any speciï¬?c point in space
and, surprisingly, still get a regularization eï¬€ect, but better results will be obtained for a value
closer to the true one, with zero being a default value that makes sense when we do not know if
the correct value should be positive or negative. Since it is far more common to regularize the
model parameters towards zero, we will focus on this special case in our exposition.
231

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

mean squared error, then the approximation is perfect. The approximation JË† is
given by
1
JË†(Î¸) = J (wâˆ—) + (w âˆ’ wâˆ— ) î€¾H (w âˆ’ wâˆ—),
(7.6)
2
where H is the Hessian matrix of J with respect to w evaluated at w âˆ—. There is
no ï¬?rst-order term in this quadratic approximation, because wâˆ— is deï¬?ned to be a
minimum, where the gradient vanishes. Likewise, because wâˆ— is the location of a
minimum of J , we can conclude that H is positive semideï¬?nite.
The minimum of JË† occurs where its gradient
Ë† w ) = H (w âˆ’ w âˆ— )
âˆ‡ w J(

(7.7)

is equal to 0.
To study the eï¬€ect of weight decay, we modify equation 7.7 by adding the
weight decay gradient. We can now solve for the minimum of the regularized
version of JÌ‚. We use the variable wÌƒ to represent the location of the minimum.
Î±wÌƒ + H (wÌƒ âˆ’ wâˆ— ) = 0

(7.8)

(H + Î±I ) wÌƒ = Hwâˆ—

(7.9)

wÌƒ = (H + Î±I )âˆ’1 Hw âˆ—.

(7.10)

As Î± approaches 0, the regularized solution wÌƒ approaches wâˆ—. But what
happens as Î± grows? Because H is real and symmetric, we can decompose it
into a diagonal matrix Î› and an orthonormal basis of eigenvectors, Q, such that
H = QÎ›Qî€¾. Applying the decomposition to equation 7.10, we obtain:
wÌƒ = (QÎ›Q î€¾ + Î±I )âˆ’1 QÎ›Q î€¾w âˆ—
î?¨
î?© âˆ’1
î€¾
= Q (Î› + Î± I )Q
QÎ›Qî€¾ wâˆ—
= Q(Î› + Î±I )âˆ’1 Î›Qî€¾ wâˆ— .

(7.11)
(7.12)
(7.13)

We see that the eï¬€ect of weight decay is to rescale w âˆ— along the axes deï¬?ned by
the eigenvectors of H. Speciï¬?cally, the component of w âˆ— that is aligned with the
i
i-th eigenvector of H is rescaled by a factor of Î»iÎ»+Î±
. (You may wish to review
how this kind of scaling works, ï¬?rst explained in ï¬?gure 2.3).
Along the directions where the eigenvalues of H are relatively large, for example,
where Î»i î€? Î±, the eï¬€ect of regularization is relatively small. However, components
with Î» i î€œ Î± will be shrunk to have nearly zero magnitude. This eï¬€ect is illustrated
in ï¬?gure 7.1.
232

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

w2

wâˆ—
wÌƒ

w1

Figure 7.1: An illustration of the eï¬€ect of L 2 (or weight decay) regularization on the value
of the optimal w. The solid ellipses represent contours of equal value of the unregularized
objective. The dotted circles represent contours of equal value of theL 2 regularizer. At
the point wÌƒ, these competing objectives reach an equilibrium. In the ï¬?rst dimension, the
eigenvalue of the Hessian of J is small. The objective function does not increase much
when moving horizontally away from w âˆ— . Because the objective function does not express
a strong preference along this direction, the regularizer has a strong eï¬€ect on this axis.
The regularizer pulls w 1 close to zero. In the second dimension, the objective function
is very sensitive to movements away from wâˆ— . The corresponding eigenvalue is large,
indicating high curvature. As a result, weight decay aï¬€ects the position ofw2 relatively
little.

Only directions along which the parameters contribute signiï¬?cantly to reducing
the objective function are preserved relatively intact. In directions that do not
contribute to reducing the objective function, a small eigenvalue of the Hessian
tells us that movement in this direction will not signiï¬?cantly increase the gradient.
Components of the weight vector corresponding to such unimportant directions
are decayed away through the use of the regularization throughout training.
So far we have discussed weight decay in terms of its eï¬€ect on the optimization
of an abstract, general, quadratic cost function. How do these eï¬€ects relate to
machine learning in particular? We can ï¬?nd out by studying linear regression, a
model for which the true cost function is quadratic and therefore amenable to the
same kind of analysis we have used so far. Applying the analysis again, we will
be able to obtain a special case of the same results, but with the solution now
phrased in terms of the training data. For linear regression, the cost function is

233

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

the sum of squared errors:
(Xw âˆ’ y )î€¾(Xw âˆ’ y ).

(7.14)

When we add L2 regularization, the objective function changes to
1
(Xw âˆ’ y )î€¾ (Xw âˆ’ y ) + Î±w î€¾ w.
2

(7.15)

This changes the normal equations for the solution from
w = (X î€¾X)âˆ’1X î€¾y

(7.16)

w = (X î€¾ X + Î±I )âˆ’1X î€¾ y.

(7.17)

to
The matrix X î€¾X in equation 7.16 is proportional to the covariance matrix m1 X î€¾X.
î€€
î€?âˆ’1
Using L2 regularization replaces this matrix with Xî€¾ X + Î±I
in equation 7.17.
The new matrix is the same as the original one, but with the addition of Î± to the
diagonal. The diagonal entries of this matrix correspond to the variance of each
input feature. We can see that L2 regularization causes the learning algorithm
to â€œperceiveâ€? the input X as having higher variance, which makes it shrink the
weights on features whose covariance with the output target is low compared to
this added variance.

7.1.2

L1 Regularization

While L 2 weight decay is the most common form of weight decay, there are other
ways to penalize the size of the model parameters. Another option is to use L 1
regularization.
Formally, L1 regularization on the model parameter w is deï¬?ned as:
î?˜
=
â„¦(Î¸) = ||w||1
|w i |,

(7.18)

i

that is, as the sum of absolute values of the individual parameters.2 We will
now discuss the eï¬€ect of L1 regularization on the simple linear regression model,
with no bias parameter, that we studied in our analysis of L2 regularization. In
particular, we are interested in delineating the diï¬€erences between L1 and L2 forms
As with L2 regularization, we could regularize the parameters towards a value that is not
(o )
1
zero, but instead towards some parameter value
that case the L regularization would
î?? w . (In
o)
(o)
introduce the term â„¦(Î¸ ) = ||w âˆ’ w ||1 = i |w i âˆ’ w i |.
2

234

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

of regularization. As with L2 weight decay, L 1 weight decay controls the strength
of the regularization by scaling the penalty â„¦ using a positive hyperparameter Î±.
Thus, the regularized objective function JËœ(w; X , y) is given by
JÌƒ (w ; X , y) = Î±||w||1 + J (w; X , y),

(7.19)

with the corresponding gradient (actually, sub-gradient):
âˆ‡w JËœ(w; X , y) = Î±sign(w) + âˆ‡w J (X , y; w)

(7.20)

where sign(w) is simply the sign of w applied element-wise.
By inspecting equation 7.20, we can see immediately that the eï¬€ect of L 1
regularization is quite diï¬€erent from that of L2 regularization. Speciï¬?cally, we can
see that the regularization contribution to the gradient no longer scales linearly
with each wi; instead it is a constant factor with a sign equal to sign(wi). One
consequence of this form of the gradient is that we will not necessarily see clean
algebraic solutions to quadratic approximations of J(X , y ; w) as we did for L 2
regularization.
Our simple linear model has a quadratic cost function that we can represent
via its Taylor s
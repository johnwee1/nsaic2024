eviously, in the formulation of the
Gaussian as an exponential family distribution, we had Âµ = Î·. So, we have

hÎ¸(x) = E[y|x; Î¸]
= Âµ
= Î·
= Î¸T x.

The ï¬rst equality follows from Assumption 2, above; the second equality
follows from the fact that y|x; Î¸ âˆ¼ N (Âµ, Ïƒ2), and so its expected value is given

33

by Âµ; the third equality follows from Assumption 1 (and our earlier derivation
showing that Âµ = Î· in the formulation of the Gaussian as an exponential
family distribution); and the last equality follows from Assumption 3.

3.2.2 Logistic regression

We now consider logistic regression. Here we are interested in binary classiï¬-
cation, so y âˆˆ {0, 1}. Given that y is binary-valued, it therefore seems natural
to choose the Bernoulli family of distributions to model the conditional dis-
tribution of y given x. In our formulation of the Bernoulli distribution as
an exponential family distribution, we had Ï† = 1/(1 + eâˆ’Î·). Furthermore,
note that if y|x; Î¸ âˆ¼ Bernoulli(Ï†), then E[y|x; Î¸] = Ï†. So, following a similar
derivation as the one for ordinary least squares, we get:

hÎ¸(x) = E[y|x; Î¸]
= Ï†
= 1/(1 + eâˆ’Î·)
= 1/(1 + eâˆ’Î¸T x)

So, this gives us hypothesis functions of the form hÎ¸(x) = 1/(1 + eâˆ’Î¸T x). If
you are previously wondering how we came up with the form of the logistic
function 1/(1 + eâˆ’z), this gives one answer: Once we assume that y condi-
tioned on x is Bernoulli, it arises as a consequence of the deï¬nition of GLMs
and exponential family distributions.

To introduce a little more terminology, the function g giving the distri-
butionâ€™s mean as a function of the natural parameter (g(Î·) = E[T (y); Î·])
is called the canonical response function. Its inverse, gâˆ’1, is called the
canonical link function. Thus, the canonical response function for the
Gaussian family is just the identify function; and the canonical response
function for the Bernoulli is the logistic function.3

3Many texts use g to denote the link function, and gâˆ’1 to denote the response function;
but the notation weâ€™re using here, inherited from the early machine learning literature,
will be more consistent with the notation used in the rest of the class.

Chapter 4

Generative learning algorithms

So far, weâ€™ve mainly been talking about learning algorithms that model
p(y|x; Î¸), the conditional distribution of y given x. For instance, logistic
regression modeled p(y|x; Î¸) as hÎ¸(x) = g(Î¸T x) where g is the sigmoid func-
tion. In these notes, weâ€™ll talk about a diï¬€erent type of learning algorithm.
Consider a classiï¬cation problem in which we want to learn to distinguish
between elephants (y = 1) and dogs (y = 0), based on some features of
an animal. Given a training set, an algorithm like logistic regression or
the perceptron algorithm (basically) tries to ï¬nd a straight lineâ€”that is, a
decision boundaryâ€”that separates the elephants and dogs. Then, to classify
a new animal as either an elephant or a dog, it checks on which side of the
decision boundary it falls, and makes its prediction accordingly.

Hereâ€™s a diï¬€erent approach. First, looking at elephants, we can build a
model of what elephants look like. Then, looking at dogs, we can build a
separate model of what dogs look like. Finally, to classify a new animal, we
can match the new animal against the elephant model, and match it against
the dog model, to see whether the new animal looks more like the elephants
or more like the dogs we had seen in the training set.

Algorithms that try to learn p(y|x) directly (such as logistic regression),
or algorithms that try to learn mappings directly from the space of inputs X
to the labels {0, 1}, (such as the perceptron algorithm) are called discrim-
inative learning algorithms. Here, weâ€™ll talk about algorithms that instead
try to model p(x|y) (and p(y)). These algorithms are called generative
learning algorithms. For instance, if y indicates whether an example is a
dog (0) or an elephant (1), then p(x|y = 0) models the distribution of dogsâ€™
features, and p(x|y = 1) models the distribution of elephantsâ€™ features.

After modeling p(y) (called the class priors) and p(x|y), our algorithm

34

35

can then use Bayes rule to derive the posterior distribution on y given x:

p(y|x) =

p(x|y)p(y)
p(x)

.

Here, the denominator is given by p(x) = p(x|y = 1)p(y = 1) + p(x|y =
0)p(y = 0) (you should be able to verify that this is true from the standard
properties of probabilities), and thus can also be expressed in terms of the
quantities p(x|y) and p(y) that weâ€™ve learned. Actually, if were calculating
p(y|x) in order to make a prediction, then we donâ€™t actually need to calculate
the denominator, since

arg max

y

p(y|x) = arg max

y

= arg max

y

p(x|y)p(y)
p(x)
p(x|y)p(y).

4.1 Gaussian discriminant analysis

The ï¬rst generative learning algorithm that weâ€™ll look at is Gaussian discrim-
inant analysis (GDA). In this model, weâ€™ll assume that p(x|y) is distributed
according to a multivariate normal distribution. Letâ€™s talk brieï¬‚y about the
properties of multivariate normal distributions before moving on to the GDA
model itself.

4.1.1 The multivariate normal distribution

The multivariate normal distribution in d-dimensions, also called the multi-
variate Gaussian distribution, is parameterized by a mean vector Âµ âˆˆ Rd
and a covariance matrix Î£ âˆˆ RdÃ—d, where Î£ â‰¥ 0 is symmetric and positive
semi-deï¬nite. Also written â€œN (Âµ, Î£)â€, its density is given by:

p(x; Âµ, Î£) =

1

(2Ï€)d/2|Î£|1/2 exp

(cid:18)

âˆ’

1
2

(x âˆ’ Âµ)T Î£âˆ’1(x âˆ’ Âµ)

(cid:19)

.

In the equation above, â€œ|Î£|â€ denotes the determinant of the matrix Î£.

For a random variable X distributed N (Âµ, Î£), the mean is (unsurpris-

ingly) given by Âµ:

E[X] =

(cid:90)

x

x p(x; Âµ, Î£)dx = Âµ

The covariance of a vector-valued random variable Z is deï¬ned as Cov(Z) =

E[(Z âˆ’ E[Z])(Z âˆ’ E[Z])T ]. This generalizes the notion of the variance of a

36

real-valued random variable. The covariance can also be deï¬ned as Cov(Z) =
E[ZZ T ] âˆ’ (E[Z])(E[Z])T . (You should be able to prove to yourself that these
two deï¬nitions are equivalent.) If X âˆ¼ N (Âµ, Î£), then

Cov(X) = Î£.

Here are some examples of what the density of a Gaussian distribution

looks like:

The left-most ï¬gure shows a Gaussian with mean zero (that is, the 2x1
zero-vector) and covariance matrix Î£ = I (the 2x2 identity matrix). A Gaus-
sian with zero mean and identity covariance is also called the standard nor-
mal distribution. The middle ï¬gure shows the density of a Gaussian with
zero mean and Î£ = 0.6I; and in the rightmost ï¬gure shows one with , Î£ = 2I.
We see that as Î£ becomes larger, the Gaussian becomes more â€œspread-out,â€
and as it becomes smaller, the distribution becomes more â€œcompressed.â€

Letâ€™s look at some more examples.

The ï¬gures above show Gaussians with mean 0, and with covariance

matrices respectively

Î£ =

(cid:20) 1
0

(cid:21)

0
1

; Î£ =

(cid:20) 1
0.5

(cid:21)

0.5
1

; Î£ =

(cid:20) 1
0.8

(cid:21)

.

0.8
1

The leftmost ï¬gure shows the familiar standard normal distribution, and we
see that as we increase the oï¬€-diagonal entry in Î£, the density becomes more

âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25â€œcompressedâ€ towards the 45â—¦ line (given by x1 = x2). We can see this more
clearly when we look at the contours of the same three densities:

37

Hereâ€™s one last set of examples generated by varying Î£:

The plots above used, respectively,
(cid:20) 1
-0.8

(cid:20) 1
-0.5

-0.5
1

; Î£ =

Î£ =

(cid:21)

(cid:21)

-0.8
1

; Î£ =

(cid:20) 3
0.8

(cid:21)

.

0.8
1

From the leftmost and middle ï¬gures, we see that by decreasing the oï¬€-
diagonal elements of the covariance matrix, the density now becomes â€œcom-
pressedâ€ again, but in the opposite direction. Lastly, as we vary the pa-
rameters, more generally the contours will form ellipses (the rightmost ï¬gure
showing an example).

As our last set of examples, ï¬xing Î£ = I, by varying Âµ, we can also move

the mean of the density around.

âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.25âˆ’3âˆ’2âˆ’10123âˆ’3âˆ’2âˆ’101230.050.10.150.20.2538

The ï¬gures above were generated using Î£ = I, and respectively

Âµ =

(cid:21)

(cid:20) 1
0

; Âµ =

(cid:21)

(cid:20) -0.5
0

; Âµ =

(cid:20) -1
-1.5

(cid:21)

.

4.1.2 The Gaussian discriminant analysis model

When we have a classiï¬cation problem in which the input features x are
continuous-valued random variables, we can then use the Gaussian Discrim-
inant Analysis (GDA) model, which models p(x|y) using a multivariate nor-
mal distribution. The model is:

y âˆ¼ Bernoulli(Ï†)

x|y = 0 âˆ¼ N (Âµ0, Î£)
x|y = 1 âˆ¼ N (Âµ1, Î£)

Writing out the distributions, this is:

p(y) = Ï†y(1 âˆ’ Ï†)1âˆ’y

p(x|y = 0) =

p(x|y = 1) =

1

(2Ï€)d/2|Î£|1/2 exp

1

(2Ï€)d/2|Î£|1/2 exp

(cid:18)

(cid:18)

âˆ’

âˆ’

1
2
1
2

(x âˆ’ Âµ0)T Î£âˆ’1(x âˆ’ Âµ0)

(x âˆ’ Âµ1)T Î£âˆ’1(x âˆ’ Âµ1)

(cid:19)

(cid:19)

Here, the parameters of our model are Ï†, Î£, Âµ0 and Âµ1. (Note that while
thereâ€™re two diï¬€erent mean vectors Âµ0 and Âµ1, this model is usually applied
using only one covariance matrix Î£.) The log-likelihood of the data is given
by

(cid:96)(Ï†, Âµ0, Âµ1, Î£) = log

= log

n
(cid:89)

i=1
n
(cid:89)

i=1

p(x(i), y(i); Ï†, Âµ0, Âµ1, Î£)

p(x(i)|y(i); Âµ0, Âµ1, Î£)p(y(i); Ï†).

By maximizing (cid:96) with respect to the parameters, we ï¬nd the maximum like-
lihood estimate of the parameters (see problem set 1) to be:

39

Ï† =

Âµ0 =

Âµ1 =

Î£ =

n
(cid:88)

1
n
(cid:80)n

1{y(i) = 1}

(cid:80)n

i=1
i=1 1{y(i) = 0}x(i)
(cid:80)n
i=1 1{y(i) = 0}
i=1 1{y(i) = 1}x(i)
(cid:80)n
i=1 1{y(i) = 1}
n
(cid:88)

1
n

i=1

(x(i) âˆ’ Âµy(i))(x(i) âˆ’ Âµy(i))T .

Pictorially, what the algorithm is doing can be seen in as follows:

Shown in the ï¬gure are the training set, as well as the contours of the
two Gaussian distributions that have been ï¬t to the data in each of the
two classes. Note that the two Gaussians have contours that are the same
shape and orientation, since they share a covariance matrix Î£, but they have
diï¬€erent means Âµ0 and Âµ1. Also shown in the ï¬gure is the straight line
giving the decision boundary at which p(y = 1|x) = 0.5. On one side of
the boundary, weâ€™ll predict y = 1 to be the most likely outcome, and on the
other side, weâ€™ll predict y = 0.

âˆ’2âˆ’101234567âˆ’7âˆ’6âˆ’5âˆ’4âˆ’3âˆ’2âˆ’10140

4.1.3 Discussion: GDA and logistic regression

The GDA model has an interesting relationship to logistic regression. If we
view the quantity p(y = 1|x; Ï†, Âµ0, Âµ1, Î£) as a function of x, weâ€™ll ï¬nd that it
can be expressed in the form

p(y = 1|x; Ï†, Î£, Âµ0, Âµ1) =

1
1 + exp(âˆ’Î¸T x)

,

where Î¸ is some appropriate function of Ï†, Î£, Âµ0, Âµ1.1 This is exactly the form
that logistic regressionâ€”a discriminative algorithmâ€”used to model p(y =
1|x).

When would we prefer one model over another? GDA and logistic regres-
sion will, in general, give diï¬€erent decision boundaries when trained on the
same dataset. Which is better?

We just argued that if p(x|y) is multivariate gaussian (with shared Î£),
then p(y|x) necessarily follows a logistic function. The converse, however,
is not true; i.e., p(y|x) being a logistic function does not imply p(x|y) is
multivariate gaussian. This shows that GDA makes stronger modeling as-
sumptions about the data than does logistic regression.
It turns out that
when these modeling assumptions are correct, then GDA will ï¬nd better ï¬ts
to the data, and is a better model. Speciï¬cally, when p(x|y) is indeed gaus-
sian (with shared Î£), then GDA is asymptotically eï¬ƒcient. Informally,
this means that in the limit of very large training sets (large n), there is no
algorithm that is strictly better than GDA (in terms of, say, how accurately
they estimate p(y|x)). In particular, it can be shown that in this setting,
GDA will be a better algorithm than logistic regression; and more generally,
even for small training set sizes, we would generally expect GDA to better.
In contrast, by making signiï¬cantly weaker assumptions, logistic regres-
sion is also more robust and less sensitive to incorrect modeling assumptions.
There are many diï¬€erent sets of assumptions that would lead to p(y|x) taking
the form of a logistic function. For example, if x|y = 0 âˆ¼ Poisson(Î»0), and
x|y = 1 âˆ¼ Poisson(Î»1), then p(y|x) will be logistic. Logistic regression will
also work well on Poisson data like this. But if we were to use GDA on such
dataâ€”and ï¬t Gaussian distributions to such non-Gaussian dataâ€”then the
results will be less predictable, and GDA may (or may not) do well.

To summarize: GDA makes stronger modeling assumptions, and is more
data eï¬ƒcient (i.e., requires less training data to learn â€œwellâ€) when the mod-
eling assumptions are correct or at least approximately correct. Logistic

1This uses the convention of redeï¬ning the x(i)â€™s on the right-hand-side to be (d + 1)-

dimensional vectors by adding the extra coordinate x(i)

0 = 1; see problem set 1.

41

regression makes weaker assumptions, and is signiï¬cantly more robust to
deviations from modeling assumptions. Speciï¬cally, when the data is in-
deed non-Gaussian, then in the limit of large datasets, logistic regression will
almost always do better than GDA. For this reason, in practice logistic re-
gression is used more often than GDA. (Some related considerations about
discriminative vs. generative models also apply for the Naive Bayes algo-
rithm that we discuss next, but the Naive Bayes algorithm is still considered
a very good, and is certainly also a very popular, classiï¬cation algorithm.)

4.2 Naive bayes (Option Reading)

In GDA, the feature vectors x were continuous, real-valued vectors. Letâ€™s
now talk about a diï¬€erent learning algorithm in which the xjâ€™s are discrete-
valued.

For our motivating example, consider building an email spam ï¬lter using
machine learning. Here, we wish to classify messages according to whether
they are unsolicited commercial (spam) email, or non-spam email. After
learning to do this, we can then have our mail reader automatically ï¬lter
out the spam messages and perhaps place them in a separate mail folder.
Classifying emails is one example of a broader set of problems called text
classiï¬cation.

Letâ€™s say we have a training set (a set of emails labeled as spam or non-
spam). Weâ€™ll begin our construction of our spam ï¬lter by specifying the
features xj used to represent an email.

We will represent an email via a feature vector whose length is equal to
the number of words in the dictionary. Speciï¬cally, if an email contains the
j-th word of the dictionary, then we will set xj = 1; otherwise, we let xj = 0.
For instance, the vector

x =

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

1
0
0
...
1
...
0

a
aardvark
aardwolf
...
buy
...
zygmurgy

is used to represent an email that contains the words â€œaâ€ 
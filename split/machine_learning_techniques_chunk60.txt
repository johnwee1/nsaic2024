as augmented or not. Simply adding white noise will not help; the
modifications should be learnable (white noise is not).

For example, you can slightly shift, rotate, and resize every picture in the training set
by  various  amounts  and  add  the  resulting  pictures  to  the  training  set  (see
Figure 14-12). This forces the model to be more tolerant to variations in the position,
orientation, and size of the objects in the pictures. For a model that’s more tolerant of
different  lighting  conditions,  you  can  similarly  generate  many  images  with  various
contrasts. In general, you can also flip the pictures horizontally (except for text, and
other  asymmetrical  objects).  By  combining  these  transformations,  you  can  greatly
increase the size of your training set.

Figure 14-12. Generating new training instances from existing ones

AlexNet also uses a competitive normalization step immediately after the ReLU step
of layers C1 and C3, called local response normalization (LRN): the most strongly acti‐
vated neurons inhibit other neurons located at the same position in neighboring fea‐
ture  maps  (such  competitive  activation  has  been  observed  in  biological  neurons).
This encourages different feature maps to specialize, pushing them apart and forcing

CNN Architectures 

| 

465

them to explore a wider range of features, ultimately improving generalization. Equa‐
tion 14-2 shows how to apply LRN.

Equation 14-2. Local response normalization (LRN)

j

high
bi = ai k + α ∑
j = j

low

−β

2

a j

with

jhigh = min i +

r
2

, f n − 1

jlow = max 0, i −

r
2

In this equation:

• bi is the normalized output of the neuron located in feature map i, at some row u
and column v (note that in this equation we consider only neurons located at this
row and column, so u and v are not shown).

• ai is the activation of that neuron after the ReLU step, but before normalization.
• k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth

radius.

• fn is the number of feature maps.

For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation
of the neurons located in the feature maps immediately above and below its own.

In AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and
k  =  1.  This  step  can  be  implemented  using  the  tf.nn.local_response_normaliza
tion()  function  (which  you  can  wrap  in  a  Lambda  layer  if  you  want  to  use  it  in  a
Keras model).

A variant of AlexNet called ZF Net12 was developed by Matthew Zeiler and Rob Fer‐
gus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked
hyperparameters (number of feature maps, kernel size, stride, etc.).

GoogLeNet
The GoogLeNet architecture was developed by Christian Szegedy et al. from Google
Research,13 and it won the ILSVRC 2014 challenge by pushing the top-five error rate
below 7%. This great performance came in large part from the fact that the network
was much deeper than previous CNNs (as you’ll see in Figure 14-14). This was made

12 Matthew D. Zeiler and Rob Fergus, “Visualizing and Understanding Convolutional Networks,” Proceedings of

the European Conference on Computer Vision (2014): 818-833.

13 Christian Szegedy et al., “Going Deeper with Convolutions,” Proceedings of the IEEE Conference on Computer

Vision and Pattern Recognition (2015): 1–9.

466 

| 

Chapter 14: Deep Computer Vision Using Convolutional Neural Networks

possible  by  subnetworks  called  inception  modules,14  which  allow  GoogLeNet  to  use
parameters  much  more  efficiently  than  previous  architectures:  GoogLeNet  actually
has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).

Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +
1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and "same" padding. The input
signal is first copied and fed to four different layers. All convolutional layers use the
ReLU activation function. Note that the second set of convolutional layers uses differ‐
ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different
scales. Also note that every single layer uses a stride of 1 and "same" padding (even
the max pooling layer), so their outputs all have the same height and width as their
inputs. This makes it possible to concatenate all the outputs along the depth dimen‐
sion  in  the  final  depth  concatenation  layer  (i.e.,  stack  the  feature  maps  from  all  four
top  convolutional  layers).  This  concatenation  layer  can  be  implemented  in  Tensor‐
Flow using the tf.concat() operation, with axis=3 (the axis is the depth).

Figure 14-13. Inception module

You  may  wonder  why  inception  modules  have  convolutional  layers  with  1  ×  1  ker‐
nels.  Surely  these  layers  cannot  capture  any  features  because  they  look  at  only  one
pixel at a time? In fact, the layers serve three purposes:

• Although  they  cannot  capture  spatial  patterns,  they  can  capture  patterns  along

the depth dimension.

• They are configured to output fewer feature maps than their inputs, so they serve
as bottleneck layers, meaning they reduce dimensionality. This cuts the computa‐

14 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams;

hence the name of these modules.

CNN Architectures 

| 

467

tional  cost  and  the  number  of  parameters,  speeding  up  training  and  improving
generalization.

• Each  pair  of  convolutional  layers  ([1  ×  1,  3  ×  3]  and  [1  ×  1,  5  ×  5])  acts  like  a
single powerful convolutional layer, capable of capturing more complex patterns.
Indeed, instead of sweeping a simple linear classifier across the image (as a single
convolutional  layer  does),  this  pair  of  convolutional  layers  sweeps  a  two-layer
neural network across the image.

In  short,  you  can  think  of  the  whole  inception  module  as  a  convolutional  layer  on
steroids, able to output feature maps that capture complex patterns at various scales.

The number of convolutional kernels for each convolutional layer
is  a  hyperparameter.  Unfortunately,  this  means  that  you  have  six
more hyperparameters to tweak for every inception layer you add.

Now  let’s  look  at  the  architecture  of  the  GoogLeNet  CNN  (see  Figure  14-14).  The
number of feature maps output by each convolutional layer and each pooling layer is
shown before the kernel size. The architecture is so deep that it has to be represented
in three columns, but GoogLeNet is actually one tall stack, including nine inception
modules (the boxes with the spinning tops). The six numbers in the inception mod‐
ules represent the number of feature maps output by each convolutional layer in the
module (in the same order as in Figure 14-13). Note that all the convolutional layers
use the ReLU activation function.

468 

| 

Chapter 14: Deep Computer Vision Using Convolutional Neural Networks

Figure 14-14. GoogLeNet architecture

Let’s go through this network:

• The first two layers divide the image’s height and width by 4 (so its area is divided
by 16), to reduce the computational load. The first layer uses a large kernel size so
that much of the information is preserved.

• Then the local response normalization layer ensures that the previous layers learn

a wide variety of features (as discussed earlier).

• Two  convolutional  layers  follow,  where  the  first  acts  like  a  bottleneck  layer.  As
explained  earlier,  you  can  think  of  this  pair  as  a  single  smarter  convolutional
layer.

• Again, a local response normalization layer ensures that the previous layers cap‐

ture a wide variety of patterns.

• Next,  a  max  pooling  layer  reduces  the  image  height  and  width  by  2,  again  to

speed up computations.

• Then  comes  the  tall  stack  of  nine  inception  modules,  interleaved  with  a  couple

max pooling layers to reduce dimensionality and speed up the net.

CNN Architectures 

| 

469

• Next, the global average pooling layer outputs the mean of each feature map: this
drops  any  remaining  spatial  information,  which  is  fine  because  there  was  not
much spatial information left at that point. Indeed, GoogLeNet input images are
typically  expected  to  be  224  ×  224  pixels,  so  after  5  max  pooling  layers,  each
dividing the height and width by 2, the feature maps are down to 7 × 7. More‐
over,  it  is  a  classification  task,  not  localization,  so  it  does  not  matter  where  the
object is. Thanks to the dimensionality reduction brought by this layer, there is
no  need  to  have  several  fully  connected  layers  at  the  top  of  the  CNN  (like  in
AlexNet),  and  this  considerably  reduces  the  number  of  parameters  in  the  net‐
work and limits the risk of overfitting.

• The last layers are self-explanatory: dropout for regularization, then a fully con‐
nected layer with 1,000 units (since there are 1,000 classes) and a softmax activa‐
tion function to output estimated class probabilities.

This diagram is slightly simplified: the original GoogLeNet architecture also included
two  auxiliary  classifiers  plugged  on  top  of  the  third  and  sixth  inception  modules.
They were both composed of one average pooling layer, one convolutional layer, two
fully  connected  layers,  and  a  softmax  activation  layer.  During  training,  their  loss
(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐
ing gradients problem and regularize the network. However, it was later shown that
their effect was relatively minor.

Several  variants  of  the  GoogLeNet  architecture  were  later  proposed  by  Google
researchers, including Inception-v3 and Inception-v4, using slightly different incep‐
tion modules and reaching even better performance.

VGGNet
The  runner-up  in  the  ILSVRC  2014  challenge  was  VGGNet,15  developed  by  Karen
Simonyan and Andrew Zisserman from the Visual Geometry Group (VGG) research
lab at Oxford University. It had a very simple and classical architecture, with 2 or 3
convolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a
pooling  layer,  and  so  on  (reaching  a  total  of  just  16  or  19  convolutional  layers,
depending on the VGG variant), plus a final dense network with 2 hidden layers and
the output layer. It used only 3 × 3 filters, but many filters.

15 Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recog‐

nition,” arXiv preprint arXiv:1409.1556 (2014).

470 

| 

Chapter 14: Deep Computer Vision Using Convolutional Neural Networks

ResNet
Kaiming  He  et  al.  won  the  ILSVRC  2015  challenge  using  a  Residual  Network  (or
ResNet),16 that delivered an astounding top-five error rate under 3.6%. The winning
variant used an extremely deep CNN composed of 152 layers (other variants had 34,
50,  and  101  layers).  It  confirmed  the  general  trend:  models  are  getting  deeper  and
deeper, with fewer and fewer parameters. The key to being able to train such a deep
network is to use skip connections (also called shortcut connections): the signal feeding
into a layer is also added to the output of a layer located a bit higher up the stack. Let’s
see why this is useful.

When training a neural network, the goal is to make it model a target function h(x).
If you add the input x to the output of the network (i.e., you add a skip connection),
then  the  network  will  be  forced  to  model  f(x)  =  h(x)  –  x  rather  than  h(x).  This  is
called residual learning (see Figure 14-15).

Figure 14-15. Residual learning

When you initialize a regular neural network, its weights are close to zero, so the net‐
work just outputs values close to zero. If you add a skip connection, the resulting net‐
work just outputs a copy of its inputs; in other words, it initially models the identity
function. If the target function is fairly close to the identity function (which is often
the case), this will speed up training considerably.

Moreover, if you add many skip connections, the network can start making progress
even if several layers have not started learning yet (see Figure 14-16). Thanks to skip
connections, the signal can easily make its way across the whole network. The deep
residual network can be seen as a stack of residual units (RUs), where each residual
unit is a small neural network with a skip connection.

16 Kaiming He et al., “Deep Residual Learning for Image Recognition,” arXiv preprint arXiv:1512:03385 (2015).

CNN Architectures 

| 

471

Figure 14-16. Regular deep neural network (left) and deep residual network (right)

Now let’s look at ResNet’s architecture (see Figure 14-17). It is surprisingly simple. It
starts  and  ends  exactly  like  GoogLeNet  (except  without  a  dropout  layer),  and  in
between is just a very deep stack of simple residual units. Each residual unit is com‐
posed of two convolutional layers (and no pooling layer!), with Batch Normalization
(BN)  and  ReLU  activation,  using  3  ×  3  kernels  and  preserving  spatial  dimensions
(stride 1, "same" padding).

472 

| 

Chapter 14: Deep Computer Vision Using Convolutional Neural Networks

Figure 14-17. ResNet architecture

Note that the number of feature maps is doubled every few residual units, at the same
time as their height and width are halved (using a convolutional layer with stride 2).
When this happens, the inputs cannot be added directly to the outputs of the residual
unit  because  they  don’t  have  the  same  shape  (for  example,  this  problem  affects  the
skip connection represented by the dashed arrow in Figure 14-17). To solve this prob‐
lem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the
right number of output feature maps (see Figure 14-18).

Figure 14-18. Skip connection when changing feature map size and depth

CNN Architectures 

| 

473

ResNet-34  is  the  ResNet  with  34  layers  (only  counting  the  convolutional  layers  and
the fully connected layer)17 containing 3 residual units that output 64 feature maps, 4
RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐
ment this architecture later in this chapter.

ResNets  deeper  than  that,  such  as  ResNet-152,  use  slightly  different  residual  units.
Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three
convolutional  layers:  first  a  1  ×  1  convolutional  layer  with  just  64  feature  maps  (4
times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer
with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature
maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs
that  output  256  maps,  then  8  RUs  with  512  maps,  a  whopping  36  RUs  with  1,024
maps, and finally 3 RUs with 2,048 maps.

Google’s  Inception-v418  architecture  merged  the  ideas  of  GoogLe‐
Net and ResNet and achieved a top-five error rate of close to 3% on
ImageNet classification.

Xception
Another  variant  of  the  GoogLeNet  architecture  is  worth  noting:  Xception19  (which
stands for Extreme Inception) was proposed in 2016 by François Chollet (the author
of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350
milli
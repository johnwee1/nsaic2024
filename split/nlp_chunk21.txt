37
37

idf
1.57
1.27
0.967
0.489
0.246
0.037
0.012
0
0

tf-idf

The tf-idf weighted value wt, d for word t in document d thus combines term

frequency tft, d (deﬁned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:

wt, d = tft, d ×

idft

(6.14)

Fig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,
using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-
sponding to the word good have now all become 0; since this word appears in every
document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which
appears in 36 out of the 37 plays, has a much lower weight.

The tf-idf weighting is the way for weighting co-occurrence matrices in infor-
mation retrieval, but also plays a role in many other aspects of natural language
processing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll look at other
weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.

3 Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

118 CHAPTER 6

• VECTOR SEMANTICS AND EMBEDDINGS

Twelfth Night
0
0
0.033
0.081

As You Like It
0.246
0
0.030
0.085

battle
good
fool
wit
Figure 6.9 A tf-idf weighted term-document matrix for four words in four Shakespeare
plays, using the counts in Fig. 6.2. For example the 0.085 value for wit in As You Like It is
the product of tf = 1 + log10(20) = 2.301 and idf = .037. Note that the idf weighting has
eliminated the importance of the ubiquitous word good and vastly reduced the impact of the
almost-ubiquitous word fool.

Julius Caesar
0.454
0
0.0012
0.048

Henry V
0.520
0
0.0019
0.054

6.6 Pointwise Mutual Information (PMI)

An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-
mation), is used for term-term-matrices, when the vector dimensions correspond to
words rather than documents. PPMI draws on the intuition that the best way to weigh
the association between two words is to ask how much more the two words co-occur
in our corpus than we would have a priori expected them to appear by chance.

Pointwise mutual information (Fano, 1961)4 is one of the most important con-
cepts in NLP. It is a measure of how often two events x and y occur, compared with
what we would expect if they were independent:

pointwise
mutual
information

I(x, y) = log2

P(x, y)
P(x)P(y)

(6.16)

The pointwise mutual information between a target word w and a context word

c (Church and Hanks 1989, Church and Hanks 1990) is then deﬁned as:

PMI(w, c) = log2

P(w, c)
P(w)P(c)

(6.17)

The numerator tells us how often we observed the two words together (assuming
we compute probability by using the MLE). The denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the probability of two independent events both occurring is just
the product of the probabilities of the two events. Thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. PMI is
a useful tool whenever we need to ﬁnd words that are strongly associated.

PMI values range from negative to positive inﬁnity. But negative PMI values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. To distinguish whether
6 occur together less often than
two words whose individual probability is each 10−
chance, we would need to be certain that the probability of the two occurring to-
12, and this kind of granularity would require an
gether is signiﬁcantly less than 10−
enormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate
such scores of ‘unrelatedness’ with human judgments. For this reason it is more

4 PMI is based on the mutual information between two random variables X and Y , deﬁned as:

I(X,Y ) =

(cid:88)

(cid:88)

x

y

P(x, y) log2

P(x, y)
P(x)P(y)

(6.15)

In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information

6.6

• POINTWISE MUTUAL INFORMATION (PMI)

119

PPMI

common to use Positive PMI (called PPMI) which replaces all negative PMI values
with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5:

PPMI(w, c) = max(log2

P(w, c)
P(w)P(c)

, 0)

(6.18)

More formally, let’s assume we have a co-occurrence matrix F with W rows (words)
and C columns (contexts), where fi j gives the number of times word wi occurs with
context c j. This can be turned into a PPMI matrix where PPMIi j gives the PPMI
value of word wi with context c j (which we can also express as PPMI(wi, c j) or
PPMI(w = i, c = j)) as follows:

pi j =

fi j
C
j=1 fi j

W
i=1

, pi

∗

=

C
j=1 fi j
C
j=1 fi j

W
(cid:80)
i=1

, p
∗

j =

W
i=1 fi j
C
j=1 fi j

W
i=1
(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

(cid:80)

PPMIi j = max(log2

pi j
p
∗

pi
∗

j

, 0)

(6.19)

(6.20)

Let’s see some PPMI calculations. We’ll use Fig. 6.10, which repeats Fig. 6.6 plus
all the count marginals, and let’s pretend for ease of calculation that these are the
only words/contexts that matter.

cherry
strawberry
digital
information

computer
2
0
1670
3325

data
8
0
1683
3982

result
9
1
85
378

pie
442
60
5
5

sugar
25
19
4
13

count(w)
486
80
3447
7703

4997

count(context)
Figure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,
together with the marginals, pretending for the purpose of this calculation that no other word-
s/contexts matter.

11716

5673

512

473

61

Thus for example we could compute PPMI(information,data), assuming we pre-
tended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-
lows:

P(w=information, c=data) =

P(w=information) =

P(c=data) =

3982
11716
7703
11716
5673
11716

= .3399

= .6575

= .4842

PPMI(information,data) = log2(.3399/(.6575

.4842)) = .0944

∗

Fig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and
Fig. 6.12 shows the PPMI values. Not surprisingly, cherry and strawberry are highly
associated with both pie and sugar, and data is mildly associated with information.
PMI has the problem of being biased toward infrequent events; very rare words
tend to have very high PMI values. One way to reduce this bias toward low frequency

5 Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the

∞ from log(0).

−

120 CHAPTER 6

• VECTOR SEMANTICS AND EMBEDDINGS

p(w,context)

cherry
strawberry
digital
information

computer
0.0002
0.0000
0.1425
0.2838

data
0.0007
0.0000
0.1436
0.3399

result
0.0008
0.0001
0.0073
0.0323

pie
0.0377
0.0051
0.0004
0.0004

sugar
0.0021
0.0016
0.0003
0.0011

p(w)
p(w)
0.0415
0.0068
0.2942
0.6575

0.0404
p(context)
Figure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals
in the right column and the bottom row.

0.0052

0.4265

0.4842

0.0437

data
0
0
0.01
0.09

computer
0
0
0.18
0.02

result
0
0
0
0.28

pie
4.38
4.10
0
0

cherry
strawberry
digital
information
Figure 6.12 The PPMI matrix showing the association between words and context words,
computed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had
a negative PMI; for example PMI(cherry,computer) = -6.7, meaning that cherry and computer
co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace
negative values by zero.

sugar
3.30
5.51
0
0

events is to slightly change the computation for P(c), using a different function Pα (c)
that raises the probability of the context word to the power of α:

PPMIα (w, c) = max(log2

P(w, c)
P(w)Pα (c)

, 0)

Pα (c) =

count(c)α
c count(c)α

(6.21)

(6.22)

Levy et al. (2015) found that a setting of α = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in Eq. 6.32). This works because raising the count to α =
0.75 increases the probability assigned to rare contexts, and hence lowers their PMI
(Pα (c) > P(c) when c is rare).

(cid:80)

Another possible solution is Laplace smoothing: Before computing PMI, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. The larger the k, the more the non-zero counts
are discounted.

6.7 Applications of the tf-idf or PPMI vector models

In summary, the vector semantics model we’ve described so far represents a target
word as a vector with dimensions corresponding either to the documents in a large
collection (the term-document matrix) or to the counts of words in some neighboring
window (the term-term matrix). The values in each dimension are counts, weighted
by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the
vectors are sparse (since most values are zero).

The model computes the similarity between two words x and y by taking the
cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model

6.8

• WORD2VEC

121

is sometimes referred to as the tf-idf model or the PPMI model, after the weighting
function.

The tf-idf model of meaning is often used for document functions like deciding
if two documents are similar. We represent a document by taking the vectors of
all the words in the document, and computing the centroid of all those vectors.
The centroid is the multidimensional version of the mean; the centroid of a set of
vectors is a single vector that has the minimum sum of squared distances to each of
the vectors in the set. Given k word vectors w1, w2, ..., wk, the centroid document
vector d is:

centroid

document
vector

d =

w1 + w2 + ... + wk
k

(6.23)

Given two documents, we can then compute their document vectors d1 and d2, and
estimate the similarity between the two documents by cos(d1, d2). Document sim-
ilarity is also useful for all sorts of applications; information retrieval, plagiarism
detection, news recommender systems, and even for digital humanities tasks like
comparing different versions of a text to see which are similar to each other.

Either the PPMI model or the tf-idf model can be used to compute word simi-
larity, for tasks like ﬁnding word paraphrases, tracking changes in word meaning, or
automatically discovering meanings of words in different corpora. For example, we
can ﬁnd the 10 most similar words to any target word w by computing the cosines
between w and each of the V

1 other words, sorting, and looking at the top 10.

−

6.8 Word2vec

In the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to words in the vocabulary or documents in a collection.
We now introduce a more powerful word representation: embeddings, short dense
vectors. Unlike the vectors we’ve seen so far, embeddings are short, with number
of dimensions d ranging from 50-1000, rather than the much larger vocabulary size
or number of documents D we’ve seen. These d dimensions don’t have a clear
V
|
interpretation. And the vectors are dense: instead of vector entries being sparse,
mostly-zero counts or functions of counts, the values will be real-valued numbers
that can be negative.

|

It turns out that dense vectors work better in every NLP task than sparse vectors.
While we don’t completely understand all the reasons for this, we have some intu-
itions. Representing words as 300-dimensional dense vectors requires our classiﬁers
to learn far fewer weights than if we represented words as 50,000-dimensional vec-
tors, and the smaller parameter space possibly helps with generalization and avoid-
ing overﬁtting. Dense vectors may also do a better job of capturing synonymy.
For example, in a sparse vector representation, dimensions for synonyms like car
and automobile dimension are distinct and unrelated; sparse vectors may thus fail
to capture the similarity between a word with car as a neighbor and a word with
automobile as a neighbor.

In this section we introduce one method for computing embeddings: skip-gram
with negative sampling, sometimes called SGNS. The skip-gram algorithm is one
of two algorithms in a software package called word2vec, and so sometimes the
algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.
2013b). The word2vec methods are fast, efﬁcient to train, and easily available on-

skip-gram

SGNS

word2vec

122 CHAPTER 6

• VECTOR SEMANTICS AND EMBEDDINGS

static
embeddings

self-supervision

line with code and pretrained embeddings. Word2vec embeddings are static em-
beddings, meaning that the method learns one ﬁxed embedding for each word in the
vocabulary. In Chapter 11 we’ll introduce methods for learning dynamic contextual
embeddings like the popular family of BERT representations, in which the vector
for each word is different in different contexts.

The intuition of word2vec is that instead of counting how often each word w oc-
curs near, say, apricot, we’ll instead train a classiﬁer on a binary prediction task: “Is
word w likely to show up near apricot?” We don’t actually care about this prediction
task; instead we’ll take the learned classiﬁer weights as the word embeddings.

The revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classiﬁer; a word c that occurs near the target
word apricot acts as gold ‘correct answer’ to the question “Is word c likely to show
up near apricot?” This method, often called self-supervision, avoids the need for
any sort of hand-labeled supervision signal. This idea was ﬁrst proposed in the task
of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)
showed that a neural language model (a neural network that learned to predict the
next word from prior words) could just use the next word in running text as its
supervision signal, and could be used to learn an embedding representation for each
word as part of doing this prediction task.

We’ll see how to do neural networks in the next chapter, but word2vec is a
much simpler model than the neural network language model, in two ways. First,
word2vec simpliﬁes the task (making it binary classiﬁcation instead of word pre-
diction). Second, word2vec simpliﬁes the architecture (training a logistic regression
classiﬁer instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). The intuition of skip-gram is:

1. Treat the target word and a neighboring context word as positive examples.
2. Randomly sample other words in the lexicon to get negative samples.
3. Use logistic regression to train a classiﬁer to distinguish those two cases.
4. Use the learned weights as the embeddings.

6.8.1 The classiﬁer

Let’s start by thinking about the classiﬁcation task, and then turn to how to train.
Imagine a sentence like the following, with a target word apricot, and assume we’re
using a window of

2 context words:

±
... lemon,

a [tablespoon of apricot jam,
c2

c3

c1

w

a] pinch ...
c4

Our goal is to train a classiﬁer such that, given a tuple (w, c) of a target word
w paired with a candidate context word c (for example (apricot, jam), or perhaps
(apricot, aardvark)) it will return the probability
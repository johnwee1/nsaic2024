 learning
rate  and  then  reduce  it  once  training  stops  making  fast  progress,  you  can  reach  a
good solution faster than with the optimal constant learning rate. There are many dif‐
ferent strategies to reduce the learning rate during training. It can also be beneficial to
start  with  a  low  learning  rate,  increase  it,  then  drop  it  again.  These  strategies  are
called learning schedules (we briefly introduced this concept in Chapter 4). These are
the most commonly used learning schedules:

Power scheduling

Set the learning rate to a function of the iteration number t: η(t) = η0 / (1 + t/s)c.
The  initial  learning  rate  η0,  the  power  c  (typically  set  to  1),  and  the  steps  s  are
hyperparameters. The learning rate drops at each step. After s steps, it is down to
η0 / 2. After s more steps, it is down to η0 / 3, then it goes down to η0 / 4, then η0 /
5,  and  so  on.  As  you  can  see,  this  schedule  first  drops  quickly,  then  more  and
more slowly. Of course, power scheduling requires tuning η0 and s (and possibly
c).

Exponential scheduling

Set the learning rate to η(t) = η0 0.1t/s. The learning rate will gradually drop by a
factor of 10 every s steps. While power scheduling reduces the learning rate more
and more slowly, exponential scheduling keeps slashing it by a factor of 10 every
s steps.

360 

| 

Chapter 11: Training Deep Neural Networks

Piecewise constant scheduling

Use a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs),
then a smaller learning rate for another number of epochs (e.g., η1 = 0.001 for 50
epochs),  and  so  on.  Although  this  solution  can  work  very  well,  it  requires  fid‐
dling around to figure out the right sequence of learning rates and how long to
use each of them.

Performance scheduling

Measure  the  validation  error  every  N  steps  (just  like  for  early  stopping),  and
reduce the learning rate by a factor of λ when the error stops dropping.

1cycle scheduling

Contrary to the other approaches, 1cycle (introduced in a 2018 paper21 by Leslie
Smith) starts by increasing the initial learning rate η0, growing linearly up to η1
halfway through training. Then it decreases the learning rate linearly down to η0
again during the second half of training, finishing the last few epochs by drop‐
ping the rate down by several orders of magnitude (still linearly). The maximum
learning rate η1 is chosen using the same approach we used to find the optimal
learning  rate,  and  the  initial  learning  rate  η0  is  chosen  to  be  roughly  10  times
lower.  When  using  a  momentum,  we  start  with  a  high  momentum  first  (e.g.,
0.95), then drop it down to a lower momentum during the first half of training
(e.g.,  down  to  0.85,  linearly),  and  then  bring  it  back  up  to  the  maximum  value
(e.g., 0.95) during the second half of training, finishing the last few epochs with
that  maximum  value.  Smith  did  many  experiments  showing  that  this  approach
was often able to speed up training considerably and reach better performance.
For  example,  on  the  popular  CIFAR10  image  dataset,  this  approach  reached
91.9%  validation  accuracy  in  just  100  epochs,  instead  of  90.3%  accuracy  in  800
epochs  through  a  standard  approach  (with  the  same  neural  network
architecture).

A  2013  paper22  by  Andrew  Senior  et  al.  compared  the  performance  of  some  of  the
most popular learning schedules when using momentum optimization to train deep
neural networks for speech recognition. The authors concluded that, in this setting,
both  performance  scheduling  and  exponential  scheduling  performed  well.  They
favored exponential scheduling because it was easy to tune and it converged slightly
faster  to  the  optimal  solution  (they  also  mentioned  that  it  was  easier  to  implement

21 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch

Size, Momentum, and Weight Decay,” arXiv preprint arXiv:1803.09820 (2018).

22 Andrew Senior et al., “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recogni‐
tion,” Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (2013):
6724–6728.

Faster Optimizers 

| 

361

than  performance  scheduling,  but  in  Keras  both  options  are  easy).  That  said,  the
1cycle approach seems to perform even better.

Implementing  power  scheduling  in  Keras  is  the  easiest  option:  just  set  the  decay
hyperparameter when creating an optimizer:

optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)

The decay is the inverse of s (the number of steps it takes to divide the learning rate
by one more unit), and Keras assumes that c is equal to 1.

Exponential scheduling and piecewise scheduling are quite simple too. You first need
to  define  a  function  that  takes  the  current  epoch  and  returns  the  learning  rate.  For
example, let’s implement exponential scheduling:

def exponential_decay_fn(epoch):
    return 0.01 * 0.1**(epoch / 20)

If you do not want to hardcode η0 and s, you can create a function that returns a con‐
figured function:

def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
        return lr0 * 0.1**(epoch / s)
    return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)

Next, create a LearningRateScheduler callback, giving it the schedule function, and
pass this callback to the fit() method:

lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])

The LearningRateScheduler will update the optimizer’s learning_rate attribute at
the  beginning  of  each  epoch.  Updating  the  learning  rate  once  per  epoch  is  usually
enough, but if you want it to be updated more often, for example at every step, you
can always write your own callback (see the “Exponential Scheduling” section of the
notebook  for  an  example).  Updating  the  learning  rate  at  every  step  makes  sense  if
there  are  many  steps  per  epoch.  Alternatively,  you  can  use  the  keras.optimiz
ers.schedules approach, described shortly.

The schedule function can optionally take the current learning rate as a second argu‐
ment. For example, the following schedule function multiplies the previous learning
rate  by  0.11/20,  which  results  in  the  same  exponential  decay  (except  the  decay  now
starts at the beginning of epoch 0 instead of 1):

def exponential_decay_fn(epoch, lr):
    return lr * 0.1**(1 / 20)

362 

| 

Chapter 11: Training Deep Neural Networks

This  implementation  relies  on  the  optimizer’s  initial  learning  rate  (contrary  to  the
previous implementation), so make sure to set it appropriately.

When you save a model, the optimizer and its learning rate get saved along with it.
This means that with this new schedule function, you could just load a trained model
and continue training where it left off, no problem. Things are not so simple if your
schedule function uses the epoch argument, however: the epoch does not get saved,
and it gets reset to 0 every time you call the fit() method. If you were to continue
training a model where it left off, this could lead to a very large learning rate, which
would likely damage your model’s weights. One solution is to manually set the fit()
method’s initial_epoch argument so the epoch starts at the right value.

For piecewise constant scheduling, you can use a schedule function like the following
one (as earlier, you can define a more general function if you want; see the “Piecewise
Constant  Scheduling”  section  of  the  notebook  for  an  example),  then  create  a  Lear
ningRateScheduler callback with this function and pass it to the fit() method, just
like we did for exponential scheduling:

def piecewise_constant_fn(epoch):
    if epoch < 5:
        return 0.01
    elif epoch < 15:
        return 0.005
    else:
        return 0.001

For  performance  scheduling,  use  the  ReduceLROnPlateau  callback.  For  example,  if
you pass the following callback to the fit() method, it will multiply the learning rate
by 0.5 whenever the best validation loss does not improve for five consecutive epochs
(other options are available; please check the documentation for more details):

lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)

Lastly, tf.keras offers an alternative way to implement learning rate scheduling: define
the  learning  rate  using  one  of  the  schedules  available  in  keras.optimizers.sched
ules, then pass this learning rate to any optimizer. This approach updates the learn‐
ing rate at each step rather than at each epoch. For example, here is how to implement
the same exponential schedule as the exponential_decay_fn() function we defined
earlier:

s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)
optimizer = keras.optimizers.SGD(learning_rate)

This  is  nice  and  simple,  plus  when  you  save  the  model,  the  learning  rate  and  its
schedule (including its state) get saved as well. This approach, however, is not part of
the Keras API; it is specific to tf.keras.

Faster Optimizers 

| 

363

As  for  the  1cycle  approach,  the  implementation  poses  no  particular  difficulty:  just
create  a  custom  callback  that  modifies  the  learning  rate  at  each  iteration  (you  can
update the optimizer’s learning rate by changing self.model.optimizer.lr). See the
“1Cycle scheduling” section of the notebook for an example.

To sum up, exponential decay, performance scheduling, and 1cycle can considerably
speed up convergence, so give them a try!

Avoiding Overfitting Through Regularization

With  four  parameters  I  can  fit  an  elephant  and  with  five  I  can  make  him  wiggle  his
trunk.

—John von Neumann, cited by Enrico Fermi in Nature 427

With thousands of parameters, you can fit the whole zoo. Deep neural networks typi‐
cally have tens of thousands of parameters, sometimes even millions. This gives them
an incredible amount of freedom and means they can fit a huge variety of complex
datasets.  But  this  great  flexibility  also  makes  the  network  prone  to  overfitting  the
training set. We need regularization.

We  already  implemented  one  of  the  best  regularization  techniques  in  Chapter  10:
early  stopping.  Moreover,  even  though  Batch  Normalization  was  designed  to  solve
the unstable gradients problems, it also acts like a pretty good regularizer. In this sec‐
tion we will examine other popular regularization techniques for neural networks: ℓ1
and ℓ2 regularization, dropout, and max-norm regularization.

ℓ1 and ℓ2 Regularization
Just like you did in Chapter 4 for simple linear models, you can use ℓ2 regularization
to  constrain  a  neural  network’s  connection  weights,  and/or  ℓ1  regularization  if  you
want a sparse model (with many weights equal to 0). Here is how to apply ℓ2 regulari‐
zation to a Keras layer’s connection weights, using a regularization factor of 0.01:

layer = keras.layers.Dense(100, activation="elu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=keras.regularizers.l2(0.01))

The l2() function returns a regularizer that will be called at each step during training
to compute the regularization loss. This is then added to the final loss. As you might
expect,  you  can  just  use  keras.regularizers.l1()  if  you  want  ℓ1  regularization;  if
you want both ℓ1 and ℓ2 regularization, use keras.regularizers.l1_l2() (specifying
both regularization factors).

Since you will typically want to apply the same regularizer to all layers in your net‐
work, as well as using the same activation function and the same initialization strat‐
egy  in  all  hidden  layers,  you  may  find  yourself  repeating  the  same  arguments.  This

364 

| 

Chapter 11: Training Deep Neural Networks

makes the code ugly and error-prone. To avoid this, you can try refactoring your code
to  use  loops.  Another  option  is  to  use  Python’s  functools.partial()  function,
which  lets  you  create  a  thin  wrapper  for  any  callable,  with  some  default  argument
values:

from functools import partial

RegularizedDense = partial(keras.layers.Dense,
                           activation="elu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=keras.regularizers.l2(0.01))

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    RegularizedDense(300),
    RegularizedDense(100),
    RegularizedDense(10, activation="softmax",
                     kernel_initializer="glorot_uniform")
])

Dropout
Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  net‐
works. It was proposed in a paper23 by Geoffrey Hinton in 2012 and further detailed
in a 2014 paper24 by Nitish Srivastava et al., and it has proven to be highly successful:
even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding
dropout. This may not sound like a lot, but when a model already has 95% accuracy,
getting  a  2%  accuracy  boost  means  dropping  the  error  rate  by  almost  40%  (going
from 5% error to roughly 3%).

It  is  a  fairly  simple  algorithm:  at  every  training  step,  every  neuron  (including  the
input neurons, but always excluding the output neurons) has a probability p of being
temporarily  “dropped  out,”  meaning  it  will  be  entirely  ignored  during  this  training
step, but it may be active during the next step (see Figure 11-9). The hyperparameter
p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20–
30% in recurrent neural nets (see Chapter 15), and closer to 40–50% in convolutional
neural  networks  (see  Chapter  14).  After  training,  neurons  don’t  get  dropped  any‐
more. And that’s all (except for a technical detail we will discuss momentarily).

23 Geoffrey E. Hinton et al., “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors,”

arXiv preprint arXiv:1207.0580 (2012).

24 Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” Journal of

Machine Learning Research 15 (2014): 1929–1958.

Avoiding Overfitting Through Regularization 

| 

365

Figure 11-9. With dropout regularization, at each training iteration a random subset of
all neurons in one or more layers—except the output layer—are “dropped out”; these
neurons output 0 at this iteration (represented by the dashed arrows)

It’s surprising at first that this destructive technique works at all. Would a company
perform  better  if  its  employees  were  told  to  toss  a  coin  every  morning  to  decide
whether  or  not  to  go  to  work?  Well,  who  knows;  perhaps  it  would!  The  company
would  be  forced  to  adapt  its  organization;  it  could  not  rely  on  any  single  person  to
work the coffee machine or perform any other critical tasks, so this expertise would
have to be spread across several people. Employees would have to learn to cooperate
with  many  of  their  coworkers,  not  just  a  handful  of  them.  The  company  would
become much more resilient. If one person quit, it wouldn’t make much of a d
hen optimize that lower-bound (M-step).4

It turns out that the summation (cid:80)n

i=1 is not essential here, and towards a
simpler exposition of the EM algorithm, we will ï¬rst consider optimizing the
the likelihood log p(x) for a single example x. After we derive the algorithm
for optimizing log p(x), we will convert it to an algorithm that works for n
examples by adding back the sum to each of the relevant equations. Thus,
now we aim to optimize log p(x; Î¸) which can be rewritten as

log p(x; Î¸) = log

p(x, z; Î¸)

(cid:88)

z

(11.5)

Let Q be a distribution over the possible values of z. That is, (cid:80)
Q(z) â‰¥ 0).

z Q(z) = 1,

Consider the following:5

log p(x; Î¸) = log

= log

(cid:88)

z
(cid:88)

z

p(x, z; Î¸)

Q(z)

p(x, z; Î¸)
Q(z)

â‰¥

(cid:88)

z

Q(z) log

p(x, z; Î¸)
Q(z)

(11.6)

(11.7)

The last step of this derivation used Jensenâ€™s inequality. Speciï¬cally,
f (x) = log x is a concave function, since f (cid:48)(cid:48)(x) = âˆ’1/x2 < 0 over its domain

3Itâ€™s mostly an empirical observation that the optimization problem is diï¬ƒcult to op-

timize.

4Empirically, the E-step and M-step can often be computed more eï¬ƒciently than op-
timizing the function (cid:96)(Â·) directly. However, it doesnâ€™t necessarily mean that alternating
the two steps can always converge to the global optimum of (cid:96)(Â·). Even for mixture of
Gaussians, the EM algorithm can either converge to a global optimum or get stuck, de-
pending on the properties of the training data. Empirically, for real-world data, often EM
can converge to a solution with relatively high likelihood (if not the optimum), and the
theory behind it is still largely not understood.

5If z were continuous, then Q would be a density, and the summations over z in our

discussion are replaced with integrals over z.

154

x âˆˆ R+. Also, the term

Q(z)

(cid:88)

z

(cid:21)

(cid:20)p(x, z; Î¸)
Q(z)

in the summation is just an expectation of the quantity [p(x, z; Î¸)/Q(z)] with
respect to z drawn according to the distribution given by Q.6 By Jensenâ€™s
inequality, we have

(cid:18)

f

Ezâˆ¼Q

(cid:21)(cid:19)

(cid:20)p(x, z; Î¸)
Q(z)

â‰¥ Ezâˆ¼Q

(cid:20)
f

(cid:18) p(x, z; Î¸)
Q(z)

(cid:19)(cid:21)

,

where the â€œz âˆ¼ Qâ€ subscripts above indicate that the expectations are with
respect to z drawn from Q. This allowed us to go from Equation (11.6) to
Equation (11.7).

Now, for any distribution Q, the formula (11.7) gives a lower-bound on
log p(x; Î¸). There are many possible choices for the Qâ€™s. Which should we
choose? Well, if we have some current guess Î¸ of the parameters, it seems
natural to try to make the lower-bound tight at that value of Î¸. I.e., we will
make the inequality above hold with equality at our particular value of Î¸.

To make the bound tight for a particular value of Î¸, we need for the step
involving Jensenâ€™s inequality in our derivation above to hold with equality.
For this to be true, we know it is suï¬ƒcient that the expectation be taken
over a â€œconstantâ€-valued random variable. I.e., we require that

p(x, z; Î¸)
Q(z)

= c

for some constant c that does not depend on z. This is easily accomplished
by choosing

Q(z) âˆ p(x, z; Î¸).

Actually, since we know (cid:80)
further tells us that

z Q(z) = 1 (because it is a distribution), this

Q(z) =

p(x, z; Î¸)
z p(x, z; Î¸)

(cid:80)

=

p(x, z; Î¸)
p(x; Î¸)
= p(z|x; Î¸)

(11.8)

6We note that the notion p(x,z;Î¸)

Q(z) only makes sense if Q(z) (cid:54)= 0 whenever p(x, z; Î¸) (cid:54)= 0.

Here we implicitly assume that we only consider those Q with such a property.

155

Thus, we simply set the Qâ€™s to be the posterior distribution of the zâ€™s given
x and the setting of the parameters Î¸.

Indeed, we can directly verify that when Q(z) = p(z|x; Î¸), then equa-

tion (11.7) is an equality because

(cid:88)

z

Q(z) log

p(x, z; Î¸)
Q(z)

p(z|x; Î¸) log

p(x, z; Î¸)
p(z|x; Î¸)

p(z|x; Î¸) log

p(z|x; Î¸)p(x; Î¸)
p(z|x; Î¸)

p(z|x; Î¸) log p(x; Î¸)

=

=

=

(cid:88)

z
(cid:88)

z
(cid:88)

z

= log p(x; Î¸)

(cid:88)

z

p(z|x; Î¸)

z p(z|x; Î¸) = 1)
For convenience, we call the expression in Equation (11.7) the evidence

= log p(x; Î¸)

(because (cid:80)

lower bound (ELBO) and we denote it by

ELBO(x; Q, Î¸) =

(cid:88)

z

Q(z) log

p(x, z; Î¸)
Q(z)

(11.9)

With this equation, we can re-write equation (11.7) as

âˆ€Q, Î¸, x,

log p(x; Î¸) â‰¥ ELBO(x; Q, Î¸)

(11.10)

Intuitively, the EM algorithm alternatively updates Q and Î¸ by a) set-
ting Q(z) = p(z|x; Î¸) following Equation (11.8) so that ELBO(x; Q, Î¸) =
log p(x; Î¸) for x and the current Î¸, and b) maximizing ELBO(x; Q, Î¸) w.r.t Î¸
while ï¬xing the choice of Q.

Recall that all the discussion above was under the assumption that we
aim to optimize the log-likelihood log p(x; Î¸) for a single example x. It turns
out that with multiple training examples, the basic idea is the same and we
only needs to take a sum over examples at relevant places. Next, we will
build the evidence lower bound for multiple training examples and make the
EM algorithm formal.

Recall we have a training set {x(1), . . . , x(n)}. Note that the optimal choice
of Q is p(z|x; Î¸), and it depends on the particular example x. Therefore here
we will introduce n distributions Q1, . . . , Qn, one for each example x(i). For
each example x(i), we can build the evidence lower bound

log p(x(i); Î¸) â‰¥ ELBO(x(i); Qi, Î¸) =

Qi(z(i)) log

p(x(i), z(i); Î¸)
Qi(z(i))

(cid:88)

z(i)

Taking sum over all the examples, we obtain a lower bound for the log-
likelihood

156

ELBO(x(i); Qi, Î¸)

(11.11)

(cid:88)

i
(cid:88)

(cid:96)(Î¸) â‰¥

=

(cid:88)

Qi(z(i)) log

p(x(i), z(i); Î¸)
Qi(z(i))

i

z(i)

For any set of distributions Q1, . . . , Qn, the formula (11.11) gives a lower-
bound on (cid:96)(Î¸), and analogous to the argument around equation (11.8), the
Qi that attains equality satisï¬es

Qi(z(i)) = p(z(i)|x(i); Î¸)

Thus, we simply set the Qiâ€™s to be the posterior distribution of the z(i)â€™s
given x(i) with the current setting of the parameters Î¸.

Now, for this choice of the Qiâ€™s, Equation (11.11) gives a lower-bound on
the loglikelihood (cid:96) that weâ€™re trying to maximize. This is the E-step. In the
M-step of the algorithm, we then maximize our formula in Equation (11.11)
with respect to the parameters to obtain a new setting of the Î¸â€™s. Repeatedly
carrying out these two steps gives us the EM algorithm, which is as follows:

Repeat until convergence {

(E-step) For each i, set

Qi(z(i)) := p(z(i)|x(i); Î¸).

n
(cid:88)

ELBO(x(i); Qi, Î¸)

i=1
(cid:88)

(cid:88)

i

z(i)

Qi(z(i)) log

p(x(i), z(i); Î¸)
Qi(z(i))

.

(11.12)

(M-step) Set

Î¸ := arg max

Î¸

= arg max

Î¸

}

How do we know if this algorithm will converge? Well, suppose Î¸(t) and
Î¸(t+1) are the parameters from two successive iterations of EM. We will now
prove that (cid:96)(Î¸(t)) â‰¤ (cid:96)(Î¸(t+1)), which shows EM always monotonically im-
proves the log-likelihood. The key to showing this result lies in our choice of

157

the Qiâ€™s. Speciï¬cally, on the iteration of EM in which the parameters had
started out as Î¸(t), we would have chosen Q(t)
i (z(i)) := p(z(i)|x(i); Î¸(t)). We
saw earlier that this choice ensures that Jensenâ€™s inequality, as applied to get
Equation (11.11), holds with equality, and hence

(cid:96)(Î¸(t)) =

n
(cid:88)

i=1

ELBO(x(i); Q(t)
i

, Î¸(t))

(11.13)

The parameters Î¸(t+1) are then obtained by maximizing the right hand side
of the equation above. Thus,

(cid:96)(Î¸(t+1)) â‰¥

â‰¥

n
(cid:88)

i=1

n
(cid:88)

ELBO(x(i); Q(t)
i

, Î¸(t+1))

(because ineqaulity (11.11) holds for all Q and Î¸)

ELBO(x(i); Q(t)
i

, Î¸(t))

(see reason below)

i=1
= (cid:96)(Î¸(t))

(by equation (11.13))

where the last inequality follows from that Î¸(t+1) is chosen explicitly to be

arg max

Î¸

n
(cid:88)

i=1

ELBO(x(i); Q(t)
i

, Î¸)

Hence, EM causes the likelihood to converge monotonically. In our de-
scription of the EM algorithm, we said weâ€™d run it until convergence. Given
the result that we just showed, one reasonable convergence test would be
to check if the increase in (cid:96)(Î¸) between successive iterations is smaller than
some tolerance parameter, and to declare convergence if EM is improving
(cid:96)(Î¸) too slowly.

Remark. If we deï¬ne (by overloading ELBO(Â·))

ELBO(Q, Î¸) =

n
(cid:88)

i=1

ELBO(x(i); Qi, Î¸) =

(cid:88)

(cid:88)

i

z(i)

Qi(z(i)) log

p(x(i), z(i); Î¸)
Qi(z(i))

(11.14)

then we know (cid:96)(Î¸) â‰¥ ELBO(Q, Î¸) from our previous derivation. The EM
can also be viewed an alternating maximization algorithm on ELBO(Q, Î¸),
in which the E-step maximizes it with respect to Q (check this yourself), and
the M-step maximizes it with respect to Î¸.

158

11.3.1 Other interpretation of ELBO
Let ELBO(x; Q, Î¸) = (cid:80)
There are several other forms of ELBO. First, we can rewrite

z Q(z) log p(x,z;Î¸)

Q(z) be deï¬ned as in equation (11.9).

ELBO(x; Q, Î¸) = Ezâˆ¼Q[log p(x, z; Î¸)] âˆ’ Ezâˆ¼Q[log Q(z)]

= Ezâˆ¼Q[log p(x|z; Î¸)] âˆ’ DKL(Q(cid:107)pz)

(11.15)

where we use pz to denote the marginal distribution of z (under the distri-
bution p(x, z; Î¸)), and DKL() denotes the KL divergence

DKL(Q(cid:107)pz) =

(cid:88)

z

Q(z) log

Q(z)
p(z)

(11.16)

In many cases, the marginal distribution of z does not depend on the param-
eter Î¸. In this case, we can see that maximizing ELBO over Î¸ is equivalent
to maximizing the ï¬rst term in (11.15). This corresponds to maximizing the
conditional likelihood of x conditioned on z, which is often a simpler question
than the original question.

Another form of ELBO(Â·) is (please verify yourself)

ELBO(x; Q, Î¸) = log p(x) âˆ’ DKL(Q(cid:107)pz|x)

(11.17)

where pz|x is the conditional distribution of z given x under the parameter
Î¸. This forms shows that the maximizer of ELBO(Q, Î¸) over Q is obtained
when Q = pz|x, which was shown in equation (11.8) before.

11.4 Mixture of Gaussians revisited

Armed with our general deï¬nition of the EM algorithm, letâ€™s go back to our
old example of ï¬tting the parameters Ï†, Âµ and Î£ in a mixture of Gaussians.
For the sake of brevity, we carry out the derivations for the M-step updates
only for Ï† and Âµj, and leave the updates for Î£j as an exercise for the reader.
The E-step is easy. Following our algorithm derivation above, we simply

calculate

w(i)

j = Qi(z(i) = j) = P (z(i) = j|x(i); Ï†, Âµ, Î£).
Here, â€œQi(z(i) = j)â€ denotes the probability of z(i) taking the value j under
the distribution Qi.

159

Next, in the M-step, we need to maximize, with respect to our parameters

Ï†, Âµ, Î£, the quantity

n
(cid:88)

(cid:88)

i=1

z(i)

Qi(z(i)) log

p(x(i), z(i); Ï†, Âµ, Î£)
Qi(z(i))

=

=

n
(cid:88)

k
(cid:88)

i=1

j=1

n
(cid:88)

k
(cid:88)

i=1

j=1

Qi(z(i) = j) log

p(x(i)|z(i) = j; Âµ, Î£)p(z(i) = j; Ï†)
Qi(z(i) = j)
2(x(i) âˆ’ Âµj)T Î£âˆ’1

(2Ï€)d/2|Î£j |1/2 exp (cid:0)âˆ’ 1

1

j (x(i) âˆ’ Âµj)(cid:1) Â· Ï†j

w(i)
j

log

w(i)
j

Letâ€™s maximize this with respect to Âµl. If we take the derivative with respect
to Âµl, we ï¬nd

âˆ‡Âµl

n
(cid:88)

k
(cid:88)

i=1

j=1

w(i)
j

log

(2Ï€)d/2|Î£j |1/2 exp (cid:0)âˆ’ 1

1

2(x(i) âˆ’ Âµj)T Î£âˆ’1

j (x(i) âˆ’ Âµj)(cid:1) Â· Ï†j

w(i)
j

= âˆ’âˆ‡Âµl

n
(cid:88)

k
(cid:88)

i=1

j=1

w(i)
j

1
2

(x(i) âˆ’ Âµj)T Î£âˆ’1

j (x(i) âˆ’ Âµj)

=

=

1
2

n
(cid:88)

i=1

w(i)

l âˆ‡Âµl2ÂµT

l Î£âˆ’1

l x(i) âˆ’ ÂµT

l Î£âˆ’1

l Âµl

n
(cid:88)

i=1

w(i)
l

(cid:0)Î£âˆ’1

l x(i) âˆ’ Î£âˆ’1

l Âµl

(cid:1)

Setting this to zero and solving for Âµl therefore yields the update rule

Âµl :=

(cid:80)n

i=1 w(i)
l x(i)
(cid:80)n
i=1 w(i)

l

,

which was what we had in the previous set of notes.

Letâ€™s do one more example, and derive the M-step update for the param-
eters Ï†j. Grouping together only the terms that depend on Ï†j, we ï¬nd that
we need to maximize

n
(cid:88)

k
(cid:88)

w(i)
j

log Ï†j.

However, there is an additional constraint that the Ï†jâ€™s sum to 1, since they
represent the probabilities Ï†j = p(z(i) = j; Ï†). To deal with the constraint

i=1

j=1

160

that (cid:80)k

j=1 Ï†j = 1, we construct the Lagrangian

L(Ï†) =

n
(cid:88)

k
(cid:88)

i=1

j=1

w(i)
j

k
(cid:88)

log Ï†j + Î²(

Ï†j âˆ’ 1),

j=1

where Î² is the Lagrange multiplier.7 Taking derivatives, we ï¬nd

âˆ‚
âˆ‚Ï†j

L(Ï†) =

n
(cid:88)

i=1

w(i)
j
Ï†j

+ Î²

Setting this to zero and solving, we get

Ï†j =

(cid:80)n

i=1 w(i)
âˆ’Î²

j

i=1 w(i)
j
(cid:80)k

I.e., Ï†j âˆ (cid:80)n
that âˆ’Î² = (cid:80)n
Qi(z(i) = j), and since probabilities sum to 1, (cid:80)
have our M-step updates for the parameters Ï†j:

. Using the constraint that (cid:80)
j=1 w(i)

i=1 1 = n. (This used the fact that w(i)
j w(i)

j Ï†j = 1, we easily ï¬nd
j =
j = 1.) We therefore

j = (cid:80)n

i=1

Ï†j :=

1
n

n
(cid:88)

i=1

w(i)
j

.

The derivation for the M-step updates to Î£j are also entirely straightfor-

ward.

11.5 Variational

inference and variational

auto-encoder (optional reading)

Loosely speaking, variational auto-encoder Kingma and Welling [2013] gen-
erally refers to a family of algorithms that extend the EM algorithms to more
complex models parameterized by neural networks. It extends the technique
of variational inference with the additional â€œre-parametrization trickâ€ which
will be introduced below. Variational auto-encoder may not give the best
performance for many datasets, but it contains several central ideas about
how to extend EM algorithms to high-dimensional continuous latent variables

7We donâ€™t need to worry about the constraint that Ï†j â‰¥ 0, because as weâ€™ll shortly see,

the solution weâ€™ll ï¬nd from this derivation will automatically satisfy that anyway.

161

with non-linear models. Understanding it will likely give you the language
and backgrounds to understand various recent papers related to it.

As a running example, we will consider the following parameterization of
p(x, z; Î¸) by a neural network. Let Î¸ be the collection of the weights of a
neural network g(z; Î¸) that maps z âˆˆ Rk to Rd. Let

z âˆ¼ N (0, IkÃ—k)

x|z âˆ¼ N (g(z; Î¸), Ïƒ2IdÃ—d)

(11.18)
(11.19)

Here IkÃ—k denotes identity matrix of dimension k by k, and Ïƒ is a scalar that
we assume to be known for simplicity.

For the Gaussian mixture models in Section 11.4, the optimal choice of
Q(z) = p(z|x; Î¸) for each ï¬xed Î¸, that is the posterior distribution of z,
can be analytically computed. In many more complex models such as the
model (11.19), itâ€™s intractable to compute the exact the posterior distribution
p(z|x; Î¸).

Recall that from equation (11.10), ELBO is always a lower bound for any
choice of Q, and therefore, we can also aim for ï¬nding an approximation of
the true posterior distribution. Often, one has to use some particular form
to approximate the true posterior distribution. Let Q be a family of Qâ€™s that
we are considering, and we will aim to ï¬nd a Q within the family of Q that is
closest to the true posterior distribution. To formalize, recall the deï¬nition of
the ELBO lower bound as a function of Q and Î¸ deï¬ned in equation (11.14)

ELBO(Q, Î¸) =

n
(cid:88)

i=1

ELBO(x(i); Qi, Î¸) =

(cid:88)

(cid:88)

i

z(i)

Qi(z(i)) log

p(x(i), z(i); Î¸)
Qi(z(i))

Recall

that EM can be viewed as alternating maximization of

ELBO(Q, Î¸). Here instead, we optimize the EBLO over Q âˆˆ Q

max
QâˆˆQ

max
Î¸

ELBO(Q, Î¸)

(11.20)

Now the next question is what form of Q (or what structural assumptions
to make about Q) allows us to eï¬ƒciently maximize the objective above. When
the la
, and to declare convergence if EM is improving
(cid:96)(θ) too slowly.

Remark. If we deﬁne (by overloading ELBO(·))

ELBO(Q, θ) =

n
(cid:88)

i=1

ELBO(x(i); Qi, θ) =

(cid:88)

(cid:88)

i

z(i)

Qi(z(i)) log

p(x(i), z(i); θ)
Qi(z(i))

(11.14)

then we know (cid:96)(θ) ≥ ELBO(Q, θ) from our previous derivation. The EM
can also be viewed an alternating maximization algorithm on ELBO(Q, θ),
in which the E-step maximizes it with respect to Q (check this yourself), and
the M-step maximizes it with respect to θ.

158

11.3.1 Other interpretation of ELBO
Let ELBO(x; Q, θ) = (cid:80)
There are several other forms of ELBO. First, we can rewrite

z Q(z) log p(x,z;θ)

Q(z) be deﬁned as in equation (11.9).

ELBO(x; Q, θ) = Ez∼Q[log p(x, z; θ)] − Ez∼Q[log Q(z)]

= Ez∼Q[log p(x|z; θ)] − DKL(Q(cid:107)pz)

(11.15)

where we use pz to denote the marginal distribution of z (under the distri-
bution p(x, z; θ)), and DKL() denotes the KL divergence

DKL(Q(cid:107)pz) =

(cid:88)

z

Q(z) log

Q(z)
p(z)

(11.16)

In many cases, the marginal distribution of z does not depend on the param-
eter θ. In this case, we can see that maximizing ELBO over θ is equivalent
to maximizing the ﬁrst term in (11.15). This corresponds to maximizing the
conditional likelihood of x conditioned on z, which is often a simpler question
than the original question.

Another form of ELBO(·) is (please verify yourself)

ELBO(x; Q, θ) = log p(x) − DKL(Q(cid:107)pz|x)

(11.17)

where pz|x is the conditional distribution of z given x under the parameter
θ. This forms shows that the maximizer of ELBO(Q, θ) over Q is obtained
when Q = pz|x, which was shown in equation (11.8) before.

11.4 Mixture of Gaussians revisited

Armed with our general deﬁnition of the EM algorithm, let’s go back to our
old example of ﬁtting the parameters φ, µ and Σ in a mixture of Gaussians.
For the sake of brevity, we carry out the derivations for the M-step updates
only for φ and µj, and leave the updates for Σj as an exercise for the reader.
The E-step is easy. Following our algorithm derivation above, we simply

calculate

w(i)

j = Qi(z(i) = j) = P (z(i) = j|x(i); φ, µ, Σ).
Here, “Qi(z(i) = j)” denotes the probability of z(i) taking the value j under
the distribution Qi.

159

Next, in the M-step, we need to maximize, with respect to our parameters

φ, µ, Σ, the quantity

n
(cid:88)

(cid:88)

i=1

z(i)

Qi(z(i)) log

p(x(i), z(i); φ, µ, Σ)
Qi(z(i))

=

=

n
(cid:88)

k
(cid:88)

i=1

j=1

n
(cid:88)

k
(cid:88)

i=1

j=1

Qi(z(i) = j) log

p(x(i)|z(i) = j; µ, Σ)p(z(i) = j; φ)
Qi(z(i) = j)
2(x(i) − µj)T Σ−1

(2π)d/2|Σj |1/2 exp (cid:0)− 1

1

j (x(i) − µj)(cid:1) · φj

w(i)
j

log

w(i)
j

Let’s maximize this with respect to µl. If we take the derivative with respect
to µl, we ﬁnd

∇µl

n
(cid:88)

k
(cid:88)

i=1

j=1

w(i)
j

log

(2π)d/2|Σj |1/2 exp (cid:0)− 1

1

2(x(i) − µj)T Σ−1

j (x(i) − µj)(cid:1) · φj

w(i)
j

= −∇µl

n
(cid:88)

k
(cid:88)

i=1

j=1

w(i)
j

1
2

(x(i) − µj)T Σ−1

j (x(i) − µj)

=

=

1
2

n
(cid:88)

i=1

w(i)

l ∇µl2µT

l Σ−1

l x(i) − µT

l Σ−1

l µl

n
(cid:88)

i=1

w(i)
l

(cid:0)Σ−1

l x(i) − Σ−1

l µl

(cid:1)

Setting this to zero and solving for µl therefore yields the update rule

µl :=

(cid:80)n

i=1 w(i)
l x(i)
(cid:80)n
i=1 w(i)

l

,

which was what we had in the previous set of notes.

Let’s do one more example, and derive the M-step update for the param-
eters φj. Grouping together only the terms that depend on φj, we ﬁnd that
we need to maximize

n
(cid:88)

k
(cid:88)

w(i)
j

log φj.

However, there is an additional constraint that the φj’s sum to 1, since they
represent the probabilities φj = p(z(i) = j; φ). To deal with the constraint

i=1

j=1

160

that (cid:80)k

j=1 φj = 1, we construct the Lagrangian

L(φ) =

n
(cid:88)

k
(cid:88)

i=1

j=1

w(i)
j

k
(cid:88)

log φj + β(

φj − 1),

j=1

where β is the Lagrange multiplier.7 Taking derivatives, we ﬁnd

∂
∂φj

L(φ) =

n
(cid:88)

i=1

w(i)
j
φj

+ β

Setting this to zero and solving, we get

φj =

(cid:80)n

i=1 w(i)
−β

j

i=1 w(i)
j
(cid:80)k

I.e., φj ∝ (cid:80)n
that −β = (cid:80)n
Qi(z(i) = j), and since probabilities sum to 1, (cid:80)
have our M-step updates for the parameters φj:

. Using the constraint that (cid:80)
j=1 w(i)

i=1 1 = n. (This used the fact that w(i)
j w(i)

j φj = 1, we easily ﬁnd
j =
j = 1.) We therefore

j = (cid:80)n

i=1

φj :=

1
n

n
(cid:88)

i=1

w(i)
j

.

The derivation for the M-step updates to Σj are also entirely straightfor-

ward.

11.5 Variational

inference and variational

auto-encoder (optional reading)

Loosely speaking, variational auto-encoder Kingma and Welling [2013] gen-
erally refers to a family of algorithms that extend the EM algorithms to more
complex models parameterized by neural networks. It extends the technique
of variational inference with the additional “re-parametrization trick” which
will be introduced below. Variational auto-encoder may not give the best
performance for many datasets, but it contains several central ideas about
how to extend EM algorithms to high-dimensional continuous latent variables

7We don’t need to worry about the constraint that φj ≥ 0, because as we’ll shortly see,

the solution we’ll ﬁnd from this derivation will automatically satisfy that anyway.

161

with non-linear models. Understanding it will likely give you the language
and backgrounds to understand various recent papers related to it.

As a running example, we will consider the following parameterization of
p(x, z; θ) by a neural network. Let θ be the collection of the weights of a
neural network g(z; θ) that maps z ∈ Rk to Rd. Let

z ∼ N (0, Ik×k)

x|z ∼ N (g(z; θ), σ2Id×d)

(11.18)
(11.19)

Here Ik×k denotes identity matrix of dimension k by k, and σ is a scalar that
we assume to be known for simplicity.

For the Gaussian mixture models in Section 11.4, the optimal choice of
Q(z) = p(z|x; θ) for each ﬁxed θ, that is the posterior distribution of z,
can be analytically computed. In many more complex models such as the
model (11.19), it’s intractable to compute the exact the posterior distribution
p(z|x; θ).

Recall that from equation (11.10), ELBO is always a lower bound for any
choice of Q, and therefore, we can also aim for ﬁnding an approximation of
the true posterior distribution. Often, one has to use some particular form
to approximate the true posterior distribution. Let Q be a family of Q’s that
we are considering, and we will aim to ﬁnd a Q within the family of Q that is
closest to the true posterior distribution. To formalize, recall the deﬁnition of
the ELBO lower bound as a function of Q and θ deﬁned in equation (11.14)

ELBO(Q, θ) =

n
(cid:88)

i=1

ELBO(x(i); Qi, θ) =

(cid:88)

(cid:88)

i

z(i)

Qi(z(i)) log

p(x(i), z(i); θ)
Qi(z(i))

Recall

that EM can be viewed as alternating maximization of

ELBO(Q, θ). Here instead, we optimize the EBLO over Q ∈ Q

max
Q∈Q

max
θ

ELBO(Q, θ)

(11.20)

Now the next question is what form of Q (or what structural assumptions
to make about Q) allows us to eﬃciently maximize the objective above. When
the latent variable z are high-dimensional discrete variables, one popular as-
sumption is the mean ﬁeld assumption, which assumes that Qi(z) gives a
distribution with independent coordinates, or in other words, Qi can be de-
composed into Qi(z) = Q1
i (zk). There are tremendous applications
of mean ﬁeld assumptions to learning generative models with discrete latent
variables, and we refer to Blei et al. [2017] for a survey of these models and

i (z1) · · · Qk

162

their impact to a wide range of applications including computational biology,
computational neuroscience, social sciences. We will not get into the details
about the discrete latent variable cases, and our main focus is to deal with
continuous latent variables, which requires not only mean ﬁeld assumptions,
but additional techniques.

When z ∈ Rk is a continuous latent variable, there are several decisions to
make towards successfully optimizing (11.20). First we need to give a succinct
representation of the distribution Qi because it is over an inﬁnite number of
points. A natural choice is to assume Qi is a Gaussian distribution with some
mean and variance. We would also like to have more succinct representation
of the means of Qi of all the examples. Note that Qi(z(i)) is supposed to
approximate p(z(i)|x(i); θ). It would make sense let all the means of the Qi’s
be some function of x(i). Concretely, let q(·; φ), v(·; φ) be two functions that
map from dimension d to k, which are parameterized by φ and ψ, we assume
that

Qi = N (q(x(i); φ), diag(v(x(i); ψ))2)

(11.21)

Here diag(w) means the k × k matrix with the entries of w ∈ Rk on the
diagonal. In other words, the distribution Qi is assumed to be a Gaussian
distribution with independent coordinates, and the mean and standard de-
viations are governed by q and v. Often in variational auto-encoder, q and v
are chosen to be neural networks.8 In recent deep learning literature, often
q, v are called encoder (in the sense of encoding the data into latent code),
whereas g(z; θ) if often referred to as the decoder.

We remark that Qi of such form in many cases are very far from a good ap-
proximation of the true posterior distribution. However, some approximation
is necessary for feasible optimization. In fact, the form of Qi needs to satisfy
other requirements (which happened to be satisﬁed by the form (11.21))

Before optimizing the ELBO, let’s ﬁrst verify whether we can eﬃciently
evaluate the value of the ELBO for ﬁxed Q of the form (11.21) and θ. We
rewrite the ELBO as a function of φ, ψ, θ by

ELBO(φ, ψ, θ) =

n
(cid:88)

i=1

(cid:20)

Ez(i)∼Qi

log

p(x(i), z(i); θ)
Qi(z(i))

(cid:21)

,

(11.22)

where Qi = N (q(x(i); φ), diag(v(x(i); ψ))2)

Note that to evaluate Qi(z(i)) inside the expectation, we should be able to
compute the density of Qi. To estimate the expectation Ez(i)∼Qi, we

8q and v can also share parameters. We sweep this level of details under the rug in this

note.

163

should be able to sample from distribution Qi so that we can build an
empirical estimator with samples. It happens that for Gaussian distribution
Qi = N (q(x(i); φ), diag(v(x(i); ψ))2), we are able to be both eﬃciently.

Now let’s optimize the ELBO. It turns out that we can run gradient ascent
over φ, ψ, θ instead of alternating maximization. There is no strong need to
compute the maximum over each variable at a much greater cost. (For Gaus-
sian mixture model in Section 11.4, computing the maximum is analytically
feasible and relatively cheap, and therefore we did alternating maximization.)
Mathematically, let η be the learning rate, the gradient ascent step is

θ := θ + η∇θELBO(φ, ψ, θ)
φ := φ + η∇φELBO(φ, ψ, θ)
ψ := ψ + η∇ψELBO(φ, ψ, θ)

Computing the gradient over θ is simple because

∇θELBO(φ, ψ, θ) = ∇θ

= ∇θ

(cid:20)

Ez(i)∼Qi

log

(cid:21)

p(x(i), z(i); θ)
Qi(z(i))

Ez(i)∼Qi

(cid:2)log p(x(i), z(i); θ)(cid:3)

n
(cid:88)

i=1
n
(cid:88)

i=1

=

n
(cid:88)

i=1

Ez(i)∼Qi

(cid:2)∇θ log p(x(i), z(i); θ)(cid:3) ,

(11.23)

But computing the gradient over φ and ψ is tricky because the sam-
pling distribution Qi depends on φ and ψ.
(Abstractly speaking, the is-
sue we face can be simpliﬁed as the problem of computing the gradi-
ent Ez∼Qφ[f (φ)] with respect to variable φ. We know that in general,
∇Ez∼Qφ[f (φ)] (cid:54)= Ez∼Qφ[∇f (φ)] because the dependency of Qφ on φ has to be
taken into account as well. )

The idea that comes to rescue is the so-called re-parameterization
trick: we rewrite z(i) ∼ Qi = N (q(x(i); φ), diag(v(x(i); ψ))2) in an equivalent
way:

z(i) = q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i) where ξ(i) ∼ N (0, Ik×k)

(11.24)

Here x (cid:12) y denotes the entry-wise product of two vectors of the same
dimension. Here we used the fact that x ∼ N (µ, σ2) is equivalent to that
x = µ+ξσ with ξ ∼ N (0, 1). We mostly just used this fact in every dimension
simultaneously for the random variable z(i) ∼ Qi.

164

(11.25)

With this re-parameterization, we have that

(cid:20)

Ez(i)∼Qi

log

(cid:21)

p(x(i), z(i); θ)
Qi(z(i))

(cid:20)

= Eξ(i)∼N (0,1)

log

p(x(i), q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i); θ)
Qi(q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i))

(cid:21)

It follows that

(cid:20)

∇φEz(i)∼Qi

log

(cid:21)

p(x(i), z(i); θ)
Qi(z(i))

= ∇φEξ(i)∼N (0,1)

log

(cid:20)

(cid:20)

= Eξ(i)∼N (0,1)

∇φ log

p(x(i), q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i); θ)
Qi(q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i))
p(x(i), q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i); θ)
Qi(q(x(i); φ) + v(x(i); ψ) (cid:12) ξ(i))

(cid:21)

(cid:21)

We can now sample multiple copies of ξ(i)’s to estimate the the expecta-
tion in the RHS of the equation above.9 We can estimate the gradient with
respect to ψ similarly, and with these, we can implement the gradient ascent
algorithm to optimize the ELBO over φ, ψ, θ.

There are not many high-dimensional distributions with analytically com-
putable density function are known to be re-parameterizable. We refer to
Kingma and Welling [2013] for a few other choices that can replace Gaussian
distribution.

9Empirically people sometimes just use one sample to estimate it for maximum com-

putational eﬃciency.

Chapter 12

Principal components analysis

In this set of notes, we will develop a method, Principal Components Analysis
(PCA), that tries to identify the subspace in which the data approximately
lies. PCA is computationally eﬃcient:
it will require only an eigenvector
calculation (easily done with the eig function in Matlab).

Suppose we are given a dataset {x(i); i = 1, . . . , n} of attributes of n dif-
ferent types of automobiles, such as their maximum speed, turn radius, and
so on. Let x(i) ∈ Rd for each i (d (cid:28) n). But unknown to us, two diﬀerent
attributes—some xi and xj—respectively give a car’s maximum speed mea-
sured in miles per hour, and the maximum speed measured in kilometers per
hour. These two attributes are therefore almost linearly dependent, up to
only small diﬀerences introduced by rounding oﬀ to the nearest mph or kph.
Thus, the data really lies approximately on an n − 1 dimensional subspace.
How can we automatically detect, and perhaps remove, this redundancy?

For a less contrived example, consider a dataset resulting from a survey of
pilots for radio-controlled helicopters, where x(i)
is a measure of the piloting
1
skill of pilot i, and x(i)
captures how much he/she enjoys ﬂying. Because
2
RC helicopters are very diﬃcult to ﬂy, only the most committed students,
ones that truly enjoy ﬂying, become good pilots. So, the two attributes
Indeed, we might posit that that the
x1 and x2 are strongly correlated.
data actually likes along some diagonal axis (the u1 direction) capturing the
intrinsic piloting “karma” of a person, with only a small amount of noise
lying oﬀ this axis. (See ﬁgure.) How can we automatically compute this u1
direction?

165

166

We will shortly develop the PCA algorithm. But prior to running PCA
per se, typically we ﬁrst preprocess the data by normalizing each feature
to have mean 0 and variance 1. We do this by subtracting the mean and
dividing by the empirical standard deviation:

x(i)
j ←

x(i)
j − µj
σj

i=1 x(i)
where µj = 1
n
feature j, respectively.

(cid:80)n

j and σ2

j = 1
n

(cid:80)n

i=1(x(i)

j − µj)2 are the mean variance of

Subtracting µj zeros out the mean and may be omitted for data known
to have zero mean (for instance, time series corresponding to speech or other
acoustic signals). Dividing by the standard deviat
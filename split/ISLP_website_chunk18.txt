ations for centering and scaling. The
transform() method applies the fitted transformation to the array of data,
and produces the model matrix.
In [12]: design = MS(['lstat '])
design = design.fit(Boston)
X = design.transform(Boston)
X[:4]
Out[12]:

0
1
2
3

intercept
1.0
1.0
1.0
1.0

lstat
4.98
9.14
4.03
2.94

In this simple case, the fit() method does very little; it simply checks that
the variable 'lstat' specified in design exists in Boston. Then transform()
constructs the model matrix with two columns: an intercept and the variable lstat.
These two operations can be combined with the fit_transform() method. .fit_
In [13]: design = MS(['lstat '])
X = design.fit_transform(Boston)
X[:4]
Out[13]:

0
1
2
3

intercept
1.0
1.0
1.0
1.0

lstat
4.98
9.14
4.03
2.94

Note that, as in the previous code chunk when the two steps were done
separately, the design object is changed as a result of the fit() operation.
The power of this pipeline will become clearer when we fit more complex
models that involve interactions and transformations.
Let’s return to our fitted regression model. The object results has several
methods that can be used for inference. We already presented a function
summarize() for showing the essentials of the fit. For a full and somewhat
exhaustive summary of the fit, we can use the summary() method (output
not shown).
In [14]: results.summary ()

The fitted coefficients can also be retrieved as the params attribute of

results.

In [15]: results.params

transform()

120

3. Linear Regression

Out[15]: intercept
34.553841
lstat
-0.950049
dtype: float64

The get_prediction() method can be used to obtain predictions, and .get_
produce confidence intervals and prediction intervals for the prediction of prediction()
medv for given values of lstat.
We first create a new data frame, in this case containing only the variable lstat, with the values for this variable at which we wish to make
predictions. We then use the transform() method of design to create the
corresponding model matrix.
In [16]: new_df = pd.DataFrame ({'lstat ':[5, 10, 15]})
newX = design.transform(new_df)
newX

Out[16]:

0
1
2

intercept
1.0
1.0
1.0

lstat
5
10
15

Next we compute the predictions at newX, and view them by extracting
the predicted_mean attribute.
In [17]: new_predictions = results.get_prediction(newX);
new_predictions.predicted_mean

Out[17]: array ([29.80359411 , 25.05334734 , 20.30310057])

We can produce confidence intervals for the predicted values.
In [18]: new_predictions.conf_int(alpha =0.05)

Out[18]: array ([[29.00741194 , 30.59977628] ,
[24.47413202 , 25.63256267] ,
[19.73158815 , 20.87461299]])

Prediction intervals are computing by setting obs=True:
In [19]: new_predictions.conf_int(obs=True , alpha =0.05)

Out[19]: array ([[17.56567478 , 42.04151344] ,
[12.82762635 , 37.27906833] ,
[ 8.0777421 , 32.52845905]])

For instance, the 95% confidence interval associated with an lstat value of
10 is (24.47, 25.63), and the 95% prediction interval is (12.82, 37.28). As
expected, the confidence and prediction intervals are centered around the
same point (a predicted value of 25.05 for medv when lstat equals 10), but
the latter are substantially wider.
Next we will plot medv and lstat using DataFrame.plot.scatter(), and .plot.
wish to add the regression line to the resulting plot.
scatter()

3.6 Lab: Linear Regression

121

Defining Functions
While there is a function within the ISLP package that adds a line to an
existing plot, we take this opportunity to define our first function to do so. def
In [20]: def abline(ax , b, m):
"Add a line with slope m and intercept b to ax"
xlim = ax.get_xlim ()
ylim = [m * xlim [0] + b, m * xlim [1] + b]
ax.plot(xlim , ylim)

A few things are illustrated above. First we see the syntax for defining a
function: def funcname(...). The function has arguments ax, b, m where
ax is an axis object for an exisiting plot, b is the intercept and m is the slope
of the desired line. Other plotting options can be passed on to ax.plot by
including additional optional arguments as follows:
In [21]: def abline(ax , b, m, *args , ** kwargs):
"Add a line with slope m and intercept b to ax"
xlim = ax.get_xlim ()
ylim = [m * xlim [0] + b, m * xlim [1] + b]
ax.plot(xlim , ylim , *args , ** kwargs)

The addition of *args allows any number of non-named arguments to
abline, while *kwargs allows any number of named arguments (such as
linewidth=3) to abline. In our function, we pass these arguments verbatim
to ax.plot above. Readers interested in learning more about functions are
referred to the section on defining functions in docs.python.org/tutorial.
Let’s use our new function to add this regression line to a plot of medv
vs. lstat.
In [22]: ax = Boston.plot.scatter('lstat ', 'medv ')
abline(ax ,
results.params [0],
results.params [1],
'r--',
linewidth =3)

Thus, the final call to ax.plot() is ax.plot(xlim, ylim, 'r--', linewidth=3).
We have used the argument 'r--' to produce a red dashed line, and added
an argument to make it of width 3. There is some evidence for non-linearity
in the relationship between lstat and medv. We will explore this issue later
in this lab.
As mentioned above, there is an existing function to add a line to a plot
— ax.axline() — but knowing how to write such functions empowers us
to create more expressive displays.
Next we examine some diagnostic plots, several of which were discussed
in Section 3.3.3. We can find the fitted values and residuals of the fit as
attributes of the results object. Various influence measures describing the
regression model are computed with the get_influence() method. As we .get_
will not use the fig component returned as the first value from subplots(), influence()
we simply capture the second returned value in ax below.
In [23]: ax = subplots(figsize =(8 ,8))[1]

122

3. Linear Regression

ax.scatter(results.fittedvalues , results.resid)
ax.set_xlabel('Fitted value ')
ax.set_ylabel('Residual ')
ax.axhline (0, c='k', ls='--');

We add a horizontal line at 0 for reference using the ax.axhline() method,
.axhline()
indicating it should be black (c='k') and have a dashed linestyle (ls='--').
On the basis of the residual plot (not shown), there is some evidence
of non-linearity. Leverage statistics can be computed for any number of
predictors using the hat_matrix_diag attribute of the value returned by the
get_influence() method.
In [24]: infl = results.get_influence ()
ax = subplots(figsize =(8 ,8))[1]
ax.scatter(np.arange(X.shape [0]) , infl.hat_matrix_diag)
ax.set_xlabel('Index ')
ax.set_ylabel('Leverage ')
np.argmax(infl.hat_matrix_diag)
Out[24]: 374

The np.argmax() function identifies the index of the largest element of an
np.argmax()
array, optionally computed over an axis of the array. In this case, we maximized over the entire array to determine which observation has the largest
leverage statistic.

3.6.3

Multiple Linear Regression

In order to fit a multiple linear regression model using least squares, we
again use the ModelSpec() transform to construct the required model matrix
and response. The arguments to ModelSpec() can be quite general, but in
this case a list of column names suffice. We consider a fit here with the two
variables lstat and age.
In [25]: X = MS(['lstat ', 'age']).fit_transform(Boston)
model1 = sm.OLS(y, X)
results1 = model1.fit()
summarize(results1)
Out[25]:

intercept
lstat
age

coef
33.2228
-1.0321
0.0345

std err
0.731
0.048
0.012

t
45.458
-21.416
2.826

P>|t|
0.000
0.000
0.005

Notice how we have compacted the first line into a succinct expression
describing the construction of X.
The Boston data set contains 12 variables, and so it would be cumbersome
to have to type all of these in order to perform a regression using all of the
predictors. Instead, we can use the following short-hand:
.columns.
In [26]: terms = Boston.columns.drop('medv ')
terms

drop()

3.6 Lab: Linear Regression

123

Out[26]: Index (['crim ', 'zn', 'indus ', 'chas ', 'nox', 'rm', 'age', 'dis',
'rad', 'tax', 'ptratio ', 'lstat '],
dtype='object ')

We can now fit the model with all the variables in terms using the same
model matrix builder.
In [27]: X = MS(terms).fit_transform(Boston)
model = sm.OLS(y, X)
results = model.fit()
summarize(results)
Out[27]:

intercept
crim
zn
indus
chas
nox
rm
age
dis
rad
tax
ptratio
lstat

coef
41.6173
-0.1214
0.0470
0.0135
2.8400
-18.7580
3.6581
0.0036
-1.4908
0.2894
-0.0127
-0.9375
-0.5520

std err
4.936
0.033
0.014
0.062
0.870
3.851
0.420
0.013
0.202
0.067
0.004
0.132
0.051

t
P>|t|
8.431
0.000
-3.678 0.000
3.384
0.001
0.217
0.829
3.264
0.001
-4.870 0.000
8.705
0.000
0.271
0.787
-7.394 0.000
4.325
0.000
-3.337 0.001
-7.091 0.000
-10.897 0.000

What if we would like to perform a regression using all of the variables but
one? For example, in the above regression output, age has a high p-value.
So we may wish to run a regression excluding this predictor. The following
syntax results in a regression using all predictors except age (output not
shown).
In [28]: minus_age = Boston.columns.drop (['medv ', 'age'])
Xma = MS(minus_age).fit_transform(Boston)
model1 = sm.OLS(y, Xma)
summarize(model1.fit())

3.6.4

Multivariate Goodness of Fit

We can access the individual components of results by name (dir(results)
shows us what is available). Hence results.rsquared gives us the R2 , and
np.sqrt(results.scale) gives us the RSE.
Variance inflation factors (section 3.3.3) are sometimes useful to assess
the effect of collinearity in the model matrix of a regression model. We will
compute the VIFs in our multiple regression fit, and use the opportunity
to introduce the idea of list comprehension.
List Comprehension
Often we encounter a sequence of objects which we would like to transform
for some other task. Below, we compute the VIF for each feature in our X
matrix and produce a data frame whose index agrees with the columns of
X. The notion of list comprehension can often make such a task easier.

list comprehension

124

3. Linear Regression

List comprehensions are simple and powerful ways to form lists of Python
objects. The language also supports dictionary and generator comprehension, though these are beyond our scope here. Let’s look at an example.
We compute the VIF for each of the variables in the model matrix X, using
the function variance_inflation_factor().
variance_

inflation_
factor()

In [29]: vals = [VIF(X, i)
for i in range (1, X.shape [1])]
vif = pd.DataFrame ({'vif':vals},
index=X.columns [1:])
vif
Out[29]:

crim
zn
indus
chas
nox
rm
age
dis
rad
tax
ptratio
lstat

vif
1.767
2.298
3.987
1.071
4.369
1.913
3.088
3.954
7.445
9.002
1.797
2.871

The function VIF() takes two arguments: a dataframe or array, and a variable column index. In the code above we call VIF() on the fly for all columns
in X. We have excluded column 0 above (the intercept), which is not of interest. In this case the VIFs are not that exciting.
The object vals above could have been constructed with the following
for loop:
In [30]: vals = []
for i in range (1, X.values.shape [1]):
vals.append(VIF(X.values , i))

List comprehension allows us to perform such repetitive operations in a
more straightforward way.

3.6.5

Interaction Terms

It is easy to include interaction terms in a linear model using ModelSpec().
Including a tuple ("lstat","age") tells the model matrix builder to include
an interaction term between lstat and age.
In [31]: X = MS(['lstat ',
'age',
('lstat ', 'age')]).fit_transform(Boston)
model2 = sm.OLS(y, X)
summarize(model2.fit())
Out[31]:

intercept
lstat

coef
36.0885
-1.3921

std err
1.470
0.167

t
24.553
-8.313

P>|t|
0.000
0.000

3.6 Lab: Linear Regression
age
lstat:age

3.6.6

-0.0007
0.0042

0.020
0.002

-0.036
2.244

125

0.971
0.025

Non-linear Transformations of the Predictors

The model matrix builder can include terms beyond just column names and
interactions. For instance, the poly() function supplied in ISLP specifies
poly()
that columns representing polynomial functions of its first argument are
added to the model matrix.
In [32]: X = MS([ poly('lstat ', degree =2), 'age']).fit_transform(Boston)
model3 = sm.OLS(y, X)
results3 = model3.fit()
summarize(results3)
Out[32]:

intercept
poly(lstat , degree =2) [0]
poly(lstat , degree =2) [1]
age

coef
17.7151
-179.2279
72.9908
0.0703

std err
0.781
6.733
5.482
0.011

t
22.681
-26.620
13.315
6.471

P>|t|
0.000
0.000
0.000
0.000

The effectively zero p-value associated with the quadratic term (i.e. the
third row above) suggests that it leads to an improved model.
By default, poly() creates a basis matrix for inclusion in the model matrix whose columns are orthogonal polynomials, which are designed for staorthogonal
ble least squares computations.13 Alternatively, had we included an argu- polynomial
ment raw=True in the above call to poly(), the basis matrix would consist
simply of lstat and lstat**2. Since either of these bases represent quadratic
polynomials, the fitted values would not change in this case, just the polynomial coefficients. Also by default, the columns created by poly() do not
include an intercept column as that is automatically added by MS().
We use the anova_lm() function to further quantify the extent to which
anova_lm()
the quadratic fit is superior to the linear fit.
In [33]: anova_lm(results1 , results3)
Out[33]:

0
1

df_resid
503.0
502.0

ssr
19168.13
14165.61

df_diff
0.0
1.0

ss_diff
NaN
5002.52

F
NaN
177.28

Pr(>F)
NaN
7.47e-35

Here results1 represents the linear submodel containing predictors lstat
and age, while results3 corresponds to the larger model above with a
quadratic term in lstat. The anova_lm() function performs a hypothesis
test comparing the two models. The null hypothesis is that the quadratic
term in the bigger model is not needed, and the alternative hypothesis is
that the bigger model is superior. Here the F-statistic is 177.28 and the
associated p-value is zero. In this case the F-statistic is the square of the
t-statistic for the quadratic term in the linear model summary for results3
— a consequence of the fact that these nested models differ by one degree of
13 Actually, poly() is a wrapper for the workhorse and standalone function Poly()
that does the work in building the model matrix.

126

3. Linear Regression

freedom. This provides very clear evidence that the quadratic polynomial
in lstat improves the linear model. This is not surprising, since earlier we
saw evidence for non-linearity in the relationship between medv and lstat.
The function anova_lm() can take more than two nested models as input,
in which case it compares every successive pair of models. That also explains
why their are NaNs in the first row above, since there is no previous model
with which to compare the first.
In [34]: ax = subplots(figsize =(8 ,8))[1]
ax.scatter(results3.fittedvalues , results3.resid)
ax.set_xlabel('Fitted value ')
ax.set_ylabel('Residual ')
ax.axhline (0, c='k', ls='--')

We see that when the quadratic term is included in the model, there is
little discernible pattern in the residuals. In order to create a cubic or
higher-degree polynomial fit, we can simply change the degree argument to
poly().

3.6.7

Qualitative Predictors

Here we use the Carseats data, which is included in the ISLP package. We
will attempt to predict Sales (child car seat sales) in 400 locations based
on a number of predictors.
In [35]: Carseats = load_data('Carseats ')
Carseats.columns
Out[35]: Index (['Sales ', 'CompPrice ', 'Income ', 'Advertising 
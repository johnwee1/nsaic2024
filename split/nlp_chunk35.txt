in language
processing with RNNs.

9.4.1 Stacked RNNs

In our examples thus far, the inputs to our RNNs have consisted of sequences of
word or character embeddings (vectors) and the outputs have been vectors useful for
predicting words, tags or sequence labels. However, nothing prevents us from using
the entire sequence of outputs from one RNN as an input sequence to another one.
Stacked RNNs consist of multiple networks where the output of one layer serves as
the input to a subsequent layer, as shown in Fig. 9.10.

Stacked RNNs

Figure 9.10 Stacked recurrent networks. The output of a lower level serves as the input to
higher levels with the output of the last network serving as the ﬁnal output.

Solong<s>andSolongand?Sampled WordSoftmaxEmbeddingInput WordRNNy1y2y3ynx1x2x3xnRNN 1RNN 2 RNN 3198 CHAPTER 9

• RNNS AND LSTMS

Stacked RNNs generally outperform single-layer networks. One reason for this
success seems to be that the network induces representations at differing levels of
abstraction across layers. Just as the early stages of the human visual system detect
edges that are then used for ﬁnding larger regions and shapes, the initial layers of
stacked networks can induce representations that serve as useful abstractions for
further layers—representations that might prove difﬁcult to induce in a single RNN.
The optimal number of stacked RNNs is speciﬁc to each application and to each
training set. However, as the number of stacks is increased the training costs rise
quickly.

9.4.2 Bidirectional RNNs

The RNN uses information from the left (prior) context to make its predictions at
time t. But in many applications we have access to the entire input sequence; in
those cases we would like to use words from the context to the right of t. One way
to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and
concatenate their representations.

In the left-to-right RNNs we’ve discussed so far, the hidden state at a given time
t represents everything the network knows about the sequence up to that point. The
state is a function of the inputs x1, ..., xt and represents the context of the network to
the left of the current time.

h f

t = RNNforward(x1, . . . , xt )

(9.16)

This new notation h f
senting everything the network has gleaned from the sequence so far.

t simply corresponds to the normal hidden state at time t, repre-

To take advantage of context to the right of the current input, we can train an
RNN on a reversed input sequence. With this approach, the hidden state at time t
represents information about the sequence to the right of the current input:

hb

t = RNNbackward(xt , . . . xn)

(9.17)

bidirectional
RNN

Here, the hidden state hb
sequence from t to the end of the sequence.

t represents all the information we have discerned about the

A bidirectional RNN (Schuster and Paliwal, 1997) combines two independent
RNNs, one where the input is processed from the start to the end, and the other from
the end to the start. We then concatenate the two representations computed by the
networks into a single vector that captures both the left and right contexts of an input
at each point in time. Here we use either the semicolon ”;” or the equivalent symbol

to mean vector concatenation:

⊕

ht = [h f
= h f

t ; hb
t ]
hb
t

t ⊕

(9.18)

Fig. 9.11 illustrates such a bidirectional network that concatenates the outputs of
the forward and backward pass. Other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. The output at
each step in time thus captures information to the left and to the right of the current
input. In sequence labeling applications, these concatenated outputs can serve as the
basis for a local labeling decision.

Bidirectional RNNs have also proven to be quite effective for sequence classiﬁ-
cation. Recall from Fig. 9.8 that for sequence classiﬁcation we used the ﬁnal hidden

9.4

• STACKED AND BIDIRECTIONAL RNN ARCHITECTURES

199

Figure 9.11 A bidirectional RNN. Separate models are trained in the forward and backward
directions, with the output of each model at each time point concatenated to represent the
bidirectional state at that time point.

state of the RNN as the input to a subsequent feedforward classiﬁer. A difﬁculty
with this approach is that the ﬁnal state naturally reﬂects more information about
the end of the sentence than its beginning. Bidirectional RNNs provide a simple
solution to this problem; as shown in Fig. 9.12, we simply combine the ﬁnal hidden
states from the forward and backward passes (for example by concatenation) and
use that as input for follow-on processing.

Figure 9.12 A bidirectional RNN for sequence classiﬁcation. The ﬁnal hidden units from
the forward and backward passes are combined to represent the entire sequence. This com-
bined representation serves as input to the subsequent classiﬁer.

RNN 2 RNN 1x1y2y1y3ynconcatenatedoutputsx2x3xnRNN 2 RNN 1x1x2x3xnhn→h1←hn→SoftmaxFFNh1←200 CHAPTER 9

• RNNS AND LSTMS

9.5 The LSTM

In practice, it is quite difﬁcult to train RNNs for tasks that require a network to make
use of information distant from the current point of processing. Despite having ac-
cess to the entire preceding sequence, the information encoded in hidden states tends
to be fairly local, more relevant to the most recent parts of the input sequence and
recent decisions. Yet distant information is critical to many language applications.
Consider the following example in the context of language modeling.

(9.19) The ﬂights the airline was canceling were full.

Assigning a high probability to was following airline is straightforward since airline
provides a strong local context for the singular agreement. However, assigning an
appropriate probability to were is quite difﬁcult, not only because the plural ﬂights
is quite distant, but also because the singular noun airline is closer in the intervening
context. Ideally, a network should be able to retain the distant information about
plural ﬂights until it is needed, while still processing the intermediate parts of the
sequence correctly.

One reason for the inability of RNNs to carry forward critical information is that
the hidden layers, and, by extension, the weights that determine the values in the hid-
den layer, are being asked to perform two tasks simultaneously: provide information
useful for the current decision, and updating and carrying forward information re-
quired for future decisions.

A second difﬁculty with training RNNs arises from the need to backpropagate
the error signal back through time. Recall from Section 9.1.2 that the hidden layer at
time t contributes to the loss at the next time step since it takes part in that calcula-
tion. As a result, during the backward pass of training, the hidden layers are subject
to repeated multiplications, as determined by the length of the sequence. A frequent
result of this process is that the gradients are eventually driven to zero, a situation
called the vanishing gradients problem.

To address these issues, more complex network architectures have been designed
to explicitly manage the task of maintaining relevant context over time, by enabling
the network to learn to forget information that is no longer needed and to remember
information required for decisions still to come.

The most commonly used such extension to RNNs is the long short-term mem-
ory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the con-
text management problem into two subproblems: removing information no longer
needed from the context, and adding information likely to be needed for later de-
cision making. The key to solving both problems is to learn how to manage this
context rather than hard-coding a strategy into the architecture. LSTMs accomplish
this by ﬁrst adding an explicit context layer to the architecture (in addition to the
usual recurrent hidden layer), and through the use of specialized neural units that
make use of gates to control the ﬂow of information into and out of the units that
comprise the network layers. These gates are implemented through the use of addi-
tional weights that operate sequentially on the input, and previous hidden layer, and
previous context layers.

The gates in an LSTM share a common design pattern; each consists of a feed-
forward layer, followed by a sigmoid activation function, followed by a pointwise
multiplication with the layer being gated. The choice of the sigmoid as the activation
function arises from its tendency to push its outputs to either 0 or 1. Combining this
with a pointwise multiplication has an effect similar to that of a binary mask. Values

vanishing
gradients

long short-term
memory

9.5

• THE LSTM 201

forget gate

in the layer being gated that align with values near 1 in the mask are passed through
nearly unchanged; values corresponding to lower values are essentially erased.

The ﬁrst gate we’ll consider is the forget gate. The purpose of this gate is
to delete information from the context that is no longer needed. The forget gate
computes a weighted sum of the previous state’s hidden layer and the current in-
put and passes that through a sigmoid. This mask is then multiplied element-wise
by the context vector to remove the information from context that is no longer re-
quired. Element-wise multiplication of two vectors (represented by the operator
,
(cid:12)
and sometimes called the Hadamard product) is the vector of the same dimension
as the two input vectors, where each element i is the product of element i in the two
input vectors:

ft = σ (U f ht
1 + W f xt )
−
ft
kt = ct

(9.20)

(9.21)

1 (cid:12)
−
The next task is to compute the actual information we need to extract from the previ-
ous hidden state and current inputs—the same basic computation we’ve been using
for all our recurrent networks.

add gate

Next, we generate the mask for the add gate to select the information to add to the
current context.

gt = tanh(Ught

1 + Wgxt )
−

(9.22)

it = σ (Uiht
it
jt = gt (cid:12)

1 + Wixt )
−

(9.23)

(9.24)

Next, we add this to the modiﬁed context vector to get our new context vector.

ct = jt + kt

(9.25)

output gate

The ﬁnal gate we’ll use is the output gate which is used to decide what informa-
tion is required for the current hidden state (as opposed to what information needs
to be preserved for future decisions).

ot = σ (Uoht
ht = ot (cid:12)

1 + Woxt )
−
tanh(ct )

(9.26)

(9.27)

Fig. 9.13 illustrates the complete computation for a single LSTM unit. Given the
appropriate weights for the various gates, an LSTM accepts as input the context
layer, and hidden layer from the previous time step, along with the current input
vector. It then generates updated context and hidden vectors as output.

It is the hidden state, ht , that provides the output for the LSTM at each time step.
This output can be used as the input to subsequent layers in a stacked RNN, or at the
ﬁnal layer of a network ht can be used to provide the ﬁnal output of the LSTM.

9.5.1 Gated Units, Layers and Networks

The neural units used in LSTMs are obviously much more complex than those used
in basic feedforward networks. Fortunately, this complexity is encapsulated within
the basic processing units, allowing us to maintain modularity and to easily exper-
iment with different architectures. To see this, consider Fig. 9.14 which illustrates
the inputs and outputs associated with each kind of unit.

202 CHAPTER 9

• RNNS AND LSTMS

Figure 9.13 A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the
current input, x, the previous hidden state, ht
1. The outputs are a new hidden
state, ht and an updated context, ct .

1, and the previous context, ct

−

−

Figure 9.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and
long short-term memory (LSTM).

At the far left, (a) is the basic feedforward unit where a single set of weights and
a single activation function determine its output, and when arranged in a layer there
are no connections among the units in the layer. Next, (b) represents the unit in a
simple recurrent network. Now there are two inputs and an additional set of weights
to go with it. However, there is still a single activation function and output.

The increased complexity of the LSTM units is encapsulated within the unit
itself. The only additional external complexity for the LSTM over the basic recurrent
unit (b) is the presence of the additional context vector as an input and output.

This modularity is key to the power and widespread applicability of LSTM units.
LSTM units (or other varieties, like GRUs) can be substituted into any of the network
architectures described in Section 9.4. And, as with simple RNNs, multi-layered
networks making use of gated units can be unrolled into deep feedforward networks
and trained in the usual fashion with backpropagation. In practice, therefore, LSTMs
rather than RNNs have become the standard unit for any modern system that makes
use of recurrent networks.

+xtht-1cthtcthtct-1ht-1xttanh+σtanhσσ+++igfo⦿⦿⦿LSTMct-1hxxtxtht-1hthtct-1ctht-1(b)(a)(c)⌃gza⌃gzLSTMUnita9.6

• SUMMARY: COMMON RNN NLP ARCHITECTURES

203

9.6 Summary: Common RNN NLP Architectures

We’ve now introduced the RNN, seen advanced components like stacking multiple
layers and using the LSTM version, and seen how the RNN can be applied to various
tasks. Let’s take a moment to summarize the architectures for these applications.

Fig. 9.15 shows the three architectures we’ve discussed so far: sequence la-
beling, sequence classiﬁcation, and language modeling. In sequence labeling (for
example for part of speech tagging), we train a model to produce a label for each
input word or token. In sequence classiﬁcation, for example for sentiment analysis,
we ignore the output for each token, and only take the value from the end of the
sequence (and similarly the model’s training signal comes from backpropagation
from that last token). In language modeling, we train the model to predict the next
word at each token step. In the next section we’ll introduce a fourth architecture, the
encoder-decoder.

Figure 9.15 Four architectures for NLP tasks. In sequence labeling (POS or named entity tagging) we map
each input token xi to an output token yi. In sequence classiﬁcation we map the entire input sequence to a single
class. In language modeling we output the next token conditioned on previous tokens. In the encoder model we
have two separate RNN models, one of which maps from an input sequence x to an intermediate representation
we call the context, and a second of which maps from the context to an output sequence y.

9.7 The Encoder-Decoder Model with RNNs

In this section we introduce a new model, the encoder-decoder model, which is used
when we are taking an input sequence and translating it to an output sequence that is
of a different length than the input, and doesn’t align with it in a word-to-word way.
Recall that in the sequence labeling task, we have two sequences, but they are the

…Encoder RNNDecoder RNNContext…x1x2xny1y2ym…RNNx1x2xn…y1y2yn…RNNx1x2xny…RNNx1x2xt-1…x2x3xta) sequence labeling b) sequence classification c) language modelingd) encoder-decoder204 CHAPTER 9

• RNNS AND LSTMS

same length (for example in part-of-speech tagging each token gets an associated
tag), each input is associated with a speciﬁc output, and the labeling for that output
takes m
one you will need is Google Cloud Storage (GCS): this is where
you  will  put  the  SavedModels,  the  training  data,  and  more.  In  the  navigation
menu, scroll down to the Storage section, and click Storage → Browser. All your
files will go in one or more buckets. Click Create Bucket and choose the bucket
name (you may need to activate the Storage API first). GCS uses a single world‐
wide namespace for buckets, so simple names like “machine-learning” will most
likely  not  be  available.  Make  sure  the  bucket  name  conforms  to  DNS  naming
conventions, as it may be used in DNS records. Moreover, bucket names are pub‐
lic,  so  do  not  put  anything  private  in  there.  It  is  common  to  use  your  domain
name or your company name as a prefix to ensure uniqueness, or simply use a
random  number  as  part  of  the  name.  Choose  the  location  where  you  want  the
bucket to be hosted, and the rest of the options should be fine by default. Then
click Create.

6. Upload  the  my_mnist_model  folder  you  created  earlier  (including  one  or  more
versions) to your bucket. To do this, just go to the GCS Browser, click the bucket,
then drag and drop the my_mnist_model folder from your system to the bucket
(see  Figure  19-4).  Alternatively,  you  can  click  “Upload  folder”  and  select  the
my_mnist_model folder to upload. By default, the maximum size for a SavedMo‐
del is 250 MB, but it is possible to request a higher quota.

Figure 19-4. Uploading a SavedModel to Google Cloud Storage

Serving a TensorFlow Model 

| 

679

7. Now you need to configure AI Platform (formerly known as ML Engine) so that
it  knows  which  models  and  versions  you  want  to  use.  In  the  navigation  menu,
scroll down to the Artificial Intelligence section, and click AI Platform → Models.
Click Activate API (it takes a few minutes), then click “Create model.” Fill in the
model details (see Figure 19-5) and click Create.

Figure 19-5. Creating a new model on Google Cloud AI Platform

8. Now that you have a model on AI Platform, you need to create a model version.
In the list of models, click the model you just created, then click “Create version”
and fill in the version details (see Figure 19-6): set the name, description, Python
version (3.5 or above), framework (TensorFlow), framework version (2.0 if avail‐
able,  or  1.13),6  ML  runtime  version  (2.0,  if  available  or  1.13),  machine  type
(choose “Single core CPU” for now), model path on GCS (this is the full path to
the  actual  version  folder,  e.g.,  gs://my-mnist-model-bucket/my_mnist_model/
0002/),  scaling  (choose  automatic),  and  minimum  number  of  TF  Serving  con‐
tainers to have running at all times (leave this field empty). Then click Save.

6 At the time of this writing, TensorFlow version 2 is not available yet on AI Platform, but that’s OK: you can

use 1.13, and it will run your TF 2 SavedModels just fine.

680 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

Figure 19-6. Creating a new model version on Google Cloud AI Platform

Congratulations,  you  have  deployed  your  first  model  on  the  cloud!  Because  you
selected  automatic  scaling,  AI  Platform  will  start  more  TF  Serving  containers  when
the  number  of  queries  per  second  increases,  and  it  will  load-balance  the  queries
between them. If the QPS goes down, it will stop containers automatically. The cost is
therefore directly linked to the QPS (as well as the type of machine you choose and
the amount of data you store on GCS). This pricing model is particularly useful for
occasional users and for services with important usage spikes, as well as for startups:
the price remains low until the startup actually starts up.

If  you  do  not  use  the  prediction  service,  AI  Platform  will  stop  all
containers. This means you will only pay for the amount of storage
you use (a few cents per gigabyte per month). Note that when you
query  the  service,  AI  Platform  will  need  to  start  up  a  TF  Serving
container, which will take a few seconds. If this delay is unaccepta‐
ble, you will have to set the minimum number of TF Serving con‐
tainers to 1 when creating the model version. Of course, this means
at least one machine will run constantly, so the monthly fee will be
higher.

Now let’s query this prediction service!

Serving a TensorFlow Model 

| 

681

Using the Prediction Service
Under the hood, AI Platform just runs TF Serving, so in principle you could use the
same code as earlier, if you knew which URL to query. There’s just one problem: GCP
also  takes  care  of  encryption  and  authentication.  Encryption  is  based  on  SSL/TLS,
and authentication is token-based: a secret authentication token must be sent to the
server  in  every  request.  So  before  your  code  can  use  the  prediction  service  (or  any
other  GCP  service),  it  must  obtain  a  token.  We  will  see  how  to  do  this  shortly,  but
first you need to configure authentication and give your application the appropriate
access rights on GCP. You have two options for authentication:

• Your application (i.e., the client code that will query the prediction service) could
authenticate  using  user  credentials  with  your  own  Google  login  and  password.
Using  user  credentials  would  give  your  application  the  exact  same  rights  as  on
GCP,  which  is  certainly  way  more  than  it  needs.  Moreover,  you  would  have  to
deploy  your  credentials  in  your  application,  so  anyone  with  access  could  steal
your credentials and fully access your GCP account. In short, do not choose this
option; it is only needed in very rare cases (e.g., when your application needs to
access its user’s GCP account).

• The  client  code  can  authenticate  with  a  service  account.  This  is  an  account  that
represents  an  application,  not  a  user.  It  is  generally  given  very  restricted  access
rights: strictly what it needs, and no more. This is the recommended option.

So, let’s create a service account for your application: in the navigation menu, go to
IAM & admin → Service accounts, then click Create Service Account, fill in the form
(service account name, ID, description), and click Create (see Figure 19-7). Next, you
must give this account some access rights. Select the ML Engine Developer role: this
will allow the service account to make predictions, and not much more. Optionally,
you can grant some users access to the service account (this is useful when your GCP
user account is part of an organization, and you wish to authorize other users in the
organization  to  deploy  applications  that  will  be  based  on  this  service  account  or  to
manage  the  service  account  itself).  Next,  click  Create  Key  to  export  the  service
account’s private key, choose JSON, and click Create. This will download the private
key in the form of a JSON file. Make sure to keep it private!

682 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

Figure 19-7. Creating a new service account in Google IAM

Great!  Now  let’s  write  a  small  script  that  will  query  the  prediction  service.  Google
provides several libraries to simplify access to its services:

Google API Client Library

This is a fairly thin layer on top of OAuth 2.0 (for the authentication) and REST.
You  can  use  it  with  all  GCP  services,  including  AI  Platform.  You  can  install  it
using pip: the library is called google-api-python-client.

Google Cloud Client Libraries

These are a bit more high-level: each one is dedicated to a particular service, such
as  GCS,  Google  BigQuery,  Google  Cloud  Natural  Language,  and  Google  Cloud
Vision. All these libraries can be installed using pip (e.g., the GCS Client Library
is  called  google-cloud-storage).  When  a  client  library  is  available  for  a  given
service, it is recommended to use it rather than the Google API Client Library, as
it implements all the best practices and will often use gRPC rather than REST, for
better performance.

At the time of this writing there is no client library for AI Platform, so we will use the
Google API Client Library. It will need to use the service account’s private key; you
can tell it where it is by setting the GOOGLE_APPLICATION_CREDENTIALS environment
variable, either before starting the script or within the script like this:

import os

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "my_service_account_key.json"

Serving a TensorFlow Model 

| 

683

If  you  deploy  your  application  to  a  virtual  machine  on  Google
Cloud  Engine  (GCE),  or  within  a  container  using  Google  Cloud
Kubernetes Engine, or as a web application on Google Cloud App
Engine, or as a microservice on Google Cloud Functions, and if the
GOOGLE_APPLICATION_CREDENTIALS  environment  variable  is  not
set, then the library will use the default service account for the host
service  (e.g.,  the  default  GCE  service  account,  if  your  application
runs on GCE).

Next, you must create a resource object that wraps access to the prediction service:7

import googleapiclient.discovery

project_id = "onyx-smoke-242003" # change this to your project ID
model_id = "my_mnist_model"
model_path = "projects/{}/models/{}".format(project_id, model_id)
ml_resource = googleapiclient.discovery.build("ml", "v1").projects()

Note  that  you  can  append  /versions/0001  (or  any  other  version  number)  to  the
model_path to specify the version you want to query: this can be useful for A/B test‐
ing or for testing a new version on a small group of users before releasing it widely
(this  is  called  a  canary).  Next,  let’s  write  a  small  function  that  will  use  the  resource
object to call the prediction service and get the predictions back:

def predict(X):
    input_data_json = {"signature_name": "serving_default",
                       "instances": X.tolist()}
    request = ml_resource.predict(name=model_path, body=input_data_json)
    response = request.execute()
    if "error" in response:
        raise RuntimeError(response["error"])
    return np.array([pred[output_name] for pred in response["predictions"]])

The function takes a NumPy array containing the input images and prepares a dictio‐
nary that the client library will convert to the JSON format (as we did earlier). Then it
prepares  a  prediction  request,  and  executes  it;  it  raises  an  exception  if  the  response
contains  an  error,  or  else  it  extracts  the  predictions  for  each  instance  and  bundles
them in a NumPy array. Let’s see if it works:

>>> Y_probas = predict(X_new)
>>> np.round(Y_probas, 2)
array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],
       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],
       [0.  , 0.96, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.  ]])

7 If you get an error saying that module google.appengine was not found, set cache_discovery=False in the

call to the build() method; see https://stackoverflow.com/q/55561354.

684 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

Yes! You now have a nice prediction service running on the cloud that can automati‐
cally scale up to any number of QPS, plus you can query it from anywhere securely.
Moreover,  it  costs  you  close  to  nothing  when  you  don’t  use  it:  you’ll  pay  just  a  few
cents per month per gigabyte used on GCS. And you can also get detailed logs and
metrics using Google Stackdriver.

But  what  if  you  want  to  deploy  your  model  to  a  mobile  app?  Or  to  an  embedded
device?

Deploying a Model to a Mobile or Embedded Device
If you need to deploy your model to a mobile or embedded device, a large model may
simply take too long to download and use too much RAM and CPU, all of which will
make your app unresponsive, heat the device, and drain its battery. To avoid this, you
need to make a mobile-friendly, lightweight, and efficient model, without sacrificing
too  much  of  its  accuracy.  The  TFLite  library  provides  several  tools8  to  help  you
deploy your models to mobile and embedded devices, with three main objectives:

• Reduce the model size, to shorten download time and reduce RAM usage.

• Reduce  the  amount  of  computations  needed  for  each  prediction,  to  reduce

latency, battery usage, and heating.

• Adapt the model to device-specific constraints.

To reduce the model size, TFLite’s model converter can take a SavedModel and com‐
press  it  to  a  much  lighter  format  based  on  FlatBuffers.  This  is  an  efficient  cross-
platform serialization library (a bit like protocol buffers) initially created by Google
for gaming. It is designed so you can load FlatBuffers straight to RAM without any
preprocessing: this reduces the loading time and memory footprint. Once the model
is loaded into a mobile or embedded device, the TFLite interpreter will execute it to
make predictions. Here is how you can convert a SavedModel to a FlatBuffer and save
it to a .tflite file:

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
tflite_model = converter.convert()
with open("converted_model.tflite", "wb") as f:
    f.write(tflite_model)

You  can  also  save  a  tf.keras  model  directly  to  a  FlatBuffer  using
from_keras_model().

8 Also check out TensorFlow’s Graph Transform Tools for modifying and optimizing computational graphs.

Deploying a Model to a Mobile or Embedded Device 

| 

685

The converter also optimizes the model, both to shrink it and to reduce its latency. It
prunes  all  the  operations  that  are  not  needed  to  make  predictions  (such  as  training
operations),  and  it  optimizes  computations  whenever  possible;  for  example,  3×a  +
4×a + 5×a will be converted to (3 + 4 + 5)×a. It also tries to fuse operations whenever
possible.  For  example,  Batch  Normalization  layers  end  up  folded  into  the  previous
layer’s addition and multiplication operations, whenever possible. To get a good idea
of how much TFLite can optimize a model, download one of the pretrained TFLite
models,  unzip  the  archive,  then  open  the  excellent  Netron  graph  visualization  tool
and  upload  the  .pb  file  to  view  the  original  model.  It’s  a  big,  complex  graph,  right?
Next, open the optimized .tflite model and marvel at its beauty!

Another way you can reduce the model size (other than simply using smaller neural
network  architectures)  is  by  using  smaller  bit-widths:  for  example,  if  you  use  half-
floats (16 bits) rather than regular floats (32 bits), the model size will shrink by a fac‐
tor  of  2,  at  the  cost  of  a  (generally  small)  accuracy  drop.  Moreover,  training  will  be
faster, and you will use roughly half the amount of GPU RAM.

TFLite’s converter can go further than that, by quantizing the model weights down to
fixed-point, 8-bit integers! This leads to a fourfold size reduction compared to using
32-bit floats. The simplest approach is called post-training quantization: it just quanti‐
zes the weights after training, using a fairly basic but efficient symmetrical quantiza‐
tion  technique.  It  finds  the  maximum  absolute  weight  value,  m,  then  it  maps  the
floating-point  range  –m  to  +m  to  the  fixed-point  (integer)  range  –127  to  +127.  For
example (see Figure 19-8), if the weights range from –1.5 to +0.8, then the bytes –127,
0, and +127 will correspond 
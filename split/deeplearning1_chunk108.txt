aux, M., Bengio, Y., and David, J.-P. (2015). Low precision arithmetic for deep
learning. In Arxiv:1412.7024, ICLRâ€™2015 Workshop. 452
Courville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by
spike-and-slab RBMs. In ICMLâ€™11 . 561, 681
731

BIBLIOGRAPHY

Courville, A., Desjardins, G., Bergstra, J., and Bengio, Y. (2014). The spike-and-slab
RBM and extensions to discrete and sparse data distributions. Pattern Analysis and
Machine Intelligence, IEEE Transactions on , 36(9), 1874â€“1887. 682
Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory, 2nd Edition.
Wiley-Interscience. 73
Cox, D. and Pinto, N. (2011). Beyond simple features: A large-scale feature search
approach to unconstrained face recognition. In Automatic Face & Gesture Recognition
and Workshops (FG 2011), 2011 IEEE International Conference on , pages 8â€“15. IEEE.
363
CramÃ©r, H. (1946). Mathematical methods of statistics. Princeton University Press. 135,
295
Crick, F. H. C. and Mitchison, G. (1983). The function of dream sleep. Nature,304,
111â€“114. 609
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics
of Control, Signals, and Systems, 2, 303â€“314. 198
Dahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010). Phone recognition
with the mean-covariance restricted Boltzmann machine. In NIPSâ€™2010 . 23
Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2012). Context-dependent pre-trained deep
neural networks for large vocabulary speech recognition. IEEE Transactions on Audio,
Speech, and Language Processing, 20(1), 33â€“42. 459
Dahl, G. E., Sainath, T. N., and Hinton, G. E. (2013). Improving deep neural networks
for LVCSR using rectiï¬?ed linear units and dropout. In ICASSPâ€™2013 . 460
Dahl, G. E., Jaitly, N., and Salakhutdinov, R. (2014). Multi-task neural networks for
QSAR predictions. arXiv:1406.1231. 26
Dauphin, Y. and Bengio, Y. (2013). Stochastic ratio matching of RBMs for sparse
high-dimensional inputs. In NIPS26 . NIPS Foundation. 619
Dauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with
reconstruction sampling. In ICMLâ€™2011 . 471
Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014).
Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In NIPSâ€™2014 . 285, 286, 288
Davis, A., Rubinstein, M., Wadhwa, N., Mysore, G., Durand, F., and Freeman, W. T.
(2014). The visual microphone: Passive recovery of sound from video. ACM Transactions
on Graphics (Proc. SIGGRAPH), 33(4), 79:1â€“79:10. 452
732

BIBLIOGRAPHY

Dayan, P. (1990). Reinforcement comparison. In Connectionist Models: Proceedings of
the 1990 Connectionist Summer School , San Mateo, CA. 691
Dayan, P. and Hinton, G. E. (1996). Varieties of Helmholtz machine. Neural Networks,
9(8), 1385â€“1403. 693
Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine.
Neural computation, 7(5), 889â€“904. 693
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M.,
Senior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep
networks. In NIPSâ€™2012 . 25, 447
Dean, T. and Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5(3), 142â€“150. 662
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990).
Indexing by latent semantic analysis. Journal of the American Society for Information
Science , 41(6), 391â€“407. 477, 482
Delalleau, O. and Bengio, Y. (2011). Shallow vs. deep sum-product networks. In NIPS .
19, 554
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A
Large-Scale Hierarchical Image Database. In CVPR09 . 21
Deng, J., Berg, A. C., Li, K., and Fei-Fei, L. (2010a). What does classifying more than
10,000 image categories tell us? In Proceedings of the 11th European Conference on
Computer Vision: Part V , ECCVâ€™10, pages 71â€“84, Berlin, Heidelberg. Springer-Verlag.
21
Deng, L. and Yu, D. (2014). Deep learning â€“ methods and applications. Foundations and
Trends in Signal Processing . 460
Deng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. (2010b). Binary
coding of speech spectrograms using a deep auto-encoder. In Interspeech 2010 , Makuhari,
Chiba, Japan. 23
Denil, M., Bazzani, L., Larochelle, H., and de Freitas, N. (2012). Learning where to attend
with deep architectures for image tracking. Neural Computation , 24 (8), 2151â€“2184. 367
Denton, E., Chintala, S., Szlam, A., and Fergus, R. (2015). Deep generative image models
using a Laplacian pyramid of adversarial networks. NIPS . 702, 719
Desjardins, G. and Bengio, Y. (2008). Empirical evaluation of convolutional RBMs for
vision. Technical Report 1327, DÃ©partement dâ€™Informatique et de Recherche OpÃ©rationnelle, UniversitÃ© de MontrÃ©al. 683
733

BIBLIOGRAPHY

Desjardins, G., Courville, A. C., Bengio, Y., Vincent, P., and Delalleau, O. (2010).
Tempered Markov chain Monte Carlo for training of restricted Boltzmann machines. In
International Conference on Artiï¬?cial Intelligence and Statistics, pages 145â€“152. 603,
614
Desjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the partition function.
In NIPSâ€™2011 . 629
Desjardins, G., Simonyan, K., Pascanu, R., et al. (2015). Natural neural networks. In
Advances in Neural Information Processing Systems , pages 2062â€“2070. 320
Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast
and robust neural network joint models for statistical machine translation. In Proc.
ACLâ€™2014 . 473
Devroye, L. (2013). Non-Uniform Random Variate Generation. SpringerLink : BÃ¼cher.
Springer New York. 694
DiCarlo, J. J. (2013). Mechanisms underlying visual object recognition: Humans vs.
neurons vs. machines. NIPS Tutorial. 26, 366
Dinh, L., Krueger, D., and Bengio, Y. (2014). NICE: Non-linear independent components
estimation. arXiv:1410.8516. 493
Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,
K., and Darrell, T. (2014). Long-term recurrent convolutional networks for visual
recognition and description. arXiv:1411.4389. 102
Donoho, D. L. and Grimes, C. (2003). Hessian eigenmaps: new locally linear embedding
techniques for high-dimensional data. Technical Report 2003-08, Dept. Statistics,
Stanford University. 164, 519
Dosovitskiy, A., Springenberg, J. T., and Brox, T. (2015). Learning to generate chairs with
convolutional neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1538â€“1546. 696, 704, 705
Doya, K. (1993). Bifurcations of recurrent neural networks in gradient descent learning.
IEEE Transactions on Neural Networks, 1, 75â€“80. 401, 403
Dreyfus, S. E. (1962). The numerical solution of variational problems. Journal of
Mathematical Analysis and Applications , 5(1), 30â€“45. 225
Dreyfus, S. E. (1973). The computational solution of optimal control problems with time
lag. IEEE Transactions on Automatic Control , 18(4), 383â€“385. 225
Drucker, H. and LeCun, Y. (1992). Improving generalisation performance using double
back-propagation. IEEE Transactions on Neural Networks, 3(6), 991â€“997. 271
734

BIBLIOGRAPHY

Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research. 307
Dudik, M., Langford, J., and Li, L. (2011). Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on Machine learning, ICML â€™11.
482
Dugas, C., Bengio, Y., BÃ©lisle, F., and Nadeau, C. (2001). Incorporating second-order
functional knowledge for better option pricing. In T. Leen, T. Dietterich, and V. Tresp,
editors, Advances in Neural Information Processing Systems 13 (NIPSâ€™00), pages
472â€“478. MIT Press. 68, 197
Dziugaite, G. K., Roy, D. M., and Ghahramani, Z. (2015). Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906 .
703
El Hihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term
dependencies. In NIPSâ€™1995 . 398, 407, 408
Elkahky, A. M., Song, Y., and He, X. (2015). A multi-view deep learning approach for
cross domain user modeling in recommendation systems. In Proceedings of the 24th
International Conference on World Wide Web, pages 278â€“288. 480
Elman, J. L. (1993). Learning and development in neural networks: The importance of
starting small. Cognition, 48, 781â€“799. 328
Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The diï¬ƒculty
of training deep architectures and the eï¬€ect of unsupervised pre-training. In Proceedings
of AISTATSâ€™2009 . 201
Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. (2010).
Why does unsupervised pre-training help deep learning? J. Machine Learning Res.
529, 533, 534
Fahlman, S. E., Hinton, G. E., and Sejnowski, T. J. (1983). Massively parallel architectures
for AI: NETL, thistle, and Boltzmann machines. In Proceedings of the National
Conference on Artiï¬?cial Intelligence AAAI-83 . 570, 654
Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., DollÃ¡r, P., Gao, J., He, X.,
Mitchell, M., Platt, J. C., Zitnick, C. L., and Zweig, G. (2015). From captions to visual
concepts and back. arXiv:1411.4952. 102
Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., and
Talay, S. (2011). Large-scale FPGA-based convolutional networks. In R. Bekkerman,
M. Bilenko, and J. Langford, editors, Scaling up Machine Learning: Parallel and
Distributed Approaches. Cambridge University Press. 523

735

BIBLIOGRAPHY

Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013). Learning hierarchical features
for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence,
35(8), 1915â€“1929. 23, 201, 360
Fei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4), 594â€“611. 538
Finn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. (2015). Learning
visual feature spaces for robotic manipulation with deep spatial autoencoders. arXiv
preprint arXiv:1509.06113 . 25
Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals
of Eugenics , 7, 179â€“188. 21, 105
FÃ¶ldiÃ¡k, P. (1989). Adaptive network for optimal linear feature extraction. In International
Joint Conference on Neural Networks (IJCNN), volume 1, pages 401â€“405, Washington
1989. IEEE, New York. 494
Franzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place,
head-direction, and spatial-view cells. 495
Franzius, M., Wilbert, N., and Wiskott, L. (2008). Invariant object recognition with slow
feature analysis. In Artiï¬?cial Neural Networks-ICANN 2008 , pages 961â€“970. Springer.
496
Frasconi, P., Gori, M., and Sperduti, A. (1997). On the eï¬ƒcient classiï¬?cation of data
structures by neural networks. In Proc. Int. Joint Conf. on Artiï¬?cial Intelligence . 401
Frasconi, P., Gori, M., and Sperduti, A. (1998). A general framework for adaptive
processing of data structures. IEEE Transactions on Neural Networks , 9(5), 768â€“786.
401
Freund, Y. and Schapire, R. E. (1996a). Experiments with a new boosting algorithm. In
Machine Learning: Proceedings of Thirteenth International Conference, pages 148â€“156,
USA. ACM. 258
Freund, Y. and Schapire, R. E. (1996b). Game theory, on-line prediction and boosting. In
Proceedings of the Ninth Annual Conference on Computational Learning Theory, pages
325â€“332. 258
Frey, B. J. (1998). Graphical models for machine learning and digital communication.
MIT Press. 705, 706
Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good
density estimators? In D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances
in Neural Information Processing Systems 8 (NIPSâ€™95), pages 661â€“670. MIT Press,
Cambridge, MA. 651
736

BIBLIOGRAPHY

Frobenius, G. (1908). Ãœber matrizen aus positiven elementen, s. B. Preuss. Akad. Wiss.
Berlin, Germany. 597
Fukushima, K. (1975). Cognitron: A self-organizing multilayered neural network. Biological
Cybernetics , 20, 121â€“136. 16, 226, 528
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a
mechanism of pattern recognition unaï¬€ected by shift in position. Biological Cybernetics ,
36, 193â€“202. 16, 24, 27, 226, 367
Gal, Y. and Ghahramani, Z. (2015). Bayesian convolutional neural networks with Bernoulli
approximate variational inference. arXiv preprint arXiv:1506.02158 . 264
Gallinari, P., LeCun, Y., Thiria, S., and Fogelman-Soulie, F. (1987). Memoires associatives
distribuees. In Proceedings of COGNITIVA 87 , Paris, La Villette. 515
Garcia-Duran, A., Bordes, A., Usunier, N., and Grandvalet, Y. (2015). Combining two
and three-way embeddings models for link prediction in knowledge bases. arXiv preprint
arXiv:1506.00999 . 484
Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., and Pallett, D. S. (1993).
Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1.
NASA STI/Recon Technical Report N , 93, 27403. 459
Garson, J. (1900). The metric system of identiï¬?cation of criminals, as used in Great
Britain and Ireland. The Journal of the Anthropological Institute of Great Britain and
Ireland , (2), 177â€“227. 21
Gers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual
prediction with LSTM. Neural computation, 12(10), 2451â€“2471. 410, 412
Ghahramani, Z. and Hinton, G. E. (1996). The EM algorithm for mixtures of factor
analyzers. Technical Report CRG-TR-96-1, Dpt. of Comp. Sci., Univ. of Toronto. 489
Gillick, D., Brunk, C., Vinyals, O., and Subramanya, A. (2015). Multilingual language
processing from bytes. arXiv preprint arXiv:1512.00103 . 477
Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2015). Region-based convolutional
networks for accurate object detection and segmentation. 426
Giudice, M. D., Manera, V., and Keysers, C. (2009). Programmed to learn? The ontogeny
of mirror neurons. Dev. Sci., 12(2), 350â€“â€“363. 656
Glorot, X. and Bengio, Y. (2010). Understanding the diï¬ƒculty of training deep feedforward
neural networks. In AISTATSâ€™2010 . 303
Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiï¬?er neural networks. In
AISTATSâ€™2011 . 16, 174, 197, 226, 227
737

BIBLIOGRAPHY

Glorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale
sentiment classiï¬?cation: A deep learning approach. In ICMLâ€™2011 . 507, 537
Goldberger, J., Roweis, S., Hinton, G. E., and Salakhutdinov, R. (2005). Neighbourhood
components analysis. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural
Information Processing Systems 17 (NIPSâ€™04). MIT Press. 115
Gong, S., McKenna, S., and Psarrou, A. (2000). Dynamic Vision: From Images to Face
Recognition. Imperial College Press. 165, 519
Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep
networks. In NIPSâ€™2009 , pages 646â€“654. 255
Goodfellow, I., Koenig, N., Muja, M., Pantofaru, C., Sorokin, A., and Takayama, L. (2010).
Help me help you: Interfaces for personal robots. In Proc. of Human Robot Interaction
(HRI), Osaka, Japan. ACM Press, ACM Press. 100
Goodfellow, I. J. (2010). Technical report: Multidimensional, downsampled co
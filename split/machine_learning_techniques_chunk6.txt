rfitting: if you look at the test set, you may stumble upon some seemingly
interesting  pattern  in  the  test  data  that  leads  you  to  select  a  particular  kind  of
Machine Learning model. When you estimate the generalization error using the test
set,  your  estimate  will  be  too  optimistic,  and  you  will  launch  a  system  that  will  not
perform as well as expected. This is called data snooping bias.

Creating  a  test  set  is  theoretically  simple:  pick  some  instances  randomly,  typically
20% of the dataset (or less if your dataset is very large), and set them aside:

Get the Data 

| 

51

import numpy as np

def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

You can then use this function like this:13

>>> train_set, test_set = split_train_test(housing, 0.2)
>>> len(train_set)
16512
>>> len(test_set)
4128

Well, this works, but it is not perfect: if you run the program again, it will generate a
different test set! Over time, you (or your Machine Learning algorithms) will get to
see the whole dataset, which is what you want to avoid.

One  solution  is  to  save  the  test  set  on  the  first  run  and  then  load  it  in  subsequent
runs. Another option is to set the random number generator’s seed (e.g., with np.ran
dom.seed(42))14 before calling np.random.permutation() so that it always generates
the same shuffled indices.

But  both  these  solutions  will  break  the  next  time  you  fetch  an  updated  dataset.  To
have a stable train/test split even after updating the dataset, a common solution is to
use  each  instance’s  identifier  to  decide  whether  or  not  it  should  go  in  the  test  set
(assuming instances have a unique and immutable identifier). For example, you could
compute a hash of each instance’s identifier and put that instance in the test set if the
hash is lower than or equal to 20% of the maximum hash value. This ensures that the
test  set  will  remain  consistent  across  multiple  runs,  even  if  you  refresh  the  dataset.
The  new  test  set  will  contain  20%  of  the  new  instances,  but  it  will  not  contain  any
instance that was previously in the training set.

Here is a possible implementation:

from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

13 In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like
in the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented
blocks), and the outputs have no prefix.

14 You will often see people set the random seed to 42. This number has no special property, other than to be the

Answer to the Ultimate Question of Life, the Universe, and Everything.

52 

| 

Chapter 2: End-to-End Machine Learning Project

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

Unfortunately, the housing dataset does not have an identifier column. The simplest
solution is to use the row index as the ID:

housing_with_id = housing.reset_index()   # adds an `index` column
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")

If you use the row index as a unique identifier, you need to make sure that new data
gets appended to the end of the dataset and that no row ever gets deleted. If this is not
possible, then you can try to use the most stable features to build a unique identifier.
For example, a district’s latitude and longitude are guaranteed to be stable for a few
million years, so you could combine them into an ID like so:15

housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")

Scikit-Learn provides a few functions to split datasets into multiple subsets in various
ways.  The  simplest  function  is  train_test_split(),  which  does  pretty  much  the
same thing as the function split_train_test(), with a couple of additional features.
First, there is a random_state parameter that allows you to set the random generator
seed. Second, you can pass it multiple datasets with an identical number of rows, and
it will split them on the same indices (this is very useful, for example, if you have a
separate DataFrame for labels):

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

So far we have considered purely random sampling methods. This is generally fine if
your dataset is large enough (especially relative to the number of attributes), but if it
is  not,  you  run  the  risk  of  introducing  a  significant  sampling  bias.  When  a  survey
company decides to call 1,000 people to ask them a few questions, they don’t just pick
1,000 people randomly in a phone book. They try to ensure that these 1,000 people
are representative of the whole population. For example, the US population is 51.3%
females and 48.7% males, so a well-conducted survey in the US would try to maintain
this  ratio  in  the  sample:  513  female  and  487  male.  This  is  called  stratified  sampling:
the  population  is  divided  into  homogeneous  subgroups  called  strata,  and  the  right
number of instances are sampled from each stratum to guarantee that the test set is
representative of the overall population. If the people running the survey used purely
random sampling, there would be about a 12% chance of sampling a skewed test set

15 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so

they will end up in the same set (test or train). This introduces some unfortunate sampling bias.

Get the Data 

| 

53

that was either less than 49% female or more than 54% female. Either way, the survey
results would be significantly biased.

Suppose  you  chatted  with  experts  who  told  you  that  the  median  income  is  a  very
important  attribute  to  predict  median  housing  prices.  You  may  want  to  ensure  that
the test set is representative of the various categories of incomes in the whole dataset.
Since the median income is a continuous numerical attribute, you first need to create
an income category attribute. Let’s look at the median income histogram more closely
(back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,
$15,000–$60,000), but some median incomes go far beyond 6. It is important to have
a sufficient number of instances in your dataset for each stratum, or else the estimate
of  a  stratum’s  importance  may  be  biased.  This  means  that  you  should  not  have  too
many strata, and each stratum should be large enough. The following code uses the
pd.cut() function to create an income category attribute with five categories (labeled
from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from
1.5 to 3, and so on:

housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])

These income categories are represented in Figure 2-9:

housing["income_cat"].hist()

Figure 2-9. Histogram of income categories

Now you are ready to do stratified sampling based on the income category. For this
you can use Scikit-Learn’s StratifiedShuffleSplit class:

54 

| 

Chapter 2: End-to-End Machine Learning Project

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]

Let’s see if this worked as expected. You can start by looking at the income category
proportions in the test set:

>>> strat_test_set["income_cat"].value_counts() / len(strat_test_set)
3    0.350533
2    0.318798
4    0.176357
5    0.114583
1    0.039729
Name: income_cat, dtype: float64

With similar code you can measure the income category proportions in the full data‐
set. Figure 2-10 compares the income category proportions in the overall dataset, in
the test set generated with stratified sampling, and in a test set generated using purely
random sampling. As you can see, the test set generated using stratified sampling has
income category proportions almost identical to those in the full dataset, whereas the
test set generated using purely random sampling is skewed.

Figure 2-10. Sampling bias comparison of stratified versus purely random sampling

Now you should remove the income_cat attribute so the data is back to its original
state:

for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)

We spent quite a bit of time on test set generation for a good reason: this is an often
neglected  but  critical  part  of  a  Machine  Learning  project.  Moreover,  many  of  these
ideas will be useful later when we discuss cross-validation. Now it’s time to move on
to the next stage: exploring the data.

Get the Data 

| 

55

Discover and Visualize the Data to Gain Insights
So far you have only taken a quick glance at the data to get a general understanding of
the kind of data you are manipulating. Now the goal is to go into a little more depth.

First, make sure you have put the test set aside and you are only exploring the train‐
ing set. Also, if the training set is very large, you may want to sample an exploration
set, to make manipulations easy and fast. In our case, the set is quite small, so you can
just  work  directly  on  the  full  set.  Let’s  create  a  copy  so  that  you  can  play  with  it
without harming the training set:

housing = strat_train_set.copy()

Visualizing Geographical Data
Since there is geographical information (latitude and longitude), it is a good idea to
create a scatterplot of all districts to visualize the data (Figure 2-11):

housing.plot(kind="scatter", x="longitude", y="latitude")

Figure 2-11. A geographical scatterplot of the data

This looks like California all right, but other than that it is hard to see any particular
pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places
where there is a high density of data points (Figure 2-12):

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)

56 

| 

Chapter 2: End-to-End Machine Learning Project

Figure 2-12. A better visualization that highlights high-density areas

Now  that’s  much  better:  you  can  clearly  see  the  high-density  areas,  namely  the  Bay
Area and around Los Angeles and San Diego, plus a long line of fairly high density in
the Central Valley, in particular around Sacramento and Fresno.

Our brains are very good at spotting patterns in pictures, but you may need to play
around with visualization parameters to make the patterns stand out.

Now let’s look at the housing prices (Figure 2-13). The radius of each circle represents
the district’s population (option s), and the color represents the price (option c). We
will  use  a  predefined  color  map  (option  cmap)  called  jet,  which  ranges  from  blue
(low values) to red (high prices):16

housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4,
    s=housing["population"]/100, label="population", figsize=(10,7),
    c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
)
plt.legend()

16 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area

down to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.

Discover and Visualize the Data to Gain Insights 

| 

57

Figure 2-13. California housing prices: red is expensive, blue is cheap, larger circles indi‐
cate areas with a larger population

This  image  tells  you  that  the  housing  prices  are  very  much  related  to  the  location
(e.g., close to the ocean) and to the population density, as you probably knew already.
A clustering algorithm should be useful for detecting the main cluster and for adding
new features that measure the proximity to the cluster centers. The ocean proximity
attribute may be useful as well, although in Northern California the housing prices in
coastal districts are not too high, so it is not a simple rule.

Looking for Correlations
Since  the  dataset  is  not  too  large,  you  can  easily  compute  the  standard  correlation
coefficient  (also  called  Pearson’s  r)  between  every  pair  of  attributes  using  the  corr()
method:

corr_matrix = housing.corr()

Now let’s look at how much each attribute correlates with the median house value:

>>> corr_matrix["median_house_value"].sort_values(ascending=False)
median_house_value    1.000000
median_income         0.687170
total_rooms           0.135231
housing_median_age    0.114220

58 

| 

Chapter 2: End-to-End Machine Learning Project

households            0.064702
total_bedrooms        0.047865
population           -0.026699
longitude            -0.047279
latitude             -0.142826
Name: median_house_value, dtype: float64

The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that
there is a strong positive correlation; for example, the median house value tends to go
up  when  the  median  income  goes  up.  When  the  coefficient  is  close  to  –1,  it  means
that  there  is  a  strong  negative  correlation;  you  can  see  a  small  negative  correlation
between the latitude and the median house value (i.e., prices have a slight tendency to
go  down  when  you  go  north).  Finally,  coefficients  close  to  0  mean  that  there  is  no
linear correlation. Figure 2-14 shows various plots along with the correlation coeffi‐
cient between their horizontal and vertical axes.

Figure 2-14. Standard correlation coefficient of various datasets (source: Wikipedia;
public domain image)

The correlation coefficient only measures linear correlations (“if x
goes up, then y generally goes up/down”). It may completely miss
out on nonlinear relationships (e.g., “if x is close to 0, then y gener‐
ally goes up”). Note how all the plots of the bottom row have a cor‐
relation  coefficient  equal  to  0,  despite  the  fact  that  their  axes  are
clearly not independent: these are examples of nonlinear relation‐
ships. Also, the second row shows examples where the correlation
coefficient  is  equal  to  1  or  –1;  notice  that  this  has  nothing  to  do
with the slope. For example, your height in inches has a correlation
coefficient of 1 with your height in feet or in nanometers.

Another  way  to  check  for  correlation  between  attributes  is  to  use  the  pandas
scatter_matrix()  function,  which  plots  every  numerical  attribute  against  every

Discover and Visualize the Data to Gain Insights 

| 

59

other numerical attribute. Since there are now 11 numerical attributes, you would get
112 = 121 plots, which would not fit on a page—so let’s just focus on a few promising
a
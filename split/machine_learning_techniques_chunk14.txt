 that  are  commonly  used  for  classification
tasks: Logistic Regression and Softmax Regression.

There will be quite a few math equations in this chapter, using basic
notions  of  linear  algebra  and  calculus.  To  understand  these  equa‐
tions, you will need to know what vectors and matrices are; how to
transpose them, multiply them, and inverse them; and what partial
derivatives are. If you are unfamiliar with these concepts, please go
through the linear algebra and calculus introductory tutorials avail‐
able as Jupyter notebooks in the online supplemental material. For
those  who  are  truly  allergic  to  mathematics,  you  should  still  go
through this chapter and simply skip the equations; hopefully, the
text will be sufficient to help you understand most of the concepts.

Linear Regression
In Chapter 1 we looked at a simple regression model of life satisfaction: life_satisfac‐
tion = θ0 + θ1 × GDP_per_capita.

This model is just a linear function of the input feature GDP_per_capita. θ0 and θ1 are
the model’s parameters.

More generally, a linear model makes a prediction by simply computing a weighted
sum of the input features, plus a constant called the bias term (also called the intercept
term), as shown in Equation 4-1.

Equation 4-1. Linear Regression model prediction
y = θ0 + θ1x1 + θ2x2 + ⋯ + θnxn

In this equation:

• ŷ is the predicted value.

• n is the number of features.
• xi is the ith feature value.
• θj is the jth model parameter (including the bias term θ0 and the feature weights

θ1, θ2, ⋯, θn).

112 

| 

Chapter 4: Training Models

This can be written much more concisely using a vectorized form, as shown in Equa‐
tion 4-2.

Equation 4-2. Linear Regression model prediction (vectorized form)

y = hθ x = θ · x

In this equation:

• θ  is  the  model’s  parameter  vector,  containing  the  bias  term  θ0  and  the  feature

weights θ1 to θn.

• x is the instance’s feature vector, containing x0 to xn, with x0 always equal to 1.
• θ · x is the dot product of the vectors θ and x, which is of course equal to θ0x0 +

θ1x1 + θ2x2 + ... + θnxn.

• hθ is the hypothesis function, using the model parameters θ.

In Machine Learning, vectors are often represented as column vec‐
tors, which are 2D arrays with a single column. If θ and x are col‐
⊺
umn  vectors,  then  the  prediction  is  y = θ
  is  the
⊺
transpose of θ (a row vector instead of a column vector) and θ
x is
⊺
the matrix multiplication of θ
 and x. It is of course the same pre‐
diction,  except  that  it  is  now  represented  as  a  single-cell  matrix
rather  than  a  scalar  value.  In  this  book  I  will  use  this  notation  to
avoid switching between dot products and matrix multiplications.

⊺
x,  where  θ

OK,  that’s  the  Linear  Regression  model—but  how  do  we  train  it?  Well,  recall  that
training a model means setting its parameters so that the model best fits the training
set. For this purpose, we first need a measure of how well (or poorly) the model fits
the training data. In Chapter 2 we saw that the most common performance measure
of a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐
fore, to train a Linear Regression model, we need to find the value of θ that minimi‐
zes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE)
than  the  RMSE,  and  it  leads  to  the  same  result  (because  the  value  that  minimizes  a
function also minimizes its square root).1

1 It is often the case that a learning algorithm will try to optimize a different function than the performance

measure used to evaluate the final model. This is generally because that function is easier to compute, because
it has useful differentiation properties that the performance measure lacks, or because we want to constrain
the model during training, as you will see when we discuss regularization.

Linear Regression 

| 

113

The MSE of a Linear Regression hypothesis hθ on a training set X is calculated using
Equation 4-3.

Equation 4-3. MSE cost function for a Linear Regression model

MSE X, hθ =

m

1
m ∑

i = 1

θ⊺x i − y i 2

Most  of  these  notations  were  presented  in  Chapter  2  (see  “Notations”  on  page  40).
The only difference is that we write hθ instead of just h to make it clear that the model
is  parametrized  by  the  vector  θ.  To  simplify  notations,  we  will  just  write  MSE(θ)
instead of MSE(X, hθ).

The Normal Equation
To find the value of θ that minimizes the cost function, there is a closed-form solution
—in other words, a mathematical equation that gives the result directly. This is called
the Normal Equation (Equation 4-4).

Equation 4-4. Normal Equation

θ = X⊺X

−1

  X⊺   y

In this equation:

• θ is the value of θ that minimizes the cost function.
• y is the vector of target values containing y(1) to y(m).

Let’s generate some linear-looking data to test this equation on (Figure 4-1):

import numpy as np

X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

114 

| 

Chapter 4: Training Models

Figure 4-1. Randomly generated linear dataset

Now let’s compute θ using the Normal Equation. We will use the inv() function from
NumPy’s linear algebra module (np.linalg) to compute the inverse of a matrix, and
the dot() method for matrix multiplication:

X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

The function that we used to generate the data is y = 4 + 3x1 + Gaussian noise. Let’s
see what the equation found:

>>> theta_best
array([[4.21509616],
       [2.77011339]])

We would have hoped for θ0 = 4 and θ1 = 3 instead of θ0 = 4.215 and θ1 = 2.770. Close
enough, but the noise made it impossible to recover the exact parameters of the origi‐
nal function.

Now we can make predictions using θ:

>>> X_new = np.array([[0], [2]])
>>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance
>>> y_predict = X_new_b.dot(theta_best)
>>> y_predict
array([[4.21509616],
       [9.75532293]])

Linear Regression 

| 

115

Let’s plot this model’s predictions (Figure 4-2):

plt.plot(X_new, y_predict, "r-")
plt.plot(X, y, "b.")
plt.axis([0, 2, 0, 15])
plt.show()

Figure 4-2. Linear Regression model predictions

Performing Linear Regression using Scikit-Learn is simple:2

>>> from sklearn.linear_model import LinearRegression
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X, y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([4.21509616]), array([[2.77011339]]))
>>> lin_reg.predict(X_new)
array([[4.21509616],
       [9.75532293]])

The  LinearRegression  class  is  based  on  the  scipy.linalg.lstsq()  function  (the
name stands for “least squares”), which you could call directly:

>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
>>> theta_best_svd
array([[4.21509616],
       [2.77011339]])

This  function  computes  θ = X+y,  where  +  is  the  pseudoinverse  of  X  (specifically,
the  Moore-Penrose  inverse).  You  can  use  np.linalg.pinv()  to  compute  the
pseudoinverse directly:

2 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_).

116 

| 

Chapter 4: Training Models

>>> np.linalg.pinv(X_b).dot(y)
array([[4.21509616],
       [2.77011339]])

The pseudoinverse itself is computed using a standard matrix factorization technique
called  Singular  Value  Decomposition  (SVD)  that  can  decompose  the  training  set
into  the  matrix  multiplication  of  three  matrices  U  Σ  V⊺  (see
matrix  X 
numpy.linalg.svd()). The pseudoinverse is computed as X+ = VΣ+U⊺. To compute
the  matrix  Σ+,  the  algorithm  takes  Σ  and  sets  to  zero  all  values  smaller  than  a  tiny
threshold value, then it replaces all the nonzero values with their inverse, and finally
it transposes the resulting matrix. This approach is more efficient than computing the
Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may
not work if the matrix X⊺X is not invertible (i.e., singular), such as if m < n or if some
features are redundant, but the pseudoinverse is always defined.

Computational Complexity
The  Normal  Equation  computes  the  inverse  of  X⊺  X,  which  is  an  (n  +  1)  ×  (n  +  1)
matrix (where n is the number of features). The computational complexity of inverting
such a matrix is typically about O(n2.4) to O(n3), depending on the implementation. In
other  words,  if  you  double  the  number  of  features,  you  multiply  the  computation
time by roughly 22.4 = 5.3 to 23 = 8.

The SVD approach used by Scikit-Learn’s LinearRegression class is about O(n2). If
you double the number of features, you multiply the computation time by roughly 4.

Both  the  Normal  Equation  and  the  SVD  approach  get  very  slow
when  the  number  of  features  grows  large  (e.g.,  100,000).  On  the
positive side, both are linear with regard to the number of instances
in  the  training  set  (they  are  O(m)),  so  they  handle  large  training
sets efficiently, provided they can fit in memory.

Also, once you have trained your Linear Regression model (using the Normal Equa‐
tion or any other algorithm), predictions are very fast: the computational complexity
is linear with regard to both the number of instances you want to make predictions
on and the number of features. In other words, making predictions on twice as many
instances (or twice as many features) will take roughly twice as much time.

Now we will look at a very different way to train a Linear Regression model, which is
better suited for cases where there are a large number of features or too many training
instances to fit in memory.

Linear Regression 

| 

117

Gradient Descent
Gradient Descent is a generic optimization algorithm capable of finding optimal solu‐
tions to a wide range of problems. The general idea of Gradient Descent is to tweak
parameters iteratively in order to minimize a cost function.

Suppose you are lost in the mountains in a dense fog, and you can only feel the slope
of  the  ground  below  your  feet.  A  good  strategy  to  get  to  the  bottom  of  the  valley
quickly  is  to  go  downhill  in  the  direction  of  the  steepest  slope.  This  is  exactly  what
Gradient  Descent  does:  it  measures  the  local  gradient  of  the  error  function  with
regard to the parameter vector θ, and it goes in the direction of descending gradient.
Once the gradient is zero, you have reached a minimum!

Concretely, you start by filling θ with random values (this is called random initializa‐
tion).  Then  you  improve  it  gradually,  taking  one  baby  step  at  a  time,  each  step
attempting to decrease the cost function (e.g., the MSE), until the algorithm converges
to a minimum (see Figure 4-3).

Figure 4-3. In this depiction of Gradient Descent, the model parameters are initialized
randomly and get tweaked repeatedly to minimize the cost function; the learning step
size is proportional to the slope of the cost function, so the steps gradually get smaller as
the parameters approach the minimum

An important parameter in Gradient Descent is the size of the steps, determined by
the learning rate hyperparameter. If the learning rate is too small, then the algorithm
will have to go through many iterations to converge, which will take a long time (see
Figure 4-4).

118 

| 

Chapter 4: Training Models

Figure 4-4. The learning rate is too small

On the other hand, if the learning rate is too high, you might jump across the valley
and  end  up  on  the  other  side,  possibly  even  higher  up  than  you  were  before.  This
might make the algorithm diverge, with larger and larger values, failing to find a good
solution (see Figure 4-5).

Figure 4-5. The learning rate is too large

Finally, not all cost functions look like nice, regular bowls. There may be holes, ridges,
plateaus, and all sorts of irregular terrains, making convergence to the minimum dif‐
ficult. Figure 4-6 shows the two main challenges with Gradient Descent. If the ran‐
dom  initialization  starts  the  algorithm  on  the  left,  then  it  will  converge  to  a  local
minimum, which is not as good as the global minimum. If it starts on the right, then it
will  take  a  very  long  time  to  cross  the  plateau.  And  if  you  stop  too  early,  you  will
never reach the global minimum.

Gradient Descent 

| 

119

Figure 4-6. Gradient Descent pitfalls

Fortunately,  the  MSE  cost  function  for  a  Linear  Regression  model  happens  to  be  a
convex function, which means that if you pick any two points on the curve, the line
segment  joining  them  never  crosses  the  curve.  This  implies  that  there  are  no  local
minima, just one global minimum. It is also a continuous function with a slope that
never changes abruptly.3 These two facts have a great consequence: Gradient Descent
is  guaranteed  to  approach  arbitrarily  close  the  global  minimum  (if  you  wait  long
enough and if the learning rate is not too high).

In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if
the features have very different scales. Figure 4-7 shows Gradient Descent on a train‐
ing set where features 1 and 2 have the same scale (on the left), and on a training set
where feature 1 has much smaller values than feature 2 (on the right).4

Figure 4-7. Gradient Descent with (left) and without (right) feature scaling

3 Technically speaking, its derivative is Lipschitz continuous.

4 Since feature 1 is smaller, it takes a larger change in θ1 to affect the cost function, which is why the bowl is

elongated along the θ1 axis.

120 

| 

Chapter 4: Training Models

As you can see, on the left the Gradient Descent algorithm goes straight toward the
minimum, thereby reaching it quickly, whereas on the right it first goes in a direction
almost  orthogonal  to  the  direction  of  the  global  minimum,  and  it  ends  with  a  long
march  down  an  almost  flat  valley.  It  will  eventually  reach  the  minimum,  but  it  will
take a long time.

When using Gradient Descent, you should ensure that all features
have  a  similar  scale  (e.g.,  using  Scikit-Learn’s  StandardScaler
class), or else it will take much longer to converge.

This  diagram  also  illustrates  the  fact  that  training  a  model  means  searching  for  a
combination of model parameters that minimizes a cost function (over the training
set). It is a search in the model’s parameter space: the more parameters a model has,
the more dimensions this space has, and the harder the search is: searching for a nee‐
dle in a 300-dimensional haystack is much trickier than in 3 dimensions. Fortunately,
since the cost function is convex in the case of Linear Regression, the needle is simply
at the bottom of the bowl.

Batch Gradient Descent
To implement Gradient Descent, you need to compute the gradient of the cost func‐
tion  with  regard  to  each  model  parameter  θj.  In  other  words,  you  need  to  calculate
how much the cost function will change if you change θj just a little bit. This is called
a partial derivative. It is like asking “What is the slope of the mountain under my feet
if I face east?” and then asking the same question facing north (and so on for all other
dimensions, if you can imagine a universe with more than three dimensions). Equa‐
tion 4-5 computes the partial derivative of the cost function with regard to parameter
θj, noted ∂ MSE(θ) / ∂θj.

Eq
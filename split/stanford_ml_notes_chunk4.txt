and â€œbuy,â€ but not

42

â€œaardvark,â€ â€œaardwolfâ€ or â€œzygmurgy.â€2 The set of words encoded into the
feature vector is called the vocabulary, so the dimension of x is equal to
the size of the vocabulary.

Having chosen our feature vector, we now want to build a generative
model. So, we have to model p(x|y). But if we have, say, a vocabulary of
50000 words, then x âˆˆ {0, 1}50000 (x is a 50000-dimensional vector of 0â€™s and
1â€™s), and if we were to model x explicitly with a multinomial distribution over
the 250000 possible outcomes, then weâ€™d end up with a (250000 âˆ’1)-dimensional
parameter vector. This is clearly too many parameters.

To model p(x|y), we will therefore make a very strong assumption. We will
assume that the xiâ€™s are conditionally independent given y. This assumption
is called the Naive Bayes (NB) assumption, and the resulting algorithm is
called the Naive Bayes classiï¬er. For instance, if y = 1 means spam email;
â€œbuyâ€ is word 2087 and â€œpriceâ€ is word 39831; then we are assuming that if
I tell you y = 1 (that a particular piece of email is spam), then knowledge
of x2087 (knowledge of whether â€œbuyâ€ appears in the message) will have no
eï¬€ect on your beliefs about the value of x39831 (whether â€œpriceâ€ appears).
More formally, this can be written p(x2087|y) = p(x2087|y, x39831). (Note that
this is not the same as saying that x2087 and x39831 are independent, which
would have been written â€œp(x2087) = p(x2087|x39831)â€; rather, we are only
assuming that x2087 and x39831 are conditionally independent given y.)

We now have:

p(x1, . . . , x50000|y)

= p(x1|y)p(x2|y, x1)p(x3|y, x1, x2) Â· Â· Â· p(x50000|y, x1, . . . , x49999)
= p(x1|y)p(x2|y)p(x3|y) Â· Â· Â· p(x50000|y)

=

d
(cid:89)

j=1

p(xj|y)

The ï¬rst equality simply follows from the usual properties of probabilities,
and the second equality used the NB assumption. We note that even though

2Actually, rather than looking through an English dictionary for the list of all English
words, in practice it is more common to look through our training set and encode in our
feature vector only the words that occur at least once there. Apart from reducing the
number of words modeled and hence reducing our computational and space requirements,
this also has the advantage of allowing us to model/include as a feature many words
that may appear in your email (such as â€œcs229â€) but that you wonâ€™t ï¬nd in a dictionary.
Sometimes (as in the homework), we also exclude the very high frequency words (which
will be words like â€œthe,â€ â€œof,â€ â€œandâ€; these high frequency, â€œcontent freeâ€ words are called
stop words) since they occur in so many documents and do little to indicate whether an
email is spam or non-spam.

43

the Naive Bayes assumption is an extremely strong assumptions, the resulting
algorithm works well on many problems.

Our model is parameterized by Ï†j|y=1 = p(xj = 1|y = 1), Ï†j|y=0 = p(xj =
1|y = 0), and Ï†y = p(y = 1). As usual, given a training set {(x(i), y(i)); i =
1, . . . , n}, we can write down the joint likelihood of the data:

L(Ï†y, Ï†j|y=0, Ï†j|y=1) =

n
(cid:89)

i=1

p(x(i), y(i)).

Maximizing this with respect to Ï†y, Ï†j|y=0 and Ï†j|y=1 gives the maximum
likelihood estimates:

Ï†j|y=1 =

Ï†j|y=0 =

Ï†y =

(cid:80)n

i=1 1{x(i)
(cid:80)n

(cid:80)n

i=1 1{x(i)
(cid:80)n

j = 1 âˆ§ y(i) = 1}

i=1 1{y(i) = 1}

j = 1 âˆ§ y(i) = 0}

i=1 1{y(i) = 0}

(cid:80)n

i=1 1{y(i) = 1}
n

In the equations above, the â€œâˆ§â€ symbol means â€œand.â€ The parameters have
a very natural interpretation. For instance, Ï†j|y=1 is just the fraction of the
spam (y = 1) emails in which word j does appear.

Having ï¬t all these parameters, to make a prediction on a new example

with features x, we then simply calculate

p(y = 1|x) =

p(x|y = 1)p(y = 1)
p(x)

(cid:16)(cid:81)d

(cid:17)
j=1 p(xj|y = 1)
(cid:17)
j=1 p(xj|y = 1)

p(y = 1) +

(cid:16)(cid:81)d

=

(cid:16)(cid:81)d

p(y = 1)

(cid:17)
j=1 p(xj|y = 0)

,

p(y = 0)

and pick whichever class has the higher posterior probability.

Lastly, we note that while we have developed the Naive Bayes algorithm
mainly for the case of problems where the features xj are binary-valued, the
generalization to where xj can take values in {1, 2, . . . , kj} is straightforward.
Here, we would simply model p(xj|y) as multinomial rather than as Bernoulli.
Indeed, even if some original input attribute (say, the living area of a house,
as in our earlier example) were continuous valued, it is quite common to
discretize itâ€”that is, turn it into a small set of discrete valuesâ€”and apply
Naive Bayes. For instance, if we use some feature xj to represent living area,
we might discretize the continuous values as follows:

44

1

xi

1200-1600 >1600

800-1200
3

Living area (sq. feet) < 400

400-800
2
Thus, for a house with living area 890 square feet, we would set the value
of the corresponding feature xj to 3. We can then apply the Naive Bayes
algorithm, and model p(xj|y) with a multinomial distribution, as described
previously. When the original, continuous-valued attributes are not well-
modeled by a multivariate normal distribution, discretizing the features and
using Naive Bayes (instead of GDA) will often result in a better classiï¬er.

5

4

4.2.1 Laplace smoothing

The Naive Bayes algorithm as we have described it will work fairly well
for many problems, but there is a simple change that makes it work much
better, especially for text classiï¬cation. Letâ€™s brieï¬‚y discuss a problem with
the algorithm in its current form, and then talk about how we can ï¬x it.

Consider spam/email classiï¬cation, and letâ€™s suppose that, we are in the
year of 20xx, after completing CS229 and having done excellent work on the
project, you decide around May 20xx to submit work you did to the NeurIPS
conference for publication.3 Because you end up discussing the conference
in your emails, you also start getting messages with the word â€œneuripsâ€
in it. But this is your ï¬rst NeurIPS paper, and until this time, you had
not previously seen any emails containing the word â€œneuripsâ€; in particular
â€œneuripsâ€ did not ever appear in your training set of spam/non-spam emails.
Assuming that â€œneuripsâ€ was the 35000th word in the dictionary, your Naive
Bayes spam ï¬lter therefore had picked its maximum likelihood estimates of
the parameters Ï†35000|y to be

Ï†35000|y=1 =

Ï†35000|y=0 =

(cid:80)n

i=1 1{x(i)
(cid:80)n

(cid:80)n

i=1 1{x(i)
(cid:80)n

35000 = 1 âˆ§ y(i) = 1}
i=1 1{y(i) = 1}
35000 = 1 âˆ§ y(i) = 0}
i=1 1{y(i) = 0}

= 0

= 0

I.e., because it has never seen â€œneuripsâ€ before in either spam or non-spam
training examples, it thinks the probability of seeing it in either type of email
is zero. Hence, when trying to decide if one of these messages containing

3NeurIPS is one of the top machine learning conferences. The deadline for submitting

a paper is typically in May-June.

â€œneuripsâ€ is spam, it calculates the class posterior probabilities, and obtains

45

p(y = 1|x) =

(cid:81)d
j=1 p(xj|y = 1)p(y = 1) + (cid:81)d

j=1 p(xj|y = 1)p(y = 1)

(cid:81)d

j=1 p(xj|y = 0)p(y = 0)

=

0
0

.

This is because each of the terms â€œ(cid:81)d
j=1 p(xj|y)â€ includes a term p(x35000|y) =
0 that is multiplied into it. Hence, our algorithm obtains 0/0, and doesnâ€™t
know how to make a prediction.

Stating the problem more broadly, it is statistically a bad idea to esti-
mate the probability of some event to be zero just because you havenâ€™t seen
it before in your ï¬nite training set. Take the problem of estimating the mean
of a multinomial random variable z taking values in {1, . . . , k}. We can pa-
rameterize our multinomial with Ï†j = p(z = j). Given a set of n independent
observations {z(1), . . . , z(n)}, the maximum likelihood estimates are given by

Ï†j =

(cid:80)n

i=1 1{z(i) = j}
n

.

As we saw previously, if we were to use these maximum likelihood estimates,
then some of the Ï†jâ€™s might end up as zero, which was a problem. To avoid
this, we can use Laplace smoothing, which replaces the above estimate
with

Ï†j =

1 + (cid:80)n

i=1 1{z(i) = j}
k + n

.

Here, weâ€™ve added 1 to the numerator, and k to the denominator. Note that
(cid:80)k
j=1 Ï†j = 1 still holds (check this yourself!), which is a desirable property
since the Ï†jâ€™s are estimates for probabilities that we know must sum to 1.
Also, Ï†j (cid:54)= 0 for all values of j, solving our problem of probabilities being
estimated as zero. Under certain (arguably quite strong) conditions, it can
be shown that the Laplace smoothing actually gives the optimal estimator
of the Ï†jâ€™s.

Returning to our Naive Bayes classiï¬er, with Laplace smoothing, we

therefore obtain the following estimates of the parameters:

Ï†j|y=1 =

Ï†j|y=0 =

1 + (cid:80)n

1 + (cid:80)n

i=1 1{x(i)
2 + (cid:80)n
i=1 1{x(i)
2 + (cid:80)n

j = 1 âˆ§ y(i) = 1}

i=1 1{y(i) = 1}

j = 1 âˆ§ y(i) = 0}

i=1 1{y(i) = 0}

46

(In practice, it usually doesnâ€™t matter much whether we apply Laplace smooth-
ing to Ï†y or not, since we will typically have a fair fraction each of spam and
non-spam messages, so Ï†y will be a reasonable estimate of p(y = 1) and will
be quite far from 0 anyway.)

4.2.2 Event models for text classiï¬cation

To close oï¬€ our discussion of generative learning algorithms, letâ€™s talk about
one more model that is speciï¬cally for text classiï¬cation. While Naive Bayes
as weâ€™ve presented it will work well for many classiï¬cation problems, for text
classiï¬cation, there is a related model that does even better.

In the speciï¬c context of text classiï¬cation, Naive Bayes as presented uses
the whatâ€™s called the Bernoulli event model (or sometimes multi-variate
Bernoulli event model). In this model, we assumed that the way an email
is generated is that ï¬rst it is randomly determined (according to the class
priors p(y)) whether a spammer or non-spammer will send you your next
message. Then, the person sending the email runs through the dictionary,
deciding whether to include each word j in that email independently and
according to the probabilities p(xj = 1|y) = Ï†j|y. Thus, the probability of a
message was given by p(y) (cid:81)d

j=1 p(xj|y).

Hereâ€™s a diï¬€erent model, called the Multinomial event model. To
describe this model, we will use a diï¬€erent notation and set of features for
representing emails. We let xj denote the identity of the j-th word in the
email. Thus, xj is now an integer taking values in {1, . . . , |V |}, where |V |
is the size of our vocabulary (dictionary). An email of d words is now rep-
resented by a vector (x1, x2, . . . , xd) of length d; note that d can vary for
diï¬€erent documents. For instance, if an email starts with â€œA NeurIPS . . . ,â€
then x1 = 1 (â€œaâ€ is the ï¬rst word in the dictionary), and x2 = 35000 (if
â€œneuripsâ€ is the 35000th word in the dictionary).

In the multinomial event model, we assume that the way an email is
generated is via a random process in which spam/non-spam is ï¬rst deter-
mined (according to p(y)) as before. Then, the sender of the email writes the
email by ï¬rst generating x1 from some multinomial distribution over words
(p(x1|y)). Next, the second word x2 is chosen independently of x1 but from
the same multinomial distribution, and similarly for x3, x4, and so on, until
all d words of the email have been generated. Thus, the overall probability of
a message is given by p(y) (cid:81)d
j=1 p(xj|y). Note that this formula looks like the
one we had earlier for the probability of a message under the Bernoulli event
model, but that the terms in the formula now mean very diï¬€erent things. In
particular xj|y is now a multinomial, rather than a Bernoulli distribution.

47

The parameters for our new model are Ï†y = p(y) as before, Ï†k|y=1 =
p(xj = k|y = 1) (for any j) and Ï†k|y=0 = p(xj = k|y = 0). Note that we have
assumed that p(xj|y) is the same for all values of j (i.e., that the distribution
according to which a word is generated does not depend on its position j
within the email).

If we are given a training set {(x(i), y(i)); i = 1, . . . , n} where x(i) =
1 , x(i)
) (here, di is the number of words in the i-training example),

2 , . . . , x(i)
di

(x(i)
the likelihood of the data is given by

L(Ï†y, Ï†k|y=0, Ï†k|y=1) =

=

n
(cid:89)

i=1

n
(cid:89)

p(x(i), y(i))

(cid:32) di(cid:89)

p(x(i)

j |y; Ï†k|y=0, Ï†k|y=1)

p(y(i); Ï†y).

(cid:33)

i=1

j=1

Maximizing this yields the maximum likelihood estimates of the parameters:

Ï†k|y=1 =

Ï†k|y=0 =

Ï†y =

j = k âˆ§ y(i) = 1}

j = k âˆ§ y(i) = 0}

(cid:80)n

i=1

(cid:80)n

i=1

(cid:80)di
j=1 1{x(i)
(cid:80)n
i=1 1{y(i) = 1}di
(cid:80)di
j=1 1{x(i)
(cid:80)n
i=1 1{y(i) = 0}di

(cid:80)n

i=1 1{y(i) = 1}
n

.

If we were to apply Laplace smoothing (which is needed in practice for good
performance) when estimating Ï†k|y=0 and Ï†k|y=1, we add 1 to the numerators
and |V | to the denominators, and obtain:

Ï†k|y=1 =

Ï†k|y=0 =

1 + (cid:80)n

j=1 1{x(i)

j = k âˆ§ y(i) = 1}

i=1 1{y(i) = 1}di

1 + (cid:80)n

j=1 1{x(i)

j = k âˆ§ y(i) = 0}

.

i=1 1{y(i) = 0}di

i=1

(cid:80)di
|V | + (cid:80)n
(cid:80)di
|V | + (cid:80)n

i=1

While not necessarily the very best classiï¬cation algorithm, the Naive Bayes
classiï¬er often works surprisingly well. It is often also a very good â€œï¬rst thing
to try,â€ given its simplicity and ease of implementation.

Chapter 5

Kernel methods

5.1 Feature maps

Recall that in our discussion about linear regression, we considered the prob-
lem of predicting the price of a house (denoted by y) from the living area of
the house (denoted by x), and we ï¬t a linear function of x to the training
data. What if the price y can be more accurately represented as a non-linear
function of x? In this case, we need a more expressive family of models than
linear models.

We start by considering ï¬tting cubic functions y = Î¸3x3 + Î¸2x2 + Î¸1x + Î¸0.
It turns out that we can view the cubic function as a linear function over
the a diï¬€erent set of feature variables (deï¬ned below). Concretely, let the
function Ï† : R â†’ R4 be deï¬ned as

Ï†(x) =

ï£¹

ï£º
ï£º
ï£»

ï£®

ï£¯
ï£¯
ï£°

1
x
x2
x3

âˆˆ R4.

(5.1)

Let Î¸ âˆˆ R4 be the vector containing Î¸0, Î¸1, Î¸2, Î¸3 as entries. Then we can

rewrite the cubic function in x as:

Î¸3x3 + Î¸2x2 + Î¸1x + Î¸0 = Î¸T Ï†(x)

Thus, a cubic function of the variable x can be viewed as a linear function
over the variables Ï†(x). To distinguish between these two sets of variables,
in the context of kernel methods, we will call the â€œoriginalâ€ input value the
input attributes of a problem (in this case, x, the living area). When the

48

49

original input is mapped to some new set of quantities Ï†(x), we will call those
new quantities the features variables. (Unfortunately, diï¬€erent authors use
diï¬€erent terms to describe these two things in diï¬€erent contexts.) We will
call Ï† a feature map, which maps the attributes to the features.

5.2 LMS (least mean squares) with features

We will derive the gradient descent algorithm for ï¬tting the model Î¸T Ï†(x).
First recall that for ordinary least square problem where we were to ï¬t Î¸T x,
the batch gradient descent update is (see the ï¬rst lecture note for its deriva-
tion):

Î¸ := Î¸ + Î±

:= Î¸ + Î±

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:0)y(i) âˆ’ hÎ¸(x(i))(cid:1) x(i)

(cid:0)y(i) âˆ’ Î¸T x(i)(cid:1) x(i).

(5.2)

Let Ï† : Rd â†’ Rp be a feature map that maps attribute x (in Rd) to the
features Ï†(x) in Rp. (In the motivating
Z *2
!!
!!
!!
!!
*B
!Z

Obs

X

Y

3

5.3

2.8

1

4.3

2.4

3

5.3

2.8

Obs

X

Y

2

2.1

1.1

3

5.3

2.8

ˆ *2

1

4.3
!!
!!
!!

2.4

X

Y

!!
!!
!!
!!
!!

2

2.1

1.1

2

2.1

1.1

1

4.3

2.4

Obs

215

ˆ *1

ˆ *B

FIGURE 5.11. A graphical illustration of the bootstrap approach on a small
sample containing n = 3 observations. Each bootstrap data set contains n observations, sampled with replacement from the original data set. Each bootstrap data
set is used to obtain an estimate of α.

5.3

Lab: Cross-Validation and the Bootstrap

In this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your
computer.
We again begin by placing most of our imports at this top level.
In [1]: import numpy as np
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS ,
summarize ,
poly)
from sklearn.model_selection import train_test_split

There are several new imports needed for this lab.
In [2]: from functools import partial
from sklearn.model_selection import \
(cross_validate ,
KFold ,
ShuffleSplit)
from sklearn.base import clone
from ISLP.models import sklearn_sm

216

5. Resampling Methods

5.3.1

The Validation Set Approach

We explore the use of the validation set approach in order to estimate the
test error rates that result from fitting various linear models on the Auto
data set.
We use the function train_test_split() to split the data into training train_test_
and validation sets. As there are 392 observations, we split into two equal split()
sets of size 196 using the argument test_size=196. It is generally a good
idea to set a random seed when performing operations like this that contain
an element of randomness, so that the results obtained can be reproduced
precisely at a later time. We set the random seed of the splitter with the
argument random_state=0.
In [3]: Auto = load_data('Auto ')
Auto_train , Auto_valid = train_test_split (Auto ,
test_size =196,
random_state =0)

Now we can fit a linear regression using only the observations corresponding to the training set Auto_train.
In [4]: hp_mm = MS(['horsepower '])
X_train = hp_mm.fit_transform(Auto_train)
y_train = Auto_train['mpg']
model = sm.OLS(y_train , X_train)
results = model.fit()

We now use the predict() method of results evaluated on the model matrix for this model created using the validation data set. We also calculate
the validation MSE of our model.
In [5]: X_valid = hp_mm.transform(Auto_valid)
y_valid = Auto_valid['mpg']
valid_pred = results.predict(X_valid)
np.mean (( y_valid - valid_pred)**2)
Out[5]: 23.6166

Hence our estimate for the validation MSE of the linear regression fit is
23.62.
We can also estimate the validation error for higher-degree polynomial
regressions. We first provide a function evalMSE() that takes a model string
as well as a training and test set and returns the MSE on the test set.
In [6]: def evalMSE(terms ,
response ,
train ,
test):
mm = MS(terms)
X_train = mm.fit_transform(train)
y_train = train[response]
X_test = mm.transform(test)
y_test = test[response]

5.3 Lab: Cross-Validation and the Bootstrap

217

results = sm.OLS(y_train , X_train).fit()
test_pred = results.predict(X_test)
return np.mean (( y_test - test_pred)**2)

Let’s use this function to estimate the validation MSE using linear,
quadratic and cubic fits. We use the enumerate() function here, which gives
enumerate()
both the values and indices of objects as one iterates over a for loop.
In [7]: MSE = np.zeros (3)
for idx , degree in enumerate(range(1, 4)):
MSE[idx] = evalMSE ([ poly('horsepower ', degree)],
'mpg',
Auto_train ,
Auto_valid)
MSE
Out[7]: array ([23.62 , 18.76 , 18.80])

These error rates are 23.62, 18.76, and 18.80, respectively. If we choose a
different training/validation split instead, then we can expect somewhat
different errors on the validation set.
In [8]: Auto_train , Auto_valid = train_test_split(Auto ,
test_size =196,
random_state =3)
MSE = np.zeros (3)
for idx , degree in enumerate(range(1, 4)):
MSE[idx] = evalMSE ([ poly('horsepower ', degree)],
'mpg',
Auto_train ,
Auto_valid)
MSE
Out[8]: array ([20.76 , 16.95 , 16.97])

Using this split of the observations into a training set and a validation
set, we find that the validation set error rates for the models with linear,
quadratic, and cubic terms are 20.76, 16.95, and 16.97, respectively.
These results are consistent with our previous findings: a model that
predicts mpg using a quadratic function of horsepower performs better than
a model that involves only a linear function of horsepower, and there is no
evidence of an improvement in using a cubic function of horsepower.

5.3.2

Cross-Validation

In theory, the cross-validation estimate can be computed for any generalized linear model. In practice, however, the simplest way to cross-validate
in Python is to use sklearn, which has a different interface or API than
statsmodels, the code we have been using to fit GLMs.
This is a problem which often confronts data scientists: “I have a function
to do task A, and need to feed it into something that performs task B, so
that I can compute B(A(D)), where D is my data.” When A and B don’t
naturally speak to each other, this requires the use of a wrapper. In the ISLP wrapper

218

5. Resampling Methods

package, we provide a wrapper, sklearn_sm(), that enables us to easily use
sklearn_sm()
the cross-validation tools of sklearn with models fit by statsmodels.
The class sklearn_sm() has as its first argument a model from statsmodels.
It can take two additional optional arguments: model_str which can be used
to specify a formula, and model_args which should be a dictionary of additional arguments used when fitting the model. For example, to fit a logistic
regression model we have to specify a family argument. This is passed as
model_args={'family':sm.families.Binomial()}.
Here is our wrapper in action:
In [9]: hp_model = sklearn_sm(sm.OLS ,
MS(['horsepower ']))
X, Y = Auto.drop(columns =['mpg']), Auto['mpg']
cv_results = cross_validate(hp_model ,
X,
Y,
cv=Auto.shape [0])
cv_err = np.mean(cv_results['test_score '])
cv_err
Out[9]: 24.2315

The arguments to cross_validate() are as follows: an object with the appropriate fit(), predict(), and score() methods, an array of features X and
a response Y. We also included an additional argument cv to cross_validate();
specifying an integer K results in K-fold cross-validation. We have provided
a value corresponding to the total number of observations, which results
in leave-one-out cross-validation (LOOCV). The cross_validate() func- cross_
tion produces a dictionary with several components; we simply want the validate()
cross-validated test score here (MSE), which is estimated to be 24.23.
We can repeat this procedure for increasingly complex polynomial fits.
To automate the process, we again use a for loop which iteratively fits
polynomial regressions of degree 1 to 5, computes the associated crossvalidation error, and stores it in the ith element of the vector cv_error.
The variable d in the for loop corresponds to the degree of the polynomial.
We begin by initializing the vector. This command may take a couple of
seconds to run.
In [10]: cv_error = np.zeros (5)
H = np.array(Auto['horsepower '])
M = sklearn_sm(sm.OLS)
for i, d in enumerate(range (1,6)):
X = np.power.outer(H, np.arange(d+1))
M_CV = cross_validate(M,
X,
Y,
cv=Auto.shape [0])
cv_error[i] = np.mean(M_CV['test_score '])
cv_error
Out[10]: array ([24.2315 , 19.2482 , 19.3350 , 19.4244 , 19.0332])

As in Figure 5.4, we see a sharp drop in the estimated test MSE between
the linear and quadratic fits, but then no clear improvement from using
higher-degree polynomials.

5.3 Lab: Cross-Validation and the Bootstrap

219

Above we introduced the outer() method of the np.power() function.
.outer()
The outer() method is applied to an operation that has two arguments, np.power()
such as add(), min(), or power(). It has two arrays as arguments, and then
forms a larger array where the operation is applied to each pair of elements
of the two arrays.
In [11]: A = np.array ([3, 5, 9])
B = np.array ([2, 4])
np.add.outer(A, B)
Out[11]: array ([[ 5, 7],
[ 7, 9],
[11, 13]])

In the CV example above, we used K = n, but of course we can also use
K < n. The code is very similar to the above (and is significantly faster).
Here we use KFold() to partition the data into K = 10 random groups. We
KFold()
use random_state to set a random seed and initialize a vector cv_error in
which we will store the CV errors corresponding to the polynomial fits of
degrees one to five.
In [12]: cv_error = np.zeros (5)
cv = KFold(n_splits =10,
shuffle=True ,
random_state =0) # use same splits for each degree
for i, d in enumerate(range (1,6)):
X = np.power.outer(H, np.arange(d+1))
M_CV = cross_validate(M,
X,
Y,
cv=cv)
cv_error[i] = np.mean(M_CV['test_score '])
cv_error
Out[12]: array ([24.2077 , 19.1853 , 19.2763 , 19.4785 , 19.1372])

Notice that the computation time is much shorter than that of LOOCV.
(In principle, the computation time for LOOCV for a least squares linear
model should be faster than for K-fold CV, due to the availability of the
formula (5.2) for LOOCV; however, the generic cross_validate() function
does not make use of this formula.) We still see little evidence that using
cubic or higher-degree polynomial terms leads to a lower test error than
simply using a quadratic fit.
The cross_validate() function is flexible and can take different splitting
mechanisms as an argument. For instance, one can use the ShuffleSplit() Shuffle
funtion to implement the validation set approach just as easily as K-fold Split()
cross-validation.
In [13]: validation = ShuffleSplit(n_splits =1,
test_size =196,
random_state =0)
results = cross_validate(hp_model ,
Auto.drop (['mpg'], axis =1),
Auto['mpg'],
cv=validation);
results['test_score ']

220

5. Resampling Methods

Out[13]: array ([23.6166])

One can estimate the variability in the test error by running the following:
In [14]: validation = ShuffleSplit(n_splits =10,
test_size =196,
random_state =0)
results = cross_validate(hp_model ,
Auto.drop (['mpg'], axis =1),
Auto['mpg'],
cv=validation)
results['test_score ']. mean (), results['test_score '].std()
Out[14]: (23.8022 , 1.4218)

Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the
randomly-selected training samples overlap and hence introduce correlations. But it does give an idea of the Monte Carlo variation incurred by
picking different random folds.

5.3.3

The Bootstrap

We illustrate the use of the bootstrap in the simple example of Section 5.2,
as well as on an example involving estimating the accuracy of the linear
regression model on the Auto data set.
Estimating the Accuracy of a Statistic of Interest
One of the great advantages of the bootstrap approach is that it can be
applied in almost all situations. No complicated mathematical calculations
are required. While there are several implementations of the bootstrap in
Python, its use for estimating standard error is simple enough that we write
our own function below for the case when our data is stored in a dataframe.
To illustrate the bootstrap, we start with a simple example. The Portfolio
data set in the ISLP package is described in Section 5.2. The goal is to estimate the sampling variance of the parameter α given in formula (5.7).
We will create a function alpha_func(), which takes as input a dataframe D
assumed to have columns X and Y, as well as a vector idx indicating which
observations should be used to estimate α. The function then outputs the
estimate for α based on the selected observations.
In [15]: Portfolio = load_data('Portfolio ')
def alpha_func(D, idx):
cov_ = np.cov(D[['X','Y']]. loc[idx], rowvar=False)
return (( cov_ [1,1] - cov_ [0 ,1]) /
(cov_ [0 ,0]+ cov_ [1 ,1] -2* cov_ [0 ,1]))

This function returns an estimate for α based on applying the minimum
variance formula (5.7) to the observations indexed by the argument idx. For
instance, the following command estimates α using all 100 observations.
In [16]: alpha_func(Portfolio , range (100))

5.3 Lab: Cross-Validation and the Bootstrap

221

Out[16]: 0.5758

Next we randomly select 100 observations from range(100), with replacement. This is equivalent to constructing a new bootstrap data set and
recomputing α̂ based on the new data set.
In [17]: rng = np.random.default_rng (0)
alpha_func(Portfolio ,
rng.choice (100,
100,
replace=True))
Out[17]: 0.6074

This process can be generalized to create a simple function boot_SE() for
computing the bootstrap standard error for arbitrary functions that take
only a data frame as an argument.
In [18]: def boot_SE(func ,
D,
n=None ,
B=1000 ,
seed =0):
rng = np.random.default_rng(seed)
first_ , second_ = 0, 0
n = n or D.shape [0]
for _ in range(B):
idx = rng.choice(D.index ,
n,
replace=True)
value = func(D, idx)
first_ += value
second_ += value **2
return np.sqrt(second_ / B - (first_ / B)**2)

Notice the use of _ as a loop variable in for _ in range(B). This is often
used if the value of the counter is unimportant and simply makes sure the
loop is executed B times.
Let’s use our function to evaluate the accuracy of our estimate of α using
B = 1,000 bootstrap replications.
In [19]: alpha_SE = boot_SE(alpha_func ,
Portfolio ,
B=1000 ,
seed =0)
alpha_SE
Out[19]: 0.0912

The final output shows that the bootstrap estimate for SE(α̂) is 0.0912.
Estimating the Accuracy of a Linear Regression Model
The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here
we use the bootstrap approach in order to assess the variability of the

222

5. Resampling Methods

estimates for β0 and β1 , the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We
will compare the estimates obtained using the bootstrap to those obtained
using the formulas for SE(β̂0 ) and SE(β̂1 ) described in Section 3.1.2.
To use our boot_SE() function, we must write a function (its first argument) that takes a data frame D and indices idx as its only arguments. But
here we want to bootstrap a specific regression model, specified by a model
formula and data. We show how to do this in a few simple steps.
We start by writing a generic function boot_OLS() for bootstrapping a
regression model that takes a formula to define the corresponding regression. We use the clone() function to make a copy of the formula that can
clone()
be refit to the new dataframe. This means that any derived features such
as those defined by poly() (which we will see shortly), will be re-fit on the
resampled data frame.
In [20]: def boot_OLS(model_matrix , response , D, idx):
D_ = D.loc[idx]
Y_ = D_[response]
X_ = clone(model_matrix).fit_transform(D_)
return sm.OLS(Y_ , X_).fit().params

This is not quite what is needed as the first argument to boot_SE(). The first
two arguments which specify the model will not change in the bootstrap
process, and we would like to freeze them. The function partial() from the
partial()
functools module does precisely this: it takes a function as an argument,
and freezes some of its arguments, starting from the left. We use it to freeze
the first two model-formula arguments of boot_OLS().
In [21]: hp_func = partial(boot_OLS , MS(['horsepower '
to its parent identiï¬?er (0=choose left or top, 1=choose right or bottom).
(Bottom)The tree divides space into regions. The 2D plane shows how a decision tree
might divide R2 . The nodes of the tree are plotted in this plane, with each internal node
drawn along the dividing line it uses to categorize examples, and leaf nodes drawn in the
center of the region of examples they receive. The result is a piecewise-constant function,
with one piece per leaf. Each leaf requires at least one training example to deï¬?ne, so it is
not possible for the decision tree to learn a function that has more local maxima than the
number of training examples.
145

CHAPTER 5. MACHINE LEARNING BASICS

Another type of learning algorithm that also breaks the input space into regions
and has separate parameters for each region is the decision tree (Breiman et al.,
1984) and its many variants. As shown in ï¬?gure 5.7, each node of the decision
tree is associated with a region in the input space, and internal nodes break that
region into one sub-region for each child of the node (typically using an axis-aligned
cut). Space is thus sub-divided into non-overlapping regions, with a one-to-one
correspondence between leaf nodes and input regions. Each leaf node usually maps
every point in its input region to the same output. Decision trees are usually
trained with specialized algorithms that are beyond the scope of this book. The
learning algorithm can be considered non-parametric if it is allowed to learn a tree
of arbitrary size, though decision trees are usually regularized with size constraints
that turn them into parametric models in practice. Decision trees as they are
typically used, with axis-aligned splits and constant outputs within each node,
struggle to solve some problems that are easy even for logistic regression. For
example, if we have a two-class problem and the positive class occurs wherever
x2 > x1 , the decision boundary is not axis-aligned. The decision tree will thus
need to approximate the decision boundary with many nodes, implementing a step
function that constantly walks back and forth across the true decision function
with axis-aligned steps.
As we have seen, nearest neighbor predictors and decision trees have many
limitations. Nonetheless, they are useful learning algorithms when computational
resources are constrained. We can also build intuition for more sophisticated
learning algorithms by thinking about the similarities and diï¬€erences between
sophisticated algorithms and k-NN or decision tree baselines.
See Murphy (2012), Bishop (2006), Hastie et al. (2001) or other machine
learning textbooks for more material on traditional supervised learning algorithms.

5.8

Unsupervised Learning Algorithms

Recall from section 5.1.3 that unsupervised algorithms are those that experience
only â€œfeaturesâ€? but not a supervision signal. The distinction between supervised
and unsupervised algorithms is not formally and rigidly deï¬?ned because there is no
objective test for distinguishing whether a value is a feature or a target provided by
a supervisor. Informally, unsupervised learning refers to most attempts to extract
information from a distribution that do not require human labor to annotate
examples. The term is usually associated with density estimation, learning to
draw samples from a distribution, learning to denoise data from some distribution,
ï¬?nding a manifold that the data lies near, or clustering the data into groups of
146

CHAPTER 5. MACHINE LEARNING BASICS

related examples.
A classic unsupervised learning task is to ï¬?nd the â€œbestâ€? representation of the
data. By â€˜bestâ€™ we can mean diï¬€erent things, but generally speaking we are looking
for a representation that preserves as much information about x as possible while
obeying some penalty or constraint aimed at keeping the representation simpler or
more accessible than x itself.
There are multiple ways of deï¬?ning a simpler representation. Three of the
most common include lower dimensional representations, sparse representations
and independent representations. Low-dimensional representations attempt to
compress as much information about x as possible in a smaller representation.
Sparse representations (Barlow, 1989; Olshausen and Field, 1996; Hinton and
Ghahramani, 1997) embed the dataset into a representation whose entries are
mostly zeroes for most inputs. The use of sparse representations typically requires
increasing the dimensionality of the representation, so that the representation
becoming mostly zeroes does not discard too much information. This results in an
overall structure of the representation that tends to distribute data along the axes
of the representation space. Independent representations attempt to disentangle
the sources of variation underlying the data distribution such that the dimensions
of the representation are statistically independent.
Of course these three criteria are certainly not mutually exclusive. Lowdimensional representations often yield elements that have fewer or weaker dependencies than the original high-dimensional data. This is because one way to
reduce the size of a representation is to ï¬?nd and remove redundancies. Identifying
and removing more redundancy allows the dimensionality reduction algorithm to
achieve more compression while discarding less information.
The notion of representation is one of the central themes of deep learning and
therefore one of the central themes in this book. In this section, we develop some
simple examples of representation learning algorithms. Together, these example
algorithms show how to operationalize all three of the criteria above. Most of the
remaining chapters introduce additional representation learning algorithms that
develop these criteria in diï¬€erent ways or introduce other criteria.

5.8.1

Principal Components Analysis

In section 2.12, we saw that the principal components analysis algorithm provides
a means of compressing data. We can also view PCA as an unsupervised learning
algorithm that learns a representation of data. This representation is based on
two of the criteria for a simple representation described above. PCA learns a
147

20

20

10

10

0

0

z2

x2

CHAPTER 5. MACHINE LEARNING BASICS

âˆ’10

âˆ’10

âˆ’20

âˆ’20
âˆ’20 âˆ’10

0
x1

10

âˆ’20 âˆ’10

20

0
z1

10

20

Figure 5.8: PCA learns a linear projection that aligns the direction of greatest variance
with the axes of the new space. (Left)The original data consists of samples ofx . In this
space, the variance might occur along directions that are not axis-aligned. (Right)The
transformed data z = xî€¾ W now varies most along the axis z1. The direction of second
most variance is now along z2.

representation that has lower dimensionality than the original input. It also learns
a representation whose elements have no linear correlation with each other. This
is a ï¬?rst step toward the criterion of learning representations whose elements are
statistically independent. To achieve full independence, a representation learning
algorithm must also remove the nonlinear relationships between variables.
PCA learns an orthogonal, linear transformation of the data that projects an
input x to a representation z as shown in ï¬?gure 5.8. In section 2.12, we saw that
we could learn a one-dimensional representation that best reconstructs the original
data (in the sense of mean squared error) and that this representation actually
corresponds to the ï¬?rst principal component of the data. Thus we can use PCA
as a simple and eï¬€ective dimensionality reduction method that preserves as much
of the information in the data as possible (again, as measured by least-squares
reconstruction error). In the following, we will study how the PCA representation
decorrelates the original data representation X .
Let us consider the m Ã— n-dimensional design matrix X. We will assume that
the data has a mean of zero, E[x] = 0. If this is not the case, the data can easily
be centered by subtracting the mean from all examples in a preprocessing step.
The unbiased sample covariance matrix associated with X is given by:
Var[x] =

1
X î€¾X.
mâˆ’1
148

(5.85)

CHAPTER 5. MACHINE LEARNING BASICS

PCA ï¬?nds a representation (through linear transformation) z = xî€¾W where
Var[z ] is diagonal.
In section 2.12, we saw that the principal components of a design matrix X
are given by the eigenvectors of Xî€¾ X. From this view,
X î€¾ X = W Î›W î€¾ .

(5.86)

In this section, we exploit an alternative derivation of the principal components. The
principal components may also be obtained via the singular value decomposition.
Speciï¬?cally, they are the right singular vectors of X . To see this, let W be the
right singular vectors in the decomposition X = U Î£W î€¾ . We then recover the
original eigenvector equation with W as the eigenvector basis:
î€?
î€‘î€¾
X î€¾ X = U Î£W î€¾
U Î£W î€¾ = W Î£2 W î€¾ .
(5.87)
The SVD is helpful to show that PCA results in a diagonal Var [z]. Using the
SVD of X , we can express the variance of X as:
1
Xî€¾ X
mâˆ’1
1
=
(U Î£W î€¾ )î€¾ U Î£W î€¾
mâˆ’1
1
=
W Î£î€¾ U î€¾U Î£W î€¾
mâˆ’1
1
=
W Î£2 W î€¾ ,
mâˆ’1

Var[x] =

(5.88)
(5.89)
(5.90)
(5.91)

where we use the fact that U î€¾ U = I because the U matrix of the singular value
decomposition is deï¬?ned to be orthogonal. This shows that if we take z = xî€¾ W ,
we can ensure that the covariance of z is diagonal as required:
1
Z î€¾Z
mâˆ’1
1
=
W î€¾X î€¾XW
mâˆ’1
1
=
W î€¾W Î£ 2 W î€¾W
mâˆ’1
1
=
Î£2,
mâˆ’1

Var[z ] =

(5.92)
(5.93)
(5.94)
(5.95)

where this time we use the fact that W î€¾W = I , again from the deï¬?nition of the
SVD.
149

CHAPTER 5. MACHINE LEARNING BASICS

The above analysis shows that when we project the data x to z, via the linear
transformation W, the resulting representation has a diagonal covariance matrix
(as given by Î£ 2) which immediately implies that the individual elements of z are
mutually uncorrelated.
This ability of PCA to transform data into a representation where the elements
are mutually uncorrelated is a very important property of PCA. It is a simple
example of a representation that attempts to disentangle the unknown factors of
variation underlying the data. In the case of PCA, this disentangling takes the
form of ï¬?nding a rotation of the input space (described by W ) that aligns the
principal axes of variance with the basis of the new representation space associated
with z.
While correlation is an important category of dependency between elements of
the data, we are also interested in learning representations that disentangle more
complicated forms of feature dependencies. For this, we will need more than what
can be done with a simple linear transformation.

5.8.2

k-means Clustering

Another example of a simple representation learning algorithm is k-means clustering.
The k-means clustering algorithm divides the training set into k diï¬€erent clusters
of examples that are near each other. We can thus think of the algorithm as
providing a k-dimensional one-hot code vector h representing an input x. If x
belongs to cluster i, then h i = 1 and all other entries of the representation h are
zero.
The one-hot code provided by k-means clustering is an example of a sparse
representation, because the majority of its entries are zero for every input. Later,
we will develop other algorithms that learn more ï¬‚exible sparse representations,
where more than one entry can be non-zero for each input x. One-hot codes
are an extreme example of sparse representations that lose many of the beneï¬?ts
of a distributed representation. The one-hot code still confers some statistical
advantages (it naturally conveys the idea that all examples in the same cluster are
similar to each other) and it confers the computational advantage that the entire
representation may be captured by a single integer.
The k-means algorithm works by initializing k diï¬€erent centroids {Âµ(1) , . . . , Âµ(k)}
to diï¬€erent values, then alternating between two diï¬€erent steps until convergence.
In one step, each training example is assigned to cluster i , where i is the index of
the nearest centroid Âµ(i) . In the other step, each centroid Âµ (i) is updated to the
mean of all training examples x(j) assigned to cluster i.
150

CHAPTER 5. MACHINE LEARNING BASICS

One diï¬ƒculty pertaining to clustering is that the clustering problem is inherently
ill-posed, in the sense that there is no single criterion that measures how well a
clustering of the data corresponds to the real world. We can measure properties of
the clustering such as the average Euclidean distance from a cluster centroid to the
members of the cluster. This allows us to tell how well we are able to reconstruct
the training data from the cluster assignments. We do not know how well the
cluster assignments correspond to properties of the real world. Moreover, there
may be many diï¬€erent clusterings that all correspond well to some property of
the real world. We may hope to ï¬?nd a clustering that relates to one feature but
obtain a diï¬€erent, equally valid clustering that is not relevant to our task. For
example, suppose that we run two clustering algorithms on a dataset consisting of
images of red trucks, images of red cars, images of gray trucks, and images of gray
cars. If we ask each clustering algorithm to ï¬?nd two clusters, one algorithm may
ï¬?nd a cluster of cars and a cluster of trucks, while another may ï¬?nd a cluster of
red vehicles and a cluster of gray vehicles. Suppose we also run a third clustering
algorithm, which is allowed to determine the number of clusters. This may assign
the examples to four clusters, red cars, red trucks, gray cars, and gray trucks. This
new clustering now at least captures information about both attributes, but it has
lost information about similarity. Red cars are in a diï¬€erent cluster from gray
cars, just as they are in a diï¬€erent cluster from gray trucks. The output of the
clustering algorithm does not tell us that red cars are more similar to gray cars
than they are to gray trucks. They are diï¬€erent from both things, and that is all
we know.
These issues illustrate some of the reasons that we may prefer a distributed
representation to a one-hot representation. A distributed representation could have
two attributes for each vehicleâ€”one representing its color and one representing
whether it is a car or a truck. It is still not entirely clear what the optimal
distributed representation is (how can the learning algorithm know whether the
two attributes we are interested in are color and car-versus-truck rather than
manufacturer and age?) but having many attributes reduces the burden on the
algorithm to guess which single attribute we care about, and allows us to measure
similarity between objects in a ï¬?ne-grained way by comparing many attributes
instead of just testing whether one attribute matches.

5.9

Stochastic Gradient Descent

Nearly all of deep learning is powered by one very important algorithm: stochastic
gradient descent or SGD. Stochastic gradient descent is an extension of the
151

CHAPTER 5. MACHINE LEARNING BASICS

gradient descent algorithm introduced in section 4.3.
A recurring problem in machine learning is that large training sets are necessary
for good generalization, but large training sets are also more computationally
expensive.
The cost function used by a machine learning algorithm often decomposes as a
sum over training e
 (2013). Modeling documents with
deep Boltzmann machines. arXiv preprint arXiv:1309.6865 . 663
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).
Dropout: A simple way to prevent neural networks from overï¬?tting. Journal of Machine
Learning Research, 15, 1929â€“1958. 258, 265, 267, 672
Srivastava, R. K., Greï¬€, K., and Schmidhuber, J. (2015).
arXiv:1505.00387 . 326

Highway networks.

Steinkrau, D., Simard, P. Y., and Buck, I. (2005). Using GPUs for machine learning
algorithms. 2013 12th International Conference on Document Analysis and Recognition,
0, 1115â€“1119. 445
Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical
model parameters given approximate inference, decoding, and model structure. In
Proceedings of the 14th International Conference on Artiï¬?cial Intelligence and Statistics
(AISTATS) , volume 15 of JMLR Workshop and Conference Proceedings , pages 725â€“733,
Fort Lauderdale. Supplementary material (4 pages) also available. 674, 698
Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. (2015). Weakly supervised memory
networks. arXiv preprint arXiv:1503.08895 . 418
Supancic, J. and Ramanan, D. (2013). Self-paced learning for long-term tracking. In
CVPRâ€™2013 . 328
Sussillo, D. (2014). Random walks: Training very deep nonlinear feed-forward networks
with smart initialization. CoRR , abs/1412.6558. 290, 303, 305, 403
Sutskever, I. (2012). Training Recurrent Neural Networks. Ph.D. thesis, Department of
computer science, University of Toronto. 406, 413
770

BIBLIOGRAPHY

Sutskever, I. and Hinton, G. E. (2008). Deep narrow sigmoid belief networks are universal
approximators. Neural Computation , 20(11), 2629â€“2636. 693
Sutskever, I. and Tieleman, T. (2010). On the Convergence Properties of Contrastive
Divergence. In Y. W. Teh and M. Titterington, editors, Proc. of the International
Conference on Artiï¬?cial Intelligence and Statistics (AISTATS), volume 9, pages 789â€“795.
612
Sutskever, I., Hinton, G., and Taylor, G. (2009). The recurrent temporal restricted
Boltzmann machine. In NIPSâ€™2008 . 685
Sutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent
neural networks. In ICMLâ€™2011 , pages 1017â€“1024. 477
Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of
initialization and momentum in deep learning. In ICML. 300, 406, 413
Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with
neural networks. In NIPSâ€™2014, arXiv:1409.3215 . 25, 101, 397, 410, 411, 474, 475
Sutton, R. and Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press.
106
Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y. (2000). Policy gradient methods
for reinforcement learning with function approximation. In NIPSâ€™1999 , pages 1057â€“
â€“1063. MIT Press. 691
Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On
autoencoders and score matching for energy based models. In ICMLâ€™2011 . ACM. 513
Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw Bayesian optimization.
arXiv preprint arXiv:1406.3896 . 436
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,
V., and Rabinovich, A. (2014a). Going deeper with convolutions. Technical report,
arXiv:1409.4842. 24, 27, 201, 258, 269, 326, 347
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and
Fergus, R. (2014b). Intriguing properties of neural networks. ICLR , abs/1312.6199.
268, 271
Szegedy, C., Vanhoucke, V., Ioï¬€e, S., Shlens, J., and Wojna, Z. (2015). Rethinking the
Inception Architecture for Computer Vision. ArXiv e-prints . 243, 322
Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). DeepFace: Closing the gap to
human-level performance in face veriï¬?cation. In CVPRâ€™2014 . 100
Tandy, D. W. (1997). Works and Days: A Translation and Commentary for the Social
Sciences . University of California Press. 1
771

BIBLIOGRAPHY

Tang, Y. and Eliasmith, C. (2010). Deep networks for robust visual recognition. In
Proceedings of the 27th International Conference on Machine Learning, June 21-24,
2010, Haifa, Israel . 241
Tang, Y., Salakhutdinov, R., and Hinton, G. (2012). Deep mixtures of factor analysers.
arXiv preprint arXiv:1206.4635 . 489
Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines
for modeling motion style. In L. Bottou and M. Littman, editors, Proceedings of
the Twenty-sixth International Conference on Machine Learning (ICMLâ€™09), pages
1025â€“1032, Montreal, Quebec, Canada. ACM. 685
Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary
latent variables. In B. SchÃ¶lkopf, J. Platt, and T. Hoï¬€man, editors, Advances in Neural
Information Processing Systems 19 (NIPSâ€™06), pages 1345â€“1352. MIT Press, Cambridge,
MA. 685
Teh, Y., Welling, M., Osindero, S., and Hinton, G. E. (2003). Energy-based models
for sparse overcomplete representations. Journal of Machine Learning Research, 4,
1235â€“1260. 491
Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework
for nonlinear dimensionality reduction. Science, 290(5500), 2319â€“2323. 164, 518, 533
Theis, L., van den Oord, A., and Bethge, M. (2015). A note on the evaluation of generative
models. arXiv:1511.01844. 698, 719
Thompson, J., Jain, A., LeCun, Y., and Bregler, C. (2014). Joint training of a convolutional
network and a graphical model for human pose estimation. In NIPSâ€™2014 . 360
Thrun, S. (1995). Learning to play the game of chess. In NIPSâ€™1994 . 271
Tibshirani, R. J. (1995). Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society B , 58, 267â€“288. 236
Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to
the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, Proceedings of the Twenty-ï¬?fth International Conference on Machine Learning (ICMLâ€™08),
pages 1064â€“1071. ACM. 612
Tieleman, T. and Hinton, G. (2009). Using fast weights to improve persistent contrastive
divergence. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-sixth
International Conference on Machine Learning (ICMLâ€™09), pages 1033â€“1040. ACM.
614
Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis.
Journal of the Royal Statistical Society B , 61(3), 611â€“622. 491
772

BIBLIOGRAPHY

Torralba, A., Fergus, R., and Weiss, Y. (2008). Small codes and large databases for
recognition. In Proceedings of the Computer Vision and Pattern Recognition Conference
(CVPRâ€™08), pages 1â€“8. 525
Touretzky, D. S. and Minton, G. E. (1985). Symbols among the neurons: Details of
a connectionist inference architecture. In Proceedings of the 9th International Joint
Conference on Artiï¬?cial Intelligence - Volume 1 , IJCAIâ€™85, pages 238â€“243, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc. 17
Tu, K. and Honavar, V. (2011). On the utility of curricula in unsupervised learning of
probabilistic grammars. In IJCAIâ€™2011 . 328
Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk,
W., and Seung, H. S. (2010). Convolutional networks can learn to generate aï¬ƒnity
graphs for image segmentation. Neural Computation, 22(2), 511â€“538. 360
Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: A simple and
general method for semi-supervised learning. In Proc. ACLâ€™2010 , pages 384â€“394. 535
TÃ¶scher, A., Jahrer, M., and Bell, R. M. (2009). The BigChaos solution to the Netï¬‚ix
grand prize. 480
Uria, B., Murray, I., and Larochelle, H. (2013). Rnade: The real-valued neural autoregressive density-estimator. In NIPSâ€™2013 . 709, 710
van den OÃ¶rd, A., Dieleman, S., and Schrauwen, B. (2013). Deep content-based music
recommendation. In NIPSâ€™2013 . 480
van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-SNE. J. Machine
Learning Res., 9. 477, 519
Vanhoucke, V., Senior, A., and Mao, M. Z. (2011). Improving the speed of neural networks
on CPUs. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop.
444, 452
Vapnik, V. N. (1982). Estimation of Dependences Based on Empirical Data . SpringerVerlag, Berlin. 114
Vapnik, V. N. (1995). The Nature of Statistical Learning Theory . Springer, New York.
114
Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative
frequencies of events to their probabilities. Theory of Probability and Its Applications,
16, 264â€“280. 114
Vincent, P. (2011). A connection between score matching and denoising autoencoders.
Neural Computation, 23(7). 513, 515, 712
773

BIBLIOGRAPHY

Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In NIPSâ€™2002 . MIT Press.
520
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and
composing robust features with denoising autoencoders. In ICML 2008 . 241, 515
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked
denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. J. Machine Learning Res., 11. 515
Vincent, P., de BrÃ©bisson, A., and Bouthillier, X. (2015). Eï¬ƒcient exact gradient update
for training deep networks with very large sparse targets. In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
Processing Systems 28 , pages 1108â€“1116. Curran Associates, Inc. 466
Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. (2014a).
Grammar as a foreign language. Technical report, arXiv:1412.7449. 410
Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014b). Show and tell: a neural image
caption generator. arXiv 1411.4555. 410
Vinyals, O., Fortunato, M., and Jaitly, N. (2015a). Pointer networks. arXiv preprint
arXiv:1506.03134 . 418
Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b). Show and tell: a neural image
caption generator. In CVPRâ€™2015 . arXiv:1411.4555. 102
Viola, P. and Jones, M. (2001). Robust real-time object detection. In International
Journal of Computer Vision. 449
Visin, F., Kastner, K., Cho, K., Matteucci, M., Courville, A., and Bengio, Y. (2015).
ReNet: A recurrent neural network based alternative to convolutional networks. arXiv
preprint arXiv:1505.00393 . 395
Von Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by retinal
projections directed to the auditory pathway. Nature, 404(6780), 871â€“876. 16
Wager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization.
In Advances in Neural Information Processing Systems 26 , pages 351â€“359. 265
Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K., and Lang, K. (1989). Phoneme
recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 37, 328â€“339. 374, 453, 459
Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. (2013). Regularization of neural
networks using dropconnect. In ICMLâ€™2013 . 266
Wang, S. and Manning, C. (2013). Fast dropout training. In ICMLâ€™2013 . 266
774

BIBLIOGRAPHY

Wang, Z., Zhang, J., Feng, J., and Chen, Z. (2014a). Knowledge graph and text jointly
embedding. In Proc. EMNLPâ€™2014 . 484
Wang, Z., Zhang, J., Feng, J., and Chen, Z. (2014b). Knowledge graph embedding by
translating on hyperplanes. In Proc. AAAIâ€™2014 . 484
Warde-Farley, D., Goodfellow, I. J., Courville, A., and Bengio, Y. (2014). An empirical
analysis of dropout in piecewise linear networks. In ICLRâ€™2014 . 262, 266, 267
Wawrzynek, J., Asanovic, K., Kingsbury, B., Johnson, D., Beck, J., and Morgan, N.
(1996). Spert-II: A vector microprocessor system. Computer , 29(3), 79â€“86. 451
Weaver, L. and Tao, N. (2001). The optimal reward baseline for gradient-based reinforcement learning. In Proc. UAIâ€™2001 , pages 538â€“545. 691
Weinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of image manifolds by
semideï¬?nite programming. In CVPRâ€™2004 , pages 988â€“995. 164, 519
Weiss, Y., Torralba, A., and Fergus, R. (2008). Spectral hashing. In NIPS , pages
1753â€“1760. 525
Welling, M., Zemel, R. S., and Hinton, G. E. (2002). Self supervised boosting. In Advances
in Neural Information Processing Systems, pages 665â€“672. 703
Welling, M., Hinton, G. E., and Osindero, S. (2003a). Learning sparse topographic
representations with products of Student-t distributions. In NIPSâ€™2002 . 680
Welling, M., Zemel, R., and Hinton, G. E. (2003b). Self-supervised boosting. In S. Becker,
S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing
Systems 15 (NIPSâ€™02), pages 665â€“672. MIT Press. 622
Welling, M., Rosen-Zvi, M., and Hinton, G. E. (2005). Exponential family harmoniums
with an application to information retrieval. In L. Saul, Y. Weiss, and L. Bottou,
editors, Advances in Neural Information Processing Systems 17 (NIPSâ€™04), volume 17,
Cambridge, MA. MIT Press. 676
Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In
Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC , pages 762â€“770. 225
Weston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning to
rank with joint word-image embeddings. Machine Learning , 81(1), 21â€“35. 401
Weston, J., Chopra, S., and Bordes, A. (2014). Memory networks. arXiv preprint
arXiv:1410.3916 . 418, 485
Widrow, B. and Hoï¬€, M. E. (1960). Adaptive switching circuits. In 1960 IRE WESCON
Convention Record , volume 4, pages 96â€“104. IRE, New York. 15, 21, 24, 27
775

BIBLIOGRAPHY

Wikipedia (2015). List of animals by number of neurons â€” Wikipedia, the free encyclopedia.
[Online; accessed 4-March-2015]. 24, 27
Williams, C. K. I. and Agakov, F. V. (2002). Products of Gaussians and Probabilistic
Minor Component Analysis. Neural Computation, 14(5), 1169â€“1182. 682
Williams, C. K. I. and Rasmussen, C. E. (1996). Gaussian processes for regression. In
D. Touretzky, M. Mozer, and M. Hasselmo, editors, Advances in Neural Information
Processing Systems 8 (NIPSâ€™95), pages 514â€“520. MIT Press, Cambridge, MA. 142
Williams, R. J. (1992). Simple statistical gradient-following algorithms connectionist
reinforcement learning. Machine Learning , 8, 229â€“256. 688, 689
Williams, R. J. and Zipser, D. (1989). A learning algorithm for continually running fully
recurrent neural networks. Neural Computation, 1, 270â€“280. 223
Wilson, D. R. and Martinez, T. R. (2003). The general ineï¬ƒciency of batch training for
gradient descent learning. Neural Networks , 16(10), 1429â€“1451. 279
Wilson, J. R. (1984). Variance reduction techniques for digital simulation. American
Journal of Mathematical and Management Sciences, 4(3), 277â€“â€“312. 690
Wiskott, L. and Sejnowski, T. J. (2002). Slow feature analysis: Unsupervised learning of
invariances. Neural Computation, 14(4), 715â€“770. 494
Wolpert, D. and MacReady, W. (1997). No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation, 1, 67â€“82. 293
Wolpert, D. H. (1996). The lack of a priori distinction between learning algorithms. Neural
Computation, 8(7), 1341â€“1390. 116
Wu, R., Yan, S., Shan, Y., Dang, Q., and Sun, G. (2015). Deep image: Scaling up image
recognition. arXiv:1501.02876. 447
Wu, Z. (1997). Global continuation for distance geometry problems. SIAM Journal of
Optimization, 7, 814â€“836. 327
Xiong, H. Y., Barash, Y., and Frey, B. J. (2011). Bayesian predicti
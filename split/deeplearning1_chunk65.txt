hus may
be parallelized easily. The computations also involve processing massive buï¬€ers of
memory, containing bitmaps describing the texture (color pattern) of each object
to be rendered. Together, this results in graphics cards having been designed to
have a high degree of parallelism and high memory bandwidth, at the cost of
having a lower clock speed and less branching capability relative to traditional
CPUs.
Neural network algorithms require the same performance characteristics as the
real-time graphics algorithms described above. Neural networks usually involve
large and numerous buï¬€ers of parameters, activation values, and gradient values,
each of which must be completely updated during every step of training. These
buï¬€ers are large enough to fall outside the cache of a traditional desktop computer
so the memory bandwidth of the system often becomes the rate limiting factor.
GPUs oï¬€er a compelling advantage over CPUs due to their high memory bandwidth.
Neural network training algorithms typically do not involve much branching or
sophisticated control, so they are appropriate for GPU hardware. Since neural
networks can be divided into multiple individual â€œneuronsâ€? that can be processed
independently from the other neurons in the same layer, neural networks easily
beneï¬?t from the parallelism of GPU computing.
GPU hardware was originally so specialized that it could only be used for
graphics tasks. Over time, GPU hardware became more ï¬‚exible, allowing custom
subroutines to be used to transform the coordinates of vertices or assign colors to
pixels. In principle, there was no requirement that these pixel values actually be
based on a rendering task. These GPUs could be used for scientiï¬?c computing by
writing the output of a computation to a buï¬€er of pixel values. Steinkrau et al.
(2005) implemented a two-layer fully connected neural network on a GPU and
reported a threefold speedup over their CPU-based baseline. Shortly thereafter,
Chellapilla et al. (2006) demonstrated that the same technique could be used to
accelerate supervised convolutional networks.
The popularity of graphics cards for neural network training exploded after
the advent of general purpose GPUs. These GP-GPUs could execute arbitrary
code, not just rendering subroutines. NVIDIAâ€™s CUDA programming language
provided a way to write this arbitrary code in a C-like language. With their
relatively convenient programming model, massive parallelism, and high memory
445

CHAPTER 12. APPLICATIONS

bandwidth, GP-GPUs now oï¬€er an ideal platform for neural network programming.
This platform was rapidly adopted by deep learning researchers soon after it became
available (Raina et al., 2009; Ciresan et al., 2010).
Writing eï¬ƒcient code for GP-GPUs remains a diï¬ƒcult task best left to specialists. The techniques required to obtain good performance on GPU are very
diï¬€erent from those used on CPU. For example, good CPU-based code is usually
designed to read information from the cache as much as possible. On GPU, most
writable memory locations are not cached, so it can actually be faster to compute
the same value twice, rather than compute it once and read it back from memory.
GPU code is also inherently multi-threaded and the diï¬€erent threads must be
coordinated with each other carefully. For example, memory operations are faster if
they can be coalesced. Coalesced reads or writes occur when several threads can
each read or write a value that they need simultaneously, as part of a single memory
transaction. Diï¬€erent models of GPUs are able to coalesce diï¬€erent kinds of read
or write patterns. Typically, memory operations are easier to coalesce if among n
threads, thread i accesses byte i + j of memory, and j is a multiple of some power
of 2. The exact speciï¬?cations diï¬€er between models of GPU. Another common
consideration for GPUs is making sure that each thread in a group executes the
same instruction simultaneously. This means that branching can be diï¬ƒcult on
GPU. Threads are divided into small groups called warps. Each thread in a warp
executes the same instruction during each cycle, so if diï¬€erent threads within the
same warp need to execute diï¬€erent code paths, these diï¬€erent code paths must
be traversed sequentially rather than in parallel.
Due to the diï¬ƒculty of writing high performance GPU code, researchers should
structure their workï¬‚ow to avoid needing to write new GPU code in order to test
new models or algorithms. Typically, one can do this by building a software library
of high performance operations like convolution and matrix multiplication, then
specifying models in terms of calls to this library of operations. For example, the
machine learning library Pylearn2 (Goodfellow et al., 2013c) speciï¬?es all of its
machine learning algorithms in terms of calls to Theano (Bergstra et al., 2010;
Bastien et al., 2012) and cuda-convnet (Krizhevsky, 2010), which provide these
high-performance operations. This factored approach can also ease support for
multiple kinds of hardware. For example, the same Theano program can run on
either CPU or GPU, without needing to change any of the calls to Theano itself.
Other libraries like TensorFlow (Abadi et al., 2015) and Torch (Collobert et al.,
2011b) provide similar features.

446

CHAPTER 12. APPLICATIONS

12.1.3

Large-Scale Distributed Implementations

In many cases, the computational resources available on a single machine are
insuï¬ƒcient. We therefore want to distribute the workload of training and inference
across many machines.
Distributing inference is simple, because each input example we want to process
can be run by a separate machine. This is known as data parallelism.
It is also possible to get model parallelism, where multiple machines work
together on a single datapoint, with each machine running a diï¬€erent part of the
model. This is feasible for both inference and training.
Data parallelism during training is somewhat harder. We can increase the size
of the minibatch used for a single SGD step, but usually we get less than linear
returns in terms of optimization performance. It would be better to allow multiple
machines to compute multiple gradient descent steps in parallel. Unfortunately,
the standard deï¬?nition of gradient descent is as a completely sequential algorithm:
the gradient at step t is a function of the parameters produced by step t âˆ’ 1.

This can be solved using asynchronous stochastic gradient descent (Bengio et al., 2001; Recht et al., 2011). In this approach, several processor cores share
the memory representing the parameters. Each core reads parameters without a
lock, then computes a gradient, then increments the parameters without a lock.
This reduces the average amount of improvement that each gradient descent step
yields, because some of the cores overwrite each otherâ€™s progress, but the increased
rate of production of steps causes the learning process to be faster overall. Dean
et al. (2012) pioneered the multi-machine implementation of this lock-free approach
to gradient descent, where the parameters are managed by a parameter server
rather than stored in shared memory. Distributed asynchronous gradient descent
remains the primary strategy for training large deep networks and is used by
most major deep learning groups in industry (Chilimbi et al., 2014; Wu et al.,
2015). Academic deep learning researchers typically cannot aï¬€ord the same scale
of distributed learning systems but some research has focused on how to build
distributed networks with relatively low-cost hardware available in the university
setting (Coates et al., 2013).

12.1.4

Model Compression

In many commercial applications, it is much more important that the time and
memory cost of running inference in a machine learning model be low than that
the time and memory cost of training be low. For applications that do not require
447

CHAPTER 12. APPLICATIONS

personalization, it is possible to train a model once, then deploy it to be used by
billions of users. In many cases, the end user is more resource-constrained than
the developer. For example, one might train a speech recognition network with a
powerful computer cluster, then deploy it on mobile phones.
A key strategy for reducing the cost of inference is model compression (BuciluaÌŒ et al., 2006). The basic idea of model compression is to replace the original,
expensive model with a smaller model that requires less memory and runtime to
store and evaluate.
Model compression is applicable when the size of the original model is driven
primarily by a need to prevent overï¬?tting. In most cases, the model with the
lowest generalization error is an ensemble of several independently trained models.
Evaluating all n ensemble members is expensive. Sometimes, even a single model
generalizes better if it is large (for example, if it is regularized with dropout).
These large models learn some function f(x), but do so using many more
parameters than are necessary for the task. Their size is necessary only due to
the limited number of training examples. As soon as we have ï¬?t this function
f(x), we can generate a training set containing inï¬?nitely many examples, simply
by applying f to randomly sampled points x. We then train the new, smaller,
model to match f (x) on these points. In order to most eï¬ƒciently use the capacity
of the new, small model, it is best to sample the new x points from a distribution
resembling the actual test inputs that will be supplied to the model later. This can
be done by corrupting training examples or by drawing points from a generative
model trained on the original training set.
Alternatively, one can train the smaller model only on the original training
points, but train it to copy other features of the model, such as its posterior
distribution over the incorrect classes (Hinton et al., 2014, 2015).

12.1.5

Dynamic Structure

One strategy for accelerating data processing systems in general is to build systems
that have dynamic structure in the graph describing the computation needed
to process an input. Data processing systems can dynamically determine which
subset of many neural networks should be run on a given input. Individual neural
networks can also exhibit dynamic structure internally by determining which subset
of features (hidden units) to compute given information from the input. This
form of dynamic structure inside neural networks is sometimes called conditional
computation (Bengio, 2013; Bengio et al., 2013b). Since many components of
the architecture may be relevant only for a small amount of possible inputs, the
448

CHAPTER 12. APPLICATIONS

system can run faster by computing these features only when they are needed.
Dynamic structure of computations is a basic computer science principle applied
generally throughout the software engineering discipline. The simplest versions
of dynamic structure applied to neural networks are based on determining which
subset of some group of neural networks (or other machine learning models) should
be applied to a particular input.
A venerable strategy for accelerating inference in a classiï¬?er is to use a cascade
of classiï¬?ers. The cascade strategy may be applied when the goal is to detect the
presence of a rare object (or event). To know for sure that the object is present,
we must use a sophisticated classiï¬?er with high capacity, that is expensive to run.
However, because the object is rare, we can usually use much less computation
to reject inputs as not containing the object. In these situations, we can train
a sequence of classiï¬?ers. The ï¬?rst classiï¬?ers in the sequence have low capacity,
and are trained to have high recall. In other words, they are trained to make sure
we do not wrongly reject an input when the object is present. The ï¬?nal classiï¬?er
is trained to have high precision. At test time, we run inference by running the
classiï¬?ers in a sequence, abandoning any example as soon as any one element in
the cascade rejects it. Overall, this allows us to verify the presence of objects with
high conï¬?dence, using a high capacity model, but does not force us to pay the cost
of full inference for every example. There are two diï¬€erent ways that the cascade
can achieve high capacity. One way is to make the later members of the cascade
individually have high capacity. In this case, the system as a whole obviously has
high capacity, because some of its individual members do. It is also possible to
make a cascade in which every individual model has low capacity but the system
as a whole has high capacity due to the combination of many small models. Viola
and Jones (2001) used a cascade of boosted decision trees to implement a fast and
robust face detector suitable for use in handheld digital cameras. Their classiï¬?er
localizes a face using essentially a sliding window approach in which many windows
are examined and rejected if they do not contain faces. Another version of cascades
uses the earlier models to implement a sort of hard attention mechanism: the
early members of the cascade localize an object and later members of the cascade
perform further processing given the location of the object. For example, Google
transcribes address numbers from Street View imagery using a two-step cascade
that ï¬?rst locates the address number with one machine learning model and then
transcribes it with another (Goodfellow et al., 2014d).
Decision trees themselves are an example of dynamic structure, because each
node in the tree determines which of its subtrees should be evaluated for each input.
A simple way to accomplish the union of deep learning and dynamic structure
449

CHAPTER 12. APPLICATIONS

is to train a decision tree in which each node uses a neural network to make the
splitting decision (Guo and Gelfand, 1992), though this has typically not been
done with the primary goal of accelerating inference computations.
In the same spirit, one can use a neural network, called the gater to select
which one out of several expert networks will be used to compute the output,
given the current input. The ï¬?rst version of this idea is called the mixture of
experts (Nowlan, 1990; Jacobs et al., 1991), in which the gater outputs a set
of probabilities or weights (obtained via a softmax nonlinearity), one per expert,
and the ï¬?nal output is obtained by the weighted combination of the output of
the experts. In that case, the use of the gater does not oï¬€er a reduction in
computational cost, but if a single expert is chosen by the gater for each example,
we obtain the hard mixture of experts (Collobert et al., 2001, 2002), which
can considerably accelerate training and inference time. This strategy works well
when the number of gating decisions is small because it is not combinatorial. But
when we want to select diï¬€erent subsets of units or parameters, it is not possible
to use a â€œsoft switchâ€? because it requires enumerating (and computing outputs for)
all the gater conï¬?gurations. To deal with this problem, several approaches have
been explored to train combinatorial gaters. Bengio et al. (2013b) experiment with
several estimators of the gradient on the gating probabilities, while Bacon et al.
(2015) and Bengio et al. (2015a) use reinforcement learning techniques (policy
gradient) to learn a form of conditional dropout on blocks of hidden units and get
an actual reduction in computational cost without imp
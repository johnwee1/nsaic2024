n(z)).
Then, the standard chain rule gives us that

âˆ€i âˆˆ {1, . . . , m},

âˆ‚J
âˆ‚zi

=

n
(cid:88)

j=1

âˆ‚J
âˆ‚uj

Â·

âˆ‚gj
âˆ‚zi

.

(7.53)

Alternatively, when z and u are both vectors, in a vectorized notation:

âˆ‚J
âˆ‚z

=

ï£®

ï£¯
ï£°

âˆ‚g1
âˆ‚z1

...

âˆ‚g1
âˆ‚zm

Â· Â· Â·
. . .
Â· Â· Â·

ï£¹

ï£º
ï£» Â·

âˆ‚gn
âˆ‚z1

...

âˆ‚gn
âˆ‚zm

âˆ‚J
âˆ‚u

.

(7.54)

In other words, the backward function is always a linear map from âˆ‚J
âˆ‚u to
âˆ‚J
âˆ‚z , though note that the mapping itself can depend on z in complex ways.
The matrix on the RHS of (7.54) is actually the transpose of the Jacobian
matrix of the function g. However, we do not discuss in-depth about Jacobian
matrices to avoid complications. Part of the reason is that when z is a matrix
(or tensor), to write an analog of equation (7.54), one has to either ï¬‚atten z
into a vector or introduce additional notations on tensor-matrix product. In
this sense, equation (7.53) is more convenient and eï¬€ective to use in all cases.
For example, when z âˆˆ RrÃ—s is a matrix, we can easily rewrite equation (7.53)

to

âˆ€i, k,

âˆ‚J
âˆ‚zik

=

n
(cid:88)

j=1

âˆ‚J
âˆ‚uj

Â·

âˆ‚gj
âˆ‚zik

.

101

(7.55)

which will indeed be used in some of the derivations in Section 7.4.3.

Key interpretation of the chain rule. We can view the formula above (equa-
tion (7.53) or (7.54)) as a way to compute âˆ‚J
âˆ‚u . Consider the following
abstract problem. Suppose J depends on z via u as deï¬ned in equation (7.52).
However, suppose the function f is not given or the function f is complex,
but we are given the value of âˆ‚J
âˆ‚u . Then, the formula in equation (7.54) gives
âˆ‚z from âˆ‚J
us a way to compute âˆ‚J
âˆ‚u .

âˆ‚z from âˆ‚J

âˆ‚J
âˆ‚u

chain rule, formula (7.54)
====================â‡’
only requires info about g(Â·) and z

âˆ‚J
âˆ‚z

.

(7.56)

Moreover, this formula only involves knowledge about g (more precisely âˆ‚gj
).
âˆ‚zi
We will repeatedly use this fact in situations where g is a building blocks of
a complex network f .

Empirically, itâ€™s often useful to modularized the mapping in (7.53) or
(7.54) into a black-box, and mathematically itâ€™s also convenient to deï¬ne a
âˆ‚u to âˆ‚J
notation for it.6 We use B[g, z] to deï¬ne the function that maps âˆ‚J
âˆ‚z ,
and write

âˆ‚J
âˆ‚z

= B[g, z]

(cid:19)

(cid:18)âˆ‚J
âˆ‚u

.

(7.57)

We call B[g, z] the backward function for the module g. Note that when z
is ï¬xed, B[g, z] is merely a linear map from Rn to Rm. Using equation (7.53),
we have

(B[g, z](v))i =

m
(cid:88)

j=1

âˆ‚gj
âˆ‚zi

Â· vj .

Or in vectorized notation, using (7.54), we have

B[g, z](v) =

ï£®

ï£¯
ï£°

âˆ‚g1
âˆ‚z1

...

âˆ‚g1
âˆ‚zm

Â· Â· Â·
. . .
Â· Â· Â·

ï£¹

ï£º
ï£» Â· v .

âˆ‚gn
âˆ‚z1

...

âˆ‚gn
âˆ‚zm

6e.g., the function is the .backward() method of the module in pytorch.

(7.58)

(7.59)

102

and therefore B[g, z] can be viewed as a matrix. However, in reality, z will be
changing and thus the backward mapping has to be recomputed for diï¬€erent
zâ€™s while g is often ï¬xed. Thus, empirically, the backward function B[g, z](v)
is often viewed as a function which takes in z (=the input to g) and v (=a
vector that is supposed to be the gradient of some variable J w.r.t to the
output of g) as the inputs, and outputs a vector that is supposed to be the
gradient of J w.r.t to z.

7.4.2 General strategy of backpropagation

We discuss the general strategy of auto-diï¬€erentiation in this section to build
a high-level understanding. Then, we will instantiate the approach to con-
crete neural networks. We take the viewpoint that neural networks are com-
plex compositions of small building blocks such as MM, Ïƒ, Conv2D, LN,
etc., deï¬ned in Section 7.3. Note that the losses (e.g., mean-squared loss, or
the cross-entropy loss) can also be abstractly viewed as additional modules.
Thus, we can abstractly write the loss function J (on a single example (x, y))
as a composition of many modules:7

J = Mk(Mkâˆ’1(Â· Â· Â· M1(x))) .

(7.60)

For example, for a binary classiï¬cation problem with a MLP Â¯hÎ¸(x) (de-
ï¬ned in equation (7.36) and (7.37)), the loss function has ber written in the
form of equation (7.60) with M1 = MMW [1],b[1], M2 = Ïƒ, M3 = MMW [2],b[2],
. . . , and Mkâˆ’1 = MMW [r],b[r] and Mk = (cid:96)logistic.

We can see from this example that some modules involve parameters, and
other modules might only involve a ï¬xed set of operations. For generality,
we assume that eachj Mi involves a set of parameters Î¸[i], though Î¸[i] could
possibly be an empty set when Mi is a ï¬xed operation such as the nonlinear
activations. We will discuss more on the granularity of the modularization,
but so far we assume all the modules Miâ€™s are simple enough.

We introduce the intermediate variables for the computation in (7.60).

7Technically, we should write J = Mk(Mkâˆ’1(Â· Â· Â· M1(x)), y). However, y is treated as a
constant for the purpose of computing the derivatives w.r.t to the parameters, and thus
we can view it as part of Mk for the sake of simplicity of notations.

103

Let

u[0] = x
u[1] = M1(u[0])
u[2] = M2(u[1])

...

J = u[k] = Mk(u[kâˆ’1]) .

(F)

Backpropgation consists of two passes, the forward pass and backward
pass. In the forward pass, the algorithm simply computes u[1], . . . , u[k] from
i = 1, . . . , k, sequentially using the deï¬nition in (F), and save all the in-
termediate variables u[i]â€™s in the memory.

In the backward pass, we ï¬rst compute the derivatives w.r.t to the
intermediate variables, that is,
âˆ‚u[1] , sequentially in this backward
order, and then compute the derivatives of the parameters âˆ‚J
âˆ‚u[i] and
u[iâˆ’1]. These two type of computations can be also interleaved with each
other because âˆ‚J
âˆ‚u[k] with
k < i.

âˆ‚u[i] and u[iâˆ’1] but not any âˆ‚J

âˆ‚Î¸[i] only depends on âˆ‚J

âˆ‚Î¸[i] from âˆ‚J

âˆ‚u[k] , . . . , âˆ‚J

âˆ‚J

âˆ‚J

We ï¬rst see why

âˆ‚u[iâˆ’1] can be computed eï¬ƒciently from âˆ‚J

âˆ‚u[i] and u[iâˆ’1]
by invoking the discussion in Section 7.4.1 on the chain rule. We in-
stantiate the discussion by setting u = u[i] and z = u[iâˆ’1], and f (u) =
Mk(Mkâˆ’1(Â· Â· Â· Mi+1(u[i]))), and g(Â·) = Mi(Â·). Note that f is very complex
but we donâ€™t need any concrete information about f . Then, the conclusive
equation (7.56) corresponds to

âˆ‚J
âˆ‚u[i]

chain rule
==========================â‡’
only requires info about Mi(Â·) and u[iâˆ’1]

âˆ‚J
âˆ‚u[iâˆ’1] .

(7.61)

More precisely, we can write, following equation (7.57)

âˆ‚J
âˆ‚u[iâˆ’1] = B[Mi, u[iâˆ’1]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚u[i]

.

Instantiating the chain rule with z = Î¸[i] and u = u[i], we also have

âˆ‚J
âˆ‚Î¸[i] = B[Mi, Î¸[i]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚u[i]

.

See Figure 7.5 for an illustration of the algorithm.

(B1)

(B2)

104

Figure 7.5: Back-propagation.

Remark 7.4.3: [Computational eï¬ƒciency and granularity of the modules]
The main underlying purpose of treating a complex network as compositions
of small modules is that small modules tend to have eï¬ƒciently implementable
backward function. In fact, the backward functions of all the atomic modules
such as addition, multiplication and ReLU can be computed as eï¬ƒciently as
the the evaluation of these modules (up to multiplicative constant factor).
Using this fact, we can prove Theorem 7.4.1 by viewing neural networks as
compositions of many atomic operations, and invoking the backpropagation
discussed above. However, in practice, itâ€™s oftentimes more convenient to
modularize the networks using modules on the level of matrix multiplication,
layernorm, etc. As we will see, naive implementation of these operationsâ€™
backward functions also have the same runtime as the evaluation of these
functions.

ð‘¥...ð‘€!ð½...ð‘€"ð‘¢[!]ð‘¢["%!]ðœ•ð½ðœ•ð½â„¬[ð‘€",ð‘¢["%!]]ðœ•ð½ðœ•ð‘¢!"#ð‘¢[&]ð‘€&ð‘¢[&%!]â„¬[ð‘€&,ð‘¢[&%!]]ðœ•ð½ðœ•ð‘¢$"#ðœ•ð½ðœ•ð‘¢$...ðœ•ð½ðœ•ð‘¢#...Forward passBackward passâ„¬[ð‘€!,ðœƒ!]ðœ•ð½ðœ•ðœƒ#â„¬[ð‘€&,ðœƒ&]ðœ•ð½ðœ•ðœƒ$â„¬[ð‘€",ðœƒ"]ðœ•ð½ðœ•ðœƒ!105

7.4.3 Backward functions for basic modules

Using the general strategy in Section 7.4.2, it suï¬ƒces to compute the back-
ward function for all modules Miâ€™s used in the networks. We compute the
backward function for the basic module MM, activations Ïƒ, and loss functions
in this section.

Backward function for MM. Suppose MMW,b(z) = W z + b is a matrix multi-
plication module where z âˆˆ Rm and W âˆˆ RnÃ—m. Then, using equation (7.59),
we have for v âˆˆ Rn

B[MM, z](v) =

ï£®

ï£¯
ï£°

âˆ‚(W z+b)1
âˆ‚z1

...

âˆ‚(W z+b)1
âˆ‚zm

Â· Â· Â·
. . .
Â· Â· Â·

ï£¹

ï£º
ï£» v .

âˆ‚(W z+b)n
âˆ‚z1

...

âˆ‚(W z+b)n
âˆ‚zm

(7.62)

Using the fact that âˆ€i âˆˆ [m], j âˆˆ [n], âˆ‚(W z+b)j
have

âˆ‚zi

= âˆ‚bj +(cid:80)m

k=1 Wjkzk
âˆ‚zi

= Wji, we

B[MM, z](v) = W (cid:62)v âˆˆ Rm .

(7.63)

In the derivation above, we have treated MM as a function of z. If we treat
MM as a function of W and b, then we can also compute the backward
function for the parameter variables W and b.
Itâ€™s less convenient to use
equation (7.59) because the variable W is a matrix and the matrix in (7.59)
will be a 4-th order tensor that is challenging for us to mathematically write
down. We use (7.58) instead:

(B[MM, W ](v))ij =

m
(cid:88)

k=1

âˆ‚(W z + b)k
âˆ‚Wij

Â· vk =

m
(cid:88)

k=1

âˆ‚ (cid:80)m

s=1 Wkszs
âˆ‚Wij

Â· vk = vizj .

In vectorized notation, we have

B[MM, W ](v) = vz(cid:62) âˆˆ RnÃ—Ã—m .

Using equation (7.59) for the variable b, we have,

B[MM, b](v) =

ï£®

ï£¯
ï£°

âˆ‚(W z+b)1
âˆ‚b1

...

âˆ‚(W z+b)1
âˆ‚bn

Â· Â· Â·
. . .
Â· Â· Â·

âˆ‚(W z+b)n
âˆ‚b1

...

âˆ‚(W z+b)n
âˆ‚bn

ï£¹

ï£º
ï£» v = v .

(7.64)

(7.65)

(7.66)

106

Here we used that âˆ‚(W z+b)j

= 0 if i (cid:54)= j and âˆ‚(W z+b)j
The computational eï¬ƒciency for computing the backward function is
O(mn), the same as evaluating the result of matrix multiplication up to
constant factor.

= 1 if i = j.

âˆ‚bi

âˆ‚bi

Backward function for the activations. Suppose M (z) = Ïƒ(z) where Ïƒ is an
element-wise activation function and z âˆˆ Rm. Then, using equation (7.59),
we have

B[Ïƒ, z](v) =

ï£¹

ï£®

...

...

ï£¯
ï£°

âˆ‚Ïƒ(z1)
âˆ‚z1

âˆ‚Ïƒ(zm)
âˆ‚z1

Â· Â· Â·
. . .
Â· Â· Â·
= diag(Ïƒ(cid:48)(z1), Â· Â· Â· , Ïƒ(cid:48)(zm))v
= Ïƒ(cid:48)(z) (cid:12) v âˆˆ Rm .

âˆ‚Ïƒ(zm)
âˆ‚zm

âˆ‚Ïƒ(z1)
âˆ‚zm

ï£º
ï£» v

(7.67)

(7.68)
(7.69)

Here, we used the fact that âˆ‚Ïƒ(zj )
= 0 when j (cid:54)= i, diag(Î»1, . . . , Î»m) denotes
âˆ‚zi
the diagonal matrix with Î»1, . . . , Î»m on the diagonal, and (cid:12) denotes the
element-wise product of two vectors with the same dimension, and Ïƒ(cid:48)(Â·) is
the element-wise application of the derivative of the activation function Ïƒ.

Regarding computation eï¬ƒciency, we note that at the ï¬rst sight, equa-
tion (7.67) appears to indicate the backward function takes O(m2) time, but
equation (7.69) shows that itâ€™s implementable in O(m) time (which is the
same as the time for evaluating of the function.) We are not supposed to be
surprised by that the possibility of simplifying equation (7.67) to (7.69)â€”if
we use smaller modules, that is, treating the vector-to-vector nonlinear ac-
tivation as m scalar-to-scalar non-linear activation, then itâ€™s more obvious
that the backward pass should have similar time to the forward pass.

Backward function for loss functions. When a module M takes in a vector
z and outputs a scalar, by equation (7.59), the backward function takes in a
scalar v and outputs a vector with entries (B[M, z](v))i = âˆ‚M
v. Therefore,
âˆ‚zi
in vectorized notation, B[M, z](v) = âˆ‚M

âˆ‚z Â· v.
Recall that squared loss (cid:96)MSE(z, y) = 1

2(z âˆ’ y)2. Thus, B[(cid:96)MSE, z](v) =

âˆ‚ 1

2 (zâˆ’y)2
âˆ‚z
For logistics loss, by equation (2.6), we have

Â· v = (z âˆ’ y) Â· v.

B[(cid:96)logistic, t](v) =

âˆ‚(cid:96)logistic(t, y)
âˆ‚t

Â· v = (1/(1 + exp(âˆ’t)) âˆ’ y) Â· v .

(7.70)

107

For cross-entropy loss, by equation (2.17), we have

B[(cid:96)ce, t](v) =

âˆ‚(cid:96)ce(t, y)
âˆ‚t

Â· v = (Ï† âˆ’ ey) Â· v ,

(7.71)

where Ï† = softmax(t).

7.4.4 Back-propagation for MLPs

Given the backward functions for every module needed in evaluating the loss
of an MLP, we follow the strategy in Section 7.4.2 to compute the gradient
of the loss w.r.t to the hidden activations and the parameters.

We consider the an r-layer MLP with a logistic loss. The loss function

can be computed via a sequence of operations (that is, the forward pass),

z[1] = MMW [1],b[1](x),
a[1] = Ïƒ(z[1])
z[2] = MMW [2],b[2](a[1])
a[2] = Ïƒ(z[2])

...

z[r] = MMW [r],b[r](a[râˆ’1])
J = (cid:96)logistic(z[r], y) .

(7.72)

We apply the backward function sequentially in a backward order. First, we
have that

âˆ‚J
âˆ‚z[r] = B[(cid:96)logistic, z[r]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚J

= B[(cid:96)logistic, z[r]](1) .

(7.73)

Then, we iteratively compute âˆ‚J
rule (equation (7.58)),

âˆ‚a[i] and âˆ‚J

âˆ‚z[i] â€™s by repeatedly invoking the chain

âˆ‚J
âˆ‚a[râˆ’1] = B[MM, a[râˆ’1]]
âˆ‚J
âˆ‚z[râˆ’1] = B[Ïƒ, z[râˆ’1]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚z[r]

(cid:18) âˆ‚J

(cid:19)

âˆ‚a[râˆ’1]

...

âˆ‚J
âˆ‚z[1] = B[Ïƒ, z[1]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚a[1]

.

(7.74)

108

Numerically, we compute these quantities by repeatedly invoking equa-
tions (7.69) and (7.63) with diï¬€erent choices of variables.

We note that the intermediate values of a[i] and z[i] are used in the back-
propagation (equation (7.74)), and therefore these values need to be stored
in the memory after the forward pass.

Next, we compute the gradient of the parameters by invoking equa-

tions (7.65) and (7.66),

(cid:19)

âˆ‚J
âˆ‚W [r] = B[MM, W [r]]
âˆ‚J
âˆ‚b[r] = B[MM, b[r]]

(cid:18) âˆ‚J
âˆ‚z[r]
(cid:19)

(cid:18) âˆ‚J
âˆ‚z[r]

...

(cid:19)

âˆ‚J
âˆ‚W [1] = B[MM, W [1]]
âˆ‚J
âˆ‚b[1] = B[MM, b[1]]

(cid:18) âˆ‚J
âˆ‚z[1]
(cid:19)

(cid:18) âˆ‚J
âˆ‚z[1]

.

(7.75)

âˆ‚J

We also note that the block of computations in equations (7.75) can be
interleaved with the block of computation in equations (7.74) because the
âˆ‚W [i] and âˆ‚J

âˆ‚b[i] can be computed as soon as âˆ‚J

âˆ‚z[i] is computed.

Putting all of

these together, and explicitly invoking the equa-
tions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).

109

Algorithm 3 Back-propagation for multi-layer neural networks.
1: Forward pass. Compute and store the values of a[k]â€™s, z[k]â€™s, and J

using the equations (7.72).

2: Backward pass. Compute the gradient of loss J with respect to z[r]:

âˆ‚J

âˆ‚z[r] = B[(cid:96)logistic, z[r]](1) = (cid:0)1/(1 + exp(âˆ’z[r])) âˆ’ y(cid:1) .

(7.76)

3: for k = r âˆ’ 1 to 0 do
4:

Compute the gradient with respect to parameters W [k+1] and b[k+1].

âˆ‚J

âˆ‚W [k+1] = B[MM, W [k+1]]
âˆ‚J

.

=

âˆ‚z[k+1] a[k](cid:62)
âˆ‚J
âˆ‚b[k+1] = B[MM, b[k+1]]
âˆ‚J
âˆ‚z[k+1] .

=

(cid:18) âˆ‚J

(cid:19)

âˆ‚z[k+1]

(cid:18) âˆ‚J

(cid:19)

âˆ‚z[k+1]

(7.77)

(7.78)

5: When k â‰¥ 1, compute the gradient with respect to z[k] and a[k].

âˆ‚J
âˆ‚a[k] = B[Ïƒ, a[k]]

(cid:18) âˆ‚J

(cid:19)

âˆ‚z[k+1]

= W [k+1](cid:62) âˆ‚J

âˆ‚z[k+1] .

âˆ‚J
âˆ‚z[k] = B[Ïƒ, z[k]]
= Ïƒ(cid:48)(z[k]) (cid:12)

(cid:19)

(cid:18) âˆ‚J
âˆ‚a[k]
âˆ‚J
âˆ‚a[k] .

(7.79)

(7.80)

7.5 Vectorization over training examples

As we discussed in Section 7.1, in the implementation of neural networks,
we will leverage the parallelism across the multiple examples. This means
that we will need to write the forward pass (the evaluation of the outputs)
of the neural network and the backward pass (backpropagation) for multiple

110

training examples in matrix notation.

The basic idea.
The basic idea is simple. Suppose
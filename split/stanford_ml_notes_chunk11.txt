i−1] but not any ∂J

∂θ[i] only depends on ∂J

∂θ[i] from ∂J

∂u[k] , . . . , ∂J

∂J

∂J

We ﬁrst see why

∂u[i−1] can be computed eﬃciently from ∂J

∂u[i] and u[i−1]
by invoking the discussion in Section 7.4.1 on the chain rule. We in-
stantiate the discussion by setting u = u[i] and z = u[i−1], and f (u) =
Mk(Mk−1(· · · Mi+1(u[i]))), and g(·) = Mi(·). Note that f is very complex
but we don’t need any concrete information about f . Then, the conclusive
equation (7.56) corresponds to

∂J
∂u[i]

chain rule
==========================⇒
only requires info about Mi(·) and u[i−1]

∂J
∂u[i−1] .

(7.61)

More precisely, we can write, following equation (7.57)

∂J
∂u[i−1] = B[Mi, u[i−1]]

(cid:19)

(cid:18) ∂J
∂u[i]

.

Instantiating the chain rule with z = θ[i] and u = u[i], we also have

∂J
∂θ[i] = B[Mi, θ[i]]

(cid:19)

(cid:18) ∂J
∂u[i]

.

See Figure 7.5 for an illustration of the algorithm.

(B1)

(B2)

104

Figure 7.5: Back-propagation.

Remark 7.4.3: [Computational eﬃciency and granularity of the modules]
The main underlying purpose of treating a complex network as compositions
of small modules is that small modules tend to have eﬃciently implementable
backward function. In fact, the backward functions of all the atomic modules
such as addition, multiplication and ReLU can be computed as eﬃciently as
the the evaluation of these modules (up to multiplicative constant factor).
Using this fact, we can prove Theorem 7.4.1 by viewing neural networks as
compositions of many atomic operations, and invoking the backpropagation
discussed above. However, in practice, it’s oftentimes more convenient to
modularize the networks using modules on the level of matrix multiplication,
layernorm, etc. As we will see, naive implementation of these operations’
backward functions also have the same runtime as the evaluation of these
functions.

𝑥...𝑀!𝐽...𝑀"𝑢[!]𝑢["%!]𝜕𝐽𝜕𝐽ℬ[𝑀",𝑢["%!]]𝜕𝐽𝜕𝑢!"#𝑢[&]𝑀&𝑢[&%!]ℬ[𝑀&,𝑢[&%!]]𝜕𝐽𝜕𝑢$"#𝜕𝐽𝜕𝑢$...𝜕𝐽𝜕𝑢#...Forward passBackward passℬ[𝑀!,𝜃!]𝜕𝐽𝜕𝜃#ℬ[𝑀&,𝜃&]𝜕𝐽𝜕𝜃$ℬ[𝑀",𝜃"]𝜕𝐽𝜕𝜃!105

7.4.3 Backward functions for basic modules

Using the general strategy in Section 7.4.2, it suﬃces to compute the back-
ward function for all modules Mi’s used in the networks. We compute the
backward function for the basic module MM, activations σ, and loss functions
in this section.

Backward function for MM. Suppose MMW,b(z) = W z + b is a matrix multi-
plication module where z ∈ Rm and W ∈ Rn×m. Then, using equation (7.59),
we have for v ∈ Rn

B[MM, z](v) =






∂(W z+b)1
∂z1

...

∂(W z+b)1
∂zm

· · ·
. . .
· · ·




 v .

∂(W z+b)n
∂z1

...

∂(W z+b)n
∂zm

(7.62)

Using the fact that ∀i ∈ [m], j ∈ [n], ∂(W z+b)j
have

∂zi

= ∂bj +(cid:80)m

k=1 Wjkzk
∂zi

= Wji, we

B[MM, z](v) = W (cid:62)v ∈ Rm .

(7.63)

In the derivation above, we have treated MM as a function of z. If we treat
MM as a function of W and b, then we can also compute the backward
function for the parameter variables W and b.
It’s less convenient to use
equation (7.59) because the variable W is a matrix and the matrix in (7.59)
will be a 4-th order tensor that is challenging for us to mathematically write
down. We use (7.58) instead:

(B[MM, W ](v))ij =

m
(cid:88)

k=1

∂(W z + b)k
∂Wij

· vk =

m
(cid:88)

k=1

∂ (cid:80)m

s=1 Wkszs
∂Wij

· vk = vizj .

In vectorized notation, we have

B[MM, W ](v) = vz(cid:62) ∈ Rn××m .

Using equation (7.59) for the variable b, we have,

B[MM, b](v) =






∂(W z+b)1
∂b1

...

∂(W z+b)1
∂bn

· · ·
. . .
· · ·

∂(W z+b)n
∂b1

...

∂(W z+b)n
∂bn




 v = v .

(7.64)

(7.65)

(7.66)

106

Here we used that ∂(W z+b)j

= 0 if i (cid:54)= j and ∂(W z+b)j
The computational eﬃciency for computing the backward function is
O(mn), the same as evaluating the result of matrix multiplication up to
constant factor.

= 1 if i = j.

∂bi

∂bi

Backward function for the activations. Suppose M (z) = σ(z) where σ is an
element-wise activation function and z ∈ Rm. Then, using equation (7.59),
we have

B[σ, z](v) =





...

...




∂σ(z1)
∂z1

∂σ(zm)
∂z1

· · ·
. . .
· · ·
= diag(σ(cid:48)(z1), · · · , σ(cid:48)(zm))v
= σ(cid:48)(z) (cid:12) v ∈ Rm .

∂σ(zm)
∂zm

∂σ(z1)
∂zm


 v

(7.67)

(7.68)
(7.69)

Here, we used the fact that ∂σ(zj )
= 0 when j (cid:54)= i, diag(λ1, . . . , λm) denotes
∂zi
the diagonal matrix with λ1, . . . , λm on the diagonal, and (cid:12) denotes the
element-wise product of two vectors with the same dimension, and σ(cid:48)(·) is
the element-wise application of the derivative of the activation function σ.

Regarding computation eﬃciency, we note that at the ﬁrst sight, equa-
tion (7.67) appears to indicate the backward function takes O(m2) time, but
equation (7.69) shows that it’s implementable in O(m) time (which is the
same as the time for evaluating of the function.) We are not supposed to be
surprised by that the possibility of simplifying equation (7.67) to (7.69)—if
we use smaller modules, that is, treating the vector-to-vector nonlinear ac-
tivation as m scalar-to-scalar non-linear activation, then it’s more obvious
that the backward pass should have similar time to the forward pass.

Backward function for loss functions. When a module M takes in a vector
z and outputs a scalar, by equation (7.59), the backward function takes in a
scalar v and outputs a vector with entries (B[M, z](v))i = ∂M
v. Therefore,
∂zi
in vectorized notation, B[M, z](v) = ∂M

∂z · v.
Recall that squared loss (cid:96)MSE(z, y) = 1

2(z − y)2. Thus, B[(cid:96)MSE, z](v) =

∂ 1

2 (z−y)2
∂z
For logistics loss, by equation (2.6), we have

· v = (z − y) · v.

B[(cid:96)logistic, t](v) =

∂(cid:96)logistic(t, y)
∂t

· v = (1/(1 + exp(−t)) − y) · v .

(7.70)

107

For cross-entropy loss, by equation (2.17), we have

B[(cid:96)ce, t](v) =

∂(cid:96)ce(t, y)
∂t

· v = (φ − ey) · v ,

(7.71)

where φ = softmax(t).

7.4.4 Back-propagation for MLPs

Given the backward functions for every module needed in evaluating the loss
of an MLP, we follow the strategy in Section 7.4.2 to compute the gradient
of the loss w.r.t to the hidden activations and the parameters.

We consider the an r-layer MLP with a logistic loss. The loss function

can be computed via a sequence of operations (that is, the forward pass),

z[1] = MMW [1],b[1](x),
a[1] = σ(z[1])
z[2] = MMW [2],b[2](a[1])
a[2] = σ(z[2])

...

z[r] = MMW [r],b[r](a[r−1])
J = (cid:96)logistic(z[r], y) .

(7.72)

We apply the backward function sequentially in a backward order. First, we
have that

∂J
∂z[r] = B[(cid:96)logistic, z[r]]

(cid:19)

(cid:18) ∂J
∂J

= B[(cid:96)logistic, z[r]](1) .

(7.73)

Then, we iteratively compute ∂J
rule (equation (7.58)),

∂a[i] and ∂J

∂z[i] ’s by repeatedly invoking the chain

∂J
∂a[r−1] = B[MM, a[r−1]]
∂J
∂z[r−1] = B[σ, z[r−1]]

(cid:19)

(cid:18) ∂J
∂z[r]

(cid:18) ∂J

(cid:19)

∂a[r−1]

...

∂J
∂z[1] = B[σ, z[1]]

(cid:19)

(cid:18) ∂J
∂a[1]

.

(7.74)

108

Numerically, we compute these quantities by repeatedly invoking equa-
tions (7.69) and (7.63) with diﬀerent choices of variables.

We note that the intermediate values of a[i] and z[i] are used in the back-
propagation (equation (7.74)), and therefore these values need to be stored
in the memory after the forward pass.

Next, we compute the gradient of the parameters by invoking equa-

tions (7.65) and (7.66),

(cid:19)

∂J
∂W [r] = B[MM, W [r]]
∂J
∂b[r] = B[MM, b[r]]

(cid:18) ∂J
∂z[r]
(cid:19)

(cid:18) ∂J
∂z[r]

...

(cid:19)

∂J
∂W [1] = B[MM, W [1]]
∂J
∂b[1] = B[MM, b[1]]

(cid:18) ∂J
∂z[1]
(cid:19)

(cid:18) ∂J
∂z[1]

.

(7.75)

∂J

We also note that the block of computations in equations (7.75) can be
interleaved with the block of computation in equations (7.74) because the
∂W [i] and ∂J

∂b[i] can be computed as soon as ∂J

∂z[i] is computed.

Putting all of

these together, and explicitly invoking the equa-
tions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).

109

Algorithm 3 Back-propagation for multi-layer neural networks.
1: Forward pass. Compute and store the values of a[k]’s, z[k]’s, and J

using the equations (7.72).

2: Backward pass. Compute the gradient of loss J with respect to z[r]:

∂J

∂z[r] = B[(cid:96)logistic, z[r]](1) = (cid:0)1/(1 + exp(−z[r])) − y(cid:1) .

(7.76)

3: for k = r − 1 to 0 do
4:

Compute the gradient with respect to parameters W [k+1] and b[k+1].

∂J

∂W [k+1] = B[MM, W [k+1]]
∂J

.

=

∂z[k+1] a[k](cid:62)
∂J
∂b[k+1] = B[MM, b[k+1]]
∂J
∂z[k+1] .

=

(cid:18) ∂J

(cid:19)

∂z[k+1]

(cid:18) ∂J

(cid:19)

∂z[k+1]

(7.77)

(7.78)

5: When k ≥ 1, compute the gradient with respect to z[k] and a[k].

∂J
∂a[k] = B[σ, a[k]]

(cid:18) ∂J

(cid:19)

∂z[k+1]

= W [k+1](cid:62) ∂J

∂z[k+1] .

∂J
∂z[k] = B[σ, z[k]]
= σ(cid:48)(z[k]) (cid:12)

(cid:19)

(cid:18) ∂J
∂a[k]
∂J
∂a[k] .

(7.79)

(7.80)

7.5 Vectorization over training examples

As we discussed in Section 7.1, in the implementation of neural networks,
we will leverage the parallelism across the multiple examples. This means
that we will need to write the forward pass (the evaluation of the outputs)
of the neural network and the backward pass (backpropagation) for multiple

110

training examples in matrix notation.

The basic idea.
The basic idea is simple. Suppose you have a training
set with three examples x(1), x(2), x(3). The ﬁrst-layer activations for each
example are as follows:

z[1](1) = W [1]x(1) + b[1]
z[1](2) = W [1]x(2) + b[1]
z[1](3) = W [1]x(3) + b[1]

Note the diﬀerence between square brackets [·], which refer to the layer num-
ber, and parenthesis (·), which refer to the training example number.
In-
tuitively, one would implement this using a for loop. It turns out, we can
vectorize these operations as well. First, deﬁne:





X =

|

|

|
x(1) x(2) x(3)
|

|

|


 ∈ Rd×3

(7.81)

Note that we are stacking training examples in columns and not rows. We
can then combine this into a single uniﬁed formulation:





Z [1] =

|

|

|
z[1](1) z[1](2) z[1](3)
|

|

|


 = W [1]X + b[1]

(7.82)

You may notice that we are attempting to add b[1] ∈ R4×1 to W [1]X ∈
R4×3. Strictly following the rules of linear algebra, this is not allowed. In
practice however, this addition is performed using broadcasting. We create
an intermediate ˜b[1] ∈ R4×3:

˜b[1] =





|
b[1]
|

|
b[1]
|





|
b[1]
|

(7.83)

We can then perform the computation: Z [1] = W [1]X + ˜b[1]. Often times, it
is not necessary to explicitly construct ˜b[1]. By inspecting the dimensions in
(7.82), you can assume b[1] ∈ R4×1 is correctly broadcast to W [1]X ∈ R4×3.

The matricization approach as above can easily generalize to multiple

layers, with one subtlety though, as discussed below.

111

Complications/Subtlety in the Implementation. All the deep learn-
ing packages or implementations put the data points in the rows of a data
matrix. (If the data point itself is a matrix or tensor, then the data are con-
centrated along the zero-th dimension.) However, most of the deep learning
papers use a similar notation to these notes where the data points are treated
as column vectors.8 There is a simple conversion to deal with the mismatch:
in the implementation, all the columns become row vectors, row vectors be-
come column vectors, all the matrices are transposed, and the orders of the
matrix multiplications are ﬂipped. In the example above, using the row ma-
jor convention, the data matrix is X ∈ R3×d, the ﬁrst layer weight matrix
has dimensionality d × m (instead of m × d as in the two layer neural net
section), and the bias vector b[1] ∈ R1×m. The computation for the hidden
activation becomes

Z [1] = XW [1] + b[1] ∈ R3×m

(7.84)

8The instructor suspects that this is mostly because in mathematics we naturally mul-

tiply a matrix to a vector on the left hand side.

Part III

Generalization and
regularization

112

Chapter 8

Generalization

This chapter discusses tools to analyze and understand the generaliza-
i.e, their performances on unseen test
tion of machine learning models,
examples. Recall that for supervised learning problems, given a train-
ing dataset {(x(i), y(i))}n
i=1, we typically learn a model hθ by minimizing a
loss/cost function J(θ), which encourages hθ to ﬁt the data. E.g., when
the loss function is the least square loss (aka mean squared error), we have
J(θ) = 1
i=1(y(i) − hθ(x(i)))2. This loss function for training purposes is
n
oftentimes referred to as the training loss/error/cost.

(cid:80)n

However, minimizing the training loss is not our ultimate goal—it is
merely our approach towards the goal of learning a predictive model. The
most important evaluation metric of a model is the loss on unseen test exam-
ples, which is oftentimes referred to as the test error. Formally, we sample a
test example (x, y) from the so-called test distribution D, and measure the
model’s error on it, by, e.g., the mean squared error, (hθ(x) − y)2. The ex-
pected loss/error over the randomness of the test example is called the test
loss/error,1

L(θ) = E(x,y)∼D[(y − hθ(x))2]

(8.1)

Note that the measurement of the error involves computing the expectation,
and in practice, it can be approximated by the average error on many sampled
test examples, which are referred to as the test dataset. Note that the key
diﬀerence here between training and test datasets is that the test examples

1In theoretical and statistical literature, we oftentimes call the uniform distribution
over the training set {(x(i), y(i))}n
i=1, denoted by (cid:98)D, an empirical distribution, and call
D the population distribution. Partly because of this, the training loss is also referred
to as the empirical loss/risk/error, and the test loss is also referred to as the population
loss/risk/error.

113

114

are unseen, in the sense that the training procedure has not used the test
examples. In classical statistical learning settings, the training examples are
also drawn from the same distribution as the test distribution D, but still
the test examples are unseen by the learning procedure whereas the training
examples are seen.2

Because of this key diﬀerence between training and test datasets, even
if they are both drawn from the same distribution D, the test error is not
necessarily always close to the training error.3 As a result, successfully min-
imizing the training error may not always lead to a small test error. We
typically say the model overﬁts the data if the model predicts accurately on
the training dataset but doesn’t generalize well to other test examples, that
is, if the training error is small but the test error is large. We say the model
underﬁts the data if the training error is relatively large4 (and in this case,
typically the test error is also relatively large.)

This chapter studies how the test error is inﬂuenced by the learning pro-
cedure, especially the choice of model parameterizations. We will decompose
the test error into “bias” and “variance” terms and study how each of them is
aﬀected by the choice of model parameterizations and their tradeoﬀs. Using
the bias-variance tradeoﬀ, we will discuss when overﬁtting and underﬁtting
will occur and be avoided. We will also discuss the double descent phe-
nomenon in Section 8.2 and some classical theoretical results in Section 8.3.

2These days, researchers have increasingly been more interested in the setting with

“domain shift”, that is, the training distribution and test distribution are diﬀerent.

3the diﬀerence between test error and training error is often referred to as the gener-
alization gap. The term generalization error in some literature means the test erro
iâˆ’1] but not any âˆ‚J

âˆ‚Î¸[i] only depends on âˆ‚J

âˆ‚Î¸[i] from âˆ‚J

âˆ‚u[k] , . . . , âˆ‚J

âˆ‚J

âˆ‚J

We ï¬rst see why

âˆ‚u[iâˆ’1] can be computed eï¬ƒciently from âˆ‚J

âˆ‚u[i] and u[iâˆ’1]
by invoking the discussion in Section 7.4.1 on the chain rule. We in-
stantiate the discussion by setting u = u[i] and z = u[iâˆ’1], and f (u) =
Mk(Mkâˆ’1(Â· Â· Â· Mi+1(u[i]))), and g(Â·) = Mi(Â·). Note that f is very complex
but we donâ€™t need any concrete information about f . Then, the conclusive
equation (7.56) corresponds to

âˆ‚J
âˆ‚u[i]

chain rule
==========================â‡’
only requires info about Mi(Â·) and u[iâˆ’1]

âˆ‚J
âˆ‚u[iâˆ’1] .

(7.61)

More precisely, we can write, following equation (7.57)

âˆ‚J
âˆ‚u[iâˆ’1] = B[Mi, u[iâˆ’1]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚u[i]

.

Instantiating the chain rule with z = Î¸[i] and u = u[i], we also have

âˆ‚J
âˆ‚Î¸[i] = B[Mi, Î¸[i]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚u[i]

.

See Figure 7.5 for an illustration of the algorithm.

(B1)

(B2)

104

Figure 7.5: Back-propagation.

Remark 7.4.3: [Computational eï¬ƒciency and granularity of the modules]
The main underlying purpose of treating a complex network as compositions
of small modules is that small modules tend to have eï¬ƒciently implementable
backward function. In fact, the backward functions of all the atomic modules
such as addition, multiplication and ReLU can be computed as eï¬ƒciently as
the the evaluation of these modules (up to multiplicative constant factor).
Using this fact, we can prove Theorem 7.4.1 by viewing neural networks as
compositions of many atomic operations, and invoking the backpropagation
discussed above. However, in practice, itâ€™s oftentimes more convenient to
modularize the networks using modules on the level of matrix multiplication,
layernorm, etc. As we will see, naive implementation of these operationsâ€™
backward functions also have the same runtime as the evaluation of these
functions.

ğ‘¥...ğ‘€!ğ½...ğ‘€"ğ‘¢[!]ğ‘¢["%!]ğœ•ğ½ğœ•ğ½â„¬[ğ‘€",ğ‘¢["%!]]ğœ•ğ½ğœ•ğ‘¢!"#ğ‘¢[&]ğ‘€&ğ‘¢[&%!]â„¬[ğ‘€&,ğ‘¢[&%!]]ğœ•ğ½ğœ•ğ‘¢$"#ğœ•ğ½ğœ•ğ‘¢$...ğœ•ğ½ğœ•ğ‘¢#...Forward passBackward passâ„¬[ğ‘€!,ğœƒ!]ğœ•ğ½ğœ•ğœƒ#â„¬[ğ‘€&,ğœƒ&]ğœ•ğ½ğœ•ğœƒ$â„¬[ğ‘€",ğœƒ"]ğœ•ğ½ğœ•ğœƒ!105

7.4.3 Backward functions for basic modules

Using the general strategy in Section 7.4.2, it suï¬ƒces to compute the back-
ward function for all modules Miâ€™s used in the networks. We compute the
backward function for the basic module MM, activations Ïƒ, and loss functions
in this section.

Backward function for MM. Suppose MMW,b(z) = W z + b is a matrix multi-
plication module where z âˆˆ Rm and W âˆˆ RnÃ—m. Then, using equation (7.59),
we have for v âˆˆ Rn

B[MM, z](v) =

ï£®

ï£¯
ï£°

âˆ‚(W z+b)1
âˆ‚z1

...

âˆ‚(W z+b)1
âˆ‚zm

Â· Â· Â·
. . .
Â· Â· Â·

ï£¹

ï£º
ï£» v .

âˆ‚(W z+b)n
âˆ‚z1

...

âˆ‚(W z+b)n
âˆ‚zm

(7.62)

Using the fact that âˆ€i âˆˆ [m], j âˆˆ [n], âˆ‚(W z+b)j
have

âˆ‚zi

= âˆ‚bj +(cid:80)m

k=1 Wjkzk
âˆ‚zi

= Wji, we

B[MM, z](v) = W (cid:62)v âˆˆ Rm .

(7.63)

In the derivation above, we have treated MM as a function of z. If we treat
MM as a function of W and b, then we can also compute the backward
function for the parameter variables W and b.
Itâ€™s less convenient to use
equation (7.59) because the variable W is a matrix and the matrix in (7.59)
will be a 4-th order tensor that is challenging for us to mathematically write
down. We use (7.58) instead:

(B[MM, W ](v))ij =

m
(cid:88)

k=1

âˆ‚(W z + b)k
âˆ‚Wij

Â· vk =

m
(cid:88)

k=1

âˆ‚ (cid:80)m

s=1 Wkszs
âˆ‚Wij

Â· vk = vizj .

In vectorized notation, we have

B[MM, W ](v) = vz(cid:62) âˆˆ RnÃ—Ã—m .

Using equation (7.59) for the variable b, we have,

B[MM, b](v) =

ï£®

ï£¯
ï£°

âˆ‚(W z+b)1
âˆ‚b1

...

âˆ‚(W z+b)1
âˆ‚bn

Â· Â· Â·
. . .
Â· Â· Â·

âˆ‚(W z+b)n
âˆ‚b1

...

âˆ‚(W z+b)n
âˆ‚bn

ï£¹

ï£º
ï£» v = v .

(7.64)

(7.65)

(7.66)

106

Here we used that âˆ‚(W z+b)j

= 0 if i (cid:54)= j and âˆ‚(W z+b)j
The computational eï¬ƒciency for computing the backward function is
O(mn), the same as evaluating the result of matrix multiplication up to
constant factor.

= 1 if i = j.

âˆ‚bi

âˆ‚bi

Backward function for the activations. Suppose M (z) = Ïƒ(z) where Ïƒ is an
element-wise activation function and z âˆˆ Rm. Then, using equation (7.59),
we have

B[Ïƒ, z](v) =

ï£¹

ï£®

...

...

ï£¯
ï£°

âˆ‚Ïƒ(z1)
âˆ‚z1

âˆ‚Ïƒ(zm)
âˆ‚z1

Â· Â· Â·
. . .
Â· Â· Â·
= diag(Ïƒ(cid:48)(z1), Â· Â· Â· , Ïƒ(cid:48)(zm))v
= Ïƒ(cid:48)(z) (cid:12) v âˆˆ Rm .

âˆ‚Ïƒ(zm)
âˆ‚zm

âˆ‚Ïƒ(z1)
âˆ‚zm

ï£º
ï£» v

(7.67)

(7.68)
(7.69)

Here, we used the fact that âˆ‚Ïƒ(zj )
= 0 when j (cid:54)= i, diag(Î»1, . . . , Î»m) denotes
âˆ‚zi
the diagonal matrix with Î»1, . . . , Î»m on the diagonal, and (cid:12) denotes the
element-wise product of two vectors with the same dimension, and Ïƒ(cid:48)(Â·) is
the element-wise application of the derivative of the activation function Ïƒ.

Regarding computation eï¬ƒciency, we note that at the ï¬rst sight, equa-
tion (7.67) appears to indicate the backward function takes O(m2) time, but
equation (7.69) shows that itâ€™s implementable in O(m) time (which is the
same as the time for evaluating of the function.) We are not supposed to be
surprised by that the possibility of simplifying equation (7.67) to (7.69)â€”if
we use smaller modules, that is, treating the vector-to-vector nonlinear ac-
tivation as m scalar-to-scalar non-linear activation, then itâ€™s more obvious
that the backward pass should have similar time to the forward pass.

Backward function for loss functions. When a module M takes in a vector
z and outputs a scalar, by equation (7.59), the backward function takes in a
scalar v and outputs a vector with entries (B[M, z](v))i = âˆ‚M
v. Therefore,
âˆ‚zi
in vectorized notation, B[M, z](v) = âˆ‚M

âˆ‚z Â· v.
Recall that squared loss (cid:96)MSE(z, y) = 1

2(z âˆ’ y)2. Thus, B[(cid:96)MSE, z](v) =

âˆ‚ 1

2 (zâˆ’y)2
âˆ‚z
For logistics loss, by equation (2.6), we have

Â· v = (z âˆ’ y) Â· v.

B[(cid:96)logistic, t](v) =

âˆ‚(cid:96)logistic(t, y)
âˆ‚t

Â· v = (1/(1 + exp(âˆ’t)) âˆ’ y) Â· v .

(7.70)

107

For cross-entropy loss, by equation (2.17), we have

B[(cid:96)ce, t](v) =

âˆ‚(cid:96)ce(t, y)
âˆ‚t

Â· v = (Ï† âˆ’ ey) Â· v ,

(7.71)

where Ï† = softmax(t).

7.4.4 Back-propagation for MLPs

Given the backward functions for every module needed in evaluating the loss
of an MLP, we follow the strategy in Section 7.4.2 to compute the gradient
of the loss w.r.t to the hidden activations and the parameters.

We consider the an r-layer MLP with a logistic loss. The loss function

can be computed via a sequence of operations (that is, the forward pass),

z[1] = MMW [1],b[1](x),
a[1] = Ïƒ(z[1])
z[2] = MMW [2],b[2](a[1])
a[2] = Ïƒ(z[2])

...

z[r] = MMW [r],b[r](a[râˆ’1])
J = (cid:96)logistic(z[r], y) .

(7.72)

We apply the backward function sequentially in a backward order. First, we
have that

âˆ‚J
âˆ‚z[r] = B[(cid:96)logistic, z[r]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚J

= B[(cid:96)logistic, z[r]](1) .

(7.73)

Then, we iteratively compute âˆ‚J
rule (equation (7.58)),

âˆ‚a[i] and âˆ‚J

âˆ‚z[i] â€™s by repeatedly invoking the chain

âˆ‚J
âˆ‚a[râˆ’1] = B[MM, a[râˆ’1]]
âˆ‚J
âˆ‚z[râˆ’1] = B[Ïƒ, z[râˆ’1]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚z[r]

(cid:18) âˆ‚J

(cid:19)

âˆ‚a[râˆ’1]

...

âˆ‚J
âˆ‚z[1] = B[Ïƒ, z[1]]

(cid:19)

(cid:18) âˆ‚J
âˆ‚a[1]

.

(7.74)

108

Numerically, we compute these quantities by repeatedly invoking equa-
tions (7.69) and (7.63) with diï¬€erent choices of variables.

We note that the intermediate values of a[i] and z[i] are used in the back-
propagation (equation (7.74)), and therefore these values need to be stored
in the memory after the forward pass.

Next, we compute the gradient of the parameters by invoking equa-

tions (7.65) and (7.66),

(cid:19)

âˆ‚J
âˆ‚W [r] = B[MM, W [r]]
âˆ‚J
âˆ‚b[r] = B[MM, b[r]]

(cid:18) âˆ‚J
âˆ‚z[r]
(cid:19)

(cid:18) âˆ‚J
âˆ‚z[r]

...

(cid:19)

âˆ‚J
âˆ‚W [1] = B[MM, W [1]]
âˆ‚J
âˆ‚b[1] = B[MM, b[1]]

(cid:18) âˆ‚J
âˆ‚z[1]
(cid:19)

(cid:18) âˆ‚J
âˆ‚z[1]

.

(7.75)

âˆ‚J

We also note that the block of computations in equations (7.75) can be
interleaved with the block of computation in equations (7.74) because the
âˆ‚W [i] and âˆ‚J

âˆ‚b[i] can be computed as soon as âˆ‚J

âˆ‚z[i] is computed.

Putting all of

these together, and explicitly invoking the equa-
tions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).

109

Algorithm 3 Back-propagation for multi-layer neural networks.
1: Forward pass. Compute and store the values of a[k]â€™s, z[k]â€™s, and J

using the equations (7.72).

2: Backward pass. Compute the gradient of loss J with respect to z[r]:

âˆ‚J

âˆ‚z[r] = B[(cid:96)logistic, z[r]](1) = (cid:0)1/(1 + exp(âˆ’z[r])) âˆ’ y(cid:1) .

(7.76)

3: for k = r âˆ’ 1 to 0 do
4:

Compute the gradient with respect to parameters W [k+1] and b[k+1].

âˆ‚J

âˆ‚W [k+1] = B[MM, W [k+1]]
âˆ‚J

.

=

âˆ‚z[k+1] a[k](cid:62)
âˆ‚J
âˆ‚b[k+1] = B[MM, b[k+1]]
âˆ‚J
âˆ‚z[k+1] .

=

(cid:18) âˆ‚J

(cid:19)

âˆ‚z[k+1]

(cid:18) âˆ‚J

(cid:19)

âˆ‚z[k+1]

(7.77)

(7.78)

5: When k â‰¥ 1, compute the gradient with respect to z[k] and a[k].

âˆ‚J
âˆ‚a[k] = B[Ïƒ, a[k]]

(cid:18) âˆ‚J

(cid:19)

âˆ‚z[k+1]

= W [k+1](cid:62) âˆ‚J

âˆ‚z[k+1] .

âˆ‚J
âˆ‚z[k] = B[Ïƒ, z[k]]
= Ïƒ(cid:48)(z[k]) (cid:12)

(cid:19)

(cid:18) âˆ‚J
âˆ‚a[k]
âˆ‚J
âˆ‚a[k] .

(7.79)

(7.80)

7.5 Vectorization over training examples

As we discussed in Section 7.1, in the implementation of neural networks,
we will leverage the parallelism across the multiple examples. This means
that we will need to write the forward pass (the evaluation of the outputs)
of the neural network and the backward pass (backpropagation) for multiple

110

training examples in matrix notation.

The basic idea.
The basic idea is simple. Suppose you have a training
set with three examples x(1), x(2), x(3). The ï¬rst-layer activations for each
example are as follows:

z[1](1) = W [1]x(1) + b[1]
z[1](2) = W [1]x(2) + b[1]
z[1](3) = W [1]x(3) + b[1]

Note the diï¬€erence between square brackets [Â·], which refer to the layer num-
ber, and parenthesis (Â·), which refer to the training example number.
In-
tuitively, one would implement this using a for loop. It turns out, we can
vectorize these operations as well. First, deï¬ne:

ï£®

ï£°

X =

|

|

|
x(1) x(2) x(3)
|

|

|

ï£¹
ï£» âˆˆ RdÃ—3

(7.81)

Note that we are stacking training examples in columns and not rows. We
can then combine this into a single uniï¬ed formulation:

ï£®

ï£°

Z [1] =

|

|

|
z[1](1) z[1](2) z[1](3)
|

|

|

ï£¹
ï£» = W [1]X + b[1]

(7.82)

You may notice that we are attempting to add b[1] âˆˆ R4Ã—1 to W [1]X âˆˆ
R4Ã—3. Strictly following the rules of linear algebra, this is not allowed. In
practice however, this addition is performed using broadcasting. We create
an intermediate Ëœb[1] âˆˆ R4Ã—3:

Ëœb[1] =

ï£®

ï£°

|
b[1]
|

|
b[1]
|

ï£¹

ï£»

|
b[1]
|

(7.83)

We can then perform the computation: Z [1] = W [1]X + Ëœb[1]. Often times, it
is not necessary to explicitly construct Ëœb[1]. By inspecting the dimensions in
(7.82), you can assume b[1] âˆˆ R4Ã—1 is correctly broadcast to W [1]X âˆˆ R4Ã—3.

The matricization approach as above can easily generalize to multiple

layers, with one subtlety though, as discussed below.

111

Complications/Subtlety in the Implementation. All the deep learn-
ing packages or implementations put the data points in the rows of a data
matrix. (If the data point itself is a matrix or tensor, then the data are con-
centrated along the zero-th dimension.) However, most of the deep learning
papers use a similar notation to these notes where the data points are treated
as column vectors.8 There is a simple conversion to deal with the mismatch:
in the implementation, all the columns become row vectors, row vectors be-
come column vectors, all the matrices are transposed, and the orders of the
matrix multiplications are ï¬‚ipped. In the example above, using the row ma-
jor convention, the data matrix is X âˆˆ R3Ã—d, the ï¬rst layer weight matrix
has dimensionality d Ã— m (instead of m Ã— d as in the two layer neural net
section), and the bias vector b[1] âˆˆ R1Ã—m. The computation for the hidden
activation becomes

Z [1] = XW [1] + b[1] âˆˆ R3Ã—m

(7.84)

8The instructor suspects that this is mostly because in mathematics we naturally mul-

tiply a matrix to a vector on the left hand side.

Part III

Generalization and
regularization

112

Chapter 8

Generalization

This chapter discusses tools to analyze and understand the generaliza-
i.e, their performances on unseen test
tion of machine learning models,
examples. Recall that for supervised learning problems, given a train-
ing dataset {(x(i), y(i))}n
i=1, we typically learn a model hÎ¸ by minimizing a
loss/cost function J(Î¸), which encourages hÎ¸ to ï¬t the data. E.g., when
the loss function is the least square loss (aka mean squared error), we have
J(Î¸) = 1
i=1(y(i) âˆ’ hÎ¸(x(i)))2. This loss function for training purposes is
n
oftentimes referred to as the training loss/error/cost.

(cid:80)n

However, minimizing the training loss is not our ultimate goalâ€”it is
merely our approach towards the goal of learning a predictive model. The
most important evaluation metric of a model is the loss on unseen test exam-
ples, which is oftentimes referred to as the test error. Formally, we sample a
test example (x, y) from the so-called test distribution D, and measure the
modelâ€™s error on it, by, e.g., the mean squared error, (hÎ¸(x) âˆ’ y)2. The ex-
pected loss/error over the randomness of the test example is called the test
loss/error,1

L(Î¸) = E(x,y)âˆ¼D[(y âˆ’ hÎ¸(x))2]

(8.1)

Note that the measurement of the error involves computing the expectation,
and in practice, it can be approximated by the average error on many sampled
test examples, which are referred to as the test dataset. Note that the key
diï¬€erence here between training and test datasets is that the test examples

1In theoretical and statistical literature, we oftentimes call the uniform distribution
over the training set {(x(i), y(i))}n
i=1, denoted by (cid:98)D, an empirical distribution, and call
D the population distribution. Partly because of this, the training loss is also referred
to as the empirical loss/risk/error, and the test loss is also referred to as the population
loss/risk/error.

113

114

are unseen, in the sense that the training procedure has not used the test
examples. In classical statistical learning settings, the training examples are
also drawn from the same distribution as the test distribution D, but still
the test examples are unseen by the learning procedure whereas the training
examples are seen.2

Because of this key diï¬€erence between training and test datasets, even
if they are both drawn from the same distribution D, the test error is not
necessarily always close to the training error.3 As a result, successfully min-
imizing the training error may not always lead to a small test error. We
typically say the model overï¬ts the data if the model predicts accurately on
the training dataset but doesnâ€™t generalize well to other test examples, that
is, if the training error is small but the test error is large. We say the model
underï¬ts the data if the training error is relatively large4 (and in this case,
typically the test error is also relatively large.)

This chapter studies how the test error is inï¬‚uenced by the learning pro-
cedure, especially the choice of model parameterizations. We will decompose
the test error into â€œbiasâ€ and â€œvarianceâ€ terms and study how each of them is
aï¬€ected by the choice of model parameterizations and their tradeoï¬€s. Using
the bias-variance tradeoï¬€, we will discuss when overï¬tting and underï¬tting
will occur and be avoided. We will also discuss the double descent phe-
nomenon in Section 8.2 and some classical theoretical results in Section 8.3.

2These days, researchers have increasingly been more interested in the setting with

â€œdomain shiftâ€, that is, the training distribution and test distribution are diï¬€erent.

3the diï¬€erence between test error and training error is often referred to as the gener-
alization gap. The term generalization error in some literature means the test erro
y  also  be  helpful  to

Exercise Solutions 

| 

737

materialize the preprocessed data, for example to inspect it or archive it. How‐
ever, this approach has a few cons. First, it’s not easy to experiment with vari‐
ous  preprocessing  logics  if  you  need  to  generate  a  preprocessed  dataset  for
each variant. Second, if you want to perform data augmentation, you have to
materialize  many  variants  of  your  dataset,  which  will  use  a  large  amount  of
disk  space  and  take  a  lot  of  time  to  generate.  Lastly,  the  trained  model  will
expect preprocessed data, so you will have to add preprocessing code in your
application before it calls the model.

• If  the  data  is  preprocessed  with  the  tf.data  pipeline,  it’s  much  easier  to  tweak
the  preprocessing  logic  and  apply  data  augmentation.  Also,  tf.data  makes  it
easy to build highly efficient preprocessing pipelines (e.g., with multithreading
and  prefetching).  However,  preprocessing  the  data  this  way  will  slow  down
training. Moreover, each training instance will be preprocessed once per epoch
rather than just once if the data was preprocessed when creating the data files.
Lastly, the trained model will still expect preprocessed data.

• If you add preprocessing layers to your model, you will only have to write the
preprocessing code once for both training and inference. If your model needs
to be deployed to many different platforms, you will not need to write the pre‐
processing  code  multiple  times.  Plus,  you  will  not  run  the  risk  of  using  the
wrong preprocessing logic for your model, since it will be part of the model.
On  the  downside,  preprocessing  the  data  will  slow  down  training,  and  each
training  instance  will  be  preprocessed  once  per  epoch.  Moreover,  by  default
the preprocessing operations will run on the GPU for the current batch (you
will not benefit from parallel preprocessing on the CPU, and prefetching). For‐
tunately,  the  upcoming  Keras  preprocessing  layers  should  be  able  to  lift  the
preprocessing operations from the preprocessing layers and run them as part
of the tf.data pipeline, so you will benefit from multithreaded execution on the
CPU and prefetching.

• Lastly,  using  TF  Transform  for  preprocessing  gives  you  many  of  the  benefits
from the previous options: the preprocessed data is materialized, each instance
is preprocessed just once (speeding up training), and preprocessing layers get
generated  automatically  so  you  only  need  to  write  the  preprocessing  code
once.  The  main  drawback  is  the  fact  that  you  need  to  learn  how  to  use  this
tool.

8. Let’s look at how to encode categorical features and text:

• To encode a categorical feature that has a natural order, such as a movie rating
(e.g., “bad,” “average,” “good”), the simplest option is to use ordinal encoding:
sort  the  categories  in  their  natural  order  and  map  each  category  to  its  rank
(e.g., “bad” maps to 0, “average” maps to 1, and “good” maps to 2). However,
most categorical features don’t have such a natural order. For example, there’s

738 

|  Appendix A: Exercise Solutions

no natural order for professions or countries. In this case, you can use one-hot
encoding or, if there are many categories, embeddings.

• For text, one option is to use a bag-of-words representation: a sentence is rep‐
resented by a vector counting the counts of each possible word. Since common
words  are  usually  not  very  important,  you’ll  want  to  use  TF-IDF  to  reduce
their weight. Instead of counting words, it is also common to count n-grams,
which  are  sequences  of  n  consecutive  words—nice  and  simple.  Alternatively,
you  can  encode  each  word  using  word  embeddings,  possibly  pretrained.
Rather  than  encoding  words,  it  is  also  possible  to  encode  each  letter,  or  sub‐
word  tokens  (e.g.,  splitting  “smartest”  into  “smart”  and  “est”).  These  last  two
options are discussed in Chapter 16.

For the solutions to exercises 9 and 10, please see the Jupyter notebooks available at
https://github.com/ageron/handson-ml2.

Chapter 14: Deep Computer Vision Using Convolutional
Neural Networks

1. These are the main advantages of a CNN over a fully connected DNN for image

classification:

• Because consecutive layers are only partially connected and because it heavily
reuses its weights, a CNN has many fewer parameters than a fully connected
DNN, which makes it much faster to train, reduces the risk of overfitting, and
requires much less training data.

• When a CNN has learned a kernel that can detect a particular feature, it can
detect that feature anywhere in the image. In contrast, when a DNN learns a
feature  in  one  location,  it  can  detect  it  only  in  that  particular  location.  Since
images  typically  have  very  repetitive  features,  CNNs  are  able  to  generalize
much better than DNNs for image processing tasks such as classification, using
fewer training examples.

• Finally, a DNN has no prior knowledge of how pixels are organized; it does not
know  that  nearby  pixels  are  close.  A  CNN’s  architecture  embeds  this  prior
knowledge. Lower layers typically identify features in small areas of the images,
while higher layers combine the lower-level features into larger features. This
works well with most natural images, giving CNNs a decisive head start com‐
pared to DNNs.

2. Let’s  compute  how  many  parameters  the  CNN  has.  Since  its  first  convolutional
layer has 3 × 3 kernels, and the input has three channels (red, green, and blue),
each feature map has 3 × 3 × 3 weights, plus a bias term. That’s 28 parameters per

Exercise Solutions 

| 

739

feature  map.  Since  this  first  convolutional  layer  has  100  feature  maps,  it  has  a
total of 2,800 parameters. The second convolutional layer has 3 × 3 kernels and
its input is the set of 100 feature maps of the previous layer, so each feature map
has 3 × 3 × 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this
layer  has  901  ×  200  =  180,200  parameters.  Finally,  the  third  and  last  convolu‐
tional layer also has 3 × 3 kernels, and its input is the set of 200 feature maps of
the previous layers, so each feature map has 3 × 3 × 200 = 1,800 weights, plus a
bias  term.  Since  it  has  400  feature  maps,  this  layer  has  a  total  of  1,801  ×  400  =
720,400 parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 = 903,400
parameters.

Now  let’s  compute  how  much  RAM  this  neural  network  will  require  (at  least)
when  making  a  prediction  for  a  single  instance.  First  let’s  compute  the  feature
map size for each layer. Since we are using a stride of 2 and "same" padding, the
horizontal and vertical dimensions of the feature maps are divided by 2 at each
layer (rounding up if necessary). So, as the input channels are 200 × 300 pixels,
the first layer’s feature maps are 100 × 150, the second layer’s feature maps are 50
× 75, and the third layer’s feature maps are 25 × 38. Since 32 bits is 4 bytes and
the first convolutional layer has 100 feature maps, this first layer takes up 4 × 100
× 150 × 100 = 6 million bytes (6 MB). The second layer takes up 4 × 50 × 75 ×
200 = 3 million bytes (3 MB). Finally, the third layer takes up 4 × 25 × 38 × 400 =
1,520,000  bytes  (about  1.5  MB).  However,  once  a  layer  has  been  computed,  the
memory occupied by the previous layer can be released, so if everything is well
optimized, only 6 + 3 = 9 million bytes (9 MB) of RAM will be required (when
the second layer has just been computed, but the memory occupied by the first
layer has not been released yet). But wait, you also need to add the memory occu‐
pied by the CNN’s parameters! We computed earlier that it has 903,400 parame‐
ters, each using up 4 bytes, so this adds 3,613,600 bytes (about 3.6 MB). The total
RAM required is therefore (at least) 12,613,600 bytes (about 12.6 MB).

Lastly, let’s compute the minimum amount of RAM required when training the
CNN on a mini-batch of 50 images. During training TensorFlow uses backpropa‐
gation, which requires keeping all values computed during the forward pass until
the reverse pass begins. So we must compute the total RAM required by all layers
for a single instance and multiply that by 50. At this point, let’s start counting in
megabytes  rather  than  bytes.  We  computed  before  that  the  three  layers  require
respectively  6,  3,  and  1.5  MB  for  each  instance.  That’s  a  total  of  10.5  MB  per
instance, so for 50 instances the total RAM required is 525 MB. Add to that the
RAM required by the input images, which is 50 × 4 × 200 × 300 × 3 = 36 million
bytes (36 MB), plus the RAM required for the model parameters, which is about
3.6 MB (computed earlier), plus some RAM for the gradients (we will neglect this
since it can be released gradually as backpropagation goes down the layers during
the reverse pass). We are up to a total of roughly 525 + 36 + 3.6 = 564.6 MB, and
that’s really an optimistic bare minimum.

740 

|  Appendix A: Exercise Solutions

3. If your GPU runs out of memory while training a CNN, here are five things you
could try to solve the problem (other than purchasing a GPU with more RAM):

• Reduce the mini-batch size.

• Reduce dimensionality using a larger stride in one or more layers.

• Remove one or more layers.

• Use 16-bit floats instead of 32-bit floats.

• Distribute the CNN across multiple devices.

4. A max pooling layer has no parameters at all, whereas a convolutional layer has

quite a few (see the previous questions).

5. A local response normalization layer makes the neurons that most strongly acti‐
vate inhibit neurons at the same location but in neighboring feature maps, which
encourages  different  feature  maps  to  specialize  and  pushes  them  apart,  forcing
them to explore a wider range of features. It is typically used in the lower layers to
have a larger pool of low-level features that the upper layers can build upon.

6. The main innovations in AlexNet compared to LeNet-5 are that it is much larger
and  deeper,  and  it  stacks  convolutional  layers  directly  on  top  of  each  other,
instead of stacking a pooling layer on top of each convolutional layer. The main
innovation in GoogLeNet is the introduction of inception modules, which make it
possible to have a much deeper net than previous CNN architectures, with fewer
parameters.  ResNet’s  main  innovation  is  the  introduction  of  skip  connections,
which make it possible to go well beyond 100 layers. Arguably, its simplicity and
consistency  are  also  rather  innovative.  SENet’s  main  innovation  was  the  idea  of
using an SE block (a two-layer dense network) after every inception module in
an inception network or every residual unit in a ResNet to recalibrate the relative
importance  of  feature  maps.  Finally,  Xception’s  main  innovation  was  the  use  of
depthwise  separable  convolutional  layers,  which  look  at  spatial  patterns  and
depthwise patterns separately.

7. Fully convolutional networks are neural networks composed exclusively of con‐
volutional and pooling layers. FCNs can efficiently process images of any width
and  height  (at  least  above  the  minimum  size).  They  are  most  useful  for  object
detection and semantic segmentation because they only need to look at the image
once  (instead  of  having  to  run  a  CNN  multiple  times  on  different  parts  of  the
image). If you have a CNN with some dense layers on top, you can convert these
dense  layers  to  convolutional  layers  to  create  an  FCN:  just  replace  the  lowest
dense layer with a convolutional layer with a kernel size equal to the layer’s input
size,  with  one  filter  per  neuron  in  the  dense  layer,  and  using  "valid"  padding.
Generally the stride should be 1, but you can set it to a higher value if you want.
The activation function should be the same as the dense layer’s. The other dense
layers should be converted the same way, but using 1 × 1 filters. It is actually pos‐

Exercise Solutions 

| 

741

sible to convert a trained CNN this way by appropriately reshaping the dense lay‐
ers’ weight matrices.

8. The main technical difficulty of semantic segmentation is the fact that a lot of the
spatial  information  gets  lost  in  a  CNN  as  the  signal  flows  through  each  layer,
especially  in  pooling  layers  and  layers  with  a  stride  greater  than  1.  This  spatial
information needs to be restored somehow to accurately predict the class of each
pixel.

For  the  solutions  to  exercises  9  to  12,  please  see  the  Jupyter  notebooks  available  at
https://github.com/ageron/handson-ml2.

Chapter 15: Processing Sequences Using RNNs and CNNs

1. Here are a few RNN applications:

• For  a  sequence-to-sequence  RNN:  predicting  the  weather  (or  any  other  time
series),  machine  translation  (using  an  Encoder–Decoder  architecture),  video
captioning,  speech  to  text,  music  generation  (or  other  sequence  generation),
identifying the chords of a song

• For a sequence-to-vector RNN: classifying music samples by music genre, ana‐
lyzing the sentiment of a book review, predicting what word an aphasic patient
is thinking of based on readings from brain implants, predicting the probabil‐
ity that a user will want to watch a movie based on their watch history (this is
one  of  many  possible  implementations  of  collaborative  filtering  for  a  recom‐
mender system)

• For  a  vector-to-sequence  RNN:  image  captioning,  creating  a  music  playlist
based on an embedding of the current artist, generating a melody based on a
set of parameters, locating pedestrians in a picture (e.g., a video frame from a
self-driving car’s camera)

2. An  RNN  layer  must  have  three-dimensional  inputs:  the  first  dimension  is  the
batch dimension (its size is the batch size), the second dimension represents the
time  (its  size  is  the  number  of  time  steps),  and  the  third  dimension  holds  the
inputs at each time step (its size is the number of input features per time step).
For example, if you want to process a batch containing 5 time series of 10 time
steps each, with 2 values per time step (e.g., the temperature and the wind speed),
the  shape  will  be  [5,  10,  2].  The  outputs  are  also  three-dimensional,  with  the
same  first  two  dimensions,  but  the  last  dimension  is  equal  to  the  number  of
neurons. For example, if an RNN layer with 32 neurons processes the batch we
just discussed, the output will have a shape of [5, 10, 32].

742 

|  Appendix A: Exercise Solutions

3. To  build  a  deep  sequence-to-sequence  RNN  using  Keras,  you  must  set
return_sequences=True for all RNN layers. To build a sequence-to-vector RNN,
you must set return_sequences=True for all RNN layers except for the top RNN
layer, which must have return_sequences=False (or do not set this argument at
all, since False is the default).

4. If you have a daily univariate time series, and you want to forecast the next seven
days, the simplest RNN architecture you can use is a stack of RNN layers (all with
return_sequences=True except for the top RNN layer), using seven neurons in
the  output  RNN  layer.  You  can  then  train  this  model  using  random  windows
from the time series (e.g., sequences of 30 consecutive days as the inpu
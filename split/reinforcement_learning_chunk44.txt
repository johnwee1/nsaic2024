with perceptual aliasing: The
perceptual distinctions approach.
In Proceedings of the Tenth National
Conference on Artiﬁcial Intelligence, pp. 183–188. AAAI/MIT Press, Menlo
Park, CA.

Christensen, J., Korf, R. E. (1986). A uniﬁed theory of heuristic evalua-
tion functions and its application to learning. In Proceedings of the Fifth
National Conference on Artiﬁcial Intelligence, pp. 148–152. Morgan Kauf-
mann, San Mateo, CA.

Cichosz, P. (1995). Truncating temporal diﬀerences: On the eﬃcient im-
plementation of TD(λ) for reinforcement learning. Journal of Artiﬁcial

15.5. OTHER FRONTIER DIMENSIONS

317

Intelligence Research, 2:287–318.

Clark, W. A., Farley, B. G. (1955). Generalization of pattern recognition in a
self-organizing system. In Proceedings of the 1955 Western Joint Computer
Conference, pp. 86–91.

Clouse, J. (1996). On Integrating Apprentice Learning and Reinforcement
Learning TITLE2. Ph.D. thesis, University of Massachusetts, Amherst.
Appeared as CMPSCI Technical Report 96-026.

Clouse, J., Utgoﬀ, P. (1992). A teaching method for reinforcement learning
In Proceedings of the Ninth International Machine Learning

systems.
Conference, pp. 92–101. Morgan Kaufmann, San Mateo, CA.

Colombetti, M., Dorigo, M. (1994). Training agent to perform sequential

behavior. Adaptive Behavior, 2(3):247–275.

Connell, J. (1989). A colony architecture for an artiﬁcial creature. Technical
Report AI-TR-1151. MIT Artiﬁcial Intelligence Laboratory, Cambridge,
MA.

Connell, J., Mahadevan, S. (1993). Robot Learning. Kluwer Academic,

Boston.

Craik, K. J. W. (1943). The Nature of Explanation. Cambridge University

Press, Cambridge.

Crites, R. H. (1996).

Large-Scale Dynamic Optimization Using Teams of
Reinforcement Learning Agents. Ph.D. thesis, University of Massachusetts,
Amherst.

Crites, R. H., Barto, A. G. (1996). Improving elevator performance using rein-
forcement learning. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo
(eds.), Advances in Neural Information Processing Systems: Proceedings
of the 1995 Conference, pp. 1017–1023. MIT Press, Cambridge, MA.

Cross, J. G. (1973). A stochastic learning model of economic behavior. The

Quarterly Journal of Economics 87 (2):239-266.

Curtiss, J. H. (1954). A theoretical comparison of the eﬃciencies of two
classical methods and a Monte Carlo method for computing one component
of the solution of a set of linear algebraic equations. In H. A. Meyer (ed.),
Symposium on Monte Carlo Methods, pp. 191–233. Wiley, New York.

Cziko, G. (1995). Without Miracles: Universal Selection Theory and the

Second Darvinian Revolution. MIT Press, Cambridge, MA.

Daniel, J. W. (1976). Splines and eﬃciency in dynamic programming. Journal

of Mathematical Analysis and Applications, 54:402–407.

318

CHAPTER 15. PROSPECTS

Dayan, P. (1991). Reinforcement comparison.

In D. S. Touretzky, J. L. El-
man, T. J. Sejnowski, and G. E. Hinton (eds.), Connectionist Models: Pro-
ceedings of the 1990 Summer School, pp. 45–51. Morgan Kaufmann, San
Mateo, CA.

Dayan, P. (1992). The convergence of TD(λ) for general λ. Machine Learning,

8:341–362.

Dayan, P., Hinton, G. E. (1993). Feudal reinforcement learning. In S. J. Han-
son, J. D. Cohen, and C. L. Giles (eds.), Advances in Neural Informa-
tion Processing Systems: Proceedings of the 1992 Conference, pp. 271–278.
Morgan Kaufmann, San Mateo, CA.

Dayan, P., Sejnowski, T. (1994). TD(λ) converges with probability 1. Machine

Learning, 14:295–301.

Dean, T., Lin, S.-H. (1995). Decomposition techniques for planning in stochas-
tic domains. In Proceedings of the Fourteenth International Joint Confer-
ence on Artiﬁcial Intelligence, pp. 1121–1127. Morgan Kaufmann. See
also Technical Report CS-95-10, Brown University, Department of Com-
puter Science, 1995.

DeJong, G., Spong, M. W. (1994). Swinging up the acrobot: An example
of intelligent control. In Proceedings of the American Control Conference,
pp. 2158–2162. American Automatic Control Council, Evanston, IL.

Denardo, E. V. (1967). Contraction mappings in the theory underlying dy-

namic programming. SIAM Review, 9:165–177.

Dennett, D. C. (1978). Brainstorms, pp. 71–89. Bradford/MIT Press, Cam-

bridge, MA.

Dick, T. (2015). A Regret-full Perspective on Policy Gradient Methods for

Reinforcement Learning. MSc Thesis, University of Alberta.

Dietterich, T. G., Flann, N. S. (1995). Explanation-based learning and rein-
forcement learning: A uniﬁed view. In A. Prieditis and S. Russell (eds.),
Proceedings of the Twelfth International Conference on Machine Learning,
pp. 176–184. Morgan Kaufmann, San Francisco.

Doya, K. (1996). Temporal diﬀerence learning in continuous time and space.
In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (eds.), Advances in
Neural Information Processing Systems: Proceedings of the 1995 Confer-
ence, pp. 1073–1079. MIT Press, Cambridge, MA.

Doyle, P. G., Snell, J. L. (1984). Random Walks and Electric Networks. The
Mathematical Association of America. Carus Mathematical Monograph
22.

15.5. OTHER FRONTIER DIMENSIONS

319

Dreyfus, S. E., Law, A. M. (1977). The Art and Theory of Dynamic Program-

ming. Academic Press, New York.

Duda, R. O., Hart, P. E. (1973). Pattern Classiﬁcation and Scene Analysis.

Wiley, New York.

Duﬀ, M. O. (1995). Q-learning for bandit problems.

In A. Prieditis and
S. Russell (eds.), Proceedings of the Twelfth International Conference on
Machine Learning, pp. 209–217. Morgan Kaufmann, San Francisco.

Estes, W. K. (1950). Toward a statistical theory of learning. Psychololgical

Review, 57:94–107.

Farley, B. G., Clark, W. A. (1954). Simulation of self-organizing systems by
digital computer. IRE Transactions on Information Theory, 4:76–84.

Feldbaum, A. A. (1965). Optimal Control Systems. Academic Press, New

York.

Fogel, L. J., Owens, A. J., Walsh, M. J. (1966). Artiﬁcial intelligence through

simulated evolution. John Wiley and Sons.

Friston, K. J., Tononi, G., Reeke, G. N., Sporns, O., Edelman, G. M. (1994).
Value-dependent selection in the brain: Simulation in a synthetic neural
model. Neuroscience, 59:229–243.

Fu, K. S. (1970). Learning control systems—Review and outlook.

IEEE

Transactions on Automatic Control, 15:210–221.

Galanter, E., Gerstenhaber, M. (1956). On thought: The extrinsic theory.

Psychological Review, 63:218–227.

Gallant, S. I. (1993). Neural Network Learning and Expert Systems. MIT

Press, Cambridge, MA.

Gallistel, C. R. (2005). Deconstructing the law of eﬀect. Games and Economic

Behavior 52 (2), 410-423.

G¨allmo, O., Asplund, L. (1995). Reinforcement learning by construction
In J. Alspector, R. Goodman, and T. X. Brown
of hypothetical targets.
(eds.), Proceedings of the International Workshop on Applications of Neural
Networks to Telecommunications 2, pp. 300–307. Erlbaum, Hillsdale, NJ.

Gardner, M. (1973). Mathematical games. Scientiﬁc American, 228(1):108–

115.

Gelperin, A., Hopﬁeld, J. J., Tank, D. W. (1985). The logic of limax learning.
In A. Selverston (ed.), Model Neural Networks and Behavior, pp. 247–261.
Plenum Press, New York.

Gittins, J. C., Jones, D. M. (1974). A dynamic allocation index for the

320

CHAPTER 15. PROSPECTS

sequential design of experiments.
In J. Gani, K. Sarkadi, and I. Vincze
(eds.), Progress in Statistics, pp. 241–266. North-Holland, Amsterdam–
London.

Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and

Machine Learning. Addison-Wesley, Reading, MA.

Goldstein, H. (1957). Classical Mechanics. Addison-Wesley, Reading, MA.

Goodwin, G. C., Sin, K. S. (1984). Adaptive Filtering Prediction and Control.

Prentice-Hall, Englewood Cliﬀs, NJ.

Gordon, G. J. (1995). Stable function approximation in dynamic program-
ming. In A. Prieditis and S. Russell (eds.), Proceedings of the Twelfth In-
ternational Conference on Machine Learning, pp. 261–268. Morgan Kauf-
mann, San Francisco. An expanded version was published as Technical Re-
port CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995.

Gordon, G. J. (1996). Chattering in SARSA(λ). CMU learning lab internal

report.

Gordon, G. J. (1996). Stable ﬁtted reinforcement learning. In D. S. Touret-
zky, M. C. Mozer, M. E. Hasselmo (eds.), Advances in Neural Information
Processing Systems: Proceedings of the 1995 Conference, pp. 1052–1058.
MIT Press, Cambridge, MA.

Gordon, G. J. (2001). Reinforcement learning with function approximation
converges to a region. Advances in neural information processing systems.

Greensmith, E., Bartlett, P. L., Baxter, J. (2001). Variance reduction tech-
In Advances in
niques for gradient estimates in reinforcement learning.
Neural Information Processing Systems: Proceedings of the 2000 Confer-
ence, pp. 1507–1514.

Greensmith, E., Bartlett, P. L., Baxter, J. (2004). Variance reduction tech-
niques for gradient estimates in reinforcement learning. Journal of Machine
Learning Research 5, 1471-1530.

Griﬃth, A. K. (1966). A new machine learning technique applied to the game
of checkers. Technical Report Project MAC, Artiﬁcial Intelligence Memo
94. Massachusetts Institute of Technology, Cambridge, MA.

Griﬃth, A. K. (1974). A comparison and evaluation of three machine learning
procedures as applied to the game of checkers. Artiﬁcial Intelligence,
5:137–148.

Gullapalli, V. (1990). A stochastic reinforcement algorithm for learning real-

valued functions. Neural Networks, 3:671–692.

Gurvits, L., Lin, L.-J., Hanson, S. J. (1994). Incremental learning of evalua-

15.5. OTHER FRONTIER DIMENSIONS

321

tion functions for absorbing Markov chains: New methods and theorems.
Preprint.

Hampson, S. E. (1983). A Neural Model of Adaptive Behavior. Ph.D. thesis,

University of California, Irvine.

Hampson, S. E. (1989). Connectionist Problem Solving: Computational As-

pects of Biological Learning. Birkhauser, Boston.

Hawkins, R. D., Kandel, E. R. (1984). Is there a cell-biological alphabet for

simple forms of learning? Psychological Review, 91:375–391.

Herrnstein, R. J. (1970). On the Law of Eﬀect. Journal of the Experimental

Analysis of Behavior 13 (2), 243-266.

Hersh, R., Griego, R. J. (1969). Brownian motion and potential theory.

Scientiﬁc American, 220:66–74.

Hesterberg, T. C. (1988), Advances in importance sampling, Ph.D. Disserta-

tion, Statistics Department, Stanford University.

Hilgard, E. R., Bower, G. H. (1975). Theories of Learning. Prentice-Hall,

Englewood Cliﬀs, NJ.

Hinton, G. E. (1984). Distributed representations. Technical Report CMU-
CS-84-157. Department of Computer Science, Carnegie-Mellon University,
Pittsburgh, PA.

Hochreiter, S., Schmidhuber, J. (1997). LTSM can solve hard time lag prob-
lems. In Advances in Neural Information Processing Systems: Proceedings
of the 1996 Conference, pp. 473–479. MIT Press, Cambridge, MA.

Holland, J. H. (1975). Adaptation in Natural and Artiﬁcial Systems. Univer-

sity of Michigan Press, Ann Arbor.

Holland, J. H. (1976). Adaptation.

In R. Rosen and F. M. Snell (eds.),
Progress in Theoretical Biology, vol. 4, pp. 263–293. Academic Press, New
York.

Holland, J. H. (1986). Escaping brittleness: The possibility of general-purpose
learning algorithms applied to rule-based systems.
In R. S. Michalski,
J. G. Carbonell, and T. M. Mitchell (eds.), Machine Learning: An Artiﬁcial
Intelligence Approach, vol. 2, pp. 593–623. Morgan Kaufmann, San Mateo,
CA.

Houk, J. C., Adams, J. L., Barto, A. G. (1995). A model of how the basal
ganglia generates and uses neural signals that predict reinforcement.
In
J. C. Houk, J. L. Davis, and D. G. Beiser (eds.), Models of Information
Processing in the Basal Ganglia, pp. 249–270. MIT Press, Cambridge, MA.

322

CHAPTER 15. PROSPECTS

Howard, R. (1960). Dynamic Programming and Markov Processes. MIT

Press, Cambridge, MA.

Hull, C. L. (1943). Principles of Behavior. Appleton-Century, New York.

Hull, C. L. (1952). A Behavior System. Wiley, New York.

Jaakkola, T., Jordan, M. I., Singh, S. P. (1994). On the convergence of
stochastic iterative dynamic programming algorithms. Neural Computa-
tion, 6:1185–1201.

Jaakkola, T., Singh, S. P., Jordan, M. I. (1995). Reinforcement learning algo-
rithm for partially observable Markov decision problems.
In G. Tesauro,
D. S. Touretzky, T. Leen (eds.), Advances in Neural Information Process-
ing Systems: Proceedings of the 1994 Conference, pp. 345–352. MIT Press,
Cambridge, MA.

Kaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Pre-
liminary results. In Proceedings of the Tenth International Conference on
Machine Learning, pp. 167–173. Morgan Kaufmann, San Mateo, CA.

Kaelbling, L. P. (1993b). Learning in Embedded Systems. MIT Press, Cam-

bridge, MA.

Kaelbling, L. P. (ed.). (1996). Special issue of Machine Learning on reinforce-

ment learning, 22.

Kaelbling, L. P., Littman, M. L., Moore, A. W. (1996). Reinforcement learn-
ing: A survey. Journal of Artiﬁcial Intelligence Research, 4:237–285.

Kakutani, S. (1945). Markov processes and the Dirichlet problem. Proceedings

of the Japan Academy, 21:227–233.

Kalos, M. H., Whitlock, P. A. (1986). Monte Carlo Methods. Wiley, New

York.

Kanerva, P. (1988). Sparse Distributed Memory. MIT Press, Cambridge, MA.

Kanerva, P. (1993).

In
M. H. Hassoun (ed.), Associative Neural Memories: Theory and Imple-
mentation, pp. 50–76. Oxford University Press, New York.

Sparse distributed memory and related models.

Kashyap, R. L., Blaydon, C. C., Fu, K. S. (1970). Stochastic approximation.
In J. M. Mendel and K. S. Fu (eds.), Adaptive, Learning, and Pattern
Recognition Systems: Theory and Applications, pp. 329–355. Academic
Press, New York.

Keerthi, S. S., Ravindran, B. (1997). Reinforcement learning. In E. Fiesler and
R. Beale (eds.), Handbook of Neural Computation, C3. Oxford University
Press, New York.

15.5. OTHER FRONTIER DIMENSIONS

323

Kimble, G. A. (1961). Hilgard and Marquis’ Conditioning and Learning.

Appleton-Century-Crofts, New York.

Kimble, G. A. (1967). Foundations of Conditioning and Learning. Appleton-

Century-Crofts, New York.

Klopf, A. H. (1972). Brain function and adaptive systems—A heterostatic
theory. Technical Report AFCRL-72-0164, Air Force Cambridge Research
Laboratories, Bedford, MA. A summary appears in Proceedings of the In-
ternational Conference on Systems, Man, and Cybernetics. IEEE Systems,
Man, and Cybernetics Society, Dallas, TX, 1974.

Klopf, A. H. (1975). A comparison of natural and artiﬁcial intelligence.

SIGART Newsletter, 53:11–13.

Klopf, A. H. (1982). The Hedonistic Neuron: A Theory of Memory, Learning,

and Intelligence. Hemisphere, Washington, DC.

Klopf, A. H. (1988). A neuronal model of classical conditioning. Psychobiol-

ogy, 16:85–125.

Kohonen, T. (1977). Associative Memory: A System Theoretic Approach.

Springer-Verlag, Berlin.

Koller, D., Friedman, N. (2009). Probabilistic Graphical Models: Principles

and Techniques. MIT Press, 2009.

Korf, R. E. (1988). Optimal path ﬁnding algorithms.

In L. N. Kanal and
V. Kumar (eds.), Search in Artiﬁcial Intelligence, pp. 223–267. Springer
Verlag, Berlin.

Koza, J. R. (1992). Genetic programming: On the programming of computers

by means of natural selection (Vol. 1). MIT press.

Kraft, L. G., Campagna, D. P. (1990). A summary comparison of CMAC
In T. Miller,
neural network and traditional adaptive control systems.
R. S. Sutton, and P. J. Werbos (eds.), Neural Networks for Control, pp. 143–
169. MIT Press, Cambridge, MA.

Kraft, L. G., Miller, W. T., Dietz, D. (1992). Development and application
of CMAC neural network-bas
ing is to impose the restriction that q
is a factorial distribution:
î?™
q(h i | v).
q(h | v) =
(19.17)
i

This is called the mean ï¬?eld approach. More generally, we can impose any graphical model structure we choose on q, to ï¬‚exibly determine how many interactions we
want our approximation to capture. This fully general graphical model approach
is called structured variational inference (Saul and Jordan, 1996).
The beauty of the variational approach is that we do not need to specify a
speciï¬?c parametric form for q. We specify how it should factorize, but then the
optimization problem determines the optimal probability distribution within those
factorization constraints. For discrete latent variables, this just means that we
use traditional optimization techniques to optimize a ï¬?nite number of variables
describing the q distribution. For continuous latent variables, this means that we
use a branch of mathematics called calculus of variations to perform optimization
over a space of functions, and actually determine which function should be used
to represent q . Calculus of variations is the origin of the names â€œvariational
learningâ€? and â€œvariational inference,â€? though these names apply even when the
latent variables are discrete and calculus of variations is not needed. In the case
of continuous latent variables, calculus of variations is a powerful technique that
removes much of the responsibility from the human designer of the model, who
now must specify only how q factorizes, rather than needing to guess how to design
a speciï¬?c q that can accurately approximate the posterior.
Because L(v, Î¸, q ) is deï¬?ned to be log p(v; Î¸) âˆ’ DKL (q(h | v )î?«p( h | v; Î¸)), we
can think of maximizing L with respect to q as minimizing DKL (q( h | v )î?«p (h | v)).
638

CHAPTER 19. APPROXIMATE INFERENCE

In this sense, we are ï¬?tting q to p . However, we are doing so with the opposite
direction of the KL divergence than we are used to using for ï¬?tting an approximation.
When we use maximum likelihood learning to ï¬?t a model to data, we minimize
D KL(pdata î?«pmodel). As illustrated in ï¬?gure 3.6, this means that maximum likelihood
encourages the model to have high probability everywhere that the data has high
probability, while our optimization-based inference procedure encourages q to
have low probability everywhere the true posterior has low probability. Both
directions of the KL divergence can have desirable and undesirable properties. The
choice of which to use depends on which properties are the highest priority for
each application. In the case of the inference optimization problem, we choose
to use DKL (q (h | v)î?«p(h | v)) for computational reasons. Speciï¬?cally, computing
D KL(q(h | v) î?«p(h | v )) involves evaluating expectations with respect to q, so by
designing q to be simple, we can simplify the required expectations. The opposite
direction of the KL divergence would require computing expectations with respect
to the true posterior. Because the form of the true posterior is determined by
the choice of model, we cannot design a reduced-cost approach to computing
D KL(p(h | v )î?«q(h | v)) exactly.

19.4.1

Discrete Latent Variables

Variational inference with discrete latent variables is relatively straightforward.
We deï¬?ne a distribution q, typically one where each factor of q is just deï¬?ned
by a lookup table over discrete states. In the simplest case, h is binary and we
make the mean ï¬?eld assumption that q factorizes over each individual hi. In this
case we can parametrize q with a vector hÌ‚ whose entries are probabilities. Then
q(hi = 1 | v) = hÌ‚ i .

After determining how to represent q , we simply optimize its parameters. In
the case of discrete latent variables, this is just a standard optimization problem.
In principle the selection of q could be done with any optimization algorithm, such
as gradient descent.

Because this optimization must occur in the inner loop of a learning algorithm,
it must be very fast. To achieve this speed, we typically use special optimization
algorithms that are designed to solve comparatively small and simple problems in
very few iterations. A popular choice is to iterate ï¬?xed point equations, in other
words, to solve
âˆ‚
L=0
(19.18)
âˆ‚hÌ‚i
for hÌ‚i . We repeatedly update diï¬€erent elements of hÌ‚ until we satisfy a convergence
639

CHAPTER 19. APPROXIMATE INFERENCE

criterion.
To make this more concrete, we show how to apply variational inference to the
binary sparse coding model (we present here the model developed by Henniges
et al. (2010) but demonstrate traditional, generic mean ï¬?eld applied to the model,
while they introduce a specialized algorithm). This derivation goes into considerable
mathematical detail and is intended for the reader who wishes to fully resolve
any ambiguity in the high-level conceptual description of variational inference and
learning we have presented so far. Readers who do not plan to derive or implement
variational learning algorithms may safely skip to the next section without missing
any new high-level concepts. Readers who proceed with the binary sparse coding
example are encouraged to review the list of useful properties of functions that
commonly arise in probabilistic models in section 3.10. We use these properties
liberally throughout the following derivations without highlighting exactly where
we use each one.
In the binary sparse coding model, the input v âˆˆ R n is generated from the
model by adding Gaussian noise to the sum of m diï¬€erent components which
can each be present or absent. Each component is switched on or oï¬€ by the
corresponding hidden unit in h âˆˆ {0, 1}m:
p(hi = 1) = Ïƒ(bi )

(19.19)

p(v | h) = N (v; W h, Î²âˆ’1 )

(19.20)

where b is a learnable set of biases, W is a learnable weight matrix, and Î² is a
learnable, diagonal precision matrix.
Training this model with maximum likelihood requires taking the derivative
with respect to the parameters. Consider the derivative with respect to one of the
biases:

=

âˆ‚
log p(v)
âˆ‚bi
âˆ‚
âˆ‚b i p(v )
p(v)
âˆ‚ î??

= âˆ‚b i
=

h p(h, v)

p(v)
î??
âˆ‚
h p( h )p( v | h )
âˆ‚b i
p(v)
640

(19.21)
(19.22)
(19.23)
(19.24)

CHAPTER 19. APPROXIMATE INFERENCE

h1

h2

v1

h3

v2

h4
h1

h3

h2

h4

v3

Figure 19.2: The graph structure of a binary sparse coding model with four hidden units.
(Left)The graph structure of p(h, v). Note that the edges are directed, and that every two
hidden units are co-parents of every visible unit. (Right)The graph structure of p(h | v ).
In order to account for the active paths between co-parents, the posterior distribution
needs an edge between all of the hidden units.

=
=

î??

âˆ‚
h p(v | h) âˆ‚b i p(h)

î?˜
h

p(v)

p(h | v )

=Ehâˆ¼p(h|v)

âˆ‚
âˆ‚bi p(h)

p(h)

âˆ‚
log p(h).
âˆ‚b i

(19.25)
(19.26)
(19.27)

This requires computing expectations with respect to p(h | v ). Unfortunately,
p(h | v ) is a complicated distribution. See ï¬?gure 19.2 for the graph structure of
p(h, v ) and p(h | v ). The posterior distribution corresponds to the complete graph
over the hidden units, so variable elimination algorithms do not help us to compute
the required expectations any faster than brute force.
We can resolve this diï¬ƒculty by using variational inference and variational
learning instead.
We can make a mean ï¬?eld approximation:
q(h | v) =

î?™
i

q(h i | v).

(19.28)

The latent variables of the binary sparse coding model are binary, so to represent
a factorial q we simply need to model m Bernoulli distributions q (hi | v). A natural
way to represent the means of the Bernoulli distributions is with a vector hÌ‚ of
probabilities, with q(h i = 1 | v) = hÌ‚i . We impose a restriction that hÌ‚ i is never
equal to 0 or to 1, in order to avoid errors when computing, for example, log hÌ‚i.
We will see that the variational inference equations never assign 0 or 1 to hÌ‚ i
641

CHAPTER 19. APPROXIMATE INFERENCE

analytically. However, in a software implementation, machine rounding error could
result in 0 or 1 values. In software, we may wish to implement binary sparse
coding using an unrestricted vector of variational parameters z and obtain hÌ‚ via
the relation hÌ‚ = Ïƒ (z ). We can thus safely compute log hÌ‚i on a computer by using
the identity log Ïƒ (zi) = âˆ’Î¶ (âˆ’z i ) relating the sigmoid and the softplus.
To begin our derivation of variational learning in the binary sparse coding
model, we show that the use of this mean ï¬?eld approximation makes learning
tractable.
The evidence lower bound is given by
L(v, Î¸, q )

(19.29)

=Ehâˆ¼q [log p(h, v )] + H (q)

(19.30)

=Ehâˆ¼q [log p(h) + log p(v | h) âˆ’ log q(h | v )]
î€¢ m
î€£
n
m
î?˜
î?˜
î?˜
=Ehâˆ¼q
log p(hi ) +
log p(v i | h) âˆ’
log q(h i | v)

(19.31)

i=1

=

m î?¨
î?˜
i=1

i=1

+ Ehâˆ¼q

i=1

=

m î?¨
î?˜
i=1

+

i=1

hÌ‚ i(log Ïƒ(bi ) âˆ’ log hÌ‚i) + (1 âˆ’ hË†i)(log Ïƒ(âˆ’b i) âˆ’ log(1 âˆ’ hÌ‚i))
î€¢ n
î?˜

log

î?²

î€’
î€“î€£
Î²i
Î²i
exp âˆ’ (vi âˆ’ Wi,:h) 2
2Ï€
2

hÌ‚ i(log Ïƒ(bi ) âˆ’ log hÌ‚i) + (1 âˆ’ hË†i)(log Ïƒ(âˆ’b i) âˆ’ log(1 âˆ’ hÌ‚i))

n
î?˜

ï£®

(19.32)

ï£«

ï£®

î?©

(19.33)
(19.34)

î?©

(19.35)
ï£¹ ï£¶ï£¹

î?˜
î?˜
1
2 Ë†
ï£°log Î²i âˆ’ Î²i ï£­v2i âˆ’ 2viWi,: hÌ‚ +
ï£°Wi,j
hj +
Wi,j Wi,k hÌ‚j hÌ‚ kï£» ï£¸ï£» .
2 i=1
2Ï€
j
kî€¶=j

(19.36)

While these equations are somewhat unappealing aesthetically, they show that L
can be expressed in a small number of simple arithmetic operations. The evidence
lower bound L is therefore tractable. We can use L as a replacement for the
intractable log-likelihood.
In principle, we could simply run gradient ascent on both v and h and this
would make a perfectly acceptable combined inference and training algorithm.
Usually, however, we do not do this, for two reasons. First, this would require
storing hÌ‚ for each v. We typically prefer algorithms that do not require perexample memory. It is diï¬ƒcult to scale learning algorithms to billions of examples
if we must remember a dynamically updated vector associated with each example.
642

CHAPTER 19. APPROXIMATE INFERENCE

Second, we would like to be able to extract the features hÌ‚ very quickly, in order to
recognize the content of v . In a realistic deployed setting, we would need to be
able to compute hÌ‚ in real time.
For both these reasons, we typically do not use gradient descent to compute
the mean ï¬?eld parameters hÌ‚. Instead, we rapidly estimate them with ï¬?xed point
equations.
The idea behind ï¬?xed point equations is that we are seeking a local maximum
with respect to hÌ‚, where âˆ‡hL(v, Î¸, hÌ‚) = 0 . We cannot eï¬ƒciently solve this
equation with respect to all of hÌ‚ simultaneously. However, we can solve for a single
variable:
âˆ‚
L(v, Î¸, hÌ‚) = 0.
(19.37)
âˆ‚ hÌ‚i
We can then iteratively apply the solution to the equation for i = 1, . . . , m,
and repeat the cycle until we satisfy a converge criterion. Common convergence
criteria include stopping when a full cycle of updates does not improve L by more
than some tolerance amount, or when the cycle does not change hÌ‚ by more than
some amount.
Iterating mean ï¬?eld ï¬?xed point equations is a general technique that can
provide fast variational inference in a broad variety of models. To make this more
concrete, we show how to derive the updates for the binary sparse coding model in
particular.
First, we must write an expression for the derivatives with respect to hÌ‚i . To
do so, we substitute equation 19.36 into the left side of equation 19.37:
âˆ‚

(19.38)
L(v, Î¸, hÌ‚)
ï£®
m
î?©
âˆ‚ ï£° î?˜î?¨
Ë†
Ë†
=
hÌ‚j (log Ïƒ(bj ) âˆ’ log h j ) + (1 âˆ’ hj )(log Ïƒ(âˆ’bj ) âˆ’ log(1 âˆ’ hÌ‚j )) (19.39)
âˆ‚ hÌ‚i j=1
ï£®
ï£«
ï£®
ï£¹ï£¶ï£¹ ï£¹
n
î?˜
î?˜
î?˜
1
2
ï£°log Î²j âˆ’ Î² j ï£­v 2j âˆ’ 2v j Wj,: hÌ‚ +
ï£° Wj,k
+
hÌ‚ k +
W j,kWj,l hÌ‚khÌ‚ l ï£»ï£¸ï£» ï£»
2
2Ï€
âˆ‚ hÌ‚i

j=1

k

= log Ïƒ (bi) âˆ’ log hÌ‚ i âˆ’ 1 + log(1 âˆ’ hÌ‚ i) + 1 âˆ’ log Ïƒ(âˆ’bi)
ï£® ï£«
ï£¶ï£¹
n
î?˜
î?˜
2
ï£°Î² j ï£­vj Wj,i âˆ’ 1 Wj,i
+
W j,kWj,i hÌ‚k ï£¸ï£»
âˆ’
2
j=1
kî€¶=i

643

lî€¶=k

(19.40)

(19.41)
(19.42)

CHAPTER 19. APPROXIMATE INFERENCE

î?˜
1 î€¾
î€¾
=bi âˆ’ log hÌ‚i + log(1 âˆ’ hÌ‚i) + v î€¾Î²W:,i âˆ’ W :,i
Î²W:,i âˆ’
W:,j
Î²W:,i hÌ‚j . (19.43)
2
j î€¶=i

To apply the ï¬?xed point update inference rule, we solve for the hÌ‚ i that sets
equation 19.43 to 0:
ï£«
ï£¶
î?˜
1
î€¾
hÌ‚i = Ïƒ ï£­ bi + v î€¾Î²W:,i âˆ’ W:,iî€¾Î²W :,i âˆ’
W:,j
Î²W:,i hÌ‚ j ï£¸ .
(19.44)
2
j î€¶=i

At this point, we can see that there is a close connection between recurrent
neural networks and inference in graphical models. Speciï¬?cally, the mean ï¬?eld
ï¬?xed point equations deï¬?ned a recurrent neural network. The task of this network
is to perform inference. We have described how to derive this network from a
model description, but it is also possible to train the inference network directly.
Several ideas based on this theme are described in chapter 20.
In the case of binary sparse coding, we can see that the recurrent network
connection speciï¬?ed by equation 19.44 consists of repeatedly updating the hidden
units based on the changing values of the neighboring hidden units. The input
always sends a ï¬?xed message of vî€¾ Î²W to the hidden units, but the hidden units
constantly update the message they send to each other. Speciï¬?cally, two units hÌ‚ i
and hÌ‚j inhibit each other when their weight vectors are aligned. This is a form of
competitionâ€”between two hidden units that both explain the input, only the one
that explains the input best will be allowed to remain active. This competition is
the mean ï¬?eld approximationâ€™s attempt to capture the explaining away interactions
in the binary sparse coding posterior. The explaining away eï¬€ect actually should
cause a multi-modal posterior, so that if we draw samples from the posterior,
some samples will have one unit active, other samples will have the other unit
active, but very few samples have both active. Unfortunately, explaining away
interactions cannot be modeled by the factorial q used for mean ï¬?eld, so the mean
ï¬?eld approximation is forced to choose one mode to model. This is an instance of
the behavior illustrated in ï¬?gure 3.6.
We can rewrite equation 19.44 into an equivalent form that reveals some further
insights:
ï£«
ï£¶
ï£«
ï£¶î€¾
î?˜
1 î€¾
ï£¬
ï£·
hÌ‚ i = Ïƒ ï£­b i + ï£­v âˆ’
W:,j hÌ‚j ï£¸ Î²W :,i âˆ’ W:,i
Î²W:,iï£¸ .
(19.45)
2
j î€¶=i
î??
In this reformulation, we see the input at each step as consisting of v âˆ’ j î€¶=i W :,j hÌ‚j
rather than v. We can thus think of unit i as attempting to encode the residual
644

CHAPTER 19. APPROXIMATE INFERENCE

error in v given the code of the other units. We can thus think of sparse coding as
an iterative autoencoder, that repeatedly encodes and decodes its input, attempting
to ï¬?x mistakes in the reconstruction after each iteration.
In this example, we have derived an update rule that updates a single unit at
a time. It would be advantageous to be able to update more units simultaneously.
Some graphical models, such as deep Boltzmann machines, are structured in such a
way that we can solve for many entries of hÌ‚ simultaneously. Unfortunately, binary
sparse coding does not admit such block updates. Instead, we can use a heuristic
technique called damping to perform block updates. In the damping approach,
we solve for the individually optimal values of every element of hÌ‚, then move all of
the values in a small step in that direction. This approach is no longer guaranteed
to increase L at each step, but works w
gful.

Larger modules can be deï¬ned via smaller modules as well, e.g., one
activation layer Ïƒ and a matrix multiplication layer MM are often combined
and called a â€œlayerâ€ in many papers. People often draw the architecture
with the basic modules in a ï¬gure by indicating the dependency between
these modules. E.g., see an illustration of an MLP in Figure 7.4, Left.

Residual connections. One of the very inï¬‚uential neural network archi-
tecture for vision application is ResNet, which uses the residual connections
that are essentially used in almost all large-scale deep learning architectures
these days. Using our notation above, a very much simpliï¬ed residual block
can be deï¬ned as

Res(z) = z + Ïƒ(MM(Ïƒ(MM(z)))).

(7.38)

A much simpliï¬ed ResNet is a composition of many residual blocks followed
by a matrix multiplication,

ResNet-S(x) = MM(Res(Res(Â· Â· Â· Res(x)))).

(7.39)

94

Figure 7.4:
layers. Right: A residual network.

Illustrative Figures for Architecture. Left: An MLP with r

We also draw the dependency of these modules in Figure 7.4, Right.

We note that the ResNet-S is still not the same as the ResNet architec-
ture introduced in the seminal paper [He et al., 2016] because ResNet uses
convolution layers instead of vanilla matrix multiplication, and adds batch
normalization between convolutions and activations. We will introduce con-
volutional layers and some variants of batch normalization below. ResNet-S
and layer normalization are part of the Transformer architecture that are
widely used in modern large language models.

Layer normalization. Layer normalization, denoted by LN in this text,
is a module that maps a vector z âˆˆ Rm to a more normalized vector LN(z) âˆˆ
Rm. It is oftentimes used after the nonlinear activations.

We ï¬rst deï¬ne a sub-module of the layer normalization, denoted by LN-S.

LN-S(z) =

ï£¹

ï£º
ï£º
ï£º
ï£»

,

ï£®

ï£¯
ï£¯
ï£¯
ï£°

z1âˆ’Ë†Âµ
Ë†Ïƒ
z2âˆ’Ë†Âµ
Ë†Ïƒ

...

zmâˆ’Ë†Âµ
Ë†Ïƒ

(7.40)

(cid:80)m

i=1 zi
m

is the empirical mean of the vector z and Ë†Ïƒ =

where Ë†Âµ =
is the empirical standard deviation of the entries of z.4 Intuitively, LN-S(z)
is a vector that is normalized to having empirical mean zero and empirical
standard deviation 1.

(cid:113) (cid:80)m

i=1(ziâˆ’Ë†Âµ2)
m

4Note that we divide by m instead of m âˆ’ 1 in the empirical standard deviation here
because we are interested in making the output of LN-S(z) have sum of squares equal to
1 (as opposed to estimating the standard deviation in statistics.)

ð‘¥Layer ð‘Ÿâˆ’1Layer ð‘–...Layer 1MLP(ð‘¥)...Layerð‘–MM!["],#["]ðœŽMM![$],#[$]ð‘¥ResRes...ResResNet-S(ð‘¥)...ResMMðœŽMMðœŽ95

Oftentimes zero mean and standard deviation 1 is not the most desired
normalization scheme, and thus layernorm introduces to parameters learnable
scalars Î² and Î³ as the desired mean and standard deviation, and use an aï¬ƒne
transformation to turn the output of LN-S(z) into a vector with mean Î² and
standard deviation Î³.

LN(z) = Î² + Î³ Â· LN-S(z) =

Ë†Ïƒ

ï£®

Î² + Î³ (cid:0) z1âˆ’Ë†Âµ
Î² + Î³ (cid:0) z2âˆ’Ë†Âµ
ï£¯
ï£¯
...
ï£¯
ï£°
Î² + Î³ (cid:0) zmâˆ’Ë†Âµ

Ë†Ïƒ

ï£¹

(cid:1)
(cid:1)

ï£º
ï£º
ï£º
ï£»
(cid:1)

Ë†Ïƒ

.

(7.41)

Here the ï¬rst occurrence of Î² should be technically interpreted as a vector
with all the entries being Î². in We also note that Ë†Âµ and Ë†Ïƒ are also functions
of z and shouldnâ€™t be treated as constants when computing the derivatives of
layernorm. Moreover, Î² and Î³ are learnable parameters and thus layernorm
is a parameterized module (as opposed to the activation layer which doesnâ€™t
have any parameters.)

Scaling-invariant property. One important property of layer normalization
is that it will make the model invariant to scaling of the parameters in the
following sense. Suppose we consider composing LN with MMW,b and get
a subnetwork LN(MMW,b(z)). Then, we have that the output of this sub-
network does not change when the parameter in MMW,b is scaled:

LN(MMÎ±W,Î±b(z)) = LN(MMW,b(z)), âˆ€Î± > 0.

(7.42)

To see this, we ï¬rst know that LN-S(Â·) is scale-invariant

LN-S(Î±z) =

ï£®

ï£¯
ï£¯
ï£¯
ï£°

ï£¹

ï£º
ï£º
ï£º
ï£»

Î±z1âˆ’Î±Ë†Âµ
Î±Ë†Ïƒ
Î±z2âˆ’Î±Ë†Âµ
Î±Ë†Ïƒ

...

Î±zmâˆ’Î±Ë†Âµ
Î±Ë†Ïƒ

=

ï£¹

ï£º
ï£º
ï£º
ï£»

ï£®

ï£¯
ï£¯
ï£¯
ï£°

z1âˆ’Ë†Âµ
Ë†Ïƒ
z2âˆ’Ë†Âµ
Ë†Ïƒ

...

zmâˆ’Ë†Âµ
Ë†Ïƒ

= LN-S(z).

(7.43)

Then we have

LN(MMÎ±W,Î±b(z)) = Î² + Î³LN-S(MMÎ±W,Î±b(z))
= Î² + Î³LN-S(Î±MMW,b(z))
= Î² + Î³LN-S(MMW,b(z))
= LN(MMW,b(z)).

(7.44)
(7.45)
(7.46)
(7.47)

Due to this property, most of the modern DL architectures for large-scale
computer vision and language applications have the following scale-invariant

96

property w.r.t all the weights that are not at the last layer. Suppose the
network f has last layerâ€™ weights Wlast, and all the rest of the weights are
denote by W . Then, we have fWlast,Î±W (x) = fWlast,W (x) for all Î± > 0. Here,
the last layers weights are special because there are typically no layernorm
or batchnorm after the last layerâ€™s weights.

Other normalization layers. There are several other normalization layers that
aim to normalize the intermediate layers of the neural networks to a more
ï¬xed and controllable scaling, such as batch-normalization [?], and group
normalization [?]. Batch normalization and group normalization are more
often used in computer vision applications whereas layer norm is used more
often in language applications.

Convolutional Layers. Convolutional Neural Networks are neural net-
works that consist of convolution layers (and many other modules), and are
particularly useful for computer vision applications. For the simplicity of
exposition, we focus on 1-D convolution in this text and only brieï¬‚y mention
2-D convolution informally at the end of this subsection. (2-D convolution
is more suitable for images which have two dimensions. 1-D convolution is
also used in natural language processing.)

We start by introducing a simpliï¬ed version of the 1-D convolution layer,
denoted by Conv1D-S(Â·) which is a type of matrix multiplication layer with
a special structure. The parameters of Conv1D-S are a ï¬lter vector w âˆˆ Rk
where k is called the ï¬lter size (oftentimes k (cid:28) m), and a bias scalar b.
Oftentimes the ï¬lter is also called a kernel (but it does not have much to do
with the kernel in kernel method.) For simplicity, we assume k = 2(cid:96) + 1 is
an odd number. We ï¬rst pad zeros to the input vector z in the sense that we
let z1âˆ’(cid:96) = z1âˆ’(cid:96)+1 = .. = z0 = 0 and zm+1 = zm+2 = .. = zm+(cid:96) = 0, and treat
z as an (m + 2(cid:96))-dimension vector. Conv1D-S outputs a vector of dimension
Rm where each output dimension is a linear combination of subsets of zjâ€™s
with coeï¬ƒcients from w,

Conv1D-S(z)i = w1ziâˆ’(cid:96) + w2ziâˆ’(cid:96)+1 + Â· Â· Â· + w2(cid:96)+1zi+(cid:96) =

2(cid:96)+1
(cid:88)

j=1

wjziâˆ’(cid:96)+(jâˆ’1).

(7.48)

Therefore, one can view Conv1D-S as a matrix multiplication with shared

parameters: Conv1D-S(z) = Qz, where

Q =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

w(cid:96)+1
w(cid:96)
...
w1
0
...
...
0
...
0

Â· Â· Â· w2(cid:96)+1
Â· Â· Â·

w2(cid:96)

0
w2(cid:96)+1

0
0

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
w1

w(cid:96)+1
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

Â· Â· Â· w2(cid:96)+1
Â· Â· Â·

w2(cid:96)

0
w2(cid:96)+1

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
0

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

Â· Â· Â·
Â· Â· Â·

0
0

0
0

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

0

w1

Â· Â· Â·

Â· Â· Â· w2(cid:96)+1

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

Â· Â· Â·

0

w1

Â· Â· Â·

w(cid:96)+1

97

.

(7.49)

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

Note that Qi,j = Qiâˆ’1,jâˆ’1 for all i, j âˆˆ {2, . . . , m}, and thus convoluation is a
matrix multiplication with parameter sharing. We also note that computing
the convolution only takes O(km) times but computing a generic matrix
multiplication takes O(m2) time. Convolution has k parameters but generic
matrix multiplication will have m2 parameters. Thus convolution is supposed
to be much more eï¬ƒcient than a generic matrix multiplication (as long as
the additional structure imposed does not hurt the ï¬‚exibility of the model
to ï¬t the data).

We also note that in practice there are many variants of the convolutional
layers that we deï¬ne here, e.g., there are other ways to pad zeros or sometimes
the dimension of the output of the convolutional layers could be diï¬€erent from
the input. We omit some of this subtleties here for simplicity.

The convolutional layers used in practice have also many â€œchannelsâ€ and
the simpliï¬ed version above corresponds to the 1-channel version. Formally,
Conv1D takes in C vectors z1, . . . , zC âˆˆ Rm as inputs, where C is referred
to as the number of channels.
In other words, the more general version,
denoted by Conv1D, takes in a matrix as input, which is the concatenation
of z1, . . . , zC and has dimension mÃ—C. It can output C (cid:48) vectors of dimension
m, denoted by Conv1D(z)1, . . . , Conv1D(z)C(cid:48), where C (cid:48) is referred to as the
output channel, or equivalently a matrix of dimension m Ã— C (cid:48). Each of the
output is a sum of the simpliï¬ed convolutions applied on various channels.

âˆ€i âˆˆ [C (cid:48)], Conv1D(z)i =

C
(cid:88)

j=1

Conv1D-Si,j(zj).

(7.50)

Note that each Conv1D-Si,j are modules with diï¬€erent parameters, and
thus the total number of parameters is k (the number of parameters in a
Conv1D-S) Ã—CC (cid:48) (the number of Conv1D-Si.jâ€™s) = kCC (cid:48).
In contrast, a
generic linear mapping from RmÃ—C and RmÃ—C(cid:48) has m2CC (cid:48) parameters. The

98

parameters can also be represented as a three-dimensional tensor of dimen-
sion k Ã— C Ã— C (cid:48).

2-D convolution (brief ). A 2-D convolution with one channel, denoted by
Conv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional input
z âˆˆ RmÃ—m and applies a ï¬lter of size k Ã— k, and outputs Conv2D-S(z) âˆˆ
RmÃ—m. The full 2-D convolutional layer, denoted by Conv2D, takes in
a sequence of matrices z1, . . . , zC âˆˆ RmÃ—m, or equivalently a 3-D ten-
sor z = (z1, . . . , zC) âˆˆ RmÃ—mÃ—C and outputs a sequence of matrices,
Conv2D(z)1, . . . , Conv2D(z)C(cid:48) âˆˆ RmÃ—m, which can also be viewed as a 3D
tensor in RmÃ—mÃ—C(cid:48). Each channel of the output is sum of the outcomes of
applying Conv2D-S layers on all the input channels.

âˆ€i âˆˆ [C (cid:48)], Conv2D(z)i =

C
(cid:88)

j=1

Conv2D-Si,j(zj).

(7.51)

Because there are CC (cid:48) number of Conv2D-S modules and each of the
Conv2D-S module has k2 parameters, the total number of parameters is
CC (cid:48)k2. The parameters can also be viewed as a 4D tensor of dimension
C Ã— C (cid:48) Ã— k Ã— k.

7.4 Backpropagation

In this section, we introduce backpropgation or auto-diï¬€erentiation, which
computes the gradient of the loss âˆ‡J(Î¸) eï¬ƒciently. We will start with an
informal theorem that states that as long as a real-valued function f can be
eï¬ƒciently computed/evaluated by a diï¬€erentiable network or circuit, then its
gradient can be eï¬ƒciently computed in a similar time. We will then show
how to do this concretely for neural networks.

Because the formality of the general theorem is not the main focus here,
we will introduce the terms with informal deï¬nitions. By a diï¬€erentiable
circuit or a diï¬€erentiable network, we mean a composition of a sequence of
diï¬€erentiable arithmetic operations (additions, subtraction, multiplication,
divisions, etc) and elementary diï¬€erentiable functions (ReLU, exp, log, sin,
cos, etc.). Let the size of the circuit be the total number of such operations
and elementary functions. We assume that each of the operations and func-
tions, and their derivatives or partial derivatives ecan be computed in O(1)
time.

Theorem 7.4.1: [backpropagation or auto-diï¬€erentiation, informally stated]
Suppose a diï¬€erentiable circuit of size N computes a real-valued function

99

f : R(cid:96) â†’ R. Then, the gradient âˆ‡f can be computed in time O(N ), by a
circuit of size O(N ).5

We note that the loss function J (j)(Î¸) for j-th example can be indeed
computed by a sequence of operations and functions involving additions,
subtraction, multiplications, and non-linear activations. Thus the theorem
suggests that we should be able to compute the âˆ‡J (j)(Î¸) in a similar time
to that for computing J (j)(Î¸) itself. This does not only apply to the fully-
connected neural network introduced in the Section 7.2, but also many other
types of neural networks that uses more advance modules.

We remark that auto-diï¬€erentiation or backpropagation is already imple-
mented in all the deep learning packages such as tensorï¬‚ow and pytorch, and
thus in practice, in most of cases a researcher does not need to write their
backpropagation algorithms. However, understanding it is very helpful for
gaining insights into the working of deep learning.

Organization of the rest of the section. In Section 7.4.1, we will start review-
ing the basic Chain rule with a new perspective that is particularly useful
for understanding backpropgation. Section 7.4.2 will introduce the general
strategy for backpropagation. Section 7.4.2 will discuss how to compute the
so-called backward function for basic modules used in neural networks, and
Section 7.4.4 will put everything together to get a concrete backprop algo-
rithm for MLPs.

7.4.1 Preliminaries on partial derivatives

Suppose a scalar variable J depend on some variables z (which could be a
scalar, matrix, or high-order tensor), we write âˆ‚J
âˆ‚z as the partial derivatives
of J w.r.t to the variable z. We stress that the convention here is that âˆ‚J
âˆ‚z
has exactly the same dimension as z itself. For example, if z âˆˆ RmÃ—n, then
âˆ‚z âˆˆ RmÃ—n, and the (i, j)-entry of âˆ‚J
âˆ‚J

âˆ‚z is equal to âˆ‚J
âˆ‚zij

.

Remark 7.4.2: When both J and z are not scalars, the partial derivatives of
J w.r.t z becomes either a matrix or tensor and the notation becomes some-
what tricky. Besides the mathematical or notational challenges in dealing

5We note if the output of the function f does not depend on some of the input co-
ordinates, then we set by default the gradient w.r.t that coordinate to zero. Setting to
zero does not count towards the total runtime here in our accounting scheme. This is why
when N â‰¤ (cid:96), we can compute the gradient in O(N ) time, which might be potentially even
less than (cid:96).

100

with these partial derivatives of multi-variate functions, they are also expen-
sive to compute and store, and thus rarely explicitly constructed empirically.
The experience of authors of this note is that itâ€™s generally more productive
to think only about derivatives of scalar function w.r.t to vector, matrices,
or tensors. For example, in this note, we will not deal with derivatives of
multi-variate functions.

Chain rule. We review the chain rule in calculus but with a perspective
and notions that are more relevant for auto-diï¬€erentiation.

Consider a scalar variable J which is obtained by the composition of f

and g on some variable z,

z âˆˆ Rm
u = g(z) âˆˆ Rn
J = f (u) âˆˆ R .

(7.52)

The same derivations below can be easily extend to the cases when z and u
are matrices or tensors; but we insist that the ï¬nal variable J is a scalar. (See
also Remark 7.4.2.) Let u = (u1, . . . , un) and let g(z) = (g1(z), Â· Â· Â· , g
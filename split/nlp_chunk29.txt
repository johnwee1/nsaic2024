nouns (including
proper nouns), verbs, adjectives, and adverbs, as well as the smaller open class of
interjections. English has all ﬁve, although not every language does.

Nouns are words for people, places, or things, but include others as well. Com-
mon nouns include concrete terms like cat and mango, abstractions like algorithm
and beauty, and verb-like terms like pacing as in His pacing to and fro became quite
annoying. Nouns in English can occur with determiners (a goat, this bandwidth)
take possessives (IBM’s annual revenue), and may occur in the plural (goats, abaci).
Many languages, including English, divide common nouns into count nouns and
mass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-
tionship/relationships) and can be counted (one goat, two goats). Mass nouns are
used when something is conceptualized as a homogeneous group. So snow, salt, and
communism are not counted (i.e., *two snows or *two communisms). Proper nouns,
like Regina, Colorado, and IBM, are names of speciﬁc persons or entities.

164 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

verb

adjective

Verbs refer to actions and processes, including main verbs like draw, provide,
and go. English verbs have inﬂections (non-third-person-singular (eat), third-person-
singular (eats), progressive (eating), past participle (eaten)). While many scholars
believe that all human languages have the categories of noun and verb, others have
argued that some languages, such as Riau Indonesian and Tongan, don’t even make
this distinction (Broschart 1997; Evans 2000; Gil 2000) .

Adjectives often describe properties or qualities of nouns, like color (white,
black), age (old, young), and value (good, bad), but there are languages without
adjectives. In Korean, for example, the words corresponding to English adjectives
act as a subclass of verbs, so what is in English an adjective “beautiful” acts in
Korean like a verb meaning “to be beautiful”.

adverb

Adverbs are a hodge-podge. All the italicized words in this example are adverbs:

Actually, I ran home extremely quickly yesterday

locative

degree

manner

temporal

Adverbs generally modify something (often verbs, hence the name “adverb”, but
also other adverbs and entire verb phrases). Directional adverbs or locative ad-
verbs (home, here, downhill) specify the direction or location of some action; degree
adverbs (extremely, very, somewhat) specify the extent of some action, process, or
property; manner adverbs (slowly, slinkily, delicately) describe the manner of some
action or process; and temporal adverbs describe the time that some action or event
took place (yesterday, Monday).

interjection

Interjections (oh, hey, alas, uh, um) are a smaller open class that also includes

preposition

particle

phrasal verb

determiner

article

conjunction

complementizer
pronoun

wh

greetings (hello, goodbye) and question responses (yes, no, uh-huh).

English adpositions occur before nouns, hence are called prepositions. They can
indicate spatial or temporal relations, whether literal (on it, before then, by the house)
or metaphorical (on time, with gusto, beside herself), and relations like marking the
agent in Hamlet was written by Shakespeare.

A particle resembles a preposition or an adverb and is used in combination with
a verb. Particles often have extended meanings that aren’t quite the same as the
prepositions they resemble, as in the particle over in she turned the paper over. A
verb and a particle acting as a single unit is called a phrasal verb. The meaning
of phrasal verbs is often non-compositional—not predictable from the individual
meanings of the verb and the particle. Thus, turn down means ‘reject’, rule out
‘eliminate’, and go on ‘continue’.

Determiners like this and that (this chapter, that page) can mark the start of an
English noun phrase. Articles like a, an, and the, are a type of determiner that mark
discourse properties of the noun and are quite frequent; the is the most common
word in written English, with a and an right behind.

Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc-
tions like and, or, and but join two elements of equal status. Subordinating conjunc-
tions are used when one of the elements has some embedded status. For example,
the subordinating conjunction that in “I thought that you might like some milk” links
the main clause I thought with the subordinate clause you might like some milk. This
clause is called subordinate because this entire clause is the “content” of the main
verb thought. Subordinating conjunctions like that which link a verb to its argument
in this way are also called complementizers.

Pronouns act as a shorthand for referring to an entity or event. Personal pro-
nouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are
forms of personal pronouns that indicate either actual possession or more often just
an abstract relation between the person and some object (my, your, his, her, its, one’s,
our, their). Wh-pronouns (what, who, whom, whoever) are used in certain question

8.2

• PART-OF-SPEECH TAGGING

165

forms, or act as complementizers (Frida, who married Diego. . . ).

auxiliary

Auxiliary verbs mark semantic features of a main verb such as its tense, whether
it is completed (aspect), whether it is negated (polarity), and whether an action is
necessary, possible, suggested, or desired (mood). English auxiliaries include the
copula verb be, the two verbs do and have, forms, as well as modal verbs used to
copula
modal mark the mood associated with the event depicted by the main verb: can indicates

ability or possibility, may permission or possibility, must necessity.

An English-speciﬁc tagset, the 45-tag Penn Treebank tagset (Marcus et al., 1993),
shown in Fig. 8.2, has been used to label many syntactically annotated corpora like
the Penn Treebank corpora, so is worth knowing about.

Tag Description
CC coord. conj.
CD cardinal number
DT determiner
EX existential ‘there’
FW foreign word
IN preposition/

Tag Description

Example
and, but, or NNP proper noun, sing.
one, two
a, the
there
mea culpa
of, in, by

Example Tag Description
IBM
TO inﬁnitive to
NNPS proper noun, plu. Carolinas UH interjection
NNS noun, plural
PDT predeterminer
POS
PRP

llamas
all, both VBD verb past tense

possessive ending ’s
personal pronoun

VBG verb gerund

VB verb base

I, you, he VBN verb past partici-

Example
to
ah, oops
eat
ate
eating
eaten

subordin-conj
JJ
adjective
JJR comparative adj
JJS superlative adj
LS list item marker
MD modal
NN sing or mass noun llama
Figure 8.2 Penn Treebank part-of-speech tags.

yellow
bigger
wildest
1, 2, One
can, should RP

PRP$ possess. pronoun
RB
adverb
RBR comparative adv
RBS superlatv. adv
particle
SYM symbol

ple

your
VBP verb non-3sg-pr
quickly
VBZ verb 3sg pres
faster
WDT wh-determ.
fastest
WP wh-pronoun
up, off
WP$ wh-possess.
+, %, & WRB wh-adverb

eat
eats
which, that
what, who
whose
how, where

Below we show some examples with each word tagged according to both the UD
and Penn tagsets. Notice that the Penn tagset distinguishes tense and participles on
verbs, and has a special tag for the existential there construction in English. Note that
since London Journal of Medicine is a proper noun, both tagsets mark its component
nouns as PROPN/NNP, including journal and medicine, which might otherwise be
labeled as common nouns (NOUN/NN).

(8.1) There/PRO/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNS

there/ADV/RB ./PUNC/.

(8.2) Preliminary/ADJ/JJ ﬁndings/NOUN/NNS were/AUX/VBD reported/VERB/VBN

in/ADP/IN today/NOUN/NN ’s/PART/POS London/PROPN/NNP
Journal/PROPN/NNP of/ADP/IN Medicine/PROPN/NNP

8.2 Part-of-Speech Tagging

part-of-speech
tagging

ambiguous

Part-of-speech tagging is the process of assigning a part-of-speech to each word in
a text. The input is a sequence x1, x2, ..., xn of (tokenized) words and a tagset, and
the output is a sequence y1, y2, ..., yn of tags, each output yi corresponding exactly to
one input xi, as shown in the intuition in Fig. 8.3.

Tagging is a disambiguation task; words are ambiguous —have more than one
possible part-of-speech—and the goal is to ﬁnd the correct tag for the situation.
For example, book can be a verb (book that ﬂight) or a noun (hand me that book).
That can be a determiner (Does that ﬂight serve dinner) or a complementizer (I

166 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

Figure 8.3 The task of part-of-speech tagging: mapping from input words x1, x2, ..., xn to
output POS tags y1, y2, ..., yn .

ambiguity
resolution

accuracy

thought that your ﬂight was earlier). The goal of POS-tagging is to resolve these
ambiguities, choosing the proper tag for the context.

The accuracy of part-of-speech tagging algorithms (the percentage of test set
tags that match human gold labels) is extremely high. One study found accuracies
over 97% across 15 languages from the Universal Dependency (UD) treebank (Wu
and Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter
the algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also
about the human performance on this task, at least for English (Manning, 2011).

Types:

Unambiguous (1 tag)
Ambiguous

(2+ tags)

Tokens:

WSJ
44,432 (86%)
7,025 (14%)

Brown
45,799 (85%)
8,050 (15%)

Unambiguous (1 tag)
Ambiguous

(2+ tags)

577,421 (45%) 384,349 (33%)
711,780 (55%) 786,646 (67%)

Figure 8.4

Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).

We’ll introduce algorithms for the task in the next few sections, but ﬁrst let’s
explore the task. Exactly how hard is it? Fig. 8.4 shows that most word types
(85-86%) are unambiguous (Janet is always NNP, hesitantly is always RB). But the
ambiguous words, though accounting for only 14-15% of the vocabulary, are very
common, and 55-67% of word tokens in running text are ambiguous. Particularly
ambiguous common words include that, back, down, put and set; here are some
examples of the 6 different parts of speech for the word back:

earnings growth took a back/JJ seat
a small building in the back/NN
a clear majority of senators back/VBP the bill
Dave began to back/VB toward the door
enable the country to buy back/RP debt
I was twenty-one back/RB then

Nonetheless, many words are easy to disambiguate, because their different tags
aren’t equally likely. For example, a can be a determiner or the letter a, but the
determiner sense is much more likely.

This idea suggests a useful baseline: given an ambiguous word, choose the tag

which is most frequent in the training corpus. This is a key concept:

Most Frequent Class Baseline: Always compare a classiﬁer against a baseline at
least as good as the most frequent class baseline (assigning each token to the class
it occurred in most often in the training set).

willNOUNAUXVERBDETNOUNJanetbackthebillPart of Speech Taggerx1x2x3x4x5y1y2y3y4y58.3

• NAMED ENTITIES AND NAMED ENTITY TAGGING

167

The most-frequent-tag baseline has an accuracy of about 92%1. The baseline

thus differs from the state-of-the-art and human ceiling (97%) by only 5%.

8.3 Named Entities and Named Entity Tagging

named entity

named entity
recognition
NER

Part of speech tagging can tell us that words like Janet, Stanford University, and
Colorado are all proper nouns; being a proper noun is a grammatical property of
these words. But viewed from a semantic perspective, these proper nouns refer to
different kinds of entities: Janet is a person, Stanford University is an organization,
and Colorado is a location.

A named entity is, roughly speaking, anything that can be referred to with a
proper name: a person, a location, an organization. The task of named entity recog-
nition (NER) is to ﬁnd spans of text that constitute proper names and tag the type of
the entity. Four entity tags are most common: PER (person), LOC (location), ORG
(organization), or GPE (geo-political entity). However, the term named entity is
commonly extended to include things that aren’t entities per se, including dates,
times, and other kinds of temporal expressions, and even numerical expressions like
prices. Here’s an example of the output of an NER tagger:

Citing high fuel prices, [ORG United Airlines] said [TIME Friday] it
has increased fares by [MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ORG American Airlines], a
unit of [ORG AMR Corp.], immediately matched the move, spokesman
[PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.],
said the increase took effect [TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [LOC Chicago]
to [LOC Dallas] and [LOC Denver] to [LOC San Francisco].

The text contains 13 mentions of named entities including 5 organizations, 4 loca-
tions, 2 times, 1 person, and 1 mention of money. Figure 8.5 shows typical generic
named entity types. Many applications will also need to use speciﬁc entity types like
proteins, genes, commercial products, or works of art.

Type
People
Organization
Location
Geo-Political Entity GPE countries, states
Figure 8.5 A list of generic named entity types with the kinds of entities they refer to.

Example sentences
Tag Sample Categories
Turing is a giant of computer science.
PER people, characters
ORG companies, sports teams
The IPCC warned about the cyclone.
LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon.

Palo Alto is raising the fees for parking.

Named entity tagging is a useful ﬁrst step in lots of natural language processing
tasks. In sentiment analysis we might want to know a consumer’s sentiment toward a
particular entity. Entities are a useful ﬁrst stage in question answering, or for linking
text to information in structured knowledge sources like Wikipedia. And named
entity tagging is also central to tasks involving building semantic representations,
like extracting events and the relationship between participants.

Unlike part-of-speech tagging, where there is no segmentation problem since
each word gets one tag, the task of named entity recognition is to ﬁnd and label
spans of text, and is difﬁcult partly because of the ambiguity of segmentation; we

1

In English, on the WSJ corpus, tested on sections 22-24.

168 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

need to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,
most words in a text will not be named entities. Another difﬁculty is caused by type
ambiguity. The mention JFK can refer to a person, the airport in New York, or any
number of schools, bridges, and streets around the United States. Some examples of
this kind of cross-type confusion are given in Figure 8.6.

[PER Washington] was born into slavery on the farm of James Burroughs.
[ORG Washington] went up 2 games to 1 in the four-game series.
Blair arrived in [LOC Washington] for what may well be his last state visit.
In June, [GPE Washington] passed a primary seatbelt law.

Figure 8.6 Examples of type ambiguities in the use of the name Washington.

BIO

The standard approach to sequence labeling for a span-recognition problem like
NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us
to treat NER like a word-by-word sequence labeling task, via tags that capture both
the boundary and the named entity type. Consider the following sentence:

[PER Jane Villanueva ] of [OR
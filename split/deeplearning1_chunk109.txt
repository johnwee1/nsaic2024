nvolution
for autoencoders. Technical report, UniversitÃ© de MontrÃ©al. 357
Goodfellow, I. J. (2014). On distinguishability criteria for estimating generative models.
In International Conference on Learning Representations, Workshops Track . 622, 700,
701
Goodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding
for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning
Hierarchical Models. 532, 538
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a).
Maxout networks. In S. Dasgupta and D. McAllester, editors, ICMLâ€™13 , pages 1319â€“
1327. 193, 264, 344, 365, 455
Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction deep
Boltzmann machines. In NIPS26 . NIPS Foundation. 100, 617, 671, 672, 673, 674, 675,
698
Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R.,
Bergstra, J., Bastien, F., and Bengio, Y. (2013c). Pylearn2: a machine learning research
library. arXiv preprint arXiv:1308.4214 . 25, 446
Goodfellow, I. J., Courville, A., and Bengio, Y. (2013d). Scaling up spike-and-slab models
for unsupervised feature learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 35(8), 1902â€“1914. 497, 498, 499, 650, 683
Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y. (2014a). An empirical
investigation of catastrophic forgeting in gradient-based neural networks. In ICLRâ€™2014 .
194
738

BIBLIOGRAPHY

Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014b). Explaining and harnessing adversarial examples. CoRR , abs/1412.6572. 268, 269, 271, 555, 556
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., and Bengio, Y. (2014c). Generative adversarial networks. In NIPSâ€™2014 .
544, 689, 699, 701, 704
Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. (2014d). Multi-digit
number recognition from Street View imagery using deep convolutional neural networks.
In International Conference on Learning Representations . 25, 101, 201, 202, 203, 391,
422, 449
Goodfellow, I. J., Vinyals, O., and Saxe, A. M. (2015). Qualitatively characterizing neural
network optimization problems. In International Conference on Learning Representations. 285, 286, 287, 291
Goodman, J. (2001). Classes for fast maximum entropy training. In International
Conference on Acoustics, Speech and Signal Processing (ICASSP), Utah. 467
Gori, M. and Tesi, A. (1992). On the problem of local minima in backpropagation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, PAMI-14(1), 76â€“86. 284
Gosset, W. S. (1908). The probable error of a mean. Biometrika , 6(1), 1â€“25. Originally
published under the pseudonym â€œStudentâ€?. 21
Gouws, S., Bengio, Y., and Corrado, G. (2014). BilBOWA: Fast bilingual distributed
representations without word alignments. Technical report, arXiv:1410.2455. 476, 539
Graf, H. P. and Jackel, L. D. (1989). Analog electronic neural network circuits. Circuits
and Devices Magazine, IEEE , 5(4), 44â€“49. 451
Graves, A. (2011). Practical variational inference for neural networks. In NIPSâ€™2011 . 242
Graves, A. (2012). Supervised Sequence Labelling with Recurrent Neural Networks . Studies
in Computational Intelligence. Springer. 374, 395, 411, 460
Graves, A. (2013). Generating sequences with recurrent neural networks. Technical report,
arXiv:1308.0850. 190, 410, 415, 420
Graves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent
neural networks. In ICMLâ€™2014 . 410
Graves, A. and Schmidhuber, J. (2005). Framewise phoneme classiï¬?cation with bidirectional LSTM and other neural network architectures. Neural Networks , 18(5), 602â€“610.
395
Graves, A. and Schmidhuber, J. (2009). Oï¬„ine handwriting recognition with multidimensional recurrent neural networks. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, NIPSâ€™2008 , pages 545â€“552. 395
739

BIBLIOGRAPHY

Graves, A., FernÃ¡ndez, S., Gomez, F., and Schmidhuber, J. (2006). Connectionist temporal
classiï¬?cation: Labelling unsegmented sequence data with recurrent neural networks. In
ICMLâ€™2006 , pages 369â€“376, Pittsburgh, USA. 460
Graves, A., Liwicki, M., Bunke, H., Schmidhuber, J., and FernÃ¡ndez, S. (2008). Unconstrained on-line handwriting recognition with recurrent neural networks. In J. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, NIPSâ€™2007 , pages 577â€“584. 395
Graves, A., Liwicki, M., FernÃ¡ndez, S., Bertolami, R., Bunke, H., and Schmidhuber, J.
(2009). A novel connectionist system for unconstrained handwriting recognition. Pattern
Analysis and Machine Intelligence, IEEE Transactions on , 31(5), 855â€“868. 410
Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent
neural networks. In ICASSPâ€™2013 , pages 6645â€“6649. 395, 398, 410, 411, 460
Graves, A., Wayne, G., and Danihelka, I. (2014a).
arXiv:1410.5401. 25

Neural Turing machines.

Graves, A., Wayne, G., and Danihelka, I. (2014b). Neural Turing machines. arXiv preprint
arXiv:1410.5401 . 418
Grefenstette, E., Hermann, K. M., Suleyman, M., and Blunsom, P. (2015). Learning to
transduce with unbounded memory. In NIPSâ€™2015 . 418
Greï¬€, K., Srivastava, R. K., KoutnÃ­k, J., Steunebrink, B. R., and Schmidhuber, J. (2015).
LSTM: a search space odyssey. arXiv preprint arXiv:1503.04069 . 412
Gregor, K. and LeCun, Y. (2010a). Emergence of complex-like cells in a temporal product
network with local receptive ï¬?elds. Technical report, arXiv:1006.0448. 352
Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In
L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International
Conference on Machine Learning (ICML-10). ACM. 652
Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. (2014). Deep
autoregressive networks. In International Conference on Machine Learning (ICMLâ€™2014).
693
Gregor, K., Danihelka, I., Graves, A., and Wierstra, D. (2015). DRAW: A recurrent neural
network for image generation. arXiv preprint arXiv:1502.04623 . 698
Gretton, A., Borgwardt, K. M., Rasch, M. J., SchÃ¶lkopf, B., and Smola, A. (2012). A
kernel two-sample test. The Journal of Machine Learning Research , 13(1), 723â€“773.
704
GÃ¼lÃ§ehre, Ã‡. and Bengio, Y. (2013). Knowledge matters: Importance of prior information
for optimization. In International Conference on Learning Representations (ICLRâ€™2013).
25
740

BIBLIOGRAPHY

Guo, H. and Gelfand, S. B. (1992). Classiï¬?cation trees with neural network feature
extraction. Neural Networks, IEEE Transactions on, 3(6), 923â€“933. 450
Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning
with limited numerical precision. CoRR , abs/1502.02551. 452
Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth
International Conference on Artiï¬?cial Intelligence and Statistics (AISTATSâ€™10). 620
Hadsell, R., Sermanet, P., Ben, J., Erkan, A., Han, J., Muller, U., and LeCun, Y.
(2007). Online learning for oï¬€road robots: Spatial label propagation to learn long-range
traversability. In Proceedings of Robotics: Science and Systems, Atlanta, GA, USA. 453
Hajnal, A., Maass, W., Pudlak, P., Szegedy, M., and Turan, G. (1993). Threshold circuits
of bounded depth. J. Comput. System. Sci., 46, 129â€“154. 199
HÃ¥stad, J. (1986). Almost optimal lower bounds for small depth circuits. In Proceedings
of the 18th annual ACM Symposium on Theory of Computing , pages 6â€“20, Berkeley,
California. ACM Press. 199
HÃ¥stad, J. and Goldmann, M. (1991). On the power of small-depth threshold circuits.
Computational Complexity , 1, 113â€“129. 199
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The elements of statistical learning:
data mining, inference and prediction. Springer Series in Statistics. Springer Verlag.
146
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Delving deep into rectiï¬?ers: Surpassing
human-level performance on ImageNet classiï¬?cation. arXiv preprint arXiv:1502.01852 .
28, 193
Hebb, D. O. (1949). The Organization of Behavior . Wiley, New York. 14, 17, 656
Henaï¬€, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011). Unsupervised learning
of sparse features for scalable audio classiï¬?cation. In ISMIRâ€™11 . 523
Henderson, J. (2003). Inducing history representations for broad coverage statistical
parsing. In HLT-NAACL, pages 103â€“110. 477
Henderson, J. (2004). Discriminative training of a neural network statistical parser. In
Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics ,
page 95. 477
Henniges, M., Puertas, G., Bornschein, J., Eggert, J., and LÃ¼cke, J. (2010). Binary sparse
coding. In Latent Variable Analysis and Signal Separation, pages 450â€“457. Springer.
640
741

BIBLIOGRAPHY

Herault, J. and Ans, B. (1984). Circuits neuronaux Ã  synapses modiï¬?ables: DÃ©codage de
messages composites par apprentissage non supervisÃ©. Comptes Rendus de lâ€™AcadÃ©mie
des Sciences, 299(III-13), 525â€“â€“528. 491
Hinton, G. (2012). Neural networks for machine learning. Coursera, video lectures. 307
Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V.,
Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic
modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82â€“97. 23,
460
Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 . 448
Hinton, G. E. (1989). Connectionist learning procedures. Artiï¬?cial Intelligence , 40,
185â€“234. 494
Hinton, G. E. (1990). Mapping part-whole hierarchies into connectionist networks. Artiï¬?cial
Intelligence, 46(1), 47â€“75. 418
Hinton, G. E. (1999). Products of experts. In ICANNâ€™1999 . 571
Hinton, G. E. (2000). Training products of experts by minimizing contrastive divergence.
Technical Report GCNU TR 2000-004, Gatsby Unit, University College London. 610,
676
Hinton, G. E. (2006). To recognize shapes, ï¬?rst learn to generate images. Technical Report
UTML TR 2006-003, University of Toronto. 528, 595
Hinton, G. E. (2007a). How to do backpropagation in a brain. Invited talk at the
NIPSâ€™2007 Deep Learning Workshop. 656
Hinton, G. E. (2007b). Learning multiple layers of representation. Trends in cognitive
sciences , 11(10), 428â€“434. 660
Hinton, G. E. (2010). A practical guide to training restricted Boltzmann machines.
Technical Report UTML TR 2010-003, Department of Computer Science, University of
Toronto. 610
Hinton, G. E. and Ghahramani, Z. (1997). Generative models for discovering sparse
distributed representations. Philosophical Transactions of the Royal Society of London .
147
Hinton, G. E. and McClelland, J. L. (1988). Learning representations by recirculation. In
NIPSâ€™1987 , pages 358â€“366. 502
Hinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding. In NIPSâ€™2002 . 519
742

BIBLIOGRAPHY

Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with
neural networks. Science, 313(5786), 504â€“507. 509, 524, 528, 529, 534
Hinton, G. E. and Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines.
In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing,
volume 1, chapter 7, pages 282â€“317. MIT Press, Cambridge. 570, 654
Hinton, G. E. and Sejnowski, T. J. (1999). Unsupervised learning: foundations of neural
computation . MIT press. 541
Hinton, G. E. and Shallice, T. (1991). Lesioning an attractor network: investigations of
acquired dyslexia. Psychological review, 98(1), 74. 13
Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and
Helmholtz free energy. In NIPSâ€™1993 . 502
Hinton, G. E., Sejnowski, T. J., and Ackley, D. H. (1984). Boltzmann machines: Constraint
satisfaction networks that learn. Technical Report TR-CMU-CS-84-119, Carnegie-Mellon
University, Dept. of Computer Science. 570, 654
Hinton, G. E., McClelland, J., and Rumelhart, D. (1986). Distributed representations.
In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing:
Explorations in the Microstructure of Cognition, volume 1, pages 77â€“109. MIT Press,
Cambridge. 17, 225, 526
Hinton, G. E., Revow, M., and Dayan, P. (1995a). Recognizing handwritten digits using
mixtures of linear models. In G. Tesauro, D. Touretzky, and T. Leen, editors, Advances
in Neural Information Processing Systems 7 (NIPSâ€™94), pages 1015â€“1022. MIT Press,
Cambridge, MA. 489
Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995b). The wake-sleep algorithm
for unsupervised neural networks. Science, 268, 1558â€“1161. 504, 651
Hinton, G. E., Dayan, P., and Revow, M. (1997). Modelling the manifolds of images of
handwritten digits. IEEE Transactions on Neural Networks , 8, 65â€“74. 499
Hinton, G. E., Welling, M., Teh, Y. W., and Osindero, S. (2001). A new view of ICA. In
Proceedings of 3rd International Conference on Independent Component Analysis and
Blind Signal Separation (ICAâ€™01), pages 746â€“751, San Diego, CA. 491
Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief
nets. Neural Computation , 18, 1527â€“1554. 14, 19, 27, 143, 528, 529, 660, 661
Hinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A.,
Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. (2012b). Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Process. Mag., 29(6), 82â€“97. 101
743

BIBLIOGRAPHY

Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012c).
Improving neural networks by preventing co-adaptation of feature detectors. Technical
report, arXiv:1207.0580. 238, 263, 267
Hinton, G. E., Vinyals, O., and Dean, J. (2014). Dark knowledge. Invited talk at the
BayLearn Bay Area Machine Learning Symposium. 448
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma
thesis, T.U. MÃ¼nchen. 18, 401, 403
Hochreiter, S. and Schmidhuber, J. (1995). Simplifying neural nets by discovering ï¬‚at
minima. In Advances in Neural Information Processing Systems 7 , pages 529â€“536. MIT
Press. 243
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation,
9(8), 1735â€“1780. 18, 410, 411
Hochreiter, S., Bengio, Y., and Frasconi, P. (2001). Gradient ï¬‚ow in recurrent nets: the
diï¬ƒculty of learning long-term dependencies. In J. Kolen and S. Kremer, editors, Field
Guide to Dynamical Recurrent Networks. IEEE Press. 411
Holi, J. L. and Hwang, J.-N. (1993). Finite precision error analysis of neural network
hardware implementations. Computers, IEEE Transactions on, 42(3), 281â€“290. 451
Holt, J. L. and Baker, T. E. (1991). Back propagation simulations using limited precision calculations. In Neural Networks, 1991., IJCNN-91-Seattle International Joint
Conference on , volume 2, pages 121â€“126. IEEE. 451
Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are
universal approximators. Neural Networks , 2, 359â€“366. 198
Hornik, K., Stinchcombe, M., and White, H. (1990). Universal approximation of an
unknown mapping and its derivatives using multilayer feedforward networks. Neural
networks, 3(5), 551â€“560. 198
Hsu, F.-H. (2002). Behind Deep Blue: Building the Computer That Defeated the Worl
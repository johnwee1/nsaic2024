 grocery store or do you need to drive everywhere).
Combining the zip code with the wealth of the neighborhood may predict
the quality of the local elementary school. Given these three derived features
(family size, walkable, school quality), we may conclude that the price of the

50010001500200025003000350040004500500001002003004005006007008009001000housing pricessquare feetprice (in $1000)home ultimately depends on these three features.

86

Figure 7.2: Diagram of a small neural network for predicting housing prices.

Formally, the input to a neural network is a set of input features
x1, x2, x3, x4. We denote the intermediate variables for â€œfamily sizeâ€, â€œwalk-
ableâ€, and â€œschool qualityâ€ by a1, a2, a3 (these aiâ€™s are often referred to as
â€œhidden unitsâ€ or â€œhidden neuronsâ€). We represent each of the aiâ€™s as a neu-
ral network with a single neuron with a subset of x1, . . . , x4 as inputs. Then
as in Figure 7.1, we will have the parameterization:

a1 = ReLU(Î¸1x1 + Î¸2x2 + Î¸3)
a2 = ReLU(Î¸4x3 + Î¸5)
a3 = ReLU(Î¸6x3 + Î¸7x4 + Î¸8)

where (Î¸1, Â· Â· Â· , Î¸8) are parameters. Now we represent the ï¬nal output Â¯hÎ¸(x)
as another linear function with a1, a2, a3 as inputs, and we get3

Â¯hÎ¸(x) = Î¸9a1 + Î¸10a2 + Î¸11a3 + Î¸12

(7.13)

where Î¸ contains all the parameters (Î¸1, Â· Â· Â· , Î¸12).

Now we represent the output as a quite complex function of x with pa-
rameters Î¸. Then you can use this parametrization Â¯hÎ¸ with the machinery of
Section 7.1 to learn the parameters Î¸.

Inspiration from Biological Neural Networks. As the name suggests,
artiï¬cial neural networks were inspired by biological neural networks. The
hidden units a1, . . . , am correspond to the neurons in a biological neural net-
work, and the parameters Î¸iâ€™s correspond to the synapses. However, itâ€™s
unclear how similar the modern deep artiï¬cial neural networks are to the bi-
ological ones. For example, perhaps not many neuroscientists think biological

3Typically, for multi-layer neural network, at the end, near the output, we donâ€™t apply

ReLU, especially when the output is not necessarily a positive number.

Family SizeSchool QualityWalkableSize# BedroomsZip CodeWealthPricey87

neural networks could have 1000 layers, while some modern artiï¬cial neural
networks do (we will elaborate more on the notion of layers.) Moreover, itâ€™s
an open question whether human brains update their neural networks in a
way similar to the way that computer scientists learn artiï¬cial neural net-
works (using backpropagation, which we will introduce in the next section.).

Two-layer Fully-Connected Neural Networks. We constructed the
neural network in equation (7.13) using a signiï¬cant amount of prior knowl-
edge/belief about how the â€œfamily sizeâ€, â€œwalkableâ€, and â€œschool qualityâ€ are
determined by the inputs. We implicitly assumed that we know the family
size is an important quantity to look at and that it can be determined by
only the â€œsizeâ€ and â€œ# bedroomsâ€. Such a prior knowledge might not be
available for other applications. It would be more ï¬‚exible and general to have
a generic parameterization. A simple way would be to write the intermediate
variable a1 as a function of all x1, . . . , x4:

a1 = ReLU(w(cid:62)
a2 = ReLU(w(cid:62)
a3 = ReLU(w(cid:62)

1 x + b1), where w1 âˆˆ R4 and b1 âˆˆ R
2 x + b2), where w2 âˆˆ R4 and b2 âˆˆ R
3 x + b3), where w3 âˆˆ R4 and b3 âˆˆ R

(7.14)

We still deï¬ne Â¯hÎ¸(x) using equation (7.13) with a1, a2, a3 being deï¬ned as
above. Thus we have a so-called fully-connected neural network because
all the intermediate variables aiâ€™s depend on all the inputs xiâ€™s.

For full generality, a two-layer fully-connected neural network with m

hidden units and d dimensional input x âˆˆ Rd is deï¬ned as

âˆ€j âˆˆ [1, ..., m],

j where w[1]

j âˆˆ Rd, b[1]

j âˆˆ R

(cid:62)

x + b[1]

zj = w[1]
j
aj = ReLU(zj),
a = [a1, . . . , am](cid:62) âˆˆ Rm

(7.15)

Â¯hÎ¸(x) = w[2](cid:62)

a + b[2] where w[2] âˆˆ Rm, b[2] âˆˆ R,

(7.16)

Note that by default the vectors in Rd are viewed as column vectors, and
in particular a is a column vector with components a1, a2, ..., am. The indices
[1] and [2] are used to distinguish two sets of parameters: the w[1]
j â€™s (each of
which is a vector in Rd) and w[2] (which is a vector in Rm). We will have
more of these later.

Vectorization. Before we introduce neural networks with more layers and
more complex structures, we will simplify the expressions for neural networks

88

with more matrix and vector notations. Another important motivation of
vectorization is the speed perspective in the implementation.
In order to
implement a neural network eï¬ƒciently, one must be careful when using for
loops. The most natural way to implement equation (7.15) in code is perhaps
to use a for loop. In practice, the dimensionalities of the inputs and hidden
units are high. As a result, code will run very slowly if you use for loops.
Leveraging the parallelism in GPUs is/was crucial for the progress of deep
learning.

This gave rise to vectorization. Instead of using for loops, vectorization
takes advantage of matrix algebra and highly optimized numerical linear
algebra packages (e.g., BLAS) to make neural network computations run
quickly. Before the deep learning era, a for loop may have been suï¬ƒcient
on smaller datasets, but modern deep networks and state-of-the-art datasets
will be infeasible to run with for loops.

We vectorize the two-layer fully-connected neural network as below. We
deï¬ne a weight matrix W [1] in RmÃ—d as the concatenation of all the vectors
w[1]

j â€™s in the following way:

W [1] =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£°

(cid:62)

(cid:62)

(cid:62)

â€” w[1]
1
â€” w[1]
2
...
â€” w[1]
m

â€”

â€”

â€”

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£»

âˆˆ RmÃ—d

(7.17)

Now by the deï¬nition of matrix vector multiplication, we can write z =

[z1, . . . , zm](cid:62) âˆˆ Rm as

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£»

z1
...
...
zm
(cid:124) (cid:123)(cid:122) (cid:125)
z âˆˆ RmÃ—1

=

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£°

(cid:124)

(cid:62)

(cid:62)

â€”

â€”

â€” w[1]
1
â€” w[1]
2
...
â€” w[1]
m
(cid:123)(cid:122)
W [1] âˆˆ RmÃ—d

â€”

(cid:62)

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£»

(cid:125)

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£»

x1
x2
...
xd
(cid:124) (cid:123)(cid:122) (cid:125)
x âˆˆ RdÃ—1

+

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£»

b[1]
1
b[1]
2
...
b[1]
m
(cid:124) (cid:123)(cid:122) (cid:125)
b[1] âˆˆ RmÃ—1

(7.18)

Or succinctly,

z = W [1]x + b[1]

(7.19)

We remark again that a vector in Rd in this notes, following the conventions
previously established, is automatically viewed as a column vector, and can

89

also be viewed as a d Ã— 1 dimensional matrix. (Note that this is diï¬€erent
from numpy where a vector is viewed as a row vector in broadcasting.)

Computing the activations a âˆˆ Rm from z âˆˆ Rm involves an element-
wise non-linear application of the ReLU function, which can be computed in
parallel eï¬ƒciently. Overloading ReLU for element-wise application of ReLU
(meaning, for a vector t âˆˆ Rd, ReLU(t) is a vector such that ReLU(t)i =
ReLU(ti)), we have

a = ReLU(z)

(7.20)

Deï¬ne W [2] = [w[2](cid:62)] âˆˆ R1Ã—m similarly. Then, the model in equa-

tion (7.16) can be summarized as

a = ReLU(W [1]x + b[1])

Â¯hÎ¸(x) = W [2]a + b[2]

(7.21)

Here Î¸ consists of W [1], W [2] (often referred to as the weight matrices) and
b[1], b[2] (referred to as the biases). The collection of W [1], b[1] is referred to as
the ï¬rst layer, and W [2], b[2] the second layer. The activation a is referred to as
the hidden layer. A two-layer neural network is also called one-hidden-layer
neural network.

Multi-layer fully-connected neural networks. With this succinct no-
tations, we can stack more layers to get a deeper fully-connected neu-
Let
ral network.
W [1], . . . , W [r], b[1], . . . , b[r] be the weight matrices and biases of all the layers.
Then a multi-layer neural network can be written as

Let r be the number of

layers (weight matrices).

a[1] = ReLU(W [1]x + b[1])
a[2] = ReLU(W [2]a[1] + b[2])
Â· Â· Â·

a[râˆ’1] = ReLU(W [râˆ’1]a[râˆ’2] + b[râˆ’1])
Â¯hÎ¸(x) = W [r]a[râˆ’1] + b[r]

(7.22)

We note that the weight matrices and biases need to have compatible
dimensions for the equations above to make sense. If a[k] has dimension mk,
then the weight matrix W [k] should be of dimension mk Ã— mkâˆ’1, and the bias
b[k] âˆˆ Rmk. Moreover, W [1] âˆˆ Rm1Ã—d and W [r] âˆˆ R1Ã—mrâˆ’1.

90

The total number of neurons in the network is m1 + Â· Â· Â· + mr, and the
total number of parameters in this network is (d + 1)m1 + (m1 + 1)m2 + Â· Â· Â· +
(mrâˆ’1 + 1)mr.

Sometimes for notational consistency we also write a[0] = x, and a[r] =

hÎ¸(x). Then we have simple recursion that

a[k] = ReLU(W [k]a[kâˆ’1] + b[k]), âˆ€k = 1, . . . , r âˆ’ 1

(7.23)

Note that this would have be true for k = r if there were an additional
ReLU in equation (7.22), but often people like to make the last layer linear
(aka without a ReLU) so that negative outputs are possible and itâ€™s easier
to interpret the last layer as a linear model. (More on the interpretability at
the â€œconnection to kernel methodâ€ paragraph of this section.)

Other activation functions. The activation function ReLU can be re-
placed by many other non-linear function Ïƒ(Â·) that maps R to R such as

Ïƒ(z) =

Ïƒ(z) =

1
1 + eâˆ’z
ez âˆ’ eâˆ’z
ez + eâˆ’z

(sigmoid)

(tanh)

Ïƒ(z) = max{z, Î³z}, Î³ âˆˆ (0, 1)

(leaky ReLU)

(cid:20)

1 + erf(

(cid:21)

z
âˆš
2

)

(GELU)

Ïƒ(z) =

Ïƒ(z) =

z
2
1
Î²

log(1 + exp(Î²z)), Î² > 0

(Softplus)

(7.28)

(7.24)

(7.25)

(7.26)

(7.27)

The activation functions are plotted in Figure 7.3. Sigmoid and tanh are
less and less used these days partly because their are bounded from both sides
and the gradient of them vanishes as z goes to both positive and negative
inï¬nity (whereas all the other activation functions still have gradients as the
input goes to positive inï¬nity.) Softplus is not used very often either in
practice and can be viewed as a smoothing of the ReLU so that it has a
proper second order derivative. GELU and leaky ReLU are both variants of
ReLU but they have some non-zero gradient even when the input is negative.
GELU (or its slight variant) is used in NLP models such as BERT and GPT
(which we will discuss in Chapter 14.)

Why do we not use the identity function for Ïƒ(z)? That is, why
not use Ïƒ(z) = z? Assume for sake of argument that b[1] and b[2] are zeros.

91

Figure 7.3: Activation functions in deep learning.

Suppose Ïƒ(z) = z, then for two-layer neural network, we have that

Â¯hÎ¸(x) = W [2]a[1]

= W [2]Ïƒ(z[1])
= W [2]z[1]
= W [2]W [1]x
= ËœW x

by deï¬nition

since Ïƒ(z) = z

from Equation (7.18)
where ËœW = W [2]W [1]

(7.29)

(7.30)

(7.31)

(7.32)

(7.33)

Notice how W [2]W [1] collapsed into ËœW .

This is because applying a linear function to another linear function will
result in a linear function over the original input (i.e., you can construct a ËœW
such that ËœW x = W [2]W [1]x). This loses much of the representational power
of the neural network as often times the output we are trying to predict
has a non-linear relationship with the inputs. Without non-linear activation
functions, the neural network will simply perform linear regression.

Connection to the Kernel Method.
In the previous lectures, we covered
the concept of feature maps. Recall that the main motivation for feature
maps is to represent functions that are non-linear in the input x by Î¸(cid:62)Ï†(x),
where Î¸ are the parameters and Ï†(x), the feature map, is a handcrafted
function non-linear in the raw input x. The performance of the learning
algorithms can signiï¬cantly depends on the choice of the feature map Ï†(x).
Oftentimes people use domain knowledge to design the feature map Ï†(x) that

92

suits the particular applications. The process of choosing the feature maps
is often referred to as feature engineering.

We can view deep learning as a way to automatically learn the right
feature map (sometimes also referred to as â€œthe representationâ€) as follows.
Suppose we denote by Î² the collection of the parameters in a fully-connected
neural networks (equation (7.22)) except those in the last layer. Then we
can abstract right a[râˆ’1] as a function of the input x and the parameters in
Î²: a[râˆ’1] = Ï†Î²(x). Now we can write the model as

Â¯hÎ¸(x) = W [r]Ï†Î²(x) + b[r]

(7.34)

When Î² is ï¬xed, then Ï†Î²(Â·) can viewed as a feature map, and therefore Â¯hÎ¸(x)
is just a linear model over the features Ï†Î²(x). However, we will train the
neural networks, both the parameters in Î² and the parameters W [r], b[r] are
optimized, and therefore we are not learning a linear model in the feature
space, but also learning a good feature map Ï†Î²(Â·) itself so that itâ€™s possi-
ble to predict accurately with a linear model on top of the feature map.
Therefore, deep learning tends to depend less on the domain knowledge of
the particular applications and requires often less feature engineering. The
penultimate layer a[r] is often (informally) referred to as the learned features
or representations in the context of deep learning.

In the example of house price prediction, a fully-connected neural network
does not need us to specify the intermediate quantity such â€œfamily sizeâ€, and
may automatically discover some useful features in the last penultimate layer
(the activation a[râˆ’1]), and use them to linearly predict the housing price.
Often the feature map / representation obtained from one datasets (that is,
the function Ï†Î²(Â·) can be also useful for other datasets, which indicates they
contain essential information about the data. However, oftentimes, the neural
network will discover complex features which are very useful for predicting
the output but may be diï¬ƒcult for a human to understand or interpret. This
is why some people refer to neural networks as a black box, as it can be
diï¬ƒcult to understand the features it has discovered.

7.3 Modules in Modern Neural Networks

The multi-layer neural network introduced in equation (7.22) of Section 7.2
is often called multi-layer perceptron (MLP) these days. Modern neural net-
works used in practice are often much more complex and consist of multiple
building blocks or multiple layers of building blocks. In this section, we will

93

introduce some of the other building blocks and discuss possible ways to
combine them.

First, each matrix multiplication can be viewed as a building block. Con-
sider a matrix multiplication operation with parameters (W, b) where W is
the weight matrix and b is the bias vector, operating on an input z,

MMW,b(z) = W z + b .

(7.35)

Note that we implicitly assume all the dimensions are chosen to be compat-
ible. We will also drop the subscripts under MM when they are clear in the
context or just for convenience when they are not essential to the discussion.
Then, the MLP can be written as as a composition of multiple matrix
multiplication modules and nonlinear activation modules (which can also be
viewed as a building block):

MLP(x) = MMW [r],b[r](Ïƒ(MMW [râˆ’1],b[râˆ’1](Ïƒ(Â· Â· Â· MMW [1],b[1](x)))).

(7.36)

Alternatively, when we drop the subscripts that indicate the parameters for
convenience, we can write

MLP(x) = MM(Ïƒ(MMÏƒ(Â· Â· Â· MM(x)))).

(7.37)

Note that in this lecture notes, by default, all the modules have diï¬€erent
sets of parameters, and the dimensions of the parameters are chosen such
that the composition is meanin
ore matrix and vector notations. Another important motivation of
vectorization is the speed perspective in the implementation.
In order to
implement a neural network eï¬ƒciently, one must be careful when using for
loops. The most natural way to implement equation (7.15) in code is perhaps
to use a for loop. In practice, the dimensionalities of the inputs and hidden
units are high. As a result, code will run very slowly if you use for loops.
Leveraging the parallelism in GPUs is/was crucial for the progress of deep
learning.

This gave rise to vectorization. Instead of using for loops, vectorization
takes advantage of matrix algebra and highly optimized numerical linear
algebra packages (e.g., BLAS) to make neural network computations run
quickly. Before the deep learning era, a for loop may have been suï¬ƒcient
on smaller datasets, but modern deep networks and state-of-the-art datasets
will be infeasible to run with for loops.

We vectorize the two-layer fully-connected neural network as below. We
deï¬ne a weight matrix W [1] in RmÃ—d as the concatenation of all the vectors
w[1]

j â€™s in the following way:

W [1] =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£°

(cid:62)

(cid:62)

(cid:62)

â€” w[1]
1
â€” w[1]
2
...
â€” w[1]
m

â€”

â€”

â€”

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£»

âˆˆ RmÃ—d

(7.17)

Now by the deï¬nition of matrix vector multiplication, we can write z =

[z1, . . . , zm](cid:62) âˆˆ Rm as

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£»

z1
...
...
zm
(cid:124) (cid:123)(cid:122) (cid:125)
z âˆˆ RmÃ—1

=

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£°

(cid:124)

(cid:62)

(cid:62)

â€”

â€”

â€” w[1]
1
â€” w[1]
2
...
â€” w[1]
m
(cid:123)(cid:122)
W [1] âˆˆ RmÃ—d

â€”

(cid:62)

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£»

(cid:125)

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£»

x1
x2
...
xd
(cid:124) (cid:123)(cid:122) (cid:125)
x âˆˆ RdÃ—1

+

ï£®

ï£¹

ï£¯
ï£¯
ï£¯
ï£°

ï£º
ï£º
ï£º
ï£»

b[1]
1
b[1]
2
...
b[1]
m
(cid:124) (cid:123)(cid:122) (cid:125)
b[1] âˆˆ RmÃ—1

(7.18)

Or succinctly,

z = W [1]x + b[1]

(7.19)

We remark again that a vector in Rd in this notes, following the conventions
previously established, is automatically viewed as a column vector, and can

89

also be viewed as a d Ã— 1 dimensional matrix. (Note that this is diï¬€erent
from numpy where a vector is viewed as a row vector in broadcasting.)

Computing the activations a âˆˆ Rm from z âˆˆ Rm involves an element-
wise non-linear application of the ReLU function, which can be computed in
parallel eï¬ƒciently. Overloading ReLU for element-wise application of ReLU
(meaning, for a vector t âˆˆ Rd, ReLU(t) is a vector such that ReLU(t)i =
ReLU(ti)), we have

a = ReLU(z)

(7.20)

Deï¬ne W [2] = [w[2](cid:62)] âˆˆ R1Ã—m similarly. Then, the model in equa-

tion (7.16) can be summarized as

a = ReLU(W [1]x + b[1])

Â¯hÎ¸(x) = W [2]a + b[2]

(7.21)

Here Î¸ consists of W [1], W [2] (often referred to as the weight matrices) and
b[1], b[2] (referred to as the biases). The collection of W [1], b[1] is referred to as
the ï¬rst layer, and W [2], b[2] the second layer. The activation a is referred to as
the hidden layer. A two-layer neural network is also called one-hidden-layer
neural network.

Multi-layer fully-connected neural networks. With this succinct no-
tations, we can stack more layers to get a deeper fully-connected neu-
Let
ral network.
W [1], . . . , W [r], b[1], . . . , b[r] be the weight matrices and biases of all the layers.
Then a multi-layer neural network can be written as

Let r be the number of

layers (weight matrices).

a[1] = ReLU(W [1]x + b[1])
a[2] = ReLU(W [2]a[1] + b[2])
Â· Â· Â·

a[râˆ’1] = ReLU(W [râˆ’1]a[râˆ’2] + b[râˆ’1])
Â¯hÎ¸(x) = W [r]a[râˆ’1] + b[r]

(7.22)

We note that the weight matrices and biases need to have compatible
dimensions for the equations above to make sense. If a[k] has dimension mk,
then the weight matrix W [k] should be of dimension mk Ã— mkâˆ’1, and the bias
b[k] âˆˆ Rmk. Moreover, W [1] âˆˆ Rm1Ã—d and W [r] âˆˆ R1Ã—mrâˆ’1.

90

The total number of neurons in the network is m1 + Â· Â· Â· + mr, and the
total number of parameters in this network is (d + 1)m1 + (m1 + 1)m2 + Â· Â· Â· +
(mrâˆ’1 + 1)mr.

Sometimes for notational consistency we also write a[0] = x, and a[r] =

hÎ¸(x). Then we have simple recursion that

a[k] = ReLU(W [k]a[kâˆ’1] + b[k]), âˆ€k = 1, . . . , r âˆ’ 1

(7.23)

Note that this would have be true for k = r if there were an additional
ReLU in equation (7.22), but often people like to make the last layer linear
(aka without a ReLU) so that negative outputs are possible and itâ€™s easier
to interpret the last layer as a linear model. (More on the interpretability at
the â€œconnection to kernel methodâ€ paragraph of this section.)

Other activation functions. The activation function ReLU can be re-
placed by many other non-linear function Ïƒ(Â·) that maps R to R such as

Ïƒ(z) =

Ïƒ(z) =

1
1 + eâˆ’z
ez âˆ’ eâˆ’z
ez + eâˆ’z

(sigmoid)

(tanh)

Ïƒ(z) = max{z, Î³z}, Î³ âˆˆ (0, 1)

(leaky ReLU)

(cid:20)

1 + erf(

(cid:21)

z
âˆš
2

)

(GELU)

Ïƒ(z) =

Ïƒ(z) =

z
2
1
Î²

log(1 + exp(Î²z)), Î² > 0

(Softplus)

(7.28)

(7.24)

(7.25)

(7.26)

(7.27)

The activation functions are plotted in Figure 7.3. Sigmoid and tanh are
less and less used these days partly because their are bounded from both sides
and the gradient of them vanishes as z goes to both positive and negative
inï¬nity (whereas all the other activation functions still have gradients as the
input goes to positive inï¬nity.) Softplus is not used very often either in
practice and can be viewed as a smoothing of the ReLU so that it has a
proper second order derivative. GELU and leaky ReLU are both variants of
ReLU but they have some non-zero gradient even when the input is negative.
GELU (or its slight variant) is used in NLP models such as BERT and GPT
(which we will discuss in Chapter 14.)

Why do we not use the identity function for Ïƒ(z)? That is, why
not use Ïƒ(z) = z? Assume for sake of argument that b[1] and b[2] are zeros.

91

Figure 7.3: Activation functions in deep learning.

Suppose Ïƒ(z) = z, then for two-layer neural network, we have that

Â¯hÎ¸(x) = W [2]a[1]

= W [2]Ïƒ(z[1])
= W [2]z[1]
= W [2]W [1]x
= ËœW x

by deï¬nition

since Ïƒ(z) = z

from Equation (7.18)
where ËœW = W [2]W [1]

(7.29)

(7.30)

(7.31)

(7.32)

(7.33)

Notice how W [2]W [1] collapsed into ËœW .

This is because applying a linear function to another linear function will
result in a linear function over the original input (i.e., you can construct a ËœW
such that ËœW x = W [2]W [1]x). This loses much of the representational power
of the neural network as often times the output we are trying to predict
has a non-linear relationship with the inputs. Without non-linear activation
functions, the neural network will simply perform linear regression.

Connection to the Kernel Method.
In the previous lectures, we covered
the concept of feature maps. Recall that the main motivation for feature
maps is to represent functions that are non-linear in the input x by Î¸(cid:62)Ï†(x),
where Î¸ are the parameters and Ï†(x), the feature map, is a handcrafted
function non-linear in the raw input x. The performance of the learning
algorithms can signiï¬cantly depends on the choice of the feature map Ï†(x).
Oftentimes people use domain knowledge to design the feature map Ï†(x) that

92

suits the particular applications. The process of choosing the feature maps
is often referred to as feature engineering.

We can view deep learning as a way to automatically learn the right
feature map (sometimes also referred to as â€œthe representationâ€) as follows.
Suppose we denote by Î² the collection of the parameters in a fully-connected
neural networks (equation (7.22)) except those in the last layer. Then we
can abstract right a[râˆ’1] as a function of the input x and the parameters in
Î²: a[râˆ’1] = Ï†Î²(x). Now we can write the model as

Â¯hÎ¸(x) = W [r]Ï†Î²(x) + b[r]

(7.34)

When Î² is ï¬xed, then Ï†Î²(Â·) can viewed as a feature map, and therefore Â¯hÎ¸(x)
is just a linear model over the features Ï†Î²(x). However, we will train the
neural networks, both the parameters in Î² and the parameters W [r], b[r] are
optimized, and therefore we are not learning a linear model in the feature
space, but also learning a good feature map Ï†Î²(Â·) itself so that itâ€™s possi-
ble to predict accurately with a linear model on top of the feature map.
Therefore, deep learning tends to depend less on the domain knowledge of
the particular applications and requires often less feature engineering. The
penultimate layer a[r] is often (informally) referred to as the learned features
or representations in the context of deep learning.

In the example of house price prediction, a fully-connected neural network
does not need us to specify the intermediate quantity such â€œfamily sizeâ€, and
may automatically discover some useful features in the last penultimate layer
(the activation a[râˆ’1]), and use them to linearly predict the housing price.
Often the feature map / representation obtained from one datasets (that is,
the function Ï†Î²(Â·) can be also useful for other datasets, which indicates they
contain essential information about the data. However, oftentimes, the neural
network will discover complex features which are very useful for predicting
the output but may be diï¬ƒcult for a human to understand or interpret. This
is why some people refer to neural networks as a black box, as it can be
diï¬ƒcult to understand the features it has discovered.

7.3 Modules in Modern Neural Networks

The multi-layer neural network introduced in equation (7.22) of Section 7.2
is often called multi-layer perceptron (MLP) these days. Modern neural net-
works used in practice are often much more complex and consist of multiple
building blocks or multiple layers of building blocks. In this section, we will

93

introduce some of the other building blocks and discuss possible ways to
combine them.

First, each matrix multiplication can be viewed as a building block. Con-
sider a matrix multiplication operation with parameters (W, b) where W is
the weight matrix and b is the bias vector, operating on an input z,

MMW,b(z) = W z + b .

(7.35)

Note that we implicitly assume all the dimensions are chosen to be compat-
ible. We will also drop the subscripts under MM when they are clear in the
context or just for convenience when they are not essential to the discussion.
Then, the MLP can be written as as a composition of multiple matrix
multiplication modules and nonlinear activation modules (which can also be
viewed as a building block):

MLP(x) = MMW [r],b[r](Ïƒ(MMW [râˆ’1],b[râˆ’1](Ïƒ(Â· Â· Â· MMW [1],b[1](x)))).

(7.36)

Alternatively, when we drop the subscripts that indicate the parameters for
convenience, we can write

MLP(x) = MM(Ïƒ(MMÏƒ(Â· Â· Â· MM(x)))).

(7.37)

Note that in this lecture notes, by default, all the modules have diï¬€erent
sets of parameters, and the dimensions of the parameters are chosen such
that the composition is meaningful.

Larger modules can be deï¬ned via smaller modules as well, e.g., one
activation layer Ïƒ and a matrix multiplication layer MM are often combined
and called a â€œlayerâ€ in many papers. People often draw the architecture
with the basic modules in a ï¬gure by indicating the dependency between
these modules. E.g., see an illustration of an MLP in Figure 7.4, Left.

Residual connections. One of the very inï¬‚uential neural network archi-
tecture for vision application is ResNet, which uses the residual connections
that are essentially used in almost all large-scale deep learning architectures
these days. Using our notation above, a very much simpliï¬ed residual block
can be deï¬ned as

Res(z) = z + Ïƒ(MM(Ïƒ(MM(z)))).

(7.38)

A much simpliï¬ed ResNet is a composition of many residual blocks followed
by a matrix multiplication,

ResNet-S(x) = MM(Res(Res(Â· Â· Â· Res(x)))).

(7.39)

94

Figure 7.4:
layers. Right: A residual network.

Illustrative Figures for Architecture. Left: An MLP with r

We also draw the dependency of these modules in Figure 7.4, Right.

We note that the ResNet-S is still not the same as the ResNet architec-
ture introduced in the seminal paper [He et al., 2016] because ResNet uses
convolution layers instead of vanilla matrix multiplication, and adds batch
normalization between convolutions and activations. We will introduce con-
volutional layers and some variants of batch normalization below. ResNet-S
and layer normalization are part of the Transformer architecture that are
widely used in modern large language models.

Layer normalization. Layer normalization, denoted by LN in this text,
is a module that maps a vector z âˆˆ Rm to a more normalized vector LN(z) âˆˆ
Rm. It is oftentimes used after the nonlinear activations.

We ï¬rst deï¬ne a sub-module of the layer normalization, denoted by LN-S.

LN-S(z) =

ï£¹

ï£º
ï£º
ï£º
ï£»

,

ï£®

ï£¯
ï£¯
ï£¯
ï£°

z1âˆ’Ë†Âµ
Ë†Ïƒ
z2âˆ’Ë†Âµ
Ë†Ïƒ

...

zmâˆ’Ë†Âµ
Ë†Ïƒ

(7.40)

(cid:80)m

i=1 zi
m

is the empirical mean of the vector z and Ë†Ïƒ =

where Ë†Âµ =
is the empirical standard deviation of the entries of z.4 Intuitively, LN-S(z)
is a vector that is normalized to having empirical mean zero and empirical
standard deviation 1.

(cid:113) (cid:80)m

i=1(ziâˆ’Ë†Âµ2)
m

4Note that we divide by m instead of m âˆ’ 1 in the empirical standard deviation here
because we are interested in making the output of LN-S(z) have sum of squares equal to
1 (as opposed to estimating the standard deviation in statistics.)

ð‘¥Layer ð‘Ÿâˆ’1Layer ð‘–...Layer 1MLP(ð‘¥)...Layerð‘–MM!["],#["]ðœŽMM![$],#[$]ð‘¥ResRes...ResResNet-S(ð‘¥)...ResMMðœŽMMðœŽ95

Oftentimes zero mean and standard deviation 1 is not the most desired
normalization scheme, and thus layernorm introduces to parameters learnable
scalars Î² and Î³ as the desired mean and standard deviation, and use an aï¬ƒne
transformation to turn the output of LN-S(z) into a vector with mean Î² and
standard deviation Î³.

LN(z) = Î² + Î³ Â· LN-S(z) =

Ë†Ïƒ

ï£®

Î² + Î³ (cid:0) z1âˆ’Ë†Âµ
Î² + Î³ (cid:0) z2âˆ’Ë†Âµ
ï£¯
ï£¯
...
ï£¯
ï£°
Î² + Î³ (cid:0) zmâˆ’Ë†Âµ

Ë†Ïƒ

ï£¹

(cid:1)
(cid:1)

ï£º
ï£º
ï£º
ï£»
(cid:1)

Ë†Ïƒ

.

(7.41)

Here the ï¬rst occurrence of Î² should be technically interpreted as a vector
with all the entries being Î². in We also note that Ë†Âµ and Ë†Ïƒ are also functions
of z and shouldnâ€™t be treated as constants when computing the derivatives of
layernorm. Moreover, Î² and Î³ are learnable parameters and thus layernorm
is a parameterized module (as opposed to the activation layer which doesnâ€™t
have any parameters.)

Scaling-invariant property. One important property of layer normalization
is that it will make the model invariant to scaling of the parameters in the
following sense. Suppose we consider composing LN with MMW,b and get
a subnetwork LN(MMW,b(z)). Then, we have that the output of this sub-
network does not change when the parameter in MMW,b is scaled:

LN(MMÎ±W,Î±b(z)) = LN(MMW,b(z)), âˆ€Î± > 0.

(7.42)

To see this, we ï¬rst know that LN-S(Â·) is scale-invariant

LN-S(Î±z) =

ï£®

ï£¯
ï£¯
ï£¯
ï£°

ï£¹

ï£º
ï£º
ï£º
ï£»

Î±z1âˆ’Î±Ë†Âµ
Î±Ë†Ïƒ
Î±z2âˆ’Î±Ë†Âµ
Î±Ë†Ïƒ

...

Î±zmâˆ’Î±Ë†Âµ
Î±Ë†Ïƒ

=

ï£¹

ï£º
ï£º
ï£º
ï£»

ï£®

ï£¯
ï£¯
ï£¯
ï£°

z1âˆ’Ë†Âµ
Ë†Ïƒ
z2âˆ’Ë†Âµ
Ë†Ïƒ

...

zmâˆ’Ë†Âµ
Ë†Ïƒ

= LN-S(z).

(7.43)

Then we have

LN(MMÎ±W,Î±b(z)) = Î² + Î³LN-S(MMÎ±W,Î±b(z))
= Î² + Î³LN-S(Î±MMW,b(z))
= Î² + Î³LN-S(MMW,b(z))
= LN(MMW,b(z)).

(7.44)
(7.45)
(7.46)
(7.47)

Due to this property, most of the modern DL architectures for large-scale
computer vision and language applications have the following scale-invariant

96

property w.r.t all the weights that are not at the last layer. Suppose the
network f has last layerâ€™ weights Wlast, and all the rest of the weights are
denote by W . Then, we have fWlast,Î±W (x) = fWlast,W (x) for all Î± > 0. Here,
the last layers weights are special because there are typically no layernorm
or batchnorm after the last layerâ€™s weights.

Other normalization layers. There are several other normalization layers that
aim to normalize the intermediate layers of the neural networks t
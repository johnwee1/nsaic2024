o a training set (the first 60,000
images) and a test set (the last 10,000 images):

X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

The training set is already shuffled for us, which is good because this guarantees that
all cross-validation folds will be similar (you don’t want one fold to be missing some
digits). Moreover, some learning algorithms are sensitive to the order of the training
instances, and they perform poorly if they get many similar instances in a row. Shuf‐
fling the dataset ensures that this won’t happen.2

2 Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as

stock market prices or weather conditions). We will explore this in the next chapters.

MNIST 

| 

87

Training a Binary Classifier
Let’s  simplify  the  problem  for  now  and  only  try  to  identify  one  digit—for  example,
the number 5. This “5-detector” will be an example of a binary classifier, capable of
distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for
this classification task:

y_train_5 = (y_train == 5)  # True for all 5s, False for all other digits
y_test_5 = (y_test == 5)

Now let’s pick a classifier and train it. A good place to start is with a Stochastic Gradi‐
ent Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This classifier
has the advantage of being capable of handling very large datasets efficiently. This is
in  part  because  SGD  deals  with  training  instances  independently,  one  at  a  time
(which also makes SGD well suited for online learning), as we will see later. Let’s cre‐
ate an SGDClassifier and train it on the whole training set:

from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)

The  SGDClassifier  relies  on  randomness  during  training  (hence
the  name  “stochastic”).  If  you  want  reproducible  results,  you
should set the random_state parameter.

Now we can use it to detect images of the number 5:

>>> sgd_clf.predict([some_digit])
array([ True])

The classifier guesses that this image represents a 5 (True). Looks like it guessed right
in this particular case! Now, let’s evaluate this model’s performance.

Performance Measures
Evaluating a classifier is often significantly trickier than evaluating a regressor, so we
will  spend  a  large  part  of  this  chapter  on  this  topic.  There  are  many  performance
measures available, so grab another coffee and get ready to learn many new concepts
and acronyms!

88 

| 

Chapter 3: Classification

Measuring Accuracy Using Cross-Validation
A good way to evaluate a model is to use cross-validation, just as you did in Chap‐
ter 2.

Implementing Cross-Validation
Occasionally you will need more control over the cross-validation process than what
Scikit-Learn provides off the shelf. In these cases, you can implement cross-validation
yourself.  The  following  code  does  roughly  the  same  thing  as  Scikit-Learn’s
cross_val_score() function, and it prints the same result:

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

skfolds = StratifiedKFold(n_splits=3, random_state=42)

for train_index, test_index in skfolds.split(X_train, y_train_5):
    clone_clf = clone(sgd_clf)
    X_train_folds = X_train[train_index]
    y_train_folds = y_train_5[train_index]
    X_test_fold = X_train[test_index]
    y_test_fold = y_train_5[test_index]

    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_fold)
    n_correct = sum(y_pred == y_test_fold)
    print(n_correct / len(y_pred))  # prints 0.9502, 0.96565, and 0.96495

The StratifiedKFold class performs stratified sampling (as explained in Chapter 2)
to produce folds that contain a representative ratio of each class. At each iteration the
code creates a clone of the classifier, trains that clone on the training folds, and makes
predictions  on  the  test  fold.  Then  it  counts  the  number  of  correct  predictions  and
outputs the ratio of correct predictions.

Let’s  use  the  cross_val_score()  function  to  evaluate  our  SGDClassifier  model,
using K-fold cross-validation with three folds. Remember that K-fold cross-validation
means splitting the training set into K folds (in this case, three), then making predic‐
tions and evaluating them on each fold using a model trained on the remaining folds
(see Chapter 2):

>>> from sklearn.model_selection import cross_val_score
>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
array([0.96355, 0.93795, 0.95615])

Wow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds?
This  looks  amazing,  doesn’t  it?  Well,  before  you  get  too  excited,  let’s  look  at  a  very
dumb classifier that just classifies every single image in the “not-5” class:

Performance Measures 

| 

89

from sklearn.base import BaseEstimator

class Never5Classifier(BaseEstimator):
    def fit(self, X, y=None):
        return self
    def predict(self, X):
        return np.zeros((len(X), 1), dtype=bool)

Can you guess this model’s accuracy? Let’s find out:

>>> never_5_clf = Never5Classifier()
>>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring="accuracy")
array([0.91125, 0.90855, 0.90915])

That’s right, it has over 90% accuracy! This is simply because only about 10% of the
images are 5s, so if you always guess that an image is not a 5, you will be right about
90% of the time. Beats Nostradamus.

This demonstrates why accuracy is generally not the preferred performance measure
for classifiers, especially when you are dealing with skewed datasets (i.e., when some
classes are much more frequent than others).

Confusion Matrix
A much better way to evaluate the performance of a classifier is to look at the confu‐
sion matrix. The general idea is to count the number of times instances of class A are
classified as class B. For example, to know the number of times the classifier confused
images of 5s with 3s, you would look in the fifth row and third column of the confu‐
sion matrix.

To compute the confusion matrix, you first need to have a set of predictions so that
they  can  be  compared  to  the  actual  targets.  You  could  make  predictions  on  the  test
set, but let’s keep it untouched for now (remember that you want to use the test set
only at the very end of your project, once you have a classifier that you are ready to
launch). Instead, you can use the cross_val_predict() function:

from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)

Just  like  the  cross_val_score()  function,  cross_val_predict()  performs  K-fold
cross-validation, but instead of returning the evaluation scores, it returns the predic‐
tions  made  on  each  test  fold.  This  means  that  you  get  a  clean  prediction  for  each
instance in the training set (“clean” meaning that the prediction is made by a model
that never saw the data during training).

Now you are ready to get the confusion matrix using the confusion_matrix() func‐
tion.  Just  pass  it  the  target  classes  (y_train_5)  and  the  predicted  classes
(y_train_pred):

90 

| 

Chapter 3: Classification

>>> from sklearn.metrics import confusion_matrix
>>> confusion_matrix(y_train_5, y_train_pred)
array([[53057,  1522],
       [ 1325,  4096]])

Each row in a confusion matrix represents an actual class, while each column repre‐
sents a predicted class. The first row of this matrix considers non-5 images (the nega‐
tive  class):  53,057  of  them  were  correctly  classified  as  non-5s  (they  are  called  true
negatives),  while  the  remaining  1,522  were  wrongly  classified  as  5s  (false  positives).
The  second  row  considers  the  images  of  5s  (the  positive  class):  1,325  were  wrongly
classified as non-5s (false negatives), while the remaining 4,096 were correctly classi‐
fied as 5s (true positives). A perfect classifier would have only true positives and true
negatives, so its confusion matrix would have nonzero values only on its main diago‐
nal (top left to bottom right):

>>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection
>>> confusion_matrix(y_train_5, y_train_perfect_predictions)
array([[54579,     0],
       [    0,  5421]])

The confusion matrix gives you a lot of information, but sometimes you may prefer a
more concise metric. An interesting one to look at is the accuracy of the positive pre‐
dictions; this is called the precision of the classifier (Equation 3-1).

Equation 3-1. Precision

precision =

TP
TP + FP

TP is the number of true positives, and FP is the number of false positives.

A trivial way to have perfect precision is to make one single positive prediction and
ensure it is correct (precision = 1/1 = 100%). But this would not be very useful, since
the classifier would ignore all but one positive instance. So precision is typically used
along with another metric named recall, also called sensitivity or the true positive rate
(TPR): this is the ratio of positive instances that are correctly detected by the classifier
(Equation 3-2).

Equation 3-2. Recall

recall =

TP
TP + FN

FN is, of course, the number of false negatives.

If you are confused about the confusion matrix, Figure 3-2 may help.

Performance Measures 

| 

91

Figure 3-2. An illustrated confusion matrix shows examples of true negatives (top left),
false positives (top right), false negatives (lower left), and true positives (lower right)

Precision and Recall
Scikit-Learn provides several functions to compute classifier metrics, including preci‐
sion and recall:

>>> from sklearn.metrics import precision_score, recall_score
>>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)
0.7290850836596654
>>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)
0.7555801512636044

Now your 5-detector does not look as shiny as it did when you looked at its accuracy.
When it claims an image represents a 5, it is correct only 72.9% of the time. More‐
over, it only detects 75.6% of the 5s.

It is often convenient to combine precision and recall into a single metric called the F1
score, in particular if you need a simple way to compare two classifiers. The F1 score is
the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean
treats all values equally, the harmonic mean gives much more weight to low values.
As a result, the classifier will only get a high F1 score if both recall and precision are
high.

Equation 3-3. F1

F1 =

2
1
precision +

1
recall

= 2 ×

precision × recall
precision + recall

=

TP
FN + FP
2

TP +

92 

| 

Chapter 3: Classification

To compute the F1 score, simply call the f1_score() function:

>>> from sklearn.metrics import f1_score
>>> f1_score(y_train_5, y_train_pred)
0.7420962043663375

The F1 score favors classifiers that have similar precision and recall. This is not always
what you want: in some contexts you mostly care about precision, and in other con‐
texts you really care about recall. For example, if you trained a classifier to detect vid‐
eos  that  are  safe  for  kids,  you  would  probably  prefer  a  classifier  that  rejects  many
good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐
sifier that has a much higher recall but lets a few really bad videos show up in your
product (in such cases, you may even want to add a human pipeline to check the clas‐
sifier’s  video  selection).  On  the  other  hand,  suppose  you  train  a  classifier  to  detect
shoplifters  in  surveillance  images:  it  is  probably  fine  if  your  classifier  has  only  30%
precision  as  long  as  it  has  99%  recall  (sure,  the  security  guards  will  get  a  few  false
alerts, but almost all shoplifters will get caught).

Unfortunately,  you  can’t  have  it  both  ways:  increasing  precision  reduces  recall,  and
vice versa. This is called the precision/recall trade-off.

Precision/Recall Trade-off
To understand this trade-off, let’s look at how the SGDClassifier makes its classifica‐
tion decisions. For each instance, it computes a score based on a decision function. If
that  score  is  greater  than  a  threshold,  it  assigns  the  instance  to  the  positive  class;
otherwise it assigns it to the negative class. Figure 3-3 shows a few digits positioned
from the lowest score on the left to the highest score on the right. Suppose the deci‐
sion threshold is positioned at the central arrow (between the two 5s): you will find 4
true positives (actual 5s) on the right of that threshold, and 1 false positive (actually a
6).  Therefore,  with  that  threshold,  the  precision  is  80%  (4  out  of  5).  But  out  of  6
actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the
threshold (move it to the arrow on the right), the false positive (the 6) becomes a true
negative, thereby increasing the precision (up to 100% in this case), but one true posi‐
tive  becomes  a  false  negative,  decreasing  recall  down  to  50%.  Conversely,  lowering
the threshold increases recall and reduces precision.

Performance Measures 

| 

93

Figure 3-3. In this precision/recall trade-off, images are ranked by their classifier score,
and those above the chosen decision threshold are considered positive; the higher the
threshold, the lower the recall, but (in general) the higher the precision

Scikit-Learn does not let you set the threshold directly, but it does give you access to
the decision scores that it uses to make predictions. Instead of calling the classifier’s
predict()  method,  you  can  call  its  decision_function()  method,  which  returns  a
score  for  each  instance,  and  then  use  any  threshold  you  want  to  make  predictions
based on those scores:

>>> y_scores = sgd_clf.decision_function([some_digit])
>>> y_scores
array([2412.53175101])
>>> threshold = 0
>>> y_some_digit_pred = (y_scores > threshold)
array([ True])

The SGDClassifier uses a threshold equal to 0, so the previous code returns the same
result as the predict() method (i.e., True). Let’s raise the threshold:

>>> threshold = 8000
>>> y_some_digit_pred = (y_scores > threshold)
>>> y_some_digit_pred
array([False])

This  confirms  that  raising  the  threshold  decreases  recall.  The  image  actually  repre‐
sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the
threshold is increased to 8,000.

How  do  you  decide  which  threshold  to  use?  First,  use  the  cross_val_predict()
function to get the scores of all instances in the training set, but this time specify that
you want to return decision scores instead of predictions:

y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,
                             method="decision_function")

With these scores, use the precision_recall_curve() function to compute precision
and recall for all possible thresholds:

94 

| 

Chapter 3: Classification

from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)

Finally, use Matplotlib to plot precision and recall as functions of the threshold value
(Figure 3-4):

def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label
of CNN and Daily Mirror news
articles.

Original Article
The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff
and offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says.
But not if you live in New England or surrounding states. “We will not ship snow to any states
in the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging
snow!”
His website and social media accounts claim to have ﬁlled more than 133 orders for snow – more
than 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a
record this winter for the snowiest month in its history. Most residents see the huge piles of snow
choking their yards and sidewalks as a nuisance, but Waring saw an opportunity.
According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-
eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston.
He joked about shipping the stuff to friends and family in warmer states, and an idea was born.
His business slogan: “Our nightmare is your dream!” At ﬁrst, ShipSnowYo sold snow packed
into empty 16.9-ounce water bottles for $19.99, but the snow usually melted before it reached its
destination...

Summary
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.

Figure 10.16 Examples of articles and summaries from the CNN/Daily Mail corpus (Hermann et al., 2015b),
(Nallapati et al., 2016).

greedy
decoding

If we take this full article and append the token tl;dr, we can use this as
the context to prime the generation process to produce a summary as illustrated
in Fig. 10.17. Again, what makes transformers able to succeed at this task (as
compared, say, to the primitive n-gram language model) is that the ability of self-
attention to incorporate information from the large context windows means that
the model has access to the original article as well as to the newly generated text
throughout the process.

Which words do we generate at each step? One simple way to generate words
is to always generate the most likely word given the context. Generating the most
likely word given the context is called greedy decoding. A greedy algorithm is one
that make a choice that is locally optimal, whether or not it will turn out to have
been the best choice with hindsight. Thus in greedy decoding, at each time step in
generation, the output yt is chosen by computing the probability for each possible
outputs (every word in the vocabulary) and then choosing the highest probability
word (the argmax):

w<t )
V P(w
ˆwt = argmaxw
|
∈

(10.46)

In practice, however, we don’t use greedy decoding with large language models.
A major problem with greedy decoding is that because the words it chooses are (by
deﬁnition) extremely predictable, the resulting text is generic and often quite repeti-
tive. Indeed, greedy decoding is so predictable that it is deterministic; if the context

234 CHAPTER 10

• TRANSFORMERS AND LARGE LANGUAGE MODELS

Figure 10.17 Summarization with large language models using the tl;dr token and context-based autore-
gressive generation.

is identical, and the probabilistic model is the same, greedy decoding will always re-
sult in generating exactly the same string. We’ll see in Chapter 13 that an extension
to greedy decoding called beam search works well in tasks like machine translation,
which are very constrained in that we are always generating a text in one language
conditioned on a very speciﬁc text in another language. In most other tasks, how-
ever, people prefer text which has been generated by more sophisticated methods,
called sampling methods, that introduce a bit more diversity into the generations.
We’ll see how to do that in the next few sections.

10.8 Large Language Models: Generation by Sampling

decoding

autoregressive
generation

sampling

The core of the generation process for large language models is the task of choosing
the single word to generate next based on the context and based on the probabilities
that the model assigns to possible words. This task of choosing a word to generate
based on the model’s probabilities is called decoding. Decoding from a language
model in a left-to-right manner (or right-to-left for languages like Arabic in which
we read from right to left), and thus repeatedly choosing the next word conditioned
on our previous choices is called autoregressive generation or causal LM genera-
tion.1 (As we’ll see, alternatives like the masked language models of Chapter 11 are
non-causal because they can predict words based on both past and future words).

The most common method for decoding in large language models is sampling.
Recall from Chapter 3 that sampling from a model’s distribution over words means
to choose random words according to their probability assigned by the model. That
is, we iteratively choose a word to generate according to its probability in context

1 Technically an autoregressive model predicts a value at time t based on a linear function of the values
at times t
2, and so on. Although language models are not linear (since they have many layers of
non-linearities), we loosely refer to this generation technique as autoregressive since the word generated
at each time step is conditioned on the word selected by the network from the previous step.

1, t

−

−

Original StoryGenerated Summary…reachedKyleitsdestinationKyleWaringWaringonlyThe…willDelimiterwilltl;dr10.8

• LARGE LANGUAGE MODELS: GENERATION BY SAMPLING

235

as deﬁned by the model. Thus we are more likely to generate words that the model
thinks have a high probability in the context and less likely to generate words that
the model thinks have a low probability.

We saw back in Chapter 3 on page 42 how to generate text from a unigram lan-
guage model , by repeatedly randomly sampling words according to their probability
until we either reach a pre-determined length or select the end-of-sentence token. To
generate text from a trained transformer language model we’ll just generalize this
model a bit: at each step we’ll sample words according to their probability condi-
tioned on our previous choices, and we’ll use a transformer language model as the
probability model that tells us this probability.

We can formalize this algorithm for generating a sequence of words W = w1, w2, . . . , wN

until we hit the end-of-sequence token, using x
pling from the distribution p(x):

∼

p(x) to mean ‘choose x by sam-

i
1
←
p(w)
wi ∼
while wi != EOS
i
i + 1
←
wi ∼

p(wi |

w<i)

random
sampling

The algorithm above is called random sampling, and it turns out random sam-
pling doesn’t work well enough. The problem is that even though random sampling
is mostly going to generate sensible, high-probable words, there are many odd, low-
probability words in the tail of the distribution, and even though each one is low-
probability, if you add up all the rare words, they constitute a large enough portion
of the distribution that they get chosen often enough to result in generating weird
sentences. For this reason, instead of random sampling, we usually use sampling
methods that avoid generating the very unlikely words.

The sampling methods we introduce below each have parameters that enable
trading off two important factors in generation: quality and diversity. Methods
that emphasize the most probable words tend to produce generations that are rated
by people as more accurate, more coherent, and more factual, but also more boring
and more repetitive. Methods that give a bit more weight to the middle-probability
words tend to be more creative and more diverse, but less factual and more likely to
be incoherent or otherwise low-quality.

10.8.1 Top-k sampling

top-k sampling

Top-k sampling is a simple generalization of greedy decoding. Instead of choosing
the single most probable word to generate, we ﬁrst truncate the distribution to the
top k most likely words, renormalize to produce a legitimate probability distribution,
and then randomly sample from within these k words according to their renormalized
probabilities. More formally:

1. Choose in advance a number of words k

2. For each word in the vocabulary V , use the language model to compute the

likelihood of this word given the context p(wt |

w<t )

3. Sort the words by their likelihood, and throw away any word that is not one of

the top k most probable words.

4. Renormalize the scores of the k words to be a legitimate probability distribu-

tion.

236 CHAPTER 10

• TRANSFORMERS AND LARGE LANGUAGE MODELS

top-p sampling

temperature
sampling

5. Randomly sample a word from within these remaining k most-probable words

according to its probability.

When k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger
number than 1 leads us to sometimes select a word which is not necessarily the most
probable, but is still probable enough, and whose choice results in generating more
diverse but still high-enough-quality text.

10.8.2 Nucleus or top-p sampling

One problem with top-k sampling is that k is ﬁxed, but the shape of the the probabil-
ity distribution over words differs in different contexts. If we set k = 10, sometimes
the top 10 words will be very likely and include most of the probability mass, but
other times the probability distribution will be ﬂatter and the top 10 words will only
include a small part of the probability mass.

An alternative, called top-p sampling or nucleus sampling (Holtzman et al.,
2020), is to keep not the top k words, but the top p percent of the probability mass.
The goal is the same; to truncate the distribution to remove the very unlikely words.
But by measuring probability rather than the number of words, the hope is that the
measure will be more robust in very different contexts, dynamically increasing and
decreasing the pool of word candidates.

Given a distribution P(wt |

words such that

w<t ), the top-p vocabulary V (p) is the smallest set of

w<t )
P(w
|

≥

p.

V (p)
(cid:88)w
∈

(10.47)

10.8.3 Temperature sampling

In temperature sampling, we don’t truncate the distribution, but instead reshape
it. The intuition for temperature sampling comes from thermodynamics, where a
system at a high temperature is very ﬂexible and can explore many possible states,
while a system at a lower temperature is likely to explore a subset of lower energy
(better) states. In low-temperature sampling, we smoothly increase the probability
of the most probable words and decrease the probability of the rare words.

We implement this intuition by simply dividing the logit by a temperature param-
eter τ before we normalize it by passing it through the softmax. In low-temperature
sampling, τ
(0, 1]. Thus instead of computing the probability distribution over the
vocabulary directly from the logit as in the following (repeated from (10.45):

∈

y = softmax(u)

we instead ﬁrst divide the logits by τ, computing the probability vector y as

y = softmax(u/τ)

(10.48)

(10.49)

Why does this work? When τ is close to 1 the distribution doesn’t change much.
But the lower τ is, the larger the scores being passed to the softmax (dividing by a
smaller fraction τ
1 results in making each score larger). Recall that one of the
useful properties of a softmax is that it tends to push high values toward 1 and low
values toward 0. Thus when larger numbers are passed to a softmax the result is
a distribution with increased probabilities of the most high-probability words and
decreased probabilities of the low probability words, making the distribution more
greedy. As τ approaches 0 the probability of the most likely word approaches 1.

≤

10.9

• LARGE LANGUAGE MODELS: TRAINING TRANSFORMERS

237

Note, by the way, that there can be other situations where we may want to do
something quite different and ﬂatten the word probability distribution instead of
making it greedy. Temperature sampling can help with this situation too, in this case
high-temperature sampling, in which case we use τ > 1.

10.9 Large Language Models: Training Transformers

self-supervision

How do we teach a transformer to be a language model? What is the algorithm and
what data do we train on?

10.9.1 Self-supervised training algorithm

To train a transformer as a language model, we use the same self-supervision (or
self-training) algorithm we saw in Section 9.2.2: we take a corpus of text as training
material and at each time step t ask the model to predict the next word. We call such
a model self-supervised because we don’t have to add any special gold labels to
the data; the natural sequence of words is its own supervision! We simply train the
model to minimize the error in predicting the true next word in the training sequence,
using cross-entropy as the loss function.

Recall that the cross-entropy loss measures the difference between a predicted

probability distribution and the correct distribution.

LCE =

−

yt [w] log ˆyt [w]

(10.50)

V
(cid:88)w
∈
In the case of language modeling, the correct distribution yt comes from knowing the
next word. This is represented as a one-hot vector corresponding to the vocabulary
where the entry for the actual next word is 1, and all the other entries are 0. Thus,
the cross-entropy loss for language modeling is determined by the probability the
model assigns to the correct next word. So at time t the CE loss in (10.50) can be
simpliﬁed as the negative log probability the model assigns to the next word in the
training sequence.

LCE ( ˆyt , yt ) =

log ˆyt [wt+1]

−

(10.51)

Thus at each word position t of the input, the model takes as input the correct se-
quence of tokens w1:t , and uses them to compute a probability distribution over
possible next words so as to compute the model’s loss for the next token wt+1. Then
we move to the next word, we ignore what the model predicted for the next word
and instead use the correct sequence of tokens w1:t+1 to estimate the probability of
token wt+2. This idea that we always give the model the correct history sequence to
predict the next word (rather than feeding the model its best case from the previous
time step) is called teacher forcing.

Fig. 10.18 illustrates the general training approach. At each step, given all the
preceding words, the ﬁnal transformer layer produces an output distribution over
the entire vocabulary. During training, the probability assigned to the correct word
is used to calculate the cross-entropy loss for each item in the sequence. As with
RNNs, the loss for a training sequence is the average cross-entropy loss over the
entire sequence. The weights in the network are adjusted to minimize the average
CE loss over the training sequence via gradient descent.

teacher forcing

238 CHAPTER 10

• TRANSFORMERS AND LARGE LANGUAGE MODELS

Figure 10.18 Training a transformer as a language model.

Note the key difference between this ﬁgure and the earlier RNN-based version
shown in Fig. 9.6. There the calculation of the outputs and the losses at each step
was inherently serial given the recurrence in the calculation of the hidden states.
With transformers, each training item can be processed in parallel since the output
for each ele
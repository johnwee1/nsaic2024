FUNDAMENTAL ALGORITHMS FOR
NLP

In the ﬁrst part of the book we introduce the fundamental suite of algorithmic
tools that make up the modern neural language model that is the heart of end-to-end
NLP systems. We begin with tokenization and preprocessing, as well as useful algo-
rithms like computing edit distance, and then proceed to the tasks of classiﬁcation,
logistic regression, neural networks, proceeding through feedforward networks, re-
current networks, and then transformers. We’ll also see the role of embeddings as a
model of word meaning.

CHAPTER

1 Introduction

[The last thing you ﬁgure out in writing a book is what to put ﬁrst.]

Pascal

3

4 CHAPTER 2

• REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE

CHAPTER

2 Regular Expressions, Text

Normalization, Edit Distance

I am unhappy.

I need some help, that much seems certain.

Perhaps I could learn to get along with my mother.

User:
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY
User:
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
User:
ELIZA: TELL ME MORE ABOUT YOUR FAMILY
User:
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU
User:
ELIZA: YOUR FATHER
User:

You are like my father in some ways.

My mother takes care of me.

My father.

ELIZA

chatbots

text
normalization

tokenization

Weizenbaum (1966)

The dialogue above is from ELIZA, an early natural language processing system
that could carry on a limited conversation with a user by imitating the responses of
a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple
program that uses pattern matching to recognize phrases like “I need X” and translate
them into suitable outputs like “What would it mean to you if you got X?”. This
simple technique succeeds in this domain because ELIZA doesn’t actually need to
know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is
one of the few dialogue genres where listeners can act as if they know nothing of the
world. ELIZA’s mimicry of human conversation was remarkably successful: many
people who interacted with ELIZA came to believe that it really understood them
and their problems, many continued to believe in ELIZA’s abilities even after the
program’s operation was explained to them (Weizenbaum, 1976), and even today
such chatbots are a fun diversion.

Of course modern conversational agents are much more than a diversion; they
can answer questions, book ﬂights, or ﬁnd restaurants, functions for which they rely
on a much more sophisticated understanding of the user’s intent, as we will see in
Chapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA
and other chatbots play a crucial role in natural language processing.

We’ll begin with the most important tool for describing text patterns: the regular
expression. Regular expressions can be used to specify strings we might want to
extract from a document, from transforming “I need X” in ELIZA above, to deﬁning
strings like $199 or $24.99 for extracting tables of prices from a document.

We’ll then turn to a set of tasks collectively called text normalization, in which
regular expressions play an important part. Normalizing text means converting it
to a more convenient, standard form. For example, most of what we are going to
do with language relies on ﬁrst separating out or tokenizing words from running
text, the task of tokenization. English words are often separated from each other
by whitespace, but whitespace is not always sufﬁcient. New York and rock ’n’ roll
are sometimes treated as large words despite the fact that they contain spaces, while
sometimes we’ll need to separate I’m into the two words I and am. For processing
tweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.

2.1

• REGULAR EXPRESSIONS

5

lemmatization

stemming

sentence
segmentation

Some languages, like Japanese, don’t have spaces between words, so word tokeniza-
tion becomes more difﬁcult.

Another part of text normalization is lemmatization, the task of determining
that two words have the same root, despite their surface differences. For example,
the words sang, sung, and sings are forms of the verb sing. The word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
Lemmatization is essential for processing morphologically complex languages like
Arabic. Stemming refers to a simpler version of lemmatization in which we mainly
just strip sufﬁxes from the end of the word. Text normalization also includes sen-
tence segmentation: breaking up a text into individual sentences, using cues like
periods or exclamation points.

Finally, we’ll need to compare words and other strings. We’ll introduce a metric
called edit distance that measures how similar two strings are based on the number
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. Edit distance is an algorithm with applications throughout language process-
ing, from spelling correction to speech recognition to coreference resolution.

2.1 Regular Expressions

regular
expression

corpus

One of the unsung successes in standardization in computer science has been the
regular expression (often shortened to regex), a language for specifying text search
strings. This practical language is used in every computer language, word processor,
and text processing tools like the Unix tools grep or Emacs. Formally, a regular ex-
pression is an algebraic notation for characterizing a set of strings. Regular expres-
sions are particularly useful for searching in texts, when we have a pattern to search
for and a corpus of texts to search through. A regular expression search function
will search through the corpus, returning all texts that match the pattern. The corpus
can be a single document or a collection. For example, the Unix command-line tool
grep takes a regular expression and returns every line of the input document that
matches the expression.

A search can be designed to return every match on a line, if there are more than
one, or just the ﬁrst match. In the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the ﬁrst
match. We’ll show regular expressions delimited by slashes but note that slashes are
not part of the regular expressions.

Regular expressions come in many variants. We’ll be describing extended regu-
lar expressions; different regular expression parsers may only recognize subsets of
these, or treat some expressions slightly differently. Using an online regular expres-
sion tester is a handy way to test out your expressions and explore these variations.

concatenation

2.1.1 Basic Regular Expression Patterns

The simplest kind of regular expression is a sequence of simple characters; putting
characters in sequence is called concatenation. To search for woodchuck, we type
/woodchuck/. The expression /Buttercup/ matches any string containing the
substring Buttercup; grep with that expression would return the line I’m called lit-
tle Buttercup. The search string can consist of a single character (like /!/) or a
sequence of characters (like /urgl/) (see Fig. 2.1).

Regular expressions are case sensitive; lower case /s/ is distinct from upper

6 CHAPTER 2

• REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE

Regex
/woodchucks/
/a/
/!/
Figure 2.1 Some simple regex searches.

Example Patterns Matched
“interesting links to woodchucks and lemurs”
“Mary Ann stopped by Mona’s”
“You’ve left the burglar behind again!” said Nori

case /S/ (/s/ matches a lower case s but not an upper case S). This means that
the pattern /woodchucks/ will not match the string Woodchucks. We can solve this
problem with the use of the square braces [ and ]. The string of characters inside the
braces speciﬁes a disjunction of characters to match. For example, Fig. 2.2 shows
that the pattern /[wW]/ matches patterns containing either w or W.

Match

Regex
/[wW]oodchuck/ Woodchuck or woodchuck
/[abc]/
/[1234567890]/

‘a’, ‘b’, or ‘c’
any digit

Example Patterns
“Woodchuck”
“In uomini, in soldati”
“plenty of 7 to 5”

Figure 2.2 The use of the brackets [] to specify a disjunction of characters.

The regular expression /[1234567890]/ speciﬁes any single digit. While such
classes of characters as digits or letters are important building blocks in expressions,
they can get awkward (e.g., it’s inconvenient to specify

/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/

range

to mean “any capital letter”). In cases where there is a well-deﬁned sequence asso-
ciated with a set of characters, the brackets can be used with the dash (-) to specify
any one character in a range. The pattern /[2-5]/ speciﬁes any one of the charac-
ters 2, 3, 4, or 5. The pattern /[b-g]/ speciﬁes one of the characters b, c, d, e, f, or
g. Some other examples are shown in Fig. 2.3.

Regex
/[A-Z]/
/[a-z]/
/[0-9]/

Match
an upper case letter
a lower case letter
a single digit

Example Patterns Matched
“we should call it ‘Drenched Blossoms’ ”
“my beans were impatient to be hoed!”
“Chapter 1: Down the Rabbit Hole”

Figure 2.3 The use of the brackets [] plus the dash - to specify a range.

The square braces can also be used to specify what a single character cannot be,
by use of the caret ˆ. If the caret ˆ is the ﬁrst symbol after the open square brace [,
the resulting pattern is negated. For example, the pattern /[ˆa]/ matches any single
character (including special characters) except a. This is only true when the caret
is the ﬁrst symbol after the open square brace. If it occurs anywhere else, it usually
stands for a caret; Fig. 2.4 shows some examples.

Regex
/[ˆA-Z]/
/[ˆSs]/
/[ˆ.]/
/[eˆ]/
/aˆb/

Match (single characters)
not an upper case letter
neither ‘S’ nor ‘s’
not a period
either ‘e’ or ‘ˆ’
the pattern ‘aˆb’

Example Patterns Matched
“Oyfn pripetchik”
“I have no exquisite reason for’t”
“our resident Djinn”
“look up ˆ now”
“look up aˆ b now”

Figure 2.4 The caret ˆ for negation or just to mean ˆ. See below re: the backslash for escaping the period.

How can we talk about optional elements, like an optional s in woodchuck and
woodchucks? We can’t use the square brackets, because while they allow us to say

2.1

• REGULAR EXPRESSIONS

7

“s or S”, they don’t allow us to say “s or nothing”. For this we use the question mark
/?/, which means “the preceding character or nothing”, as shown in Fig. 2.5.

Regex
/woodchucks?/
/colou?r/

Example Patterns Matched
“woodchuck”
“color”
Figure 2.5 The question mark ? marks optionality of the previous expression.

Match
woodchuck or woodchucks
color or colour

We can think of the question mark as meaning “zero or one instances of the
previous character”. That is, it’s a way of specifying how many of something that
we want, something that is very important in regular expressions. For example,
consider the language of certain sheep, which consists of strings that look like the
following:

baa!
baaa!
baaaa!
baaaaa!
. . .

Kleene *

Kleene +

This language consists of strings with a b, followed by at least two a’s, followed
by an exclamation point. The set of operators that allows us to say things like “some
number of as” are based on the asterisk or *, commonly called the Kleene * (gen-
erally pronounced “cleany star”). The Kleene star means “zero or more occurrences
of the immediately previous character or regular expression”. So /a*/ means “any
string of zero or more as”. This will match a or aaaaaa, but it will also match the
empty string at the start of Off Minor since the string Off Minor starts with zero a’s.
So the regular expression for matching one or more a is /aa*/, meaning one a fol-
lowed by zero or more as. More complex patterns can also be repeated. So /[ab]*/
means “zero or more a’s or b’s” (not “zero or more right square braces”). This will
match strings like aaaa or ababab or bbbb.

For specifying multiple digits (useful for ﬁnding prices) we can extend /[0-9]/,
the regular expression for a single digit. An integer (a string of digits) is thus
/[0-9][0-9]*/. (Why isn’t it just /[0-9]*/?)

Sometimes it’s annoying to have to write the regular expression for digits twice,
so there is a shorter way to specify “at least one” of some character. This is the
Kleene +, which means “one or more occurrences of the immediately preceding
character or regular expression”. Thus, the expression /[0-9]+/ is the normal way
to specify “a sequence of digits”. There are thus two ways to specify the sheep
language: /baaa*!/ or /baa+!/.

One very important special character is the period (/./), a wildcard expression

that matches any single character (except a carriage return), as shown in Fig. 2.6.

Regex
/beg.n/

Match
any character between beg and n

Example Matches
begin, beg’n, begun

Figure 2.6 The use of the period . to specify any character.

The wildcard is often used together with the Kleene star to mean “any string of
characters”. For example, suppose we want to ﬁnd any line in which a particular
word, for example, aardvark, appears twice. We can specify this with the regular
expression /aardvark.*aardvark/.

anchors

Anchors are special characters that anchor regular expressions to particular places

8 CHAPTER 2

• REGULAR EXPRESSIONS, TEXT NORMALIZATION, EDIT DISTANCE

in a string. The most common anchors are the caret ˆ and the dollar sign $. The caret
ˆ matches the start of a line. The pattern /ˆThe/ matches the word The only at the
start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to in-
dicate a negation inside of square brackets, and just to mean a caret. (What are the
contexts that allow grep or Python to know which function a given caret is supposed
to have?) The dollar sign $ matches the end of a line. So the pattern (cid:32)$ is a useful
pattern for matching a space at the end of a line, and /ˆThe dog\.$/ matches a
line that contains only the phrase The dog. (We have to use the backslash here since
we want the . to mean “period” and not the wildcard.)

Regex
ˆ
$
\b
\B

Match
start of line
end of line
word boundary
non-word boundary

Figure 2.7 Anchors in regular expressions.

There are also two other anchors: \b matches a word boundary, and \B matches
a non-boundary. Thus, /\bthe\b/ matches the word the but not the word other.
More technically, a “word” for the purposes of a regular expression is deﬁned as any
sequence of digits, underscores, or letters; this is based on the deﬁnition of “words”
in programming languages. For example, /\b99\b/ will match the string 99 in
There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in
There are 299 bottles of beer on the wall (since 99 follows a number). But it will
match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore,
or letter).

2.1.2 Disjunction, Grouping, and Precedence

Suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. In such a case, we might want to search for either the string cat or
the string dog. Since we can’t use the square brackets to search for “cat or dog” (why
can’t we say /[catdog]/?), we need a new operator, the disjunction operator, also
called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or
the string dog.

Sometimes we need to use this disjunction operator in the midst of a larger se-
quence. For example, suppose I want to search for information about pet ﬁsh for
my cousin David. How can I specif
al examples can be used to compute a conï¬?dence interval around the mean
(equation 5.47). While these conï¬?dence intervals are not well-justiï¬?ed after the
use of cross-validation, it is still common practice to use them to declare that
algorithm A is better than algorithm B only if the conï¬?dence interval of the error
of algorithm A lies below and does not intersect the conï¬?dence interval of algorithm
B.
Deï¬?ne KFoldXV(D, A, L, k):
Require: D, the given dataset, with elements z(i)
Require: A, the learning algorithm, seen as a function that takes a dataset as
input and outputs a learned function
Require: L, the loss function, seen as a function from a learned function f and
an example z(i) âˆˆ D to a scalar âˆˆ R
Require: k, the number of folds
Split D into k mutually exclusive subsets Di, whose union is D.
for i from 1 to k do
fi = A(D\D i)
for z(j) in D i do
ej = L(fi , z(j) )
end for
end for
Return e

123

CHAPTER 5. MACHINE LEARNING BASICS

(i.i.d.) data points. A point estimator or statistic is any function of the data:
Î¸Ì‚m = g(x(1) , . . . , x(m) ).

(5.19)

The deï¬?nition does not require that g return a value that is close to the true
Î¸ or even that the range of g is the same as the set of allowable values of Î¸.
This deï¬?nition of a point estimator is very general and allows the designer of an
estimator great ï¬‚exibility. While almost any function thus qualiï¬?es as an estimator,
a good estimator is a function whose output is close to the true underlying Î¸ that
generated the training data.
For now, we take the frequentist perspective on statistics. That is, we assume
that the true parameter value Î¸ is ï¬?xed but unknown, while the point estimate
Î¸Ì‚ is a function of the data. Since the data is drawn from a random process, any
function of the data is random. Therefore Î¸Ì‚ is a random variable.
Point estimation can also refer to the estimation of the relationship between
input and target variables. We refer to these types of point estimates as function
estimators.
Function Estimation As we mentioned above, sometimes we are interested in
performing function estimation (or function approximation). Here we are trying to
predict a variable y given an input vector x. We assume that there is a function
f (x) that describes the approximate relationship between y and x. For example,
we may assume that y = f(x) + î€?, where î€? stands for the part of y that is not
predictable from x. In function estimation, we are interested in approximating
Ë† Function estimation is really just the same as
f with a model or estimate f.
estimating a parameter Î¸; the function estimator fË† is simply a point estimator in
function space. The linear regression example (discussed above in section 5.1.4) and
the polynomial regression example (discussed in section 5.2) are both examples of
scenarios that may be interpreted either as estimating a parameter w or estimating
a function fË† mapping from x to y.
We now review the most commonly studied properties of point estimators and
discuss what they tell us about these estimators.

5.4.2

Bias

The bias of an estimator is deï¬?ned as:
bias(Î¸Ì‚m ) = E( Î¸Ì‚m ) âˆ’ Î¸
124

(5.20)

CHAPTER 5. MACHINE LEARNING BASICS

where the expectation is over the data (seen as samples from a random variable)
and Î¸ is the true underlying value of Î¸ used to deï¬?ne the data generating distribution. An estimator Î¸Ì‚ m is said to be unbiased if bias( Î¸Ì‚m) = 0, which implies
that E(Î¸Ì‚m) = Î¸. An estimator Î¸Ë†m is said to be asymptotically unbiased if
limmâ†’âˆž bias(Î¸Ì‚ m ) = 0, which implies that limmâ†’âˆž E(Î¸Ë†m) = Î¸.
Example: Bernoulli Distribution Consider a set of samples {x(1) , . . . , x(m) }
that are independently and identically distributed according to a Bernoulli distribution with mean Î¸:
( i)
( i)
P (x(i); Î¸) = Î¸x (1 âˆ’ Î¸)(1âˆ’x ) .
(5.21)
A common estimator for the Î¸ parameter of this distribution is the mean of the
training samples:
m
1 î?˜ (i )
Î¸Ì‚m =
x .
(5.22)
m
i=1

To determine whether this estimator is biased, we can substitute equation 5.22
into equation 5.20:
bias(Î¸Ì‚m) = E[Î¸Ë†m] âˆ’ Î¸
î€¢
î€£
m
1 î?˜ (i )
=E
x
âˆ’Î¸
m i=1
m
1 î?˜ î?¨ ( i) î?©
=
âˆ’Î¸
E x
m
i=1
m
î?˜

1
î€?
î€‘
î?˜
1
( i) x ( i)
(1âˆ’x(i) )
=
x Î¸ (1 âˆ’ Î¸)
âˆ’Î¸
m i=1 (i)

=

x

(5.23)
(5.24)
(5.25)

(5.26)

=0

m
î?˜

1
(Î¸ ) âˆ’ Î¸
m i=1

=Î¸âˆ’Î¸ =0

(5.27)
(5.28)

Since bias( Î¸Ì‚) = 0, we say that our estimator Î¸Ì‚ is unbiased.
Example: Gaussian Distribution Estimator of the Mean Now, consider
a set of samples {x(1) , . . . , x(m)} that are independently and identically distributed
according to a Gaussian distribution p(x (i)) = N (x(i); Âµ, Ïƒ2 ), where i âˆˆ {1, . . . , m}.
125

CHAPTER 5. MACHINE LEARNING BASICS

Recall that the Gaussian probability density function is given by
î€ 
î€¡
(
i
)
2
1
1 (x âˆ’ Âµ)
p(x(i); Âµ, Ïƒ 2 ) = âˆš
exp âˆ’
.
2
Ïƒ2
2Ï€Ïƒ2

(5.29)

A common estimator of the Gaussian mean parameter is known as the sample
mean:
m
1 î?˜ (i )
ÂµÌ‚m =
x
(5.30)
m i=1

To determine the bias of the sample mean, we are again interested in calculating
its expectation:
bias(ÂµÌ‚m ) = E[Ë†
Âµm ] âˆ’ Âµ
î€¢
î€£
m
1 î?˜ (i )
=E
x
âˆ’Âµ
m i=1
î€ 
î€¡
m
1 î?˜ î?¨ (i ) î?©
âˆ’Âµ
E x
=
m i=1
î€ 
î€¡
m
1 î?˜
=
Âµ âˆ’Âµ
m

(5.31)
(5.32)
(5.33)
(5.34)

i=1

=Âµâˆ’Âµ=0

(5.35)

Thus we ï¬?nd that the sample mean is an unbiased estimator of Gaussian mean
parameter.
Example: Estimators of the Variance of a Gaussian Distribution As an
example, we compare two diï¬€erent estimators of the variance parameter Ïƒ2 of a
Gaussian distribution. We are interested in knowing if either estimator is biased.
The ï¬?rst estimator of Ïƒ2 we consider is known as the sample variance:
ÏƒÌ‚2m =

m
î€‘2
1 î?˜ î€? ( i)
x âˆ’ ÂµÌ‚ m ,
m i=1

(5.36)

where ÂµÌ‚m is the sample mean, deï¬?ned above. More formally, we are interested in
computing
2 ] âˆ’ Ïƒ2
bias(ÏƒÌ‚ 2m) = E[Ë†
Ïƒm
(5.37)
126

CHAPTER 5. MACHINE LEARNING BASICS

2
We begin by evaluating the term E[ÏƒÌ‚m
]:
î€¢
î€£
m î€?
î€‘
î?˜
2
1
2
E[ÏƒÌ‚m
] =E
x(i) âˆ’ ÂµÌ‚ m
m i=1

=

mâˆ’1 2
Ïƒ
m

(5.38)
(5.39)

Returning to equation 5.37, we conclude that the bias of ÏƒÌ‚ 2m is âˆ’Ïƒ 2/m. Therefore,
the sample variance is a biased estimator.
The unbiased sample variance estimator
ÏƒÌƒ 2m =

m
î€‘2
1 î?˜ î€? (i )
x âˆ’ ÂµÌ‚m
m âˆ’ 1 i=1

(5.40)

provides an alternative approach. As the name suggests this estimator is unbiased.
That is, we ï¬?nd that E[ÏƒÌƒ2m ] = Ïƒ2:
î€¢
î€£
m î€?
î€‘2
î?˜
1
E[ÏƒÌƒ2m] = E
x (i) âˆ’ ÂµÌ‚m
(5.41)
mâˆ’1
i=1
m
2
=
E[ÏƒÌ‚m
]
(5.42)
mâˆ’1î€’
î€“
m
mâˆ’1 2
=
Ïƒ
(5.43)
mâˆ’1
m
= Ïƒ 2.

(5.44)

We have two estimators: one is biased and the other is not. While unbiased
estimators are clearly desirable, they are not always the â€œbestâ€? estimators. As we
will see we often use biased estimators that possess other important properties.

5.4.3

Variance and Standard Error

Another property of the estimator that we might want to consider is how much
we expect it to vary as a function of the data sample. Just as we computed the
expectation of the estimator to determine its bias, we can compute its variance.
The variance of an estimator is simply the variance
Ë†
Var(Î¸)

(5.45)

where the random variable is the training set. Alternately, the square root of the
variance is called the standard error, denoted SE(Î¸Ì‚).
127

CHAPTER 5. MACHINE LEARNING BASICS

The variance or the standard error of an estimator provides a measure of how
we would expect the estimate we compute from data to vary as we independently
resample the dataset from the underlying data generating process. Just as we
might like an estimator to exhibit low bias we would also like it to have relatively
low variance.
When we compute any statistic using a ï¬?nite number of samples, our estimate
of the true underlying parameter is uncertain, in the sense that we could have
obtained other samples from the same distribution and their statistics would have
been diï¬€erent. The expected degree of variation in any estimator is a source of
error that we want to quantify.
The standard error of the mean is given by
î?¶
î€¢
î€£
î?µ
m
î?µ
î?˜
1
Ïƒ
SE(ÂµÌ‚m) = î?´ Var
x (i) = âˆš ,
m i=1
m

(5.46)

where Ïƒ2 is the true variance of the samples xi . The standard error is often
estimated by using an estimate of Ïƒ. Unfortunately, neither the square root of
the sample variance nor the square root of the unbiased estimator of the variance
provide an unbiased estimate of the standard deviation. Both approaches tend
to underestimate the true standard deviation, but are still used in practice. The
square root of the unbiased estimator of the variance is less of an underestimate.
For large m, the approximation is quite reasonable.
The standard error of the mean is very useful in machine learning experiments.
We often estimate the generalization error by computing the sample mean of the
error on the test set. The number of examples in the test set determines the
accuracy of this estimate. Taking advantage of the central limit theorem, which
tells us that the mean will be approximately distributed with a normal distribution,
we can use the standard error to compute the probability that the true expectation
falls in any chosen interval. For example, the 95% conï¬?dence interval centered on
the mean ÂµÌ‚m is
(ÂµÌ‚m âˆ’ 1.96SE(Ë†
Ë†m + 1.96SE(Ë†
Âµm )),
(5.47)
Âµ m), Âµ
under the normal distribution with mean ÂµÌ‚m and variance SE(ÂµÌ‚m )2 . In machine
learning experiments, it is common to say that algorithm A is better than algorithm
B if the upper bound of the 95% conï¬?dence interval for the error of algorithm A is
less than the lower bound of the 95% conï¬?dence interval for the error of algorithm
B.

128

CHAPTER 5. MACHINE LEARNING BASICS

Example: Bernoulli Distribution We once again consider a set of samples
{x(1) , . . . , x(m)} drawn independently and identically from a Bernoulli distribution
(i )
( i)
(recall P(x(i); Î¸) = Î¸ x (1 âˆ’ Î¸)(1âˆ’x ) ).î??This time we are interested in computing
m
1
(i )
the variance of the estimator Î¸Ì‚m = m
i=1 x .
î€ 
î€¡
m
î€? î€‘
1 î?˜ (i)
Var Î¸Ì‚m = Var
x
(5.48)
m i=1
î€? î€‘
1 î?˜
= 2
Var x(i)
m

(5.49)

=

(5.50)

m

i=1
m
î?˜

1
Î¸(1 âˆ’ Î¸)
m2 i=1

1
mÎ¸(1 âˆ’ Î¸)
m2
1
= Î¸(1 âˆ’ Î¸)
m
=

(5.51)
(5.52)

The variance of the estimator decreases as a function of m, the number of examples
in the dataset. This is a common property of popular estimators that we will
return to when we discuss consistency (see section 5.4.5).

5.4.4

Trading oï¬€ Bias and Variance to Minimize Mean Squared
Error

Bias and variance measure two diï¬€erent sources of error in an estimator. Bias
measures the expected deviation from the true value of the function or parameter.
Variance on the other hand, provides a measure of the deviation from the expected
estimator value that any particular sampling of the data is likely to cause.
What happens when we are given a choice between two estimators, one with
more bias and one with more variance? How do we choose between them? For
example, imagine that we are interested in approximating the function shown in
ï¬?gure 5.2 and we are only oï¬€ered the choice between a model with large bias and
one that suï¬€ers from large variance. How do we choose between them?
The most common way to negotiate this trade-oï¬€ is to use cross-validation.
Empirically, cross-validation is highly successful on many real-world tasks. Alternatively, we can also compare the mean squared error (MSE) of the estimates:
MSE = E[(Î¸Ì‚m âˆ’ Î¸)2 ]

= Bias( Î¸Ì‚m) 2 + Var(Î¸Ë†m )
129

(5.53)
(5.54)

CHAPTER 5. MACHINE LEARNING BASICS

The MSE measures the overall expected deviationâ€”in a squared error senseâ€”
between the estimator and the true value of the parameter Î¸. As is clear from
equation 5.54, evaluating the MSE incorporates both the bias and the variance.
Desirable estimators are those with small MSE and these are estimators that
manage to keep both their bias and variance somewhat in check.

Underï¬?tting zone

Bias

Overï¬?tting zone

Generalization
error

Optimal
capacity

Variance

Capacity

Figure 5.6: As capacity increases (x-axis), bias (dotted) tends to decrease and variance
(dashed) tends to increase, yielding another U-shaped curve for generalization error (bold
curve). If we vary capacity along one axis, there is an optimal capacity, with underï¬?tting
when the capacity is below this optimum and overï¬?tting when it is above. This relationship
is similar to the relationship between capacity, underï¬?tting, and overï¬?tting, discussed in
section 5.2 and ï¬?gure 5.3.

The relationship between bias and variance is tightly linked to the machine
learning concepts of capacity, underï¬?tting and overï¬?tting. In the case where generalization error is measured by the MSE (where bias and variance are meaningful
components of generalization error), increasing capacity tends to increase variance
and decrease bias. This is illustrated in ï¬?gure 5.6, where we see again the U-shaped
curve of generalization error as a function of capacity.

5.4.5

Consistency

So far we have discussed the properties of various estimators for a training set of
ï¬?xed size. Usually, we are also concerned with the behavior of an estimator as the
amount of training data grows. In particular, we usually wish that, as the number
of data points m in our dataset increases, our point estimates converge to the true
130

CHAPTER 5. MACHINE LEARNING BASICS

value of the corresponding parameters. More formally, we would like that
plimmâ†’âˆž Î¸Ì‚m = Î¸.

(5.55)

The symbol plim indicates convergence in probability, meaning that for any î€? > 0,
P (|Î¸Ë†m âˆ’ Î¸| > î€?) â†’ 0 as m â†’ âˆž. The condition described by equation 5.55 is
known as consistency. It is sometimes referred to as weak consistency, with
strong consistency referring to the almost sure convergence of Î¸Ì‚ to Î¸. Almost
sure convergence of a sequence of random variables x (1) , x (2), . . . to a value x
occurs when p(limmâ†’âˆž x(m) = x) = 1.
Consistency ensures that the bias induced by the estimator diminishes as the
number of data examples grows. However, the reverse is not trueâ€”asymptotic
unbiasedness does not imply consistency. For example, consider estimating the
mean parameter Âµ of a normal distribution N (x; Âµ, Ïƒ 2 ), with a dataset consisting
of m samples: {x(1) , . . . , x(m)}. We could use the ï¬?rst sample x (1) of the dataset
as an unbiased estimator: Î¸Ì‚ = x (1). In that case, E(Î¸Ë†m) = Î¸ so the estimator
is unbiased no matter how many data points are seen. This, of course, implies
that the estimate is asymptotically unbiased. However, this is not a consistent
estimator as it is not the case that Î¸Ë†m â†’ Î¸ as m â†’ âˆž.

5.5

Maximum Likelihood Estimation

Previously, we have seen some deï¬?nitions of common estimators and analyzed
their properties. But where did these estimators come from? Rather than guessing
that some function might make a good estimator and then analyzing its bias and
variance, we would like to have some principle from which we can derive speciï¬?c
functions that are good estimators for diï¬€erent models.
The most common such principle is the maximum likelihood principle.
Consider a set of m examples X = {x (1) , . . . , x (m)} drawn independently from
the true but unknown data generating distribution pdata (x).
Let p model(x; Î¸) be a parametric family of probability distributions over the
same space indexed by Î¸. In other words, p model(x; Î¸ ) maps any con
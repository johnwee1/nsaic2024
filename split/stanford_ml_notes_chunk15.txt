rizer R(Î¸) is typically chosen to be some measure of the com-
plexity of the model Î¸. Thus, when using the regularized loss, we aim to
ï¬nd a model that both ï¬t the data (a small loss J(Î¸)) and have a small

1Here our notations generally omit the dependency on the training dataset for
simplicityâ€”we write J(Î¸) even though it obviously needs to depend on the training dataset.

135

136

model complexity (a small R(Î¸)). The balance between the two objectives is
controlled by the regularization parameter Î». When Î» = 0, the regularized
loss is equivalent to the original loss. When Î» is a suï¬ƒciently small positive
number, minimizing the regularized loss is eï¬€ectively minimizing the original
loss with the regularizer as the tie-breaker. When the regularizer is extremely
large, then the original loss is not eï¬€ective (and likely the model will have a
large bias.)

2(cid:107)Î¸(cid:107)2
2.

The most commonly used regularization is perhaps (cid:96)2 regularization,
where R(Î¸) = 1
It encourages the optimizer to ï¬nd a model with
small (cid:96)2 norm. In deep learning, itâ€™s oftentimes referred to as weight de-
cay, because gradient descent with learning rate Î· on the regularized loss
RÎ»(Î¸) is equivalent to shrinking/decaying Î¸ by a scalar factor of 1 âˆ’ Î·Î» and
then applying the standard gradient

Î¸ â† Î¸ âˆ’ Î·âˆ‡JÎ»(Î¸) = Î¸ âˆ’ Î·Î»Î¸ âˆ’ Î·âˆ‡J(Î¸)

= (1 âˆ’ Î»Î·)Î¸
(cid:125)

(cid:123)(cid:122)
decaying weights

(cid:124)

âˆ’Î·âˆ‡J(Î¸)

(9.2)

Besides encouraging simpler models, regularization can also impose in-
ductive biases or structures on the model parameters. For example, suppose
we had a prior belief that the number of non-zeros in the ground-truth model
parameters is small,2â€”which is oftentimes called sparsity of the modelâ€”, we
can impose a regularization on the number of non-zeros in Î¸, denoted by
(cid:107)Î¸(cid:107)0, to leverage such a prior belief. Imposing additional structure of the
parameters narrows our search space and makes the complexity of the model
family smaller,â€”e.g., the family of sparse models can be thought of as having
lower complexity than the family of all modelsâ€”, and thus tends to lead to a
better generalization. On the other hand, imposing additional structure may
risk increasing the bias. For example, if we regularize the sparsity strongly
but no sparse models can predict the label accurately, we will suï¬€er from
large bias (analogously to the situation when we use linear models to learn
data than can only be represented by quadratic functions in Section 8.1.)

The sparsity of the parameters is not a continuous function of the param-
eters, and thus we cannot optimize it with (stochastic) gradient descent. A
common relaxation is to use R(Î¸) = (cid:107)Î¸(cid:107)1 as a continuous surrogate.3

2For linear models, this means the model just uses a few coordinates of the inputs to

make an accurate prediction.

3There has been a rich line of theoretical work that explains why (cid:107)Î¸(cid:107)1 is a good sur-
rogate for encouraging sparsity, but itâ€™s beyond the scope of this course. An intuition is:
assuming the parameter is on the unit sphere, the parameter with smallest (cid:96)1 norm also

137

The R(Î¸) = (cid:107)Î¸(cid:107)1 (also called LASSO) and R(Î¸) = 1

2 are perhaps
among the most commonly used regularizers for linear models. Other norm
and powers of norms are sometimes also used. The (cid:96)2 norm regularization is
much more commonly used with kernel methods because (cid:96)1 regularization is
typically not compatible with the kernel trick (the optimal solution cannot
be written as functions of inner products of features.)

2(cid:107)Î¸(cid:107)2

In deep learning, the most commonly used regularizer is (cid:96)2 regularization
or weight decay. Other common ones include dropout, data augmentation,
regularizing the spectral norm of the weight matrices, and regularizing the
Lipschitzness of the model, etc. Regularization in deep learning is an ac-
tive research area, and itâ€™s known that there is another implicit source of
regularization, as discussed in the next section.

9.2 Implicit regularization eï¬€ect (optional

reading)

The implicit regularization eï¬€ect of optimizers, or implicit bias or algorithmic
regularization, is a new concept/phenomenon observed in the deep learning
era. It largely refers to that the optimizers can implicitly impose structures
on parameters beyond what has been imposed by the regularized loss.

In most classical settings, the loss or regularized loss has a unique global
minimum, and thus any reasonable optimizer should converge to that global
minimum and cannot impose any additional preferences. However, in deep
learning, oftentimes the loss or regularized loss has more than one (approx-
imate) global minima, and diï¬€erence optimizers may converge to diï¬€erent
global minima. Though these global minima have the same or similar train-
ing losses, they may be of diï¬€erent nature and have dramatically diï¬€erent
generalization performance. See Figures 9.1 and 9.2 and its caption for an
illustration and some experiment results. For example, itâ€™s possible that one
global minimum gives a much more Lipschitz or sparse model than others
and thus has a better test error. It turns out that many commonly-used op-
timizers (or their components) prefer or bias towards ï¬nding global minima
of certain properties, leading to a better test performance.

happen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and
(cid:96)1 norm gives the same extremal points to some extent.

138

Figure 9.1: An Illustration that diï¬€erent global minima of the training loss
can have diï¬€erent test performance.

Figure 9.2: Left: Performance of neural networks trained by two diï¬€erent
learning rates schedules on the CIFAR-10 dataset. Although both exper-
iments used exactly the same regularized losses and the optimizers ï¬t the
training data perfectly, the modelsâ€™ generalization performance diï¬€er much.
Right: On a diï¬€erent synthetic dataset, optimizers with diï¬€erent initializa-
tions have the same training error but diï¬€erent generalization performance.4

In summary, the takehome message here is that the choice of optimizer
does not only aï¬€ect minimizing the training loss, but also imposes implicit
regularization and aï¬€ects the generalization of the model. Even if your cur-
rent optimizer already converges to a small training error perfectly, you may
still need to tune your optimizer for a better generalization, .

4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]

Î¸loss139

One may wonder which components of the optimizers bias towards what
type of global minima and what type of global minima may generalize bet-
ter. These are open questions that researchers are actively investigating.
Empirical and theoretical research have oï¬€ered some clues and heuristics.
In many (but deï¬nitely far from all) situations, among those setting where
optimization can succeed in minimizing the training loss, the use of larger
initial learning rate, smaller initialization, smaller batch size, and momen-
tum appears to help with biasing towards more generalizable solutions. A
conjecture (that can be proven in certain simpliï¬ed case) is that stochas-
ticity in the optimization process help the optimizer to ï¬nd ï¬‚atter global
minima (global minima where the curvature of the loss is small), and ï¬‚at
global minima tend to give more Lipschitz models and better generalization.
Characterizing the implicit regularization eï¬€ect formally is still a challenging
open research question.

9.3 Model selection via cross validation

Suppose we are trying select among several diï¬€erent models for a learning
problem. For instance, we might be using a polynomial regression model
hÎ¸(x) = g(Î¸0 + Î¸1x + Î¸2x2 + Â· Â· Â· + Î¸kxk), and wish to decide if k should be
0, 1, . . . , or 10. How can we automatically select a model that represents
a good tradeoï¬€ between the twin evils of bias and variance5? Alternatively,
suppose we want to automatically choose the bandwidth parameter Ï„ for
locally weighted regression, or the parameter C for our (cid:96)1-regularized SVM.
How can we do that?

For the sake of concreteness, in these notes we assume we have some
ï¬nite set of models M = {M1, . . . , Md} that weâ€™re trying to select among.
For instance, in our ï¬rst example above, the model Mi would be an i-th
degree polynomial regression model.
(The generalization to inï¬nite M is
not hard.6) Alternatively, if we are trying to decide between using an SVM,
a neural network or logistic regression, then M may contain these models.

5Given that we said in the previous set of notes that bias and variance are two very
diï¬€erent beasts, some readers may be wondering if we should be calling them â€œtwinâ€ evils
here. Perhaps itâ€™d be better to think of them as non-identical twins. The phrase â€œthe
fraternal twin evils of bias and varianceâ€ doesnâ€™t have the same ring to it, though.

6If we are trying to choose from an inï¬nite set of models, say corresponding to the
possible values of the bandwidth Ï„ âˆˆ R+, we may discretize Ï„ and consider only a ï¬nite
number of possible values for it. More generally, most of the algorithms described here
can all be viewed as performing optimization search in the space of models, and we can
perform this search over inï¬nite model classes as well.

140

Cross validation. Lets suppose we are, as usual, given a training set S.
Given what we know about empirical risk minimization, hereâ€™s what might
initially seem like a algorithm, resulting from using empirical risk minimiza-
tion for model selection:

1. Train each model Mi on S, to get some hypothesis hi.

2. Pick the hypotheses with the smallest training error.

This algorithm does not work. Consider choosing the degree of a poly-
nomial. The higher the degree of the polynomial, the better it will ï¬t the
training set S, and thus the lower the training error. Hence, this method will
always select a high-variance, high-degree polynomial model, which we saw
previously is often poor choice.

Hereâ€™s an algorithm that works better. In hold-out cross validation

(also called simple cross validation), we do the following:

1. Randomly split S into Strain (say, 70% of the data) and Scv (the remain-

ing 30%). Here, Scv is called the hold-out cross validation set.

2. Train each model Mi on Strain only, to get some hypothesis hi.

3. Select and output the hypothesis hi that had the smallest error Ë†ÎµScv(hi)
on the hold out cross validation set. (Here Ë†ÎµScv(h) denotes the average
error of h on the set of examples in Scv.) The error on the hold out
validation set is also referred to as the validation error.

By testing/validating on a set of examples Scv that the models were not
trained on, we obtain a better estimate of each hypothesis hiâ€™s true general-
ization/test error. Thus, this approach is essentially picking the model with
the smallest estimated generalization/test error. The size of the validation
set depends on the total number of available examples. Usually, somewhere
between 1/4âˆ’1/3 of the data is used in the hold out cross validation set, and
30% is a typical choice. However, when the total dataset is huge, validation
set can be a smaller fraction of the total examples as long as the absolute
number of validation examples is decent. For example, for the ImageNet
dataset that has about 1M training images, the validation set is sometimes
set to be 50K images, which is only about 5% of the total examples.

Optionally, step 3 in the algorithm may also be replaced with selecting
the model Mi according to arg mini Ë†ÎµScv(hi), and then retraining Mi on the
entire training set S. (This is often a good idea, with one exception being
learning algorithms that are be very sensitive to perturbations of the initial

141

conditions and/or data. For these methods, Mi doing well on Strain does not
necessarily mean it will also do well on Scv, and it might be better to forgo
this retraining step.)

The disadvantage of using hold out cross validation is that it â€œwastesâ€
about 30% of the data. Even if we were to take the optional step of retraining
the model on the entire training set, itâ€™s still as if weâ€™re trying to ï¬nd a good
model for a learning problem in which we had 0.7n training examples, rather
than n training examples, since weâ€™re testing models that were trained on
only 0.7n examples each time. While this is ï¬ne if data is abundant and/or
cheap, in learning problems in which data is scarce (consider a problem with
n = 20, say), weâ€™d like to do something better.

Here is a method, called k-fold cross validation, that holds out less

data each time:

1. Randomly split S into k disjoint subsets of m/k training examples each.

Lets call these subsets S1, . . . , Sk.

2. For each model Mi, we evaluate it as follows:

For j = 1, . . . , k

Train the model Mi on S1 âˆª Â· Â· Â· âˆª Sjâˆ’1 âˆª Sj+1 âˆª Â· Â· Â· Sk (i.e., train
on all the data except Sj) to get some hypothesis hij.
Test the hypothesis hij on Sj, to get Ë†ÎµSj (hij).

The estimated generalization error of model Mi is then calculated
as the average of the Ë†ÎµSj (hij)â€™s (averaged over j).

3. Pick the model Mi with the lowest estimated generalization error, and
retrain that model on the entire training set S. The resulting hypothesis
is then output as our ï¬nal answer.

A typical choice for the number of folds to use here would be k = 10.
While the fraction of data held out each time is now 1/kâ€”much smaller
than beforeâ€”this procedure may also be more computationally expensive
than hold-out cross validation, since we now need train to each model k
times.

While k = 10 is a commonly used choice, in problems in which data is
really scarce, sometimes we will use the extreme choice of k = m in order
to leave out as little data as possible each time. In this setting, we would
repeatedly train on all but one of the training examples in S, and test on that
held-out example. The resulting m = k errors are then averaged together to
obtain our estimate of the generalization error of a model. This method has

142

its own name; since weâ€™re holding out one training example at a time, this
method is called leave-one-out cross validation.

Finally, even though we have described the diï¬€erent versions of cross vali-
dation as methods for selecting a model, they can also be used more simply to
evaluate a single model or algorithm. For example, if you have implemented
some learning algorithm and want to estimate how well it performs for your
application (or if you have invented a novel learning algorithm and want to
report in a technical paper how well it performs on various test sets), cross
validation would give a reasonable way of doing so.

9.4 Bayesian statistics and regularization

In this section, we will talk about one more tool in our arsenal for our battle
against overï¬tting.

At the beginning of the quarter, we talked about parameter ï¬tting using
maximum likelihood estimation (MLE), and chose our parameters according
to

Î¸MLE = arg max

Î¸

p(y(i)|x(i); Î¸).

n
(cid:89)

i=1

Throughout our subsequent discussions, we viewed Î¸ as an unknown param-
eter of the world. This view of the Î¸ as being constant-valued but unknown
is taken in frequentist statistics. In the frequentist this view of the world, Î¸
is not randomâ€”it just happens to be unknown
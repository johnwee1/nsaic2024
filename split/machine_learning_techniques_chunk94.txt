ry
large training set, you are often better off training the model on a
single  machine  with  a  single  powerful  GPU  with  a  large  memory
bandwidth.

Saturation is more severe for large dense models, since they have a lot of parameters
and gradients to transfer. It is less severe for small models (but the parallelization gain
is limited) and for large sparse models, where the gradients are typically mostly zeros
and  so  can  be  communicated  efficiently.  Jeff  Dean,  initiator  and  lead  of  the  Google
Brain  project,  reported  typical  speedups  of  25–40×  when  distributing  computations
across  50  GPUs  for  dense  models,  and  a  300×  speedup  for  sparser  models  trained
across 500 GPUs. As you can see, sparse models really do scale better. Here are a few
concrete examples:

20 Jianmin Chen et al., “Revisiting Distributed Synchronous SGD,” arXiv preprint arXiv:1604.00981 (2016).

708 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

• Neural machine translation: 6× speedup on 8 GPUs

• Inception/ImageNet: 32× speedup on 50 GPUs

• RankBrain: 300× speedup on 500 GPUs

Beyond  a  few  dozen  GPUs  for  a  dense  model  or  few  hundred  GPUs  for  a  sparse
model,  saturation  kicks  in  and  performance  degrades.  There  is  plenty  of  research
going on to solve this problem (exploring peer-to-peer architectures rather than cen‐
tralized  parameter  servers,  using  lossy  model  compression,  optimizing  when  and
what the replicas need to communicate, and so on), so there will likely be a lot of pro‐
gress in parallelizing neural networks in the next few years.

In the meantime, to reduce the saturation problem, you probably want to use a few
powerful  GPUs  rather  than  plenty  of  weak  GPUs,  and  you  should  also  group  your
GPUs  on  few  and  very  well  interconnected  servers.  You  can  also  try  dropping  the
float  precision  from  32  bits  (tf.float32)  to  16  bits  (tf.bfloat16).  This  will  cut  in
half the amount of data to transfer, often without much impact on the convergence
rate or the model’s performance. Lastly, if you are using centralized parameters, you
can  shard  (split)  the  parameters  across  multiple  parameter  servers:  adding  more
parameter servers will reduce the network load on each server and limit the risk of
bandwidth saturation.

OK, now let’s train a model across multiple GPUs!

Training at Scale Using the Distribution Strategies API
Many  models  can  be  trained  quite  well  on  a  single  GPU,  or  even  on  a  CPU.  But  if
training  is  too  slow,  you  can  try  distributing  it  across  multiple  GPUs  on  the  same
machine. If that’s still too slow, try using more powerful GPUs, or add more GPUs to
the machine. If your model performs heavy computations (such as large matrix mul‐
tiplications), then it will run much faster on powerful GPUs, and you could even try
to use TPUs on Google Cloud AI Platform, which will usually run even faster for such
models. But if you can’t fit any more GPUs on the same machine, and if TPUs aren’t
for  you  (e.g.,  perhaps  your  model  doesn’t  benefit  much  from  TPUs,  or  perhaps  you
want  to  use  your  own  hardware  infrastructure),  then  you  can  try  training  it  across
several servers, each with multiple GPUs (if this is still not enough, as a last resort you
can  try  adding  some  model  parallelism,  but  this  requires  a  lot  more  effort).  In  this
section we will see how to train models at scale, starting with multiple GPUs on the
same  machine  (or  TPUs)  and  then  moving  on  to  multiple  GPUs  across  multiple
machines.

Luckily, TensorFlow comes with a very simple API that takes care of all the complex‐
ity for you: the Distribution Strategies API. To train a Keras model across all available
GPUs  (on  a  single  machine,  for  now)  using  data  parallelism  with  the  mirrored

Training Models Across Multiple Devices 

| 

709

strategy, create a MirroredStrategy object, call its scope() method to get a distribu‐
tion context, and wrap the creation and compilation of your model inside that con‐
text. Then call the model’s fit() method normally:

distribution = tf.distribute.MirroredStrategy()

with distribution.scope():
    mirrored_model = keras.models.Sequential([...])
    mirrored_model.compile([...])

batch_size = 100 # must be divisible by the number of replicas
history = mirrored_model.fit(X_train, y_train, epochs=10)

Under the hood, tf.keras is distribution-aware, so in this MirroredStrategy context it
knows  that  it  must  replicate  all  variables  and  operations  across  all  available  GPU
devices.  Note  that  the  fit()  method  will  automatically  split  each  training  batch
across all the replicas, so it’s important that the batch size be divisible by the number
of replicas. And that’s all! Training will generally be significantly faster than using a
single device, and the code change was really minimal.

Once you have finished training your model, you can use it to make predictions effi‐
ciently: call the predict() method, and it will automatically split the batch across all
replicas, making predictions in parallel (again, the batch size must be divisible by the
number of replicas). If you call the model’s save() method, it will be saved as a regu‐
lar model, not as a mirrored model with multiple replicas. So when you load it, it will
run like a regular model, on a single device (by default GPU 0, or the CPU if there are
no GPUs). If you want to load a model and run it on all available devices, you must
call keras.models.load_model() within a distribution context:

with distribution.scope():
    mirrored_model = keras.models.load_model("my_mnist_model.h5")

If you only want to use a subset of all the available GPU devices, you can pass the list
to the MirroredStrategy’s constructor:

distribution = tf.distribute.MirroredStrategy(["/gpu:0", "/gpu:1"])

By default, the MirroredStrategy class uses the NVIDIA Collective Communications
Library (NCCL) for the AllReduce mean operation, but you can change it by setting
the cross_device_ops argument to an instance of the tf.distribute.Hierarchical
CopyAllReduce  class,  or  an  instance  of  the  tf.distribute.ReductionToOneDevice
class. The default NCCL option is based on the tf.distribute.NcclAllReduce class,
which  is  usually  faster,  but  this  depends  on  the  number  and  types  of  GPUs,  so  you
may want to give the alternatives a try.21

21 For more details on AllReduce algorithms, read this great post by Yuichiro Ueno, and this page on scaling

with NCCL.

710 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

If  you  want  to  try  using  data  parallelism  with  centralized  parameters,  replace  the
MirroredStrategy with the CentralStorageStrategy:

distribution = tf.distribute.experimental.CentralStorageStrategy()

You  can  optionally  set  the  compute_devices  argument  to  specify  the  list  of  devices
you  want  to  use  as  workers  (by  default  it  will  use  all  available  GPUs),  and  you  can
optionally set the parameter_device argument to specify the device you want to store
the parameters on (by default it will use the CPU, or the GPU if there is just one).

Now let’s see how to train a model across a cluster of TensorFlow servers!

Training a Model on a TensorFlow Cluster
A TensorFlow cluster is a group of TensorFlow processes running in parallel, usually
on different machines, and talking to each other to complete some work—for exam‐
ple, training or executing a neural network. Each TF process in the cluster is called a
task, or a TF server. It has an IP address, a port, and a type (also called its role or its
job).  The  type  can  be  either  "worker",  "chief",  "ps"  (parameter  server),  or
"evaluator":

• Each  worker  performs  computations,  usually  on  a  machine  with  one  or  more

GPUs.

• The chief performs computations as well (it is a worker), but it also handles extra
work such as writing TensorBoard logs or saving checkpoints. There is a single
chief in a cluster. If no chief is specified, then the first worker is the chief.

• A parameter server only keeps track of variable values, and it is usually on a CPU-
only machine. This type of task is only used with the ParameterServerStrategy.

• An evaluator obviously takes care of evaluation.

To  start  a  TensorFlow  cluster,  you  must  first  specify  it.  This  means  defining  each
task’s IP address, TCP port, and type. For example, the following cluster specification
defines  a  cluster  with  three  tasks  (two  workers  and  one  parameter  server;  see
Figure 19-21). The cluster spec is a dictionary with one key per job, and the values are
lists of task addresses (IP:port):

cluster_spec = {
    "worker": [
        "machine-a.example.com:2222",  # /job:worker/task:0
        "machine-b.example.com:2222"   # /job:worker/task:1
    ],
    "ps": ["machine-a.example.com:2221"] # /job:ps/task:0
}

Training Models Across Multiple Devices 

| 

711

Figure 19-21. TensorFlow cluster

In general there will be a single task per machine, but as this example shows, you can
configure  multiple  tasks  on  the  same  machine  if  you  want  (if  they  share  the  same
GPUs, make sure the RAM is split appropriately, as discussed earlier).

By  default,  every  task  in  the  cluster  may  communicate  with  every
other task, so make sure to configure your firewall to authorize all
communications  between  these  machines  on  these  ports  (it’s  usu‐
ally simpler if you use the same port on every machine).

When you start a task, you must give it the cluster spec, and you must also tell it what
its type and index are (e.g., worker 0). The simplest way to specify everything at once
(both the cluster spec and the current task’s type and index) is to set the TF_CONFIG
environment variable before starting TensorFlow. It must be a JSON-encoded dictio‐
nary  containing  a  cluster  specification  (under  the  "cluster"  key)  and  the  type  and
index of the current task (under the "task" key). For example, the following TF_CON
FIG environment variable uses the cluster we just defined and specifies that the task
to start is the first worker:

import os
import json

os.environ["TF_CONFIG"] = json.dumps({
    "cluster": cluster_spec,
    "task": {"type": "worker", "index": 0}
})

712 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale

In general you want to define the TF_CONFIG environment variable
outside of Python, so the code does not need to include the current
task’s  type  and  index  (this  makes  it  possible  to  use  the  same  code
across all workers).

Now let’s train a model on a cluster! We will start with the mirrored strategy—it’s sur‐
prisingly simple! First, you need to set the TF_CONFIG environment variable appropri‐
ately for each task. There should be no parameter server (remove the “ps” key in the
cluster spec), and in general you will want a single worker per machine. Make extra
sure  you  set  a  different  task  index  for  each  task.  Finally,  run  the  following  training
code on every worker:

distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()

with distribution.scope():
    mirrored_model = keras.models.Sequential([...])
    mirrored_model.compile([...])

batch_size = 100 # must be divisible by the number of replicas
history = mirrored_model.fit(X_train, y_train, epochs=10)

Yes,  that’s  exactly  the  same  code  we  used  earlier,  except  this  time  we  are  using  the
MultiWorkerMirroredStrategy (in future versions, the MirroredStrategy will prob‐
ably  handle  both  the  single  machine  and  multimachine  cases).  When  you  start  this
script on the first workers, they will remain blocked at the AllReduce step, but as soon
as the last worker starts up training will begin, and you will see them all advancing at
exactly the same rate (since they synchronize at each step).

You can choose from two AllReduce implementations for this distribution strategy: a
ring  AllReduce  algorithm  based  on  gRPC  for  the  network  communications,  and
NCCL’s implementation. The best algorithm to use depends on the number of work‐
ers,  the  number  and  types  of  GPUs,  and  the  network.  By  default,  TensorFlow  will
apply some heuristics to select the right algorithm for you, but if you want to force
one  algorithm,  pass  CollectiveCommunication.RING  or  CollectiveCommunica
tion.NCCL (from tf.distribute.experimental) to the strategy’s constructor.

If  you  prefer  to  implement  asynchronous  data  parallelism  with  parameter  servers,
change  the  strategy  to  ParameterServerStrategy,  add  one  or  more  parameter
servers, and configure TF_CONFIG appropriately for each task. Note that although the
workers  will  work  asynchronously,  the  replicas  on  each  worker  will  work
synchronously.

Lastly,  if  you  have  access  to  TPUs  on  Google  Cloud,  you  can  create  a  TPUStrategy
like this (then use it like the other strategies):

Training Models Across Multiple Devices 

| 

713

resolver = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.tpu.experimental.initialize_tpu_system(resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)

If you are a researcher, you may be eligible to use TPUs for free; see
https://tensorflow.org/tfrc for more details.

You can now train models across multiple GPUs and multiple servers: give yourself a
pat on the back! If you want to train a large model, you will need many GPUs, across
many servers, which will require either buying a lot of hardware or managing a lot of
cloud VMs. In many cases, it’s going to be less hassle and less expensive to use a cloud
service  that  takes  care  of  provisioning  and  managing  all  this  infrastructure  for  you,
just when you need it. Let’s see how to do that on GCP.

Running Large Training Jobs on Google Cloud AI Platform
If you decide to use Google AI Platform, you can deploy a training job with the same
training code as you would run on your own TF cluster, and the platform will take
care of provisioning and configuring as many GPU VMs as you desire (within your
quotas).

To  start  the  job,  you  will  need  the  gcloud  command-line  tool,  which  is  part  of  the
Google Cloud SDK. You can either install the SDK on your own machine, or just use
the Google Cloud Shell on GCP. This is a terminal you can use directly in your web
browser;  it  runs  on  a  free  Linux  VM  (Debian),  with  the  SDK  already  installed  and
preconfigured for you. The Cloud Shell is available anywhere in GCP: just click the
Activate Cloud Shell icon at the top right of the page (see Figure 19-22).

Figure 19-22. Activating the Google Cloud Shell

If you prefer to install the SDK on your machine, once you have installed it, you need
to  initialize  it  by  running  gcloud  init:  you  will  need  to  log  in  to  GCP  and  grant
access  to  your  GCP  resources,  then  select  the  GCP  project  you  want  to  use  (if  you
have more than one), as well as the region where you want the job to run. The gcloud
command gives you access to every GCP feature, including the ones we used earlier.
You don’t have to go through the web interface every time; you can write scripts that
start or stop VMs for you, deploy models, or perform any other GCP action.

714 

| 

Chapter 19: Training and Deploying TensorFlow Models at Scale
a standard result in linear algebra
(and, indeed, one of the ways of deï¬ning determinants) that the volume of
C2 is given by |A|. Now, suppose s is uniformly distributed in [0, 1]d, so its
density is ps(s) = 1{s âˆˆ C1}. Then clearly x will be uniformly distributed
in C2. Its density is therefore found to be px(x) = 1{x âˆˆ C2}/vol(C2) (since
it must integrate over C2 to 1). But using the fact that the determinant
of the inverse of a matrix is just the inverse of the determinant, we have
1/vol(C2) = 1/|A| = |Aâˆ’1| = |W |. Thus, px(x) = 1{x âˆˆ C2}|W | = 1{W x âˆˆ
C1}|W | = ps(W x)|W |.

13.3

ICA algorithm

We are now ready to derive an ICA algorithm. We describe an algorithm
by Bell and Sejnowski, and we give an interpretation of their algorithm as a
method for maximum likelihood estimation. (This is diï¬€erent from their orig-
inal interpretation involving a complicated idea called the infomax principal
which is no longer necessary given the modern understanding of ICA.)

We suppose that the distribution of each source sj is given by a density

ps, and that the joint distribution of the sources s is given by

p(s) =

d
(cid:89)

j=1

ps(sj).

175

Note that by modeling the joint distribution as a product of marginals, we
capture the assumption that the sources are independent. Using our formulas
from the previous section, this implies the following density on x = As =
W âˆ’1s:

d
(cid:89)

p(x) =

ps(wT

j x) Â· |W |.

j=1

All that remains is to specify a density for the individual sources ps.

Recall that, given a real-valued random variable z, its cumulative distri-
âˆ’âˆž pz(z)dz and

bution function (cdf) F is deï¬ned by F (z0) = P (z â‰¤ z0) = (cid:82) z0
the density is the derivative of the cdf: pz(z) = F (cid:48)(z).

Thus, to specify a density for the siâ€™s, all we need to do is to specify some
cdf for it. A cdf has to be a monotonic function that increases from zero
to one. Following our previous discussion, we cannot choose the Gaussian
cdf, as ICA doesnâ€™t work on Gaussian data. What weâ€™ll choose instead as
a reasonable â€œdefaultâ€ cdf that slowly increases from 0 to 1, is the sigmoid
function g(s) = 1/(1 + eâˆ’s). Hence, ps(s) = g(cid:48)(s).1

The square matrix W is the parameter in our model. Given a training

set {x(i); i = 1, . . . , n}, the log likelihood is given by

(cid:96)(W ) =

n
(cid:88)

(cid:32) d

(cid:88)

i=1

j=1

log g(cid:48)(wT

j x(i)) + log |W |

.

(cid:33)

We would like to maximize this in terms W . By taking derivatives and using
the fact (from the ï¬rst set of notes) that âˆ‡W |W | = |W |(W âˆ’1)T , we easily
derive a stochastic gradient ascent learning rule. For a training example x(i),
the update rule is:

W := W + Î±

ï£«

ï£®

ï£¬
ï£¬
ï£¬
ï£­

ï£¯
ï£¯
ï£¯
ï£°

1 âˆ’ 2g(wT
1 âˆ’ 2g(wT

1 x(i))
2 x(i))

...

1 âˆ’ 2g(wT

d x(i))

ï£¹

ï£º
ï£º
ï£º
ï£»

x(i)T + (W T )âˆ’1

ï£¶

ï£·
ï£·
ï£·
ï£¸

,

1If you have prior knowledge that the sourcesâ€™ densities take a certain form, then it
is a good idea to substitute that in here. But in the absence of such knowledge, the
sigmoid function can be thought of as a reasonable default that seems to work well for
many problems. Also, the presentation here assumes that either the data x(i) has been
preprocessed to have zero mean, or that it can naturally be expected to have zero mean
(such as acoustic signals). This is necessary because our assumption that ps(s) = g(cid:48)(s)
implies E[s] = 0 (the derivative of the logistic function is a symmetric function, and
hence gives a density corresponding to a random variable with zero mean), which implies
E[x] = E[As] = 0.

176

where Î± is the learning rate.

After the algorithm converges, we then compute s(i) = W x(i) to recover

the original sources.

Remark. When writing down the likelihood of the data, we implicitly as-
sumed that the x(i)â€™s were independent of each other (for diï¬€erent values
of i; note this issue is diï¬€erent from whether the diï¬€erent coordinates of
x(i) are independent), so that the likelihood of the training set was given
by (cid:81)
i p(x(i); W ). This assumption is clearly incorrect for speech data and
other time series where the x(i)â€™s are dependent, but it can be shown that
having correlated training examples will not hurt the performance of the al-
gorithm if we have suï¬ƒcient data. However, for problems where successive
training examples are correlated, when implementing stochastic gradient as-
cent, it sometimes helps accelerate convergence if we visit training examples
(I.e., run stochastic gradient ascent on a
in a randomly permuted order.
randomly shuï¬„ed copy of the training set.)

Chapter 14

Self-supervised learning and
foundation models

Despite its huge success, supervised learning with neural networks typically
relies on the availability of a labeled dataset of decent size, which is some-
times costly to collect. Recently, AI and machine learning are undergoing a
paradigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and
GPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and
are adaptable to a wide range of downstream tasks. These models, called
foundation models by Bommasani et al. [2021], oftentimes leverage massive
unlabeled data so that much fewer labeled data in the downstream tasks are
needed. Moreover, though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent capabil-
ities. These models are typically (pre-)trained by self-supervised learning
methods where the supervisions/labels come from parts of the inputs.

This chapter will introduce the paradigm of foundation models and basic

related concepts.

14.1 Pretraining and adaptation

The foundation models paradigm consists of two phases: pretraining (or sim-
ply training) and adaptation. We ï¬rst pretrain a large model on a massive
unlabeled dataset (e.g., billions of unlabeled images).1 Then, we adapt the
pretrained model to a downstream task (e.g., detecting cancer from scan im-
ages). These downstream tasks are often prediction tasks with limited or

1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-

geNet dataset).

177

178

even no labeled data. The intuition is that the pretrained models learn good
representations that capture intrinsic semantic structure/ information about
the data, and the adaptation phase customizes the model to a particular
downstream task by, e.g., retrieving the information speciï¬c to it. For ex-
ample, a model pretrained on massive unlabeled image data may learn good
general visual representations/features, and we adapt the representations to
solve biomedical imagining tasks.

We formalize the two phases below.

Pretraining.
Suppose we have an unlabeled pretraining dataset
{x(1), x(2) Â· Â· Â· , x(n)} that consists of n examples in Rd. Let Ï†Î¸ be a model that
is parameterized by Î¸ and maps the input x to some m-dimensional represen-
tation Ï†Î¸(x). (People also call Ï†Î¸(x) âˆˆ Rm the embedding or features of the
example x.) We pretrain the model Î¸ with a pretraining loss, which is often
an average of loss functions on all the examples: Lpre(Î¸) = 1
i=1 (cid:96)pre(Î¸, x(i)).
n
Here (cid:96)pre is a so-called self-supervised loss on a single datapoint x(i), because
as shown later, e.g., in Section 14.3, the â€œsupervisionâ€ comes from the data
point x(i) itself.
It is also possible that the pretraining loss is not a sum
of losses on individual examples. We will discuss two pretraining losses in
Section 14.2 and Section 14.3.

(cid:80)n

We use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,
2014]) to minimize Lpre(Î¸). We denote the obtained pretrained model by Ë†Î¸.

task

, y(ntask)
task

task, y(1)

task), Â· Â· Â· , (x(ntask)

Adaptation. For a downstream task, we usually have a labeled dataset
{(x(1)
)} with ntask examples. The setting when
ntask = 0 is called zero-shot learningâ€”the downstream task doesnâ€™t have any
labeled examples. When ntask is relatively small (say, between 1 and 50), the
setting is called few-shot learning. Itâ€™s also pretty common to have a larger
ntask on the order of ranging from hundreds to tens of thousands.

An adaptation algorithm generally takes in a downstream dataset and the
pretrained model Ë†Î¸, and outputs a variant of Ë†Î¸ that solves the downstream
task. We will discuss below two popular and general adaptation methods,
linear probe and ï¬netuning. In addition, two other methods speciï¬c to lan-
guage problems are introduced in 14.3.2.

The linear probe approach uses a linear head on top of the representation
to predict the downstream labels. Mathematically, the adapted model out-
puts w(cid:62)Ï†Ë†Î¸(x), where w âˆˆ Rm is a parameter to be learned, and Ë†Î¸ is exactly
the pretrained model (ï¬xed). We can use SGD (or other optimizers) to train

w on the downstream task loss to predict the task label

min
wâˆˆRm

1
ntask

ntask(cid:88)

i=1

(cid:96)task(y(i)

task, w(cid:62)Ï†Ë†Î¸(x(i)

task))

179

(14.1)

if the downstream task is a regression problem, we will have

E.g.,
(cid:96)task(ytask, w(cid:62)Ï†Ë†Î¸(xtask)) = (ytask âˆ’ w(cid:62)Ï†Ë†Î¸(xtask))2.

The ï¬netuning algorithm uses a similar structure for the downstream
prediction model, but also further ï¬netunes the pretrained model (instead
of keeping it ï¬xed). Concretely, the prediction model is w(cid:62)Ï†Î¸(x) with pa-
rameters w and Î¸. We optimize both w and Î¸ to ï¬t the downstream data,
but initialize Î¸ with the pretrained model Ë†Î¸. The linear head w is usually
initialized randomly.

minimize
w,Î¸

1
ntask

ntask(cid:88)

i=1

(cid:96)task(y(i)

task, w(cid:62)Ï†Î¸(x(i)

task))

with initialization w â† random vector

Î¸ â† Ë†Î¸

(14.2)

(14.3)

(14.4)

Various other adaptation methods exists and are sometimes specialized
to the particular pretraining methods. We will discuss one of them in Sec-
tion 14.3.2.

14.2 Pretraining methods in computer vision

This section introduces two concrete pretraining methods for computer vi-
sion: supervised pretraining and contrastive learning.

Supervised pretraining. Here, the pretraining dataset is a large-scale
labeled dataset (e.g., ImageNet), and the pretrained models are simply a
neural network trained with vanilla supervised learning (with the last layer
being removed). Concretely, suppose we write the learned neural network as
U Ï†Ë†Î¸(x), where U is the last (fully-connected) layer parameters, Ë†Î¸ corresponds
to the parameters of all the other layers, and Ï†Ë†Î¸(x) are the penultimate
activations layer (which serves as the representation). We simply discard U
and use Ï†Ë†Î¸(x) as the pretrained model.
Contrastive learning. Contrastive learning is a self-supervised pretraining
method that uses only unlabeled data. The main intuition is that a good
representation function Ï†Î¸(Â·) should map semantically similar images to sim-
ilar representations, and that random pair of images should generally have

180

distinct representations. E.g., we may want to map images of two huskies to
similar representations, but a husky and an elephant should have diï¬€erent
representations. One deï¬nition of similarity is that images from the same
class are similar. Using this deï¬nition will result in the so-called supervised
contrastive algorithms that work well when labeled pretraining datasets are
available.

Without labeled data, we can use data augmentation to generate a pair
of â€œsimilarâ€ augmented images given an original image x. Data augmenta-
tion typically means that we apply random cropping, ï¬‚ipping, and/or color
transformation on the original image x to generate a variant. We can take
two random augmentations, denoted by Ë†x and Ëœx, of the same original image
x, and call them a positive pair. We observe that positive pairs of images
are often semantically related because they are augmentations of the same
image. We will design a loss function for Î¸ such that the representations of
a positive pair, Ï†Î¸(Ë†x), Ï†Î¸(Ëœx), as close to each other as possible.

On the other hand, we can also take another random image z from the
pretraining dataset and generate an augmentation Ë†z from z. Note that (Ë†x, Ë†z)
are from diï¬€erent images; therefore, with a good chance, they are not seman-
tically related. We call (Ë†x, Ë†z) a negative or random pair.2 We will design a
loss to push the representation of random pairs, Ï†Î¸(Ë†x), Ï†Î¸(Ë†z), far away from
each other.

There are many recent algorithms based on the contrastive learning prin-
ciple, and here we introduce SIMCLR [Chen et al., 2020] as an concrete
example. The loss function is deï¬ned on a batch of examples (x1, Â· Â· Â· , x(B))
with batch size B. The algorithm computes two random augmentations for
each example x(i) in the batch, denoted by Ë†x(i) and Ëœx(i). As a result, we
have the augmented batch of 2B examples: Ë†x1, Â· Â· Â· , Ë†x(B), Ëœx1, Â· Â· Â· , Ëœx(B). The
SIMCLR loss is deï¬ned as3

Lpre(Î¸) = âˆ’

B
(cid:88)

i=1

log

exp (cid:0)Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(i))(cid:1)

exp (Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(i))) + (cid:80)

j(cid:54)=i exp (Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(j)))

.

The intuition is as follows. The loss is increasing in Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(j)), and
thus minimizing the loss encourages Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(j)) to be small, making
Ï†Î¸(Ë†x(i)) far away from Ï†Î¸(Ëœx(j)). On the other hand, the loss is decreasing in

2Random pair may be a more accurate term because itâ€™s still possible (though not
likely) that x and z are semantically related, so are Ë†x and Ë†z. But in the literature, the
term negative pair seems to be also common.

3This is a variant and simpliï¬cation of the original loss that does not change the essence

(but may change the eï¬ƒciency slightly).

181

Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(i)), and thus minimizing the loss encourages Ï†Î¸(Ë†x(i))(cid:62)Ï†Î¸(Ëœx(i))
to be large, resulting in Ï†Î¸(Ë†x(i)) and Ï†Î¸(Ëœx(i)) to be close.4

14.3 Pretrained large language models

Natural language processing is another area where pretraining models are
In language problems, an example typically corre-
particularly successful.
sponds to a document or generally a sequence/trunk of words,5 denoted
by x = (x1, Â· Â· Â· , xT ) where T is the length of the document/sequence,
xi âˆˆ {1, Â· Â· Â· , V } are words in the document, and V is the vocabulary size.6
A language model is a probabilistic model representing the probability of
a document, denoted by p(x1, Â· Â· Â· , xT ). This probability distribution is very
complex because its support size is V T â€” exponential in the length of the
document. Instead of modeling the distribution of a document itself, we can
apply the chain rule of conditional probability to decompose it as follows:

p(x1, Â· Â· Â· , xT ) = p(x1)p(x2|x1) Â· Â· Â· p(xT |x1, Â· Â· Â· , xT âˆ’1).

(14.5)

Now the support size of each of the conditional probability p(xt|x1, Â· Â· Â· , xtâˆ’1)
is V .

We will model the conditional probability p(xt|x1, Â· Â· Â· , xtâˆ’1) as a function

of x1, . . . , xtâˆ’1 parameterized by some parameter Î¸.

A parameterized model takes in numerical inputs and therefore we ï¬rst
introduce embeddings or representations fo the words. Let ei âˆˆ Rd be the
embedding of the word i âˆˆ {1, 2, Â· Â· Â· , V }. We call [e1, Â· Â· Â· , eV ] âˆˆ RdÃ—V the
embedding matrix.

The most commonly used model is Transformer [Vaswani et al., 2017]. In
this subsection, we will introduce the input-output interface of a Transformer,
but 
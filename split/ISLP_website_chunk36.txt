losely mimic best subset selection while retaining the computational advantages of forward and
backward stepwise selection.

6.1.3

Choosing the Optimal Model

Best subset selection, forward selection, and backward selection result in
the creation of a set of models, each of which contains a subset of the p
3 Like forward stepwise selection, backward stepwise selection performs a guided search
over model space, and so effectively considers substantially more than 1 + p(p + 1)/2
models.

236

6. Linear Model Selection and Regularization

predictors. To apply these methods, we need a way to determine which of
these models is best. As we discussed in Section 6.1.1, the model containing
all of the predictors will always have the smallest RSS and the largest R2 ,
since these quantities are related to the training error. Instead, we wish to
choose a model with a low test error. As is evident here, and as we show
in Chapter 2, the training error can be a poor estimate of the test error.
Therefore, RSS and R2 are not suitable for selecting the best model among
a collection of models with different numbers of predictors.
In order to select the best model with respect to test error, we need to
estimate this test error. There are two common approaches:
1. We can indirectly estimate test error by making an adjustment to the
training error to account for the bias due to overfitting.
2. We can directly estimate the test error, using either a validation set
approach or a cross-validation approach, as discussed in Chapter 5.
We consider both of these approaches below.
Cp , AIC, BIC, and Adjusted R2
We show in Chapter 2 that the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.) This is because
when we fit a model to the training data using least squares, we specifically estimate the regression coefficients such that the training RSS (but
not the test RSS) is as small as possible. In particular, the training error
will decrease as more variables are included in the model, but the test error
may not. Therefore, training set RSS and training set R2 cannot be used
to select from among a set of models with different numbers of variables.
However, a number of techniques for adjusting the training error for the
model size are available. These approaches can be used to select among a set
of models with different numbers of variables. We now consider four such
approaches: Cp , Akaike information criterion (AIC), Bayesian information
Cp
criterion (BIC), and adjusted R2 . Figure 6.2 displays Cp , BIC, and adjusted
Akaike
R2 for the best model of each size produced by best subset selection on the information
Credit data set.
criterion
For a fitted least squares model containing d predictors, the Cp estimate Bayesian
information
of test MSE is computed using the equation
(
1'
Cp =
RSS + 2dσ̂ 2 ,
n

(6.2)

where σ̂ 2 is an estimate of the variance of the error " associated with each
response measurement in (6.1).4 Typically σ̂ 2 is estimated using the full
model containing all predictors. Essentially, the Cp statistic adds a penalty
of 2dσ̂ 2 to the training RSS in order to adjust for the fact that the training
error tends to underestimate the test error. Clearly, the penalty increases as
the number of predictors in the model increases; this is intended to adjust
4 Mallow’s C

!
2
p is sometimes defined as Cp = RSS/σ̂ + 2d − n. This is equivalent to
1 2
the definition given above in the sense that Cp = n σ̂ (Cp! + n), and so the model with
smallest Cp also has smallest Cp! .

criterion
adjusted R2

4

6

8

10

0.94
0.92
0.90
0.86

0.88

Adjusted R2

10000
2

2

Number of Predictors

237

0.96

30000
25000

BIC

15000

20000

20000
10000

15000

Cp

25000

30000

6.1 Subset Selection

4

6

8

10

Number of Predictors

2

4

6

8

10

Number of Predictors

FIGURE 6.2. Cp , BIC, and adjusted R2 are shown for the best models of each
size for the Credit data set (the lower frontier in Figure 6.1). Cp and BIC are
estimates of test MSE. In the middle plot we see that the BIC estimate of test
error shows an increase after four variables are selected. The other two plots are
rather flat after four variables are included.

for the corresponding decrease in training RSS. Though it is beyond the
scope of this book, one can show that if σ̂ 2 is an unbiased estimate of σ 2 in
(6.2), then Cp is an unbiased estimate of test MSE. As a consequence, the
Cp statistic tends to take on a small value for models with a low test error,
so when determining which of a set of models is best, we choose the model
with the lowest Cp value. In Figure 6.2, Cp selects the six-variable model
containing the predictors income, limit, rating, cards, age and student.
The AIC criterion is defined for a large class of models fit by maximum
likelihood. In the case of the model (6.1) with Gaussian errors, maximum
likelihood and least squares are the same thing. In this case AIC is given by
AIC =

(
1'
RSS + 2dσ̂ 2 ,
n

where, for simplicity, we have omitted irrelevant constants.5 Hence for least
squares models, Cp and AIC are proportional to each other, and so only
Cp is displayed in Figure 6.2.
BIC is derived from a Bayesian point of view, but ends up looking similar
to Cp (and AIC) as well. For the least squares model with d predictors, the
BIC is, up to irrelevant constants, given by
BIC =

(
1'
RSS + log(n)dσ̂ 2 .
n

(6.3)

Like Cp , the BIC will tend to take on a small value for a model with a
low test error, and so generally we select the model that has the lowest
BIC value. Notice that BIC replaces the 2dσ̂ 2 used by Cp with a log(n)dσ̂ 2
term, where n is the number of observations. Since log n > 2 for any n > 7,
5 There are two formulas for AIC for least squares regression. The formula that we
provide here requires an expression for σ 2 , which we obtain using the full model containing all predictors. The second formula is appropriate when σ 2 is unknown and we do
not want to explicitly estimate it; that formula has a log(RSS) term instead of an RSS
term. Detailed derivations of these two formulas are outside of the scope of this book.

238

6. Linear Model Selection and Regularization

the BIC statistic generally places a heavier penalty on models with many
variables, and hence results in the selection of smaller models than Cp .
In Figure 6.2, we see that this is indeed the case for the Credit data set;
BIC chooses a model that contains only the four predictors income, limit,
cards, and student. In this case the curves are very flat and so there does
not appear to be much difference in accuracy between the four-variable and
six-variable models.
The adjusted R2 statistic is another popular approach for selecting among
a set of models that contain different numbers of variables. Recall from
Chapter
3 that the usual R2 is defined as 1 − RSS/TSS, where TSS =
)
(yi − y)2 is the total sum of squares for the response. Since RSS always
decreases as more variables are added to the model, the R2 always increases
as more variables are added. For a least squares model with d variables,
the adjusted R2 statistic is calculated as
Adjusted R2 = 1 −

RSS/(n − d − 1)
.
TSS/(n − 1)

(6.4)

Unlike Cp , AIC, and BIC, for which a small value indicates a model with
a low test error, a large value of adjusted R2 indicates a model with a
small test error. Maximizing the adjusted R2 is equivalent to minimizing
RSS
n−d−1 . While RSS always decreases as the number of variables in the model
RSS
increases, n−d−1
may increase or decrease, due to the presence of d in the
denominator.
The intuition behind the adjusted R2 is that once all of the correct
variables have been included in the model, adding additional noise variables
will lead to only a very small decrease in RSS. Since adding noise variables
RSS
leads to an increase in d, such variables will lead to an increase in n−d−1
,
2
and consequently a decrease in the adjusted R . Therefore, in theory, the
model with the largest adjusted R2 will have only correct variables and
no noise variables. Unlike the R2 statistic, the adjusted R2 statistic pays
a price for the inclusion of unnecessary variables in the model. Figure 6.2
displays the adjusted R2 for the Credit data set. Using this statistic results
in the selection of a model that contains seven variables, adding own to the
model selected by Cp and AIC.
Cp , AIC, and BIC all have rigorous theoretical justifications that are
beyond the scope of this book. These justifications rely on asymptotic arguments (scenarios where the sample size n is very large). Despite its popularity, and even though it is quite intuitive, the adjusted R2 is not as well
motivated in statistical theory as AIC, BIC, and Cp . All of these measures
are simple to use and compute. Here we have presented their formulas in
the case of a linear model fit using least squares; however, AIC and BIC
can also be defined for more general types of models.
Validation and Cross-Validation
As an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods
discussed in Chapter 5. We can compute the validation set error or the
cross-validation error for each model under consideration, and then select

2

4

6

8

10

Number of Predictors

239

200
180
160
140
100

120

Cross−Validation Error

220

220
200
180
160
100

100

120

140

Validation Set Error

200
180
160
140
120

Square Root of BIC

220

6.1 Subset Selection

2

4

6

8

10

Number of Predictors

2

4

6

8

10

Number of Predictors

FIGURE 6.3. For the Credit data set, three quantities are displayed for the
best model containing d predictors, for d ranging from 1 to 11. The overall best
model, based on each of these quantities, is shown as a blue cross. Left: Square
root of BIC. Center: Validation set errors. Right: Cross-validation errors.

the model for which the resulting estimated test error is smallest. This procedure has an advantage relative to AIC, BIC, Cp , and adjusted R2 , in that
it provides a direct estimate of the test error, and makes fewer assumptions
about the true underlying model. It can also be used in a wider range of
model selection tasks, even in cases where it is hard to pinpoint the model
degrees of freedom (e.g. the number of predictors in the model) or hard
to estimate the error variance σ 2 . Note that when cross-validation is used,
the sequence of models Mk in Algorithms 6.1–6.3 is determined separately
for each training fold, and the validation errors are averaged over all folds
for each model size k. This means, for example with best-subset regression,
that Mk , the best subset of size k, can differ across the folds. Once the
best size k is chosen, we find the best model of that size on the full data
set.
In the past, performing cross-validation was computationally prohibitive
for many problems with large p and/or large n, and so AIC, BIC, Cp ,
and adjusted R2 were more attractive approaches for choosing among a
set of models. However, nowadays with fast computers, the computations
required to perform cross-validation are hardly ever an issue. Thus, crossvalidation is a very attractive approach for selecting from among a number
of models under consideration.
Figure 6.3 displays, as a function of d, the BIC, validation set errors, and
cross-validation errors on the Credit data, for the best d-variable model.
The validation errors were calculated by randomly selecting three-quarters
of the observations as the training set, and the remainder as the validation set. The cross-validation errors were computed using k = 10 folds.
In this case, the validation and cross-validation methods both result in a
six-variable model. However, all three approaches suggest that the four-,
five-, and six-variable models are roughly equivalent in terms of their test
errors.
In fact, the estimated test error curves displayed in the center and righthand panels of Figure 6.3 are quite flat. While a three-variable model clearly
has lower estimated test error than a two-variable model, the estimated test
errors of the 3- to 11-variable models are quite similar. Furthermore, if we

240

6. Linear Model Selection and Regularization

repeated the validation set approach using a different split of the data into
a training set and a validation set, or if we repeated cross-validation using
a different set of cross-validation folds, then the precise model with the
lowest estimated test error would surely change. In this setting, we can
select a model using the one-standard-error rule. We first calculate the onestandard error of the estimated test MSE for each model size, and then standardselect the smallest model for which the estimated test error is within one error
standard error of the lowest point on the curve. The rationale here is that rule
if a set of models appear to be more or less equally good, then we might
as well choose the simplest model—that is, the model with the smallest
number of predictors. In this case, applying the one-standard-error rule
to the validation set or cross-validation approach leads to selection of the
three-variable model.

6.2

Shrinkage Methods

The subset selection methods described in Section 6.1 involve using least
squares to fit a linear model that contains a subset of the predictors. As an
alternative, we can fit a model containing all p predictors using a technique
that constrains or regularizes the coefficient estimates, or equivalently, that
shrinks the coefficient estimates towards zero. It may not be immediately
obvious why such a constraint should improve the fit, but it turns out that
shrinking the coefficient estimates can significantly reduce their variance.
The two best-known techniques for shrinking the regression coefficients
towards zero are ridge regression and the lasso.

6.2.1

Ridge Regression

Recall from Chapter 3 that the least squares fitting procedure estimates
β0 , β1 , . . . , βp using the values that minimize

2
p
n
0
0
y i − β 0 −
RSS =
βj xij  .
i=1

j=1

Ridge regression is very similar to least squares, except that the coefficients
ridge
are estimated by minimizing a slightly different quantity. In particular, the regression
ridge regression coefficient estimates β̂ R are the values that minimize

2
p
p
p
n
0
0
0
0
2
yi − β0 −

βj xij
+λ
βj = RSS + λ
βj2 ,
(6.5)
i=1

j=1

j=1

j=1

where λ ≥ 0 is a tuning parameter, to be determined separately. Equatuning
tion 6.5 trades off two different criteria. As with least squares, ridge regres- parameter
sion seeks coefficient estimates that fit
) the data well, by making the RSS
small. However, the second term, λ j βj2 , called a shrinkage penalty, is
shrinkage
small when β1 , . . . , βp are close to zero, and so it has the effect of shrinking penalty
the estimates of βj towards zero. The tuning parameter λ serves to control

1e−02

300
200
100
0
−100
−300

Standardized Coefficients

300
200
100
0
−100
−300

Standardized Coefficients

400

Income
Limit
Rating
Student

241

400

6.2 Shrinkage Methods

1e+00

1e+02

1e+04

λ

0.0

0.2

0.4

0.6

0.8

1.0

!β̂λR !2 /!β̂!2

FIGURE 6.4. The standardized ridge regression coefficients are displayed for
the Credit data set, as a function of λ and $β̂λR $2 /$β̂$2 .

the relative impact of these two terms on the regression coefficient estimates. When λ = 0, the penalty term has no effect, and ridge regression
will produce the least squares estimates. However, as λ → ∞, the impact of
the shrinkage pe
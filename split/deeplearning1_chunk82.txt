LEARNING

We provide here a list of these generic regularization strategies. The list is
clearly not exhaustive, but gives some concrete examples of ways that learning
algorithms can be encouraged to discover features that correspond to underlying
factors. This list was introduced in section 3.1 of Bengio et al. (2013d) and has
been partially expanded here.
â€¢ Smoothness: This is the assumption that f(x + î€?d) â‰ˆ f (x) for unit d and
small î€?. This assumption allows the learner to generalize from training
examples to nearby points in input space. Many machine learning algorithms
leverage this idea, but it is insuï¬ƒcient to overcome the curse of dimensionality.
â€¢ Linearity: Many learning algorithms assume that relationships between some
variables are linear. This allows the algorithm to make predictions even
very far from the observed data, but can sometimes lead to overly extreme
predictions. Most simple machine learning algorithms that do not make the
smoothness assumption instead make the linearity assumption. These are
in fact diï¬€erent assumptionsâ€”linear functions with large weights applied
to high-dimensional spaces may not be very smooth. See Goodfellow et al.
(2014b) for a further discussion of the limitations of the linearity assumption.
â€¢ Multiple explanatory factors: Many representation learning algorithms are
motivated by the assumption that the data is generated by multiple underlying
explanatory factors, and that most tasks can be solved easily given the state
of each of these factors. Section 15.3 describes how this view motivates semisupervised learning via representation learning. Learning the structure of p(x)
requires learning some of the same features that are useful for modeling p(y |
x) because both refer to the same underlying explanatory factors. Section 15.4
describes how this view motivates the use of distributed representations, with
separate directions in representation space corresponding to separate factors
of variation.
â€¢ Causal factors: the model is constructed in such a way that it treats the
factors of variation described by the learned representation h as the causes
of the observed data x, and not vice-versa. As discussed in section 15.3, this
is advantageous for semi-supervised learning and makes the learned model
more robust when the distribution over the underlying causes changes or
when we use the model for a new task.
â€¢ Depth, or a hierarchical organization of explanatory factors: High-level,
abstract concepts can be deï¬?ned in terms of simple concepts, forming a
hierarchy. From another point of view, the use of a deep architecture
555

CHAPTER 15. REPRESENTATION LEARNING

expresses our belief that the task should be accomplished via a multi-step
program, with each step referring back to the output of the processing
accomplished via previous steps.
â€¢ Shared factors across tasks: In the context where we have many tasks,
corresponding to diï¬€erent yi variables sharing the same input x or where
each task is associated with a subset or a function f (i)(x) of a global input
x, the assumption is that each y i is associated with a diï¬€erent subset from a
common pool of relevant factors h. Because these subsets overlap, learning
all the P (y i | x) via a shared intermediate representation P (h | x) allows
sharing of statistical strength between the tasks.
â€¢ Manifolds: Probability mass concentrates, and the regions in which it concentrates are locally connected and occupy a tiny volume. In the continuous
case, these regions can be approximated by low-dimensional manifolds with
a much smaller dimensionality than the original space where the data lives.
Many machine learning algorithms behave sensibly only on this manifold
(Goodfellow et al., 2014b). Some machine learning algorithms, especially
autoencoders, attempt to explicitly learn the structure of the manifold.
â€¢ Natural clustering: Many machine learning algorithms assume that each
connected manifold in the input space may be assigned to a single class. The
data may lie on many disconnected manifolds, but the class remains constant
within each one of these. This assumption motivates a variety of learning
algorithms, including tangent propagation, double backprop, the manifold
tangent classiï¬?er and adversarial training.
â€¢ Temporal and spatial coherence: Slow feature analysis and related algorithms
make the assumption that the most important explanatory factors change
slowly over time, or at least that it is easier to predict the true underlying
explanatory factors than to predict raw observations such as pixel values.
See section 13.3 for further description of this approach.
â€¢ Sparsity: Most features should presumably not be relevant to describing most
inputsâ€”there is no need to use a feature that detects elephant trunks when
representing an image of a cat. It is therefore reasonable to impose a prior
that any feature that can be interpreted as â€œpresentâ€? or â€œabsentâ€? should be
absent most of the time.
â€¢ Simplicity of Factor Dependencies: In good high-level representations, the
factors are related to each other through simple dependencies. The simplest
556

CHAPTER 15. REPRESENTATION LEARNING

î?‘
possible is marginal independence, P (h) = i P (hi), but linear dependencies
or those captured by a shallow autoencoder are also reasonable assumptions.
This can be seen in many laws of physics, and is assumed when plugging a
linear predictor or a factorized prior on top of a learned representation.
The concept of representation learning ties together all of the many forms
of deep learning. Feedforward and recurrent networks, autoencoders and deep
probabilistic models all learn and exploit representations. Learning the best
possible representation remains an exciting avenue of research.

557

Chapter 16

Structured Probabilistic Models
for Deep Learning
Deep learning draws upon many modeling formalisms that researchers can use to
guide their design eï¬€orts and describe their algorithms. One of these formalisms
is the idea of structured probabilistic models. We have already discussed
structured probabilistic models brieï¬‚y in section 3.14. That brief presentation was
suï¬ƒcient to understand how to use structured probabilistic models as a language to
describe some of the algorithms in part II. Now, in part III, structured probabilistic
models are a key ingredient of many of the most important research topics in deep
learning. In order to prepare to discuss these research ideas, this chapter describes
structured probabilistic models in much greater detail. This chapter is intended
to be self-contained; the reader does not need to review the earlier introduction
before continuing with this chapter.
A structured probabilistic model is a way of describing a probability distribution,
using a graph to describe which random variables in the probability distribution
interact with each other directly. Here we use â€œgraphâ€? in the graph theory senseâ€”a
set of vertices connected to one another by a set of edges. Because the structure
of the model is deï¬?ned by a graph, these models are often also referred to as
graphical models.
The graphical models research community is large and has developed many
diï¬€erent models, training algorithms, and inference algorithms. In this chapter, we
provide basic background on some of the most central ideas of graphical models,
with an emphasis on the concepts that have proven most useful to the deep learning
research community. If you already have a strong background in graphical models,
you may wish to skip most of this chapter. However, even a graphical model expert
558

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

may beneï¬?t from reading the ï¬?nal section of this chapter, section 16.7, in which we
highlight some of the unique ways that graphical models are used for deep learning
algorithms. Deep learning practitioners tend to use very diï¬€erent model structures,
learning algorithms and inference procedures than are commonly used by the rest
of the graphical models research community. In this chapter, we identify these
diï¬€erences in preferences and explain the reasons for them.
In this chapter we ï¬?rst describe the challenges of building large-scale probabilistic models. Next, we describe how to use a graph to describe the structure
of a probability distribution. While this approach allows us to overcome many
challenges, it is not without its own complications. One of the major diï¬ƒculties in
graphical modeling is understanding which variables need to be able to interact
directly, i.e., which graph structures are most suitable for a given problem. We
outline two approaches to resolving this diï¬ƒculty by learning about the dependencies in section 16.5. Finally, we close with a discussion of the unique emphasis that
deep learning practitioners place on speciï¬?c approaches to graphical modeling in
section 16.7.

16.1

The Challenge of Unstructured Modeling

The goal of deep learning is to scale machine learning to the kinds of challenges
needed to solve artiï¬?cial intelligence. This means being able to understand highdimensional data with rich structure. For example, we would like AI algorithms to
be able to understand natural images,1 audio waveforms representing speech, and
documents containing multiple words and punctuation characters.
Classiï¬?cation algorithms can take an input from such a rich high-dimensional
distribution and summarize it with a categorical labelâ€”what object is in a photo,
what word is spoken in a recording, what topic a document is about. The process
of classiï¬?cation discards most of the information in the input and produces a
single output (or a probability distribution over values of that single output). The
classiï¬?er is also often able to ignore many parts of the input. For example, when
recognizing an object in a photo, it is usually possible to ignore the background of
the photo.
It is possible to ask probabilistic models to do many other tasks. These tasks are
often more expensive than classiï¬?cation. Some of them require producing multiple
output values. Most require a complete understanding of the entire structure of
1

A natural image is an image that might be captured by a camera in a reasonably ordinary
environment, as opposed to a synthetically rendered image, a screenshot of a web page, etc.
559

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

the input, with no option to ignore sections of it. These tasks include the following:
â€¢ Density estimation: given an input x, the machine learning system returns
an estimate of the true density p(x) under the data generating distribution.
This requires only a single output, but it does require a complete understanding of the entire input. If even one element of the vector is unusual, the
system must assign it a low probability.
â€¢ Denoising: given a damaged or incorrectly observed input xÌƒ, the machine
learning system returns an estimate of the original or correct x. For example,
the machine learning system might be asked to remove dust or scratches
from an old photograph. This requires multiple outputs (every element of the
estimated clean example x) and an understanding of the entire input (since
even one damaged area will still reveal the ï¬?nal estimate as being damaged).
â€¢ Missing value imputation: given the observations of some elements of x,
the model is asked to return estimates of or a probability distribution over
some or all of the unobserved elements of x. This requires multiple outputs.
Because the model could be asked to restore any of the elements of x, it
must understand the entire input.
â€¢ Sampling: the model generates new samples from the distribution p(x).
Applications include speech synthesis, i.e. producing new waveforms that
sound like natural human speech. This requires multiple output values and a
good model of the entire input. If the samples have even one element drawn
from the wrong distribution, then the sampling process is wrong.
For an example of a sampling task using small natural images, see ï¬?gure 16.1.
Modeling a rich distribution over thousands or millions of random variables is a
challenging task, both computationally and statistically. Suppose we only wanted
to model binary variables. This is the simplest possible case, and yet already it
seems overwhelming. For a small, 32 Ã— 32 pixel color (RGB) image, there are 2 3072
possible binary images of this form. This number is over 10800 times larger than
the estimated number of atoms in the universe.
In general, if we wish to model a distribution over a random vector x containing
n discrete variables capable of taking on k values each, then the naive approach of
representing P (x) by storing a lookup table with one probability value per possible
outcome requires kn parameters!
This is not feasible for several reasons:
560

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Figure 16.1: Probabilistic modeling of natural images. (Top)Example 32Ã— 32 pixel color
images from the CIFAR-10 dataset (Krizhevsky and Hinton, 2009). (Bottom)Samples
drawn from a structured probabilistic model trained on this dataset. Each sample appears
at the same position in the grid as the training example that is closest to it in Euclidean
space. This comparison allows us to see that the model is truly synthesizing new images,
rather than memorizing the training data. Contrast of both sets of images has been
adjusted for display. Figure reproduced with permission from Courville et al. (2011).

561

CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

â€¢ Memory: the cost of storing the representation: For all but very small values
of n and k, representing the distribution as a table will require too many
values to store.
â€¢ Statistical eï¬ƒciency: As the number of parameters in a model increases,
so does the amount of training data needed to choose the values of those
parameters using a statistical estimator. Because the table-based model
has an astronomical number of parameters, it will require an astronomically
large training set to ï¬?t accurately. Any such model will overï¬?t the training
set very badly unless additional assumptions are made linking the diï¬€erent
entries in the table (for example, like in back-oï¬€ or smoothed n-gram models,
section 12.4.1).
â€¢ Runtime: the cost of inference: Suppose we want to perform an inference
task where we use our model of the joint distribution P (x) to compute some
other distribution, such as the marginal distribution P (x1) or the conditional
distribution P (x2 | x1 ). Computing these distributions will require summing
across the entire table, so the runtime of these operations is as high as the
intractable memory cost of storing the model.
â€¢ Runtime: the cost of sampling: Likewise, suppose we want to draw a sample
from the model. The naive way to do this is to sample some value u âˆ¼ U (0, 1),
then iterate through the table, adding up the probability values until they
exceed u and return the outcome corresponding to that position in the table.
This requires reading through the whole table in the worst case, so it has
the same exponential cost as the other operations.
The problem with the table-based approach is that we are explicitly modeling
every possible kind of interaction between every possible subset of variables. The
probability distributions we encounter in real tasks are much simpl
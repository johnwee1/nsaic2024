of view of learning a useful representation of h, we would like h to encode
enough information about x to reconstruct it well, which implies that h and x
should have very high mutual information. These two goals are at odds with each
other. We often learn generative models that very precisely encode x into h but
are not able to mix very well. This situation arises frequently with Boltzmann
machinesâ€”the sharper the distribution a Boltzmann machine learns, the harder
it is for a Markov chain sampling from the model distribution to mix well. This
problem is illustrated in ï¬?gure 17.2.
All this could make MCMC methods less useful when the distribution of interest
has a manifold structure with a separate manifold for each class: the distribution
is concentrated around many modes and these modes are separated by vast regions
of high energy. This type of distribution is what we expect in many classiï¬?cation
problems and would make MCMC methods converge very slowly because of poor
mixing between modes.

602

CHAPTER 17. MONTE CARLO METHODS

17.5.1

Tempering to Mix between Modes

When a distribution has sharp peaks of high probability surrounded by regions of
low probability, it is diï¬ƒcult to mix between the diï¬€erent modes of the distribution.
Several techniques for faster mixing are based on constructing alternative versions
of the target distribution in which the peaks are not as high and the surrounding
valleys are not as low. Energy-based models provide a particularly simple way to
do so. So far, we have described an energy-based model as deï¬?ning a probability
distribution
p(x) âˆ? exp (âˆ’E (x)) .
(17.25)
Energy-based models may be augmented with an extra parameter Î² controlling
how sharply peaked the distribution is:
p Î² (x) âˆ? exp (âˆ’Î²E (x)) .

(17.26)

The Î² parameter is often described as being the reciprocal of the temperature,
reï¬‚ecting the origin of energy-based models in statistical physics. When the
temperature falls to zero and Î² rises to inï¬?nity, the energy-based model becomes
deterministic. When the temperature rises to inï¬?nity and Î² falls to zero, the
distribution (for discrete x) becomes uniform.
Typically, a model is trained to be evaluated at Î² = 1. However, we can make
use of other temperatures, particularly those where Î² < 1. Tempering is a general
strategy of mixing between modes of p1 rapidly by drawing samples with Î² < 1.
Markov chains based on tempered transitions (Neal, 1994) temporarily
sample from higher-temperature distributions in order to mix to diï¬€erent modes,
then resume sampling from the unit temperature distribution. These techniques
have been applied to models such as RBMs (Salakhutdinov, 2010). Another
approach is to use parallel tempering (Iba, 2001), in which the Markov chain
simulates many diï¬€erent states in parallel, at diï¬€erent temperatures. The highest
temperature states mix slowly, while the lowest temperature states, at temperature
1, provide accurate samples from the model. The transition operator includes
stochastically swapping states between two diï¬€erent temperature levels, so that a
suï¬ƒciently high-probability sample from a high-temperature slot can jump into a
lower temperature slot. This approach has also been applied to RBMs (Desjardins
et al., 2010; Cho et al., 2010). Although tempering is a promising approach, at
this point it has not allowed researchers to make a strong advance in solving the
challenge of sampling from complex EBMs. One possible reason is that there
are critical temperatures around which the temperature transition must be
very slow (as the temperature is gradually reduced) in order for tempering to be
eï¬€ective.
603

CHAPTER 17. MONTE CARLO METHODS

17.5.2

Depth May Help Mixing

When drawing samples from a latent variable model p(h, x), we have seen that if
p(h | x) encodes x too well, then sampling from p(x | h) will not change x very
much and mixing will be poor. One way to resolve this problem is to make h be a
deep representation, that encodes x into h in such a way that a Markov chain in
the space of h can mix more easily. Many representation learning algorithms, such
as autoencoders and RBMs, tend to yield a marginal distribution over h that is
more uniform and more unimodal than the original data distribution over x. It can
be argued that this arises from trying to minimize reconstruction error while using
all of the available representation space, because minimizing reconstruction error
over the training examples will be better achieved when diï¬€erent training examples
are easily distinguishable from each other in h-space, and thus well separated.
Bengio et al. (2013a) observed that deeper stacks of regularized autoencoders or
RBMs yield marginal distributions in the top-level h-space that appeared more
spread out and more uniform, with less of a gap between the regions corresponding
to diï¬€erent modes (categories, in the experiments). Training an RBM in that
higher-level space allowed Gibbs sampling to mix faster between modes. It remains
however unclear how to exploit this observation to help better train and sample
from deep generative models.
Despite the diï¬ƒculty of mixing, Monte Carlo techniques are useful and are
often the best tool available. Indeed, they are the primary tool used to confront
the intractable partition function of undirected models, discussed next.

604

Chapter 18

Confronting the Partition
Function
In section 16.2.2 we saw that many probabilistic models (commonly known as undirected graphical models) are deï¬?ned by an unnormalized probability distribution
pÌƒ(x; Î¸). We must normalize pÌƒ by dividing by a partition function Z(Î¸) in order to
obtain a valid probability distribution:
p(x; Î¸) =

1
pÌƒ(x; Î¸).
Z(Î¸)

(18.1)

The partition function is an integral (for continuous variables) or sum (for discrete
variables) over the unnormalized probability of all states:
î?š
pÌƒ(x)dx
(18.2)
or

î?˜

pÌƒ(x).

(18.3)

x

This operation is intractable for many interesting models.
As we will see in chapter 20, several deep learning models are designed to
have a tractable normalizing constant, or are designed to be used in ways that do
not involve computing p(x) at all. However, other models directly confront the
challenge of intractable partition functions. In this chapter, we describe techniques
used for training and evaluating models that have intractable partition functions.
605

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

18.1

The Log-Likelihood Gradient

What makes learning undirected models by maximum likelihood particularly
diï¬ƒcult is that the partition function depends on the parameters. The gradient of
the log-likelihood with respect to the parameters has a term corresponding to the
gradient of the partition function:
âˆ‡Î¸ log p(x; Î¸) = âˆ‡Î¸ log pÌƒ(x; Î¸) âˆ’ âˆ‡Î¸ log Z (Î¸).

(18.4)

This is a well-known decomposition into the positive phase and negative
phase of learning.
For most undirected models of interest, the negative phase is diï¬ƒcult. Models
with no latent variables or with few interactions between latent variables typically
have a tractable positive phase. The quintessential example of a model with a
straightforward positive phase and diï¬ƒcult negative phase is the RBM, which has
hidden units that are conditionally independent from each other given the visible
units. The case where the positive phase is diï¬ƒcult, with complicated interactions
between latent variables, is primarily covered in chapter 19. This chapter focuses
on the diï¬ƒculties of the negative phase.
Let us look more closely at the gradient of log Z :
âˆ‡Î¸ log Z
âˆ‡Î¸ Z
Z
î??
âˆ‡Î¸ x pÌƒ(x)
=
Z
î??
âˆ‡ Î¸ pÌƒ(x)
= x
.
Z
=

(18.5)
(18.6)
(18.7)
(18.8)

For models that guarantee p(x) > 0 for all x, we can substitute exp (log pÌƒ(x))
for pÌƒ(x):
î??
x âˆ‡ Î¸ exp (log pÌƒ(x))
(18.9)
Z
î??
x exp (log pÌƒ(x)) âˆ‡Î¸ log pÌƒ(x)
=
(18.10)
Z
î??
pÌƒ(x)âˆ‡ Î¸ log pÌƒ(x)
= x
(18.11)
Z
î?˜
=
(18.12)
p(x)âˆ‡Î¸ log pÌƒ(x)
x

606

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

= Exâˆ¼p(x) âˆ‡Î¸ log pÌƒ(x).

(18.13)

This derivation made use of summation over discrete x, but a similar result
applies using integration over continuous x. In the continuous version of the
derivation, we use Leibnizâ€™s rule for diï¬€erentiation under the integral sign to obtain
the identity
î?š
î?š
âˆ‡Î¸

pÌƒ(x)dx =

âˆ‡Î¸ pÌƒ(x)dx.

(18.14)

This identity is applicable only under certain regularity conditions on pÌƒ and âˆ‡Î¸ pÌƒ(x).
In measure theoretic terms, the conditions are: (i) The unnormalized distribution pÌƒ
must be a Lebesgue-integrable function of x for every value of Î¸; (ii) The gradient
âˆ‡ Î¸ pÌƒ(x) must exist for all Î¸ and almost all x; (iii) There must exist an integrable
function R(x) that bounds âˆ‡ Î¸pÌƒ (x) in the sense that maxi | âˆ‚Î¸âˆ‚ i pÌƒ(x) | â‰¤ R(x) for all
Î¸ and almost all x. Fortunately, most machine learning models of interest have
these properties.
This identity
âˆ‡Î¸ log Z = Exâˆ¼p(x) âˆ‡Î¸ log pÌƒ(x)

(18.15)

is the basis for a variety of Monte Carlo methods for approximately maximizing
the likelihood of models with intractable partition functions.
The Monte Carlo approach to learning undirected models provides an intuitive
framework in which we can think of both the positive phase and the negative
phase. In the positive phase, we increase log pÌƒ(x) for x drawn from the data. In
the negative phase, we decrease the partition function by decreasing log pÌƒ(x) drawn
from the model distribution.
In the deep learning literature, it is common to parametrize log pÌƒ in terms of
an energy function (equation 16.7). In this case, we can interpret the positive
phase as pushing down on the energy of training examples and the negative phase
as pushing up on the energy of samples drawn from the model, as illustrated in
ï¬?gure 18.1.

18.2

Stochastic Maximum Likelihood and Contrastive
Divergence

The naive way of implementing equation 18.15 is to compute it by burning in
a set of Markov chains from a random initialization every time the gradient is
needed. When learning is performed using stochastic gradient descent, this means
the chains must be burned in once per gradient step. This approach leads to the
607

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

training procedure presented in algorithm 18.1. The high cost of burning in the
Markov chains in the inner loop makes this procedure computationally infeasible,
but this procedure is the starting point that other more practical algorithms aim
to approximate.
Algorithm 18.1 A naive MCMC algorithm for maximizing the log-likelihood
with an intractable partition function using gradient ascent.
Set î€?, the step size, to a small positive number.
Set k, the number of Gibbs steps, high enough to allow burn in. Perhaps 100 to
train an RBM on a small image patch.
while not converged do
Sample î??
a minibatch of m examples {x(1) , . . . , x(m)} from the training set.
(i )
g â†? m1 m
i=1 âˆ‡Î¸ log pÌƒ(x ; Î¸ ).
Initialize a set of m samples { xÌƒ(1) , . . . , xÌƒ(m)} to random values (e.g., from
a uniform or normal distribution, or possibly a distribution with marginals
matched to the modelâ€™s marginals).
for i = 1 to k do
for j = 1 to m do
xÌƒ(j) â†? gibbs_update(xÌƒ(j) ).
end for
end for
î??
(i )
g â†? g âˆ’ m1 m
i=1 âˆ‡Î¸ log pÌƒ(xÌƒ ; Î¸ ).
Î¸ â†? Î¸ + î€?g.
end while
We can view the MCMC approach to maximum likelihood as trying to achieve
balance between two forces, one pushing up on the model distribution where the
data occurs, and another pushing down on the model distribution where the model
samples occur. Figure 18.1 illustrates this process. The two forces correspond to
maximizing log pÌƒ and minimizing log Z. Several approximations to the negative
phase are possible. Each of these approximations can be understood as making
the negative phase computationally cheaper but also making it push down in the
wrong locations.
Because the negative phase involves drawing samples from the modelâ€™s distribution, we can think of it as ï¬?nding points that the model believes in strongly.
Because the negative phase acts to reduce the probability of those points, they
are generally considered to represent the modelâ€™s incorrect beliefs about the world.
They are frequently referred to in the literature as â€œhallucinationsâ€? or â€œfantasy
particles.â€? In fact, the negative phase has been proposed as a possible explanation
608

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

The positive phase

The negative phase
pmodel (x)
pdata(x)
p(x)

p(x)

pmodel (x)
pdata(x)

x

x

Figure 18.1: The view of algorithm 18.1 as having a â€œpositive phaseâ€? and â€œnegative phase.â€?
(Left)In the positive phase, we sample points from the data distribution, and push up on
their unnormalized probability. This means points that are likely in the data get pushed
up on more. (Right)In the negative phase, we sample points from the model distribution,
and push down on their unnormalized probability. This counteracts the positive phaseâ€™s
tendency to just add a large constant to the unnormalized probability everywhere. When
the data distribution and the model distribution are equal, the positive phase has the
same chance to push up at a point as the negative phase has to push down. When this
occurs, there is no longer any gradient (in expectation) and training must terminate.

for dreaming in humans and other animals (Crick and Mitchison, 1983), the idea
being that the brain maintains a probabilistic model of the world and follows
the gradient of log pÌƒ while experiencing real events while awake and follows the
negative gradient of log pÌƒ to minimize log Z while sleeping and experiencing events
sampled from the current model. This view explains much of the language used to
describe algorithms with a positive and negative phase, but it has not been proven
to be correct with neuroscientiï¬?c experiments. In machine learning models, it is
usually necessary to use the positive and negative phase simultaneously, rather
than in separate time periods of wakefulness and REM sleep. As we will see in
section 19.5, other machine learning algorithms draw samples from the model
distribution for other purposes and such algorithms could also provide an account
for the function of dream sleep.
Given this understanding of the role of the positive and negative phase of
learning, we can attempt to design a less expensive alternative to algorithm 18.1.
The main cost of the naive MCMC algorithm is the cost of burning in the Markov
chains from a random initialization at each step. A natural solution is to initialize
the Markov chains from a distribution that is very close to the model distribution,
609

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

so that the burn in operation does not take as many steps.
The contrastive divergence (CD, or CD-k to indicate CD with k Gibbs steps)
algorithm initializes the Markov chain at each step with samples from the data
distribution (Hinton, 2000, 2010). This approach is presented as algorithm 18.2.
Obtaining samples from the data distribution is free, because they are already
available in the data set. Initially, the data distribution is not close to the model
distribution, so the negative phase is not very accurate. Fortunately, the positive
phase can still accurately increase the modelâ€™s probability of the data. After the
positive phase has had some time to act, the model distribution is closer to the
data distribution, and the negat
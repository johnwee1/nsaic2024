n hyperplane,
then the maximal margin classifier classifies the test observation x∗ based
on the sign of f (x∗ ) = β0 + β1 x∗1 + β2 x∗2 + · · · + βp x∗p .
Figure 9.3 shows the maximal margin hyperplane on the data set of
Figure 9.2. Comparing the right-hand panel of Figure 9.2 to Figure 9.3,
we see that the maximal margin hyperplane shown in Figure 9.3 does indeed result in a greater minimal distance between the observations and the
separating hyperplane—that is, a larger margin. In a sense, the maximal
margin hyperplane represents the mid-line of the widest “slab” that we can
insert between the two classes.
Examining Figure 9.3, we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines
indicating the width of the margin. These three observations are known as
support vectors, since they are vectors in p-dimensional space (in Figure 9.3, support
p = 2) and they “support” the maximal margin hyperplane in the sense vector
that if these points were moved slightly then the maximal margin hyperplane would move as well. Interestingly, the maximal margin hyperplane
depends directly on the support vectors, but not on the other observations:
a movement to any of the other observations would not affect the separating
hyperplane, provided that the observation’s movement does not cause it to

372

9. Support Vector Machines

cross the boundary set by the margin. The fact that the maximal margin
hyperplane depends directly on only a small subset of the observations is
an important property that will arise later in this chapter when we discuss
the support vector classifier and support vector machines.

9.1.4

Construction of the Maximal Margin Classifier

We now consider the task of constructing the maximal margin hyperplane
based on a set of n training observations x1 , . . . , xn ∈ Rp and associated
class labels y1 , . . . , yn ∈ {−1, 1}. Briefly, the maximal margin hyperplane
is the solution to the optimization problem
(9.9)

maximize M

β0 ,β1 ,...,βp ,M

subject to

p
0

βj2 = 1,

(9.10)

j=1

yi (β0 + β1 xi1 + β2 xi2 + · · · + βp xip ) ≥ M ∀ i = 1, . . . , n. (9.11)
This optimization problem (9.9)–(9.11) is actually simpler than it looks.
First of all, the constraint in (9.11) that
yi (β0 + β1 xi1 + β2 xi2 + · · · + βp xip ) ≥ M ∀ i = 1, . . . , n
guarantees that each observation will be on the correct side of the hyperplane, provided that M is positive. (Actually, for each observation to be
on the correct side of the hyperplane we would simply need yi (β0 + β1 xi1 +
β2 xi2 +· · ·+βp xip ) > 0, so the constraint in (9.11) in fact requires that each
observation be on the correct side of the hyperplane, with some cushion,
provided that M is positive.)
Second, note that (9.10) is not really a constraint on the hyperplane, since
if β0 + β1 xi1 + β2 xi2 + · · · + βp xip = 0 defines a hyperplane, then so does
k(β0 + β1 xi1 + β2 xi2 + · · · + βp xip ) = 0 for any k =
% 0. However, (9.10) adds
meaning to (9.11); one can show that with this constraint the perpendicular
distance from the ith observation to the hyperplane is given by
yi (β0 + β1 xi1 + β2 xi2 + · · · + βp xip ).
Therefore, the constraints (9.10) and (9.11) ensure that each observation
is on the correct side of the hyperplane and at least a distance M from the
hyperplane. Hence, M represents the margin of our hyperplane, and the
optimization problem chooses β0 , β1 , . . . , βp to maximize M . This is exactly
the definition of the maximal margin hyperplane! The problem (9.9)–(9.11)
can be solved efficiently, but details of this optimization are outside of the
scope of this book.

9.1.5

The Non-separable Case

The maximal margin classifier is a very natural way to perform classification, if a separating hyperplane exists. However, as we have hinted, in
many cases no separating hyperplane exists, and so there is no maximal

373

X2

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

9.2 Support Vector Classifiers

0

1

2

3

X1

FIGURE 9.4. There are two classes of observations, shown in blue and in
purple. In this case, the two classes are not separable by a hyperplane, and so the
maximal margin classifier cannot be used.

margin classifier. In this case, the optimization problem (9.9)–(9.11) has no
solution with M > 0. An example is shown in Figure 9.4. In this case, we
cannot exactly separate the two classes. However, as we will see in the next
section, we can extend the concept of a separating hyperplane in order to
develop a hyperplane that almost separates the classes, using a so-called
soft margin. The generalization of the maximal margin classifier to the
non-separable case is known as the support vector classifier.

9.2

Support Vector Classifiers

9.2.1

Overview of the Support Vector Classifier

In Figure 9.4, we see that observations that belong to two classes are not
necessarily separable by a hyperplane. In fact, even if a separating hyperplane does exist, then there are instances in which a classifier based on
a separating hyperplane might not be desirable. A classifier based on a
separating hyperplane will necessarily perfectly classify all of the training
observations; this can lead to sensitivity to individual observations. An example is shown in Figure 9.5. The addition of a single observation in the
right-hand panel of Figure 9.5 leads to a dramatic change in the maximal margin hyperplane. The resulting maximal margin hyperplane is not
satisfactory—for one thing, it has only a tiny margin. This is problematic
because as discussed previously, the distance of an observation from the
hyperplane can be seen as a measure of our confidence that the observation was correctly classified. Moreover, the fact that the maximal margin hyperplane is extremely sensitive to a change in a single observation
suggests that it may have overfit the training data.
In this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of

2
1

X2

−1

0

1
−1

0

X2

2

3

9. Support Vector Machines

3

374

−1

0

1

2

3

−1

0

X1

1

2

3

X1

FIGURE 9.5. Left: Two classes of observations are shown in blue and in
purple, along with the maximal margin hyperplane. Right: An additional blue
observation has been added, leading to a dramatic shift in the maximal margin
hyperplane shown as a solid line. The dashed line indicates the maximal margin
hyperplane that was obtained in the absence of this additional point.

• Greater robustness to individual observations, and
• Better classification of most of the training observations.
That is, it could be worthwhile to misclassify a few training observations
in order to do a better job in classifying the remaining observations.
The support vector classifier, sometimes called a soft margin classifier, support
does exactly this. Rather than seeking the largest possible margin so that vector
every observation is not only on the correct side of the hyperplane but classifier
also on the correct side of the margin, we instead allow some observations soft margin
to be on the incorrect side of the margin, or even the incorrect side of classifier
the hyperplane. (The margin is soft because it can be violated by some
of the training observations.) An example is shown in the left-hand panel
of Figure 9.6. Most of the observations are on the correct side of the margin.
However, a small subset of the observations are on the wrong side of the
margin.
An observation can be not only on the wrong side of the margin, but also
on the wrong side of the hyperplane. In fact, when there is no separating
hyperplane, such a situation is inevitable. Observations on the wrong side of
the hyperplane correspond to training observations that are misclassified by
the support vector classifier. The right-hand panel of Figure 9.6 illustrates
such a scenario.

9.2.2

Details of the Support Vector Classifier

The support vector classifier classifies a test observation depending on
which side of a hyperplane it lies. The hyperplane is chosen to correctly
separate most of the training observations into the two classes, but may

10

10

7
11

9

9

X2

2

8

1

1

X2

2

8

1
3

0

0

375

3

3

7

4

4

9.2 Support Vector Classifiers

5

−0.5

0.0

0.5

1.0

5

4

2
−1

−1

4
6

1.5

2.0

2.5

2

6
−0.5

X1

1

12

3

0.0

0.5

1.0

1.5

2.0

2.5

X1

FIGURE 9.6. Left: A support vector classifier was fit to a small data set. The
hyperplane is shown as a solid line and the margins are shown as dashed lines.
Purple observations: Observations 3, 4, 5, and 6 are on the correct side of the
margin, observation 2 is on the margin, and observation 1 is on the wrong side of
the margin. Blue observations: Observations 7 and 10 are on the correct side of
the margin, observation 9 is on the margin, and observation 8 is on the wrong side
of the margin. No observations are on the wrong side of the hyperplane. Right:
Same as left panel with two additional points, 11 and 12. These two observations
are on the wrong side of the hyperplane and the wrong side of the margin.

misclassify a few observations. It is the solution to the optimization problem
maximize

M

(9.12)

βj2 = 1,

(9.13)

β0 ,β1 ,...,βp ,&1 ,...,&n , M

subject to

p
0
j=1

yi (β0 + β1 xi1 + β2 xi2 + · · · + βp xip ) ≥ M (1 − "i ),
n
0
"i ≥ 0,
"i ≤ C,

(9.14)
(9.15)

i=1

where C is a nonnegative tuning parameter. As in (9.11), M is the width
of the margin; we seek to make this quantity as large as possible. In (9.14),
"1 , . . . , "n are slack variables that allow individual observations to be on
slack
the wrong side of the margin or the hyperplane; we will explain them in variable
greater detail momentarily. Once we have solved (9.12)–(9.15), we classify
a test observation x∗ as before, by simply determining on which side of the
hyperplane it lies. That is, we classify the test observation based on the
sign of f (x∗ ) = β0 + β1 x∗1 + · · · + βp x∗p .
The problem (9.12)–(9.15) seems complex, but insight into its behavior
can be made through a series of simple observations presented below. First
of all, the slack variable "i tells us where the ith observation is located,
relative to the hyperplane and relative to the margin. If "i = 0 then the ith
observation is on the correct side of the margin, as we saw in Section 9.1.4.
If "i > 0 then the ith observation is on the wrong side of the margin, and
we say that the ith observation has violated the margin. If "i > 1 then it is
on the wrong side of the hyperplane.

376

9. Support Vector Machines

We now consider the role of the tuning parameter C. In (9.15), C bounds
the sum of the "i ’s, and so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. We can
think of C as a budget for the amount that the margin can be violated
by the n observations. If C = 0 then there is no budget for violations to
the margin, and it must be the case that "1 = · · · = "n = 0, in which case
(9.12)–(9.15) simply amounts to the maximal margin hyperplane optimization problem (9.9)–(9.11). (Of course, a maximal margin hyperplane exists
only if the two classes are separable.) For C > 0 no more than C observations can be on the wrong side of the hyperplane, because if an observation
is on)
the wrong side of the hyperplane then "i > 1, and (9.15) requires
n
that i=1 "i ≤ C. As the budget C increases, we become more tolerant of
violations to the margin, and so the margin will widen. Conversely, as C
decreases, we become less tolerant of violations to the margin and so the
margin narrows. An example is shown in Figure 9.7.
In practice, C is treated as a tuning parameter that is generally chosen via
cross-validation. As with the tuning parameters that we have seen throughout this book, C controls the bias-variance trade-off of the statistical learning technique. When C is small, we seek narrow margins that are rarely
violated; this amounts to a classifier that is highly fit to the data, which
may have low bias but high variance. On the other hand, when C is larger,
the margin is wider and we allow more violations to it; this amounts to
fitting the data less hard and obtaining a classifier that is potentially more
biased but may have lower variance.
The optimization problem (9.12)–(9.15) has a very interesting property:
it turns out that only observations that either lie on the margin or that
violate the margin will affect the hyperplane, and hence the classifier obtained. In other words, an observation that lies strictly on the correct side
of the margin does not affect the support vector classifier! Changing the
position of that observation would not change the classifier at all, provided
that its position remains on the correct side of the margin. Observations
that lie directly on the margin, or on the wrong side of the margin for
their class, are known as support vectors. These observations do affect the
support vector classifier.
The fact that only support vectors affect the classifier is in line with our
previous assertion that C controls the bias-variance trade-off of the support
vector classifier. When the tuning parameter C is large, then the margin is
wide, many observations violate the margin, and so there are many support
vectors. In this case, many observations are involved in determining the
hyperplane. The top left panel in Figure 9.7 illustrates this setting: this
classifier has low variance (since many observations are support vectors)
but potentially high bias. In contrast, if C is small, then there will be fewer
support vectors and hence the resulting classifier will have low bias but
high variance. The bottom right panel in Figure 9.7 illustrates this setting,
with only eight support vectors.
The fact that the support vector classifier’s decision rule is based only
on a potentially small subset of the training observations (the support vectors) means that it is quite robust to the behavior of observations that
are far away from the hyperplane. This property is distinct from some of

3
2
1
0

X2

−2
−3
−1

0

1

2

−1

0

1

2

1

2

2
1
0
−1
−2
−3

−3

−2

−1

0

X2

1

2

3

X1

3

X1

X2

377

−1

0
−3

−2

−1

X2

1

2

3

9.3 Support Vector Machines

−1

0

1

2

X1

−1

0

X1

FIGURE 9.7. A support vector classifier was fit using four different values
of the tuning parameter C in (9.12)–(9.15). The largest value of C was used
in the top left panel, and smaller values were used in the top right, bottom left,
and bottom right panels. When C is large, then there is a high tolerance for
observations being on the wrong side of the margin, and so the margin will be
large. As C decreases, the tolerance for observations being on the wrong side of
the margin decreases, and the margin narrows.

the other classification methods that we have seen in preceding chapters,
such as linear discriminant analysis. Recall that the LDA classification rule
depends on the mean of all of the observations within each class, as well as
the within-class covariance matrix computed using all of the observations.
In contrast, logistic regression, unlike LDA, has very low sensitivity to observations far from the decision boundary. In fact we will see in Section 9.5
that the support vector classifier and logistic regression are closely related.

9.3

Support Vector Machines

We first discuss a general mechanism for converting a linear classifier into
one that produces non-linear decision boundaries. We then int
features.

The output layer of the original model should usually be replaced because it is most
likely not useful at all for the new task, and it may not even have the right number of
outputs for the new task.

Similarly, the upper hidden layers of the original model are less likely to be as useful
as the lower layers, since the high-level features that are most useful for the new task
may differ significantly from the ones that were most useful for the original task. You
want to find the right number of layers to reuse.

346 

| 

Chapter 11: Training Deep Neural Networks

The more similar the tasks are, the more layers you want to reuse
(starting with the lower layers). For very similar tasks, try keeping
all the hidden layers and just replacing the output layer.

Try freezing all the reused layers first (i.e., make their weights non-trainable so that
Gradient  Descent  won’t  modify  them),  then  train  your  model  and  see  how  it  per‐
forms. Then try unfreezing one or two of the top hidden layers to let backpropaga‐
tion tweak them and see if performance improves. The more training data you have,
the  more  layers  you  can  unfreeze.  It  is  also  useful  to  reduce  the  learning  rate  when
you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.

If you still cannot get good performance, and you have little training data, try drop‐
ping the top hidden layer(s) and freezing all the remaining hidden layers again. You
can  iterate  until  you  find  the  right  number  of  layers  to  reuse.  If  you  have  plenty  of
training data, you may try replacing the top hidden layers instead of dropping them,
and even adding more hidden layers.

Transfer Learning with Keras
Let’s  look  at  an  example.  Suppose  the  Fashion  MNIST  dataset  only  contained  eight
classes—for  example,  all  the  classes  except  for  sandal  and  shirt.  Someone  built  and
trained a Keras model on that set and got reasonably good performance (>90% accu‐
racy). Let’s call this model A. You now want to tackle a different task: you have images
of  sandals  and  shirts,  and  you  want  to  train  a  binary  classifier  (positive=shirt,
negative=sandal).  Your  dataset  is  quite  small;  you  only  have  200  labeled  images.
When you train a new model for this task (let’s call it model B) with the same archi‐
tecture  as  model  A,  it  performs  reasonably  well  (97.2%  accuracy).  But  since  it’s  a
much easier task (there are just two classes), you were hoping for more. While drink‐
ing your morning coffee, you realize that your task is quite similar to task A, so per‐
haps transfer learning can help? Let’s find out!

First, you need to load model A and create a new model based on that model’s layers.
Let’s reuse all the layers except for the output layer:

model_A = keras.models.load_model("my_model_A.h5")
model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))

Note  that  model_A  and  model_B_on_A  now  share  some  layers.  When  you  train
model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone
model_A before you reuse its layers. To do this, you clone model A’s architecture with
clone_model(),  then  copy  its  weights  (since  clone_model()  does  not  clone  the
weights):

Reusing Pretrained Layers 

| 

347

model_A_clone = keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())

Now you could train model_B_on_A for task B, but since the new output layer was ini‐
tialized  randomly  it  will  make  large  errors  (at  least  during  the  first  few  epochs),  so
there will be large error gradients that may wreck the reused weights. To avoid this,
one approach is to freeze the reused layers during the first few epochs, giving the new
layer  some  time  to  learn  reasonable  weights.  To  do  this,  set  every  layer’s  trainable
attribute to False and compile the model:

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

model_B_on_A.compile(loss="binary_crossentropy", optimizer="sgd",
                     metrics=["accuracy"])

You must always compile your model after you freeze or unfreeze
layers.

Now you can train the model for a few epochs, then unfreeze the reused layers (which
requires  compiling  the  model  again)  and  continue  training  to  fine-tune  the  reused
layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce
the learning rate, once again to avoid damaging the reused weights:

history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                           validation_data=(X_valid_B, y_valid_B))

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                           validation_data=(X_valid_B, y_valid_B))

So, what’s the final verdict? Well, this model’s test accuracy is 99.25%, which means
that transfer learning reduced the error rate from 2.8% down to almost 0.7%! That’s a
factor of four!

>>> model_B_on_A.evaluate(X_test_B, y_test_B)
[0.06887910133600235, 0.9925]

Are you convinced? You shouldn’t be: I cheated! I tried many configurations until I
found one that demonstrated a strong improvement. If you try to change the classes
or the random seed, you will see that the improvement generally drops, or even van‐
ishes or reverses. What I did is called “torturing the data until it confesses.” When a

348 

| 

Chapter 11: Training Deep Neural Networks

paper just looks too positive, you should be suspicious: perhaps the flashy new tech‐
nique does not actually help much (in fact, it may even degrade performance), but the
authors tried many variants and reported only the best results (which may be due to
sheer  luck),  without  mentioning  how  many  failures  they  encountered  on  the  way.
Most  of  the  time,  this  is  not  malicious  at  all,  but  it  is  part  of  the  reason  so  many
results in science can never be reproduced.

Why  did  I  cheat?  It  turns  out  that  transfer  learning  does  not  work  very  well  with
small  dense  networks,  presumably  because  small  networks  learn  few  patterns,  and
dense networks learn very specific patterns, which are unlikely to be useful in other
tasks. Transfer learning works best with deep convolutional neural networks, which
tend  to  learn  feature  detectors  that  are  much  more  general  (especially  in  the  lower
layers). We will revisit transfer learning in Chapter 14, using the techniques we just
discussed (and this time there will be no cheating, I promise!).

Unsupervised Pretraining
Suppose  you  want  to  tackle  a  complex  task  for  which  you  don’t  have  much  labeled
training  data,  but  unfortunately  you  cannot  find  a  model  trained  on  a  similar  task.
Don’t lose hope! First, you should try to gather more labeled training data, but if you
can’t,  you  may  still  be  able  to  perform  unsupervised  pretraining  (see  Figure  11-5).
Indeed, it is often cheap to gather unlabeled training examples, but expensive to label
them. If you can gather plenty of unlabeled training data, you can try to use it to train
an unsupervised model, such as an autoencoder or a generative adversarial network
(see Chapter 17). Then you can reuse the lower layers of the autoencoder or the lower
layers of the GAN’s discriminator, add the output layer for your task on top, and fine-
tune  the  final  network  using  supervised  learning  (i.e.,  with  the  labeled  training
examples).

It is this technique that Geoffrey Hinton and his team used in 2006 and which led to
the revival of neural networks and the success of Deep Learning. Until 2010, unsuper‐
vised pretraining—typically with restricted Boltzmann machines (RBMs; see Appen‐
dix E)—was the norm for deep nets, and only after the vanishing gradients problem
was alleviated did it become much more common to train DNNs purely using super‐
vised  learning.  Unsupervised  pretraining  (today  typically  using  autoencoders  or
GANs  rather  than  RBMs)  is  still  a  good  option  when  you  have  a  complex  task  to
solve,  no  similar  model  you  can  reuse,  and  little  labeled  training  data  but  plenty  of
unlabeled training data.

Note that in the early days of Deep Learning it was difficult to train deep models, so
people  would  use  a  technique  called  greedy  layer-wise  pretraining  (depicted  in
Figure 11-5). They would first train an unsupervised model with a single layer, typi‐
cally  an  RBM,  then  they  would  freeze  that  layer  and  add  another  one  on  top  of  it,
then  train  the  model  again  (effectively  just  training  the  new  layer),  then  freeze  the

Reusing Pretrained Layers 

| 

349

new layer and add another layer on top of it, train the model again, and so on. Nowa‐
days, things are much simpler: people generally train the full unsupervised model in
one shot (i.e., in Figure 11-5, just start directly at step three) and use autoencoders or
GANs rather than RBMs.

Figure 11-5. In unsupervised training, a model is trained on the unlabeled data (or on
all the data) using an unsupervised learning technique, then it is fine-tuned for the final
task on the labeled data using a supervised learning technique; the unsupervised part
may train one layer at a time as shown here, or it may train the full model directly

Pretraining on an Auxiliary Task
If you do not have much labeled training data, one last option is to train a first neural
network  on  an  auxiliary  task  for  which  you  can  easily  obtain  or  generate  labeled
training  data,  then  reuse  the  lower  layers  of  that  network  for  your  actual  task.  The
first neural network’s lower layers will learn feature detectors that will likely be reusa‐
ble by the second neural network.

For example, if you want to build a system to recognize faces, you may only have a
few pictures of each individual—clearly not enough to train a good classifier. Gather‐
ing hundreds of pictures of each person would not be practical. You could, however,
gather a lot of pictures of random people on the web and train a first neural network
to  detect  whether  or  not  two  different  pictures  feature  the  same  person.  Such  a

350 

| 

Chapter 11: Training Deep Neural Networks

network  would  learn  good  feature  detectors  for  faces,  so  reusing  its  lower  layers
would allow you to train a good face classifier that uses little training data.

For  natural  language  processing  (NLP)  applications,  you  can  download  a  corpus  of
millions of text documents and automatically generate labeled data from it. For exam‐
ple, you could randomly mask out some words and train a model to predict what the
missing words are (e.g., it should predict that the missing word in the sentence “What
___ you saying?” is probably “are” or “were”). If you can train a model to reach good
performance  on  this  task,  then  it  will  already  know  quite  a  lot  about  language,  and
you  can  certainly  reuse  it  for  your  actual  task  and  fine-tune  it  on  your  labeled  data
(we will discuss more pretraining tasks in Chapter 15).

Self-supervised  learning  is  when  you  automatically  generate  the
labels from the data itself, then you train a model on the resulting
“labeled”  dataset  using  supervised  learning  techniques.  Since  this
approach  requires  no  human  labeling  whatsoever,  it  is  best  classi‐
fied as a form of unsupervised learning.

Faster Optimizers
Training a very large deep neural network can be painfully slow. So far we have seen
four ways to speed up training (and reach a better solution): applying a good initiali‐
zation  strategy  for  the  connection  weights,  using  a  good  activation  function,  using
Batch Normalization, and reusing parts of a pretrained network (possibly built on an
auxiliary task or using unsupervised learning). Another huge speed boost comes from
using a faster optimizer than the regular Gradient Descent optimizer. In this section
we  will  present  the  most  popular  algorithms:  momentum  optimization,  Nesterov
finally  Adam  and  Nadam
Accelerated  Gradient,  AdaGrad,  RMSProp,  and 
optimization.

Momentum Optimization
Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start
out slowly, but it will quickly pick up momentum until it eventually reaches terminal
velocity (if there is some friction or air resistance). This is the very simple idea behind
momentum  optimization,  proposed  by  Boris  Polyak  in  1964.13  In  contrast,  regular
Gradient  Descent  will  simply  take  small,  regular  steps  down  the  slope,  so  the  algo‐
rithm will take much more time to reach the bottom.

13 Boris T. Polyak, “Some Methods of Speeding Up the Convergence of Iteration Methods,” USSR Computational

Mathematics and Mathematical Physics 4, no. 5 (1964): 1–17.

Faster Optimizers 

| 

351

Recall that Gradient Descent updates the weights θ by directly subtracting the gradi‐
ent  of  the  cost  function  J(θ)  with  regard  to  the  weights  (∇θJ(θ))  multiplied  by  the
learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the ear‐
lier gradients were. If the local gradient is tiny, it goes very slowly.

Momentum  optimization  cares  a  great  deal  about  what  previous  gradients  were:  at
each  iteration,  it  subtracts  the  local  gradient  from  the  momentum  vector  m  (multi‐
plied  by  the  learning  rate  η),  and  it  updates  the  weights  by  adding  this  momentum
vector (see Equation 11-4). In other words, the gradient is used for acceleration, not
for speed. To simulate some sort of friction mechanism and prevent the momentum
from growing too large, the algorithm introduces a new hyperparameter β, called the
momentum, which must be set between 0 (high friction) and 1 (no friction). A typical
momentum value is 0.9.

Equation 11-4. Momentum algorithm
1 . m βm − η∇θJ θ
θ + m
2 .

θ

You can easily verify that if the gradient remains constant, the terminal velocity (i.e.,
the maximum size of the weight updates) is equal to that gradient multiplied by the
learning rate η multiplied by 1/(1–β) (ignoring the sign). For example, if β = 0.9, then
the  terminal  velocity  is  equal  to  10  times  the  gradient  times  the  learning  rate,  so
momentum optimization ends up going 10 times faster than Gradient Descent! This
allows  momentum  optimization  to  escape  from  plateaus  much  faster  than  Gradient
Descent. We saw in Chapter 4 that when the inputs have very different scales, the cost
function  will  look  like  an  elongated  bowl  (see  Figure  4-7).  Gradient  Descent  goes
down the steep slope quite fast, but then it takes a very long time to go down the val‐
ley.  In  contrast,  momentum  optimization  will  roll  down  the  valley  faster  and  faster
until  it  reaches  the  bottom  (the  optimum).  In  deep  neural  networks  that  don’t  use
Batch Normalization, the upper layers will often end up having inputs with very dif‐
ferent scales, so using momentum optimization helps a lot. It can also help roll past
local optima.

Due  to  the  momentum,  the  optimizer  may  overshoot  a  bit,  then
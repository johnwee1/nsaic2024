g
tf.io.parse_tensor().

Instead of parsing examples one by one using tf.io.parse_single_example(), you
may want to parse them batch by batch using tf.io.parse_example():

dataset = tf.data.TFRecordDataset(["my_contacts.tfrecord"]).batch(10)
for serialized_examples in dataset:
    parsed_examples = tf.io.parse_example(serialized_examples,
                                          feature_description)

As you can see, the Example protobuf will probably be sufficient for most use cases.
However, it may be a bit cumbersome to use when you are dealing with lists of lists.
For  example,  suppose  you  want  to  classify  text  documents.  Each  document  may  be
represented  as  a  list  of  sentences,  where  each  sentence  is  represented  as  a  list  of
words.  And  perhaps  each  document  also  has  a  list  of  comments,  where  each  com‐
ment is represented as a list of words. There may be some contextual data too, such as
the  document’s  author,  title,  and  publication  date.  TensorFlow’s  SequenceExample
protobuf is designed for such use cases.

Handling Lists of Lists Using the SequenceExample Protobuf
Here is the definition of the SequenceExample protobuf:

message FeatureList { repeated Feature feature = 1; };
message FeatureLists { map<string, FeatureList> feature_list = 1; };
message SequenceExample {
    Features context = 1;
    FeatureLists feature_lists = 2;
};

The TFRecord Format 

| 

429

A  SequenceExample  contains  a  Features  object  for  the  contextual  data  and  a  Fea
tureLists object that contains one or more named FeatureList objects (e.g., a Fea
tureList  named  "content"  and  another  named  "comments").  Each  FeatureList
contains a list of Feature objects, each of which may be a list of byte strings, a list of
64-bit  integers,  or  a  list  of  floats  (in  this  example,  each  Feature  would  represent  a
sentence or a comment, perhaps in the form of a list of word identifiers). Building a
SequenceExample, serializing it, and parsing it is similar to building, serializing, and
parsing an Example, but you must use tf.io.parse_single_sequence_example() to
parse  a  single  SequenceExample  or  tf.io.parse_sequence_example()  to  parse  a
batch. Both functions return a tuple containing the context features (as a dictionary)
and  the  feature  lists  (also  as  a  dictionary).  If  the  feature  lists  contain  sequences  of
varying sizes (as in the preceding example), you may want to convert them to ragged
tensors, using tf.RaggedTensor.from_sparse() (see the notebook for the full code):

parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(
    serialized_sequence_example, context_feature_descriptions,
    sequence_feature_descriptions)
parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists["content"])

Now that you know how to efficiently store, load, and parse data, the next step is to
prepare it so that it can be fed to a neural network.

Preprocessing the Input Features
Preparing your data for a neural network requires converting all features into numer‐
ical features, generally normalizing them, and more. In particular, if your data con‐
tains categorical features or text features, they need to be converted to numbers. This
can  be  done  ahead  of  time  when  preparing  your  data  files,  using  any  tool  you  like
(e.g., NumPy, pandas, or Scikit-Learn). Alternatively, you can preprocess your data on
the fly when loading it with the Data API (e.g., using the dataset’s map() method, as
we saw earlier), or you can include a preprocessing layer directly in your model. Let’s
look at this last option now.

For example, here is how you can implement a standardization layer using a Lambda
layer.  For  each  feature,  it  subtracts  the  mean  and  divides  by  its  standard  deviation
(plus a tiny smoothing term to avoid division by zero):

means = np.mean(X_train, axis=0, keepdims=True)
stds = np.std(X_train, axis=0, keepdims=True)
eps = keras.backend.epsilon()
model = keras.models.Sequential([
    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),
    [...] # other layers
])

430 

| 

Chapter 13: Loading and Preprocessing Data with TensorFlow

That’s  not  too  hard!  However,  you  may  prefer  to  use  a  nice  self-contained  custom
layer (much like Scikit-Learn’s StandardScaler), rather than having global variables
like means and stds dangling around:

class Standardization(keras.layers.Layer):
    def adapt(self, data_sample):
        self.means_ = np.mean(data_sample, axis=0, keepdims=True)
        self.stds_ = np.std(data_sample, axis=0, keepdims=True)
    def call(self, inputs):
        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())

Before you can use this standardization layer, you will need to adapt it to your dataset
by calling the adapt() method and passing it a data sample. This will allow it to use
the appropriate mean and standard deviation for each feature:

std_layer = Standardization()
std_layer.adapt(data_sample)

This sample must be large enough to be representative of your dataset, but it does not
have to be the full training set: in general, a few hundred randomly selected instances
will suffice (however, this depends on your task). Next, you can use this preprocess‐
ing layer like a normal layer:

model = keras.Sequential()
model.add(std_layer)
[...] # create the rest of the model
model.compile([...])
model.fit([...])

If  you  are  thinking  that  Keras  should  contain  a  standardization  layer  like  this  one,
here’s some good news for you: by the time you read this, the keras.layers.Normal
ization  layer  will  probably  be  available.  It  will  work  very  much  like  our  custom
Standardization layer: first, create the layer, then adapt it to your dataset by passing
a data sample to the adapt() method, and finally use the layer normally.

Now  let’s  look  at  categorical  features.  We  will  start  by  encoding  them  as  one-hot
vectors.

Encoding Categorical Features Using One-Hot Vectors
Consider the ocean_proximity feature in the California housing dataset we explored
in  Chapter  2:  it  is  a  categorical  feature  with  five  possible  values:  "<1H  OCEAN",
"INLAND", "NEAR OCEAN", "NEAR BAY", and "ISLAND". We need to encode this feature
before we feed it to a neural network. Since there are very few categories, we can use
one-hot encoding. For this, we first need to map each category to its index (0 to 4),
which can be done using a lookup table:

vocab = ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
indices = tf.range(len(vocab), dtype=tf.int64)

Preprocessing the Input Features 

| 

431

table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)
num_oov_buckets = 2
table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)

Let’s go through this code:

• We first define the vocabulary: this is the list of all possible categories.

• Then we create a tensor with the corresponding indices (0 to 4).

• Next, we create an initializer for the lookup table, passing it the list of categories
and their corresponding indices. In this example, we already have this data, so we
use a KeyValueTensorInitializer; but if the categories were listed in a text file
(with one category per line), we would use a TextFileInitializer instead.

• In the last two lines we create the lookup table, giving it the initializer and speci‐
fying  the  number  of  out-of-vocabulary  (oov)  buckets.  If  we  look  up  a  category
that does not exist in the vocabulary, the lookup table will compute a hash of this
category  and  use  it  to  assign  the  unknown  category  to  one  of  the  oov  buckets.
Their indices start after the known categories, so in this example the indices of
the two oov buckets are 5 and 6.

Why use oov buckets? Well, if the number of categories is large (e.g., zip codes, cities,
words, products, or users) and the dataset is large as well, or it keeps changing, then
getting the full list of categories may not be convenient. One solution is to define the
vocabulary based on a data sample (rather than the whole training set) and add some
oov  buckets  for  the  other  categories  that  were  not  in  the  data  sample.  The  more
unknown  categories  you  expect  to  find  during  training,  the  more  oov  buckets  you
should use. Indeed, if there are not enough oov buckets, there will be collisions: dif‐
ferent  categories  will  end  up  in  the  same  bucket,  so  the  neural  network  will  not  be
able to distinguish them (at least not based on this feature).

Now let’s use the lookup table to encode a small batch of categorical features to one-
hot vectors:

>>> categories = tf.constant(["NEAR BAY", "DESERT", "INLAND", "INLAND"])
>>> cat_indices = table.lookup(categories)
>>> cat_indices
<tf.Tensor: id=514, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>
>>> cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)
>>> cat_one_hot
<tf.Tensor: id=524, shape=(4, 7), dtype=float32, numpy=
array([[0., 0., 0., 1., 0., 0., 0.],
       [0., 0., 0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>

As you can see, "NEAR BAY" was mapped to index 3, the unknown category "DESERT"
was mapped to one of the two oov buckets (at index 5), and "INLAND" was mapped to

432 

| 

Chapter 13: Loading and Preprocessing Data with TensorFlow

index 1, twice. Then we used tf.one_hot() to one-hot encode these indices. Notice
that  we  have  to  tell  this  function  the  total  number  of  indices,  which  is  equal  to  the
vocabulary size plus the number of oov buckets. Now you know how to encode cate‐
gorical features to one-hot vectors using TensorFlow!

Just like earlier, it wouldn’t be too difficult to bundle all of this logic into a nice self-
contained class. Its adapt() method would take a data sample and extract all the dis‐
tinct categories it contains. It would create a lookup table to map each category to its
index  (including  unknown  categories  using  oov  buckets).  Then  its  call()  method
would use the lookup table to map the input categories to their indices. Well, here’s
more good news: by the time you read this, Keras will probably include a layer called
keras.layers.TextVectorization,  which  will  be  capable  of  doing  exactly  that:  its
adapt()  method  will  extract  the  vocabulary  from  a  data  sample,  and  its  call()
method will convert each category to its index in the vocabulary. You could add this
layer at the beginning of your model, followed by a Lambda layer that would apply the
tf.one_hot() function, if you want to convert these indices to one-hot vectors.

This  may  not  be  the  best  solution,  though.  The  size  of  each  one-hot  vector  is  the
vocabulary length plus the number of oov buckets. This is fine when there are just a
few  possible  categories,  but  if  the  vocabulary  is  large,  it  is  much  more  efficient  to
encode them using embeddings instead.

As  a  rule  of  thumb,  if  the  number  of  categories  is  lower  than  10,
then one-hot encoding is generally the way to go (but your mileage
may vary!). If the number of categories is greater than 50 (which is
often  the  case  when  you  use  hash  buckets),  then  embeddings  are
usually preferable. In between 10 and 50 categories, you may want
to experiment with both options and see which one works best for
your use case.

Encoding Categorical Features Using Embeddings
An  embedding  is  a  trainable  dense  vector  that  represents  a  category.  By  default,
embeddings are initialized randomly, so for example the "NEAR BAY" category could
be represented initially by a random vector such as [0.131, 0.890], while the "NEAR
OCEAN"  category  might  be  represented  by  another  random  vector  such  as  [0.631,
0.791]. In this example, we use 2D embeddings, but the number of dimensions is a
hyperparameter you can tweak. Since these embeddings are trainable, they will grad‐
ually improve during training; and as they represent fairly similar categories, Gradi‐
ent Descent will certainly end up pushing them closer together, while it will tend to
move them away from the "INLAND" category’s embedding (see Figure 13-4). Indeed,
the  better  the  representation,  the  easier  it  will  be  for  the  neural  network  to  make
accurate predictions, so training tends to make embeddings useful representations of

Preprocessing the Input Features 

| 

433

the categories. This is called representation learning (we will see other types of repre‐
sentation learning in Chapter 17).

Figure 13-4. Embeddings will gradually improve during training

Word Embeddings
Not only will embeddings generally be useful representations for the task at hand, but
quite  often  these  same  embeddings  can  be  reused  successfully  for  other  tasks.  The
most  common  example  of  this  is  word  embeddings  (i.e.,  embeddings  of  individual
words): when you are working on a natural language processing task, you are often
better off reusing pretrained word embeddings than training your own.

The  idea  of  using  vectors  to  represent  words  dates  back  to  the  1960s,  and  many
sophisticated  techniques  have  been  used  to  generate  useful  vectors,  including  using
neural networks. But things really took off in 2013, when Tomáš Mikolov and other
Google researchers published a paper9 describing an efficient technique to learn word
embeddings  using  neural  networks,  significantly  outperforming  previous  attempts.
This allowed them to learn embeddings on a very large corpus of text: they trained a
neural network to predict the words near any given word, and obtained astounding
word embeddings. For example, synonyms had very close embeddings, and semanti‐
cally related words such as France, Spain, and Italy ended up clustered together.

It’s  not  just  about  proximity,  though:  word  embeddings  were  also  organized  along
meaningful axes in the embedding space. Here is a famous example: if you compute
King  –  Man  +  Woman  (adding  and  subtracting  the  embedding  vectors  of  these
words), then the result will be very close to the embedding of the word Queen (see
Figure  13-5).  In  other  words,  the  word  embeddings  encode  the  concept  of  gender!

9 Tomas Mikolov et al., “Distributed Representations of Words and Phrases and Their Compositionality,” Pro‐
ceedings of the 26th International Conference on Neural Information Processing Systems 2 (2013): 3111–3119.

434 

| 

Chapter 13: Loading and Preprocessing Data with TensorFlow

Similarly, you can compute Madrid – Spain + France, and the result is close to Paris,
which  seems  to  show  that  the  notion  of  capital  city  was  also  encoded  in  the
embeddings.

Figure 13-5. Word embeddings of similar words tend to be close, and some axes seem to
encode meaningful concepts

Unfortunately,  word  embeddings  sometimes  capture  our  worst  biases.  For  example,
although  they  correctly  learn  that  Man  is  to  King  as  Woman  is  to  Queen,  they  also
seem to learn that Man is to Doctor as Woman is to Nurse: quite a sexist bias! To be
fair,  this  particular  example  is  probably  exaggerated,  as  was  pointed  out  in  a  2019
paper10  by  Malvina  Nissim  et  al.  Nevertheless,  ensuring  fairness  in  Deep  Learning
algorithms is an important and active research topic.

Let’s look at 
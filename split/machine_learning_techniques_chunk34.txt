n  the  ith  input  neuron  and  the  jth  output

neuron.

• xi is the ith input value of the current training instance.
• y j is the output of the jth output neuron for the current training instance.
• yj is the target output of the jth output neuron for the current training instance.
• η is the learning rate.

The decision boundary of each output neuron is linear, so Perceptrons are incapable
of learning complex patterns (just like Logistic Regression classifiers). However, if the
training instances are linearly separable, Rosenblatt demonstrated that this algorithm
would converge to a solution.8 This is called the Perceptron convergence theorem.

Scikit-Learn  provides  a  Perceptron  class  that  implements  a  single-TLU  network.  It
can be used pretty much as you would expect—for example, on the iris dataset (intro‐
duced in Chapter 4):

import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:, (2, 3)]  # petal length, petal width
y = (iris.target == 0).astype(np.int)  # Iris setosa?

per_clf = Perceptron()
per_clf.fit(X, y)

y_pred = per_clf.predict([[2, 0.5]])

8 Note that this solution is not unique: when data points are linearly separable, there is an infinity of hyper‐

planes that can separate them.

From Biological to Artificial Neurons 

| 

287

You may have noticed that the Perceptron learning algorithm strongly resembles Sto‐
chastic  Gradient  Descent.  In  fact,  Scikit-Learn’s  Perceptron  class  is  equivalent  to
using  an  SGDClassifier  with  the  following  hyperparameters:  loss="perceptron",
learning_rate="constant",  eta0=1  (the  learning  rate),  and  penalty=None  (no
regularization).

Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class
probability; rather, they make predictions based on a hard threshold. This is one rea‐
son to prefer Logistic Regression over Perceptrons.

In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert highligh‐
ted a number of serious weaknesses of Perceptrons—in particular, the fact that they
are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR) classifi‐
cation problem; see the left side of Figure 10-6). This is true of any other linear classi‐
fication model (such as Logistic Regression classifiers), but researchers had expected
much  more  from  Perceptrons,  and  some  were  so  disappointed  that  they  dropped
neural networks altogether in favor of higher-level problems such as logic, problem
solving, and search.

It turns out that some of the limitations of Perceptrons can be eliminated by stacking
multiple Perceptrons. The resulting ANN is called a Multilayer Perceptron (MLP). An
MLP can solve the XOR problem, as you can verify by computing the output of the
MLP represented on the right side of Figure 10-6: with inputs (0, 0) or (1, 1), the net‐
work outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. All connections have a
weight equal to 1, except the four connections where the weight is shown. Try verify‐
ing that this network indeed solves the XOR problem!

Figure 10-6. XOR classification problem and an MLP that solves it

288 

| 

Chapter 10: Introduction to Artificial Neural Networks with Keras

The Multilayer Perceptron and Backpropagation
An MLP is composed of one (passthrough) input layer, one or more layers of TLUs,
called  hidden  layers,  and  one  final  layer  of  TLUs  called  the  output  layer  (see
Figure 10-7). The layers close to the input layer are usually called the lower layers, and
the ones close to the outputs are usually called the upper layers. Every layer except the
output layer includes a bias neuron and is fully connected to the next layer.

Figure 10-7. Architecture of a Multilayer Perceptron with two inputs, one hidden layer of
four neurons, and three output neurons (the bias neurons are shown here, but usually
they are implicit)

The signal flows only in one direction (from the inputs to the out‐
puts), so this architecture is an example of a feedforward neural net‐
work (FNN).

When an ANN contains a deep stack of hidden layers,9 it is called a deep neural net‐
work (DNN). The field of Deep Learning studies DNNs, and more generally models
containing  deep  stacks  of  computations.  Even  so,  many  people  talk  about  Deep
Learning whenever neural networks are involved (even shallow ones).

For  many  years  researchers  struggled  to  find  a  way  to  train  MLPs,  without  success.
But  in  1986,  David  Rumelhart,  Geoffrey  Hinton,  and  Ronald  Williams  published  a

9 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see

ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.

From Biological to Artificial Neurons 

| 

289

groundbreaking  paper10  that  introduced  the  backpropagation  training  algorithm,
which  is  still  used  today.  In  short,  it  is  Gradient  Descent  (introduced  in  Chapter  4)
using an efficient technique for computing the gradients automatically:11 in just two
passes through the network (one forward, one backward), the backpropagation algo‐
rithm is able to compute the gradient of the network’s error with regard to every sin‐
gle model parameter. In other words, it can find out how each connection weight and
each bias term should be tweaked in order to reduce the error. Once it has these gra‐
dients,  it  just  performs  a  regular  Gradient  Descent  step,  and  the  whole  process  is
repeated until the network converges to the solution.

Automatically  computing  gradients  is  called  automatic  differentia‐
tion, or autodiff. There are various autodiff techniques, with differ‐
ent  pros  and  cons.  The  one  used  by  backpropagation  is  called
reverse-mode autodiff. It is fast and precise, and is well suited when
the  function  to  differentiate  has  many  variables  (e.g.,  connection
weights) and few outputs (e.g., one loss). If you want to learn more
about autodiff, check out Appendix D.

Let’s run through this algorithm in a bit more detail:

• It handles one mini-batch at a time (for example, containing 32 instances each),
and  it  goes  through  the  full  training  set  multiple  times.  Each  pass  is  called  an
epoch.

• Each mini-batch is passed to the network’s input layer, which sends it to the first
hidden layer. The algorithm then computes the output of all the neurons in this
layer (for every instance in the mini-batch). The result is passed on to the next
layer, its output is computed and passed to the next layer, and so on until we get
the output of the last layer, the output layer. This is the forward pass: it is exactly
like making predictions, except all intermediate results are preserved since they
are needed for the backward pass.

• Next, the algorithm measures the network’s output error (i.e., it uses a loss func‐
tion that compares the desired output and the actual output of the network, and
returns some measure of the error).

• Then  it  computes  how  much  each  output  connection  contributed  to  the  error.
This is done analytically by applying the chain rule (perhaps the most fundamen‐
tal rule in calculus), which makes this step fast and precise.

10 David Rumelhart et al. “Learning Internal Representations by Error Propagation,” (Defense Technical Infor‐

mation Center technical report, September 1985).

11 This technique was actually independently invented several times by various researchers in different fields,

starting with Paul Werbos in 1974.

290 

| 

Chapter 10: Introduction to Artificial Neural Networks with Keras

• The algorithm then measures how much of these error contributions came from
each connection in the layer below, again using the chain rule, working backward
until the algorithm reaches the input layer. As explained earlier, this reverse pass
efficiently  measures  the  error  gradient  across  all  the  connection  weights  in  the
network by propagating the error gradient backward through the network (hence
the name of the algorithm).

• Finally, the algorithm performs a Gradient Descent step to tweak all the connec‐

tion weights in the network, using the error gradients it just computed.

This algorithm is so important that it’s worth summarizing it again: for each training
instance, the backpropagation algorithm first makes a prediction (forward pass) and
measures the error, then goes through each layer in reverse to measure the error con‐
tribution  from  each  connection  (reverse  pass),  and  finally  tweaks  the  connection
weights to reduce the error (Gradient Descent step).

It is important to initialize all the hidden layers’ connection weights
randomly, or else training will fail. For example, if you initialize all
weights and biases to zero, then all neurons in a given layer will be
perfectly  identical,  and  thus  backpropagation  will  affect  them  in
exactly the same way, so they will remain identical. In other words,
despite having hundreds of neurons per layer, your model will act
as  if  it  had  only  one  neuron  per  layer:  it  won’t  be  too  smart.  If
instead you randomly initialize the weights, you break the symme‐
try and allow backpropagation to train a diverse team of neurons.

In  order  for  this  algorithm  to  work  properly,  its  authors  made  a  key  change  to  the
MLP’s architecture: they replaced the step function with the logistic (sigmoid) func‐
tion,  σ(z)  =  1  /  (1  +  exp(–z)).  This  was  essential  because  the  step  function  contains
only  flat  segments,  so  there  is  no  gradient  to  work  with  (Gradient  Descent  cannot
move on a flat surface), while the logistic function has a well-defined nonzero deriva‐
tive everywhere, allowing Gradient Descent to make some progress at every step. In
fact, the backpropagation algorithm works well with many other activation functions,
not just the logistic function. Here are two other popular choices:

The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1

Just  like  the  logistic  function,  this  activation  function  is  S-shaped,  continuous,
and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in
the  case  of  the  logistic  function).  That  range  tends  to  make  each  layer’s  output
more  or  less  centered  around  0  at  the  beginning  of  training,  which  often  helps
speed up convergence.

From Biological to Artificial Neurons 

| 

291

The Rectified Linear Unit function: ReLU(z) = max(0, z)

The  ReLU  function  is  continuous  but  unfortunately  not  differentiable  at  z  =  0
(the slope changes abruptly, which can make Gradient Descent bounce around),
and its derivative is 0 for z < 0. In practice, however, it works very well and has
the  advantage  of  being  fast  to  compute,  so  it  has  become  the  default.12  Most
importantly, the fact that it does not have a maximum output value helps reduce
some issues during Gradient Descent (we will come back to this in Chapter 11).

These  popular  activation  functions  and  their  derivatives  are  represented  in
Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if
you  chain  several  linear  transformations,  all  you  get  is  a  linear  transformation.  For
example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions
gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t
have some nonlinearity between layers, then even a deep stack of layers is equivalent
to a single layer, and you can’t solve very complex problems with that. Conversely, a
large enough DNN with nonlinear activations can theoretically approximate any con‐
tinuous function.

Figure 10-8. Activation functions and their derivatives

OK! You know where neural nets came from, what their architecture is, and how to
compute their outputs. You’ve also learned about the backpropagation algorithm. But
what exactly can you do with them?

Regression MLPs
First, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,
the price of a house, given many of its features), then you just need a single output
neuron: its output is the predicted value. For multivariate regression (i.e., to predict

12 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck
to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is
one of the cases where the biological analogy was misleading.

292 

| 

Chapter 10: Introduction to Artificial Neural Networks with Keras

multiple  values  at  once),  you  need  one  output  neuron  per  output  dimension.  For
example, to locate the center of an object in an image, you need to predict 2D coordi‐
nates,  so  you  need  two  output  neurons.  If  you  also  want  to  place  a  bounding  box
around the object, then you need two more numbers: the width and the height of the
object. So, you end up with four output neurons.

In general, when building an MLP for regression, you do not want to use any activa‐
tion function for the output neurons, so they are free to output any range of values. If
you want to guarantee that the output will always be positive, then you can use the
ReLU  activation  function  in  the  output  layer.  Alternatively,  you  can  use  the  softplus
activation function, which is a smooth variant of ReLU: softplus(z) = log(1 + exp(z)).
It is close to 0 when z is negative, and close to z when z is positive. Finally, if you want
to guarantee that the predictions will fall within a given range of values, then you can
use  the  logistic  function  or  the  hyperbolic  tangent,  and  then  scale  the  labels  to  the
appropriate  range:  0  to  1  for  the  logistic  function  and  –1  to  1  for  the  hyperbolic
tangent.

The loss function to use during training is typically the mean squared error, but if you
have  a  lot  of  outliers  in  the  training  set,  you  may  prefer  to  use  the  mean  absolute
error  instead.  Alternatively,  you  can  use  the  Huber  loss,  which  is  a  combination  of
both.

The Huber loss is quadratic when the error is smaller than a thres‐
hold δ (typically 1) but linear when the error is larger than δ. The
linear part makes it less sensitive to outliers than the mean squared
error,  and  the  quadratic  part  allows  it  to  converge  faster  and  be
more precise than the mean absolute error.

Table 10-1 summarizes the typical architecture of a regression MLP.

Table 10-1. Typical regression MLP architecture

Hyperparameter
# input neurons

# hidden layers

Typical value
One per input feature (e.g., 28 x 28 = 784 for MNIST)

Depends on the problem, but typically 1 to 5

# neurons per hidden layer Depends on the problem, but typically 10 to 100

# output neurons

Hidden activation

Output activation

Loss function

1 per prediction dimension

ReLU (or SELU, see Chapter 11)

None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs)

MSE or MAE/Huber (if outliers)

From Biological to Artificial Neurons 

| 

293

Classification MLPs
MLPs  can  also  be  used  for  classification  tasks.  For  a  binary  classification  problem,
you just need a single output neuron using the logistic activation function: the output
will be a number between 0 and 1, which you can interpret as the esti
 when
evaluated on an i.i.d. test set. It is natural therefore to wonder whether these
models have obtained a true human-level understanding of these tasks. In order
to probe the level of understanding a network has of the underlying task, we can
search for examples that the model misclassiï¬?es. Szegedy et al. (2014b) found that
even neural networks that perform at human level accuracy have a nearly 100%
error rate on examples that are intentionally constructed by using an optimization
procedure to search for an input xî€° near a data point x such that the model
output is very diï¬€erent at xî€° . In many cases, xî€° can be so similar to x that a
human observer cannot tell the diï¬€erence between the original example and the
adversarial example, but the network can make highly diï¬€erent predictions. See
ï¬?gure 7.8 for an example.
Adversarial examples have many implications, for example, in computer security,
that are beyond the scope of this chapter. However, they are interesting in the
context of regularization because one can reduce the error rate on the original i.i.d.
test set via adversarial trainingâ€”training on adversarially perturbed examples
from the training set (Szegedy et al., 2014b; Goodfellow et al., 2014b).
Goodfellow et al. (2014b) showed that one of the primary causes of these
adversarial examples is excessive linearity. Neural networks are built out of
primarily linear building blocks. In some experiments the overall function they
implement proves to be highly linear as a result. These linear functions are easy
268

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

+ .007 Ã—

x
y =â€œpandaâ€?
w/ 57.7%
conï¬?dence

=

sign(âˆ‡x J (Î¸, x, y))
â€œnematodeâ€?
w/ 8.2%
conï¬?dence

x+
î€? sign(âˆ‡x J(Î¸, x, y ))
â€œgibbonâ€?
w/ 99.3 %
conï¬?dence

Figure 7.8: A demonstration of adversarial example generation applied to GoogLeNet
(Szegedy et al., 2014a) on ImageNet. By adding an imperceptibly small vector whose
elements are equal to the sign of the elements of the gradient of the cost function with
respect to the input, we can change GoogLeNetâ€™s classiï¬?cation of the image. Reproduced
with permission from Goodfellow et al. (2014b).

to optimize. Unfortunately, the value of a linear function can change very rapidly
if it has numerous inputs. If we change each input by î€?, then a linear function
with weights w can change by as much as î€?||w||1, which can be a very large
amount if w is high-dimensional. Adversarial training discourages this highly
sensitive locally linear behavior by encouraging the network to be locally constant
in the neighborhood of the training data. This can be seen as a way of explicitly
introducing a local constancy prior into supervised neural nets.
Adversarial training helps to illustrate the power of using a large function
family in combination with aggressive regularization. Purely linear models, like
logistic regression, are not able to resist adversarial examples because they are
forced to be linear. Neural networks are able to represent functions that can range
from nearly linear to nearly locally constant and thus have the ï¬‚exibility to capture
linear trends in the training data while still learning to resist local perturbation.
Adversarial examples also provide a means of accomplishing semi-supervised
learning. At a point x that is not associated with a label in the dataset, the
model itself assigns some label yÌ‚ . The modelâ€™s label yÌ‚ may not be the true label,
but if the model is high quality, then yÌ‚ has a high probability of providing the
true label. We can seek an adversarial example xî€° that causes the classiï¬?er to
output a label yî€° with y î€° î€¶= yÌ‚. Adversarial examples generated using not the true
label but a label provided by a trained model are called virtual adversarial
examples (Miyato et al., 2015). The classiï¬?er may then be trained to assign the
same label to x and xî€° . This encourages the classiï¬?er to learn a function that is
269

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

robust to small changes anywhere along the manifold where the unlabeled data
lies. The assumption motivating this approach is that diï¬€erent classes usually lie
on disconnected manifolds, and a small perturbation should not be able to jump
from one class manifold to another class manifold.

7.14

Tangent Distance, Tangent Prop, and Manifold
Tangent Classiï¬?er

Many machine learning algorithms aim to overcome the curse of dimensionality
by assuming that the data lies near a low-dimensional manifold, as described in
section 5.11.3.
One of the early attempts to take advantage of the manifold hypothesis is the
tangent distance algorithm (Simard et al., 1993, 1998). It is a non-parametric
nearest-neighbor algorithm in which the metric used is not the generic Euclidean
distance but one that is derived from knowledge of the manifolds near which
probability concentrates. It is assumed that we are trying to classify examples and
that examples on the same manifold share the same category. Since the classiï¬?er
should be invariant to the local factors of variation that correspond to movement
on the manifold, it would make sense to use as nearest-neighbor distance between
points x1 and x2 the distance between the manifolds M 1 and M2 to which they
respectively belong. Although that may be computationally diï¬ƒcult (it would
require solving an optimization problem, to ï¬?nd the nearest pair of points on M1
and M2), a cheap alternative that makes sense locally is to approximate Mi by its
tangent plane at xi and measure the distance between the two tangents, or between
a tangent plane and a point. That can be achieved by solving a low-dimensional
linear system (in the dimension of the manifolds). Of course, this algorithm requires
one to specify the tangent vectors.
In a related spirit, the tangent prop algorithm (Simard et al., 1992) (ï¬?gure 7.9)
trains a neural net classiï¬?er with an extra penalty to make each output f(x) of
the neural net locally invariant to known factors of variation. These factors of
variation correspond to movement along the manifold near which examples of the
same class concentrate. Local invariance is achieved by requiring âˆ‡x f (x) to be
orthogonal to the known manifold tangent vectors v(i) at x, or equivalently that
the directional derivative of f at x in the directions v (i) be small by adding a
regularization penalty â„¦:
â„¦(f ) =

î?˜î€?

î€¾

(âˆ‡x f(x)) v

i

270

(i )

î€‘2

.

(7.67)

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

This regularizer can of course be scaled by an appropriate hyperparameter, and, for
most neural networks, we would need to sum over many outputs rather than the lone
output f(x) described here for simplicity. As with the tangent distance algorithm,
the tangent vectors are derived a priori, usually from the formal knowledge of
the eï¬€ect of transformations such as translation, rotation, and scaling in images.
Tangent prop has been used not just for supervised learning (Simard et al., 1992)
but also in the context of reinforcement learning (Thrun, 1995).
Tangent propagation is closely related to dataset augmentation. In both
cases, the user of the algorithm encodes his or her prior knowledge of the task
by specifying a set of transformations that should not alter the output of the
network. The diï¬€erence is that in the case of dataset augmentation, the network is
explicitly trained to correctly classify distinct inputs that were created by applying
more than an inï¬?nitesimal amount of these transformations. Tangent propagation
does not require explicitly visiting a new input point. Instead, it analytically
regularizes the model to resist perturbation in the directions corresponding to
the speciï¬?ed transformation. While this analytical approach is intellectually
elegant, it has two major drawbacks. First, it only regularizes the model to resist
inï¬?nitesimal perturbation. Explicit dataset augmentation confers resistance to
larger perturbations. Second, the inï¬?nitesimal approach poses diï¬ƒculties for models
based on rectiï¬?ed linear units. These models can only shrink their derivatives
by turning units oï¬€ or shrinking their weights. They are not able to shrink their
derivatives by saturating at a high value with large weights, as sigmoid or tanh
units can. Dataset augmentation works well with rectiï¬?ed linear units because
diï¬€erent subsets of rectiï¬?ed units can activate for diï¬€erent transformed versions of
each original input.
Tangent propagation is also related to double backprop (Drucker and LeCun,
1992) and adversarial training (Szegedy et al., 2014b; Goodfellow et al., 2014b).
Double backprop regularizes the Jacobian to be small, while adversarial training
ï¬?nds inputs near the original inputs and trains the model to produce the same
output on these as on the original inputs. Tangent propagation and dataset
augmentation using manually speciï¬?ed transformations both require that the
model should be invariant to certain speciï¬?ed directions of change in the input.
Double backprop and adversarial training both require that the model should be
invariant to all directions of change in the input so long as the change is small. Just
as dataset augmentation is the non-inï¬?nitesimal version of tangent propagation,
adversarial training is the non-inï¬?nitesimal version of double backprop.
The manifold tangent classiï¬?er (Rifai et al., 2011c), eliminates the need to
know the tangent vectors a priori. As we will see in chapter 14, autoencoders can
271

x2

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

Normal

Tangent

x1

Figure 7.9: Illustration of the main idea of the tangent prop algorithm (Simard et al.,
1992) and manifold tangent classiï¬?er (Rifai et al., 2011c), which both regularize the
classiï¬?er output function f(x). Each curve represents the manifold for a diï¬€erent class,
illustrated here as a one-dimensional manifold embedded in a two-dimensional space.
On one curve, we have chosen a single point and drawn a vector that is tangent to the
class manifold (parallel to and touching the manifold) and a vector that is normal to the
class manifold (orthogonal to the manifold). In multiple dimensions there may be many
tangent directions and many normal directions. We expect the classiï¬?cation function to
change rapidly as it moves in the direction normal to the manifold, and not to change as
it moves along the class manifold. Both tangent propagation and the manifold tangent
classiï¬?er regularize f (x) to not change very much as x moves along the manifold. Tangent
propagation requires the user to manually specify functions that compute the tangent
directions (such as specifying that small translations of images remain in the same class
manifold) while the manifold tangent classiï¬?er estimates the manifold tangent directions
by training an autoencoder to ï¬?t the training data. The use of autoencoders to estimate
manifolds will be described in chapter 14.

estimate the manifold tangent vectors. The manifold tangent classiï¬?er makes use
of this technique to avoid needing user-speciï¬?ed tangent vectors. As illustrated
in ï¬?gure 14.10, these estimated tangent vectors go beyond the classical invariants
that arise out of the geometry of images (such as translation, rotation and scaling)
and include factors that must be learned because they are object-speciï¬?c (such as
moving body parts). The algorithm proposed with the manifold tangent classiï¬?er
is therefore simple: (1) use an autoencoder to learn the manifold structure by
unsupervised learning, and (2) use these tangents to regularize a neural net classiï¬?er
as in tangent prop (equation 7.67).
This chapter has described most of the general strategies used to regularize
neural networks. Regularization is a central theme of machine learning and as such
272

CHAPTER 7. REGULARIZATION FOR DEEP LEARNING

will be revisited periodically by most of the remaining chapters. Another central
theme of machine learning is optimization, described next.

273

Chapter 8

Optimization for Training Deep
Models
Deep learning algorithms involve optimization in many contexts. For example,
performing inference in models such as PCA involves solving an optimization
problem. We often use analytical optimization to write proofs or design algorithms.
Of all of the many optimization problems involved in deep learning, the most
diï¬ƒcult is neural network training. It is quite common to invest days to months of
time on hundreds of machines in order to solve even a single instance of the neural
network training problem. Because this problem is so important and so expensive,
a specialized set of optimization techniques have been developed for solving it.
This chapter presents these optimization techniques for neural network training.
If you are unfamiliar with the basic principles of gradient-based optimization,
we suggest reviewing chapter 4. That chapter includes a brief overview of numerical
optimization in general.
This chapter focuses on one particular case of optimization: ï¬?nding the parameters Î¸ of a neural network that signiï¬?cantly reduce a cost function J(Î¸), which
typically includes a performance measure evaluated on the entire training set as
well as additional regularization terms.
We begin with a description of how optimization used as a training algorithm
for a machine learning task diï¬€ers from pure optimization. Next, we present several
of the concrete challenges that make optimization of neural networks diï¬ƒcult. We
then deï¬?ne several practical algorithms, including both optimization algorithms
themselves and strategies for initializing the parameters. More advanced algorithms
adapt their learning rates during training or leverage information contained in
274

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

the second derivatives of the cost function. Finally, we conclude with a review of
several optimization strategies that are formed by combining simple optimization
algorithms into higher-level procedures.

8.1

How Learning Diï¬€ers from Pure Optimization

Optimization algorithms used for training of deep models diï¬€er from traditional
optimization algorithms in several ways. Machine learning usually acts indirectly.
In most machine learning scenarios, we care about some performance measure
P , that is deï¬?ned with respect to the test set and may also be intractable. We
therefore optimize P only indirectly. We reduce a diï¬€erent cost function J(Î¸) in
the hope that doing so will improve P . This is in contrast to pure optimization,
where minimizing J is a goal in and of itself. Optimization algorithms for training
deep models also typically include some specialization on the speciï¬?c structure of
machine learning objective functions.
Typically, the cost function can be written as an average over the training set,
such as
J(Î¸) = E(x,y)âˆ¼pË†data L(f (x; Î¸), y),
(8.1)
where L is the per-example loss function, f (x; Î¸) is the predicted output when
the input is x, pÌ‚data is the empirical distribution. In the supervised learning case,
y is the target output. Throughout this chapter, we develop the unregularized
supervised case, where the arguments to L are f(x; Î¸) and y. However, it is trivial
to extend this development, for example, to include Î¸ or x as arguments, or to
exclude y as arguments, in order to develop various forms of regularization or
unsupervised learning.
Equation 8.1 deï¬?nes an objective function with resp
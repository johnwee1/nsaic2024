on of tissue-regulated
splicing using RNA sequence and cellular context. Bioinformatics , 27(18), 2554â€“2562.
265
Xu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S., and
Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual
attention. In ICMLâ€™2015, arXiv:1502.03044 . 102, 410, 691
Yildiz, I. B., Jaeger, H., and Kiebel, S. J. (2012). Re-visiting the echo state property.
Neural networks , 35, 1â€“9. 405
776

BIBLIOGRAPHY

Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features
in deep neural networks? In NIPSâ€™2014 . 325, 536
Younes, L. (1998). On the convergence of Markovian stochastic algorithms with rapidly
decreasing ergodicity rates. In Stochastics and Stochastics Models, pages 177â€“228. 612
Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured
conditional random ï¬?elds. IEEE Journal of Selected Topics in Signal Processing. 323
Zaremba, W. and Sutskever, I. (2014). Learning to execute. arXiv 1410.4615. 329
Zaremba, W. and Sutskever, I. (2015). Reinforcement learning neural Turing machines.
arXiv:1505.00521 . 419
Zaslavsky, T. (1975). Facing Up to Arrangements: Face-Count Formulas for Partitions
of Space by Hyperplanes. Number no. 154 in Memoirs of the American Mathematical
Society. American Mathematical Society. 550
Zeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks.
In ECCVâ€™14 . 6
Zeiler, M. D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q., Nguyen, P., Senior,
A., Vanhoucke, V., Dean, J., and Hinton, G. E. (2013). On rectiï¬?ed linear units for
speech processing. In ICASSP 2013 . 460
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2015). Object detectors
emerge in deep scene CNNs. ICLRâ€™2015, arXiv:1412.6856. 551
Zhou, J. and Troyanskaya, O. G. (2014). Deep supervised and convolutional generative
stochastic network for protein secondary structure prediction. In ICMLâ€™2014 . 715
Zhou, Y. and Chellappa, R. (1988). Computation of optical ï¬‚ow using a neural network.
In Neural Networks, 1988., IEEE International Conference on, pages 71â€“78. IEEE. 339
ZÃ¶hrer, M. and Pernkopf, F. (2014). General stochastic networks for classiï¬?cation. In
NIPSâ€™2014 . 716

777

Index
0-1 loss, 102, 274

Bag of words, 467
Bagging, 252
Absolute value rectiï¬?cation, 191
Batch normalization, 264, 422
Accuracy, 420
Bayes error, 116
Activation function, 169
Bayesâ€™ rule, 69
Active constraint, 94
Bayesian hyperparameter optimization, 433
AdaGrad, 305
Bayesian network, see directed graphical
ADALINE, see adaptive linear element
model
Adam, 307, 422
Bayesian probability, 54
Adaptive linear element, 15, 23, 26
Bayesian statistics, 134
Adversarial example, 265
Belief network, see directed graphical model
Adversarial training, 266, 268, 526
Bernoulli distribution, 61
Aï¬ƒne, 109
BFGS, 314
AIS, see annealed importance sampling
Bias, 123, 227
Almost everywhere, 70
Bias parameter, 109
Almost sure convergence, 128
Biased importance sampling, 589
Ancestral sampling, 576, 591
Bigram, 458
ANN, see Artiï¬?cial neural network
Binary relation, 478
Annealed importance sampling, 621, 662, Block Gibbs sampling, 595
711
Boltzmann distribution, 566
Approximate Bayesian computation, 710
Boltzmann machine, 566, 648
Approximate inference, 579
BPTT, see back-propagation through time
Artiï¬?cial intelligence, 1
Broadcasting, 33
Artiï¬?cial neural network, see Neural net- Burn-in, 593
work
ASR, see automatic speech recognition
CAE, see contractive autoencoder
Asymptotically unbiased, 123
Calculus of variations, 178
Audio, 101, 357, 455
Categorical distribution, see multinoulli disAutoencoder, 4, 353, 498
tribution
Automatic speech recognition, 455
CD, see contrastive divergence
Centering trick (DBM), 667
Back-propagation, 201
Central limit theorem, 63
Back-propagation through time, 381
Chain rule (calculus), 203
Backprop, see back-propagation
Chain rule of probability, 58

778

INDEX

Chess, 2
Critical temperature, 599
Chord, 575
Cross-correlation, 329
Chordal graph, 575
Cross-entropy, 74, 131
Class-based language models, 460
Cross-validation, 121
Classical dynamical system, 372
CTC, see connectionist temporal classiï¬?caClassiï¬?cation, 99
tion
Clique potential, see factor (graphical model) Curriculum learning, 326
CNN, see convolutional neural network
Curse of dimensionality, 153
Collaborative Filtering, 474
Cyc, 2
Collider, see explaining away
D-separation, 568
Color images, 357
DAE, see denoising autoencoder
Complex cell, 362
Data generating distribution, 110, 130
Computational graph, 202
Data generating process, 110
Computer vision, 449
Data parallelism, 444
Concept drift, 533
Dataset, 103
Condition number, 277
Conditional computation, see dynamic struc- Dataset augmentation, 268, 454
DBM, see deep Boltzmann machine
ture
DCGAN, 547, 548, 695
Conditional independence, xiii, 59
Decision tree, 144, 544
Conditional probability, 58
Decoder, 4
Conditional RBM, 679
Deep belief network, 26, 525, 626, 651, 654,
Connectionism, 17, 440
678, 686
Connectionist temporal classiï¬?cation, 457
Deep Blue, 2
Consistency, 128, 509
Deep Boltzmann machine, 23, 26, 525, 626,
Constrained optimization, 92, 235
647, 651, 657, 666, 678
Content-based addressing, 416
Content-based recommender systems, 475 Deep feedforward network, 166, 422
Deep learning, 2, 5
Context-speciï¬?c independence, 569
Denoising autoencoder, 506, 683
Contextual bandits, 476
Denoising score matching, 615
Continuation methods, 324
Density estimation, 102
Contractive autoencoder, 516
Derivative, xiii, 82
Contrast, 451
Design matrix, 105
Contrastive divergence, 289, 606, 666
Detector layer, 336
Convex optimization, 140
Determinant, xii
Convolution, 327, 677
Diagonal matrix, 40
Convolutional network, 16
Convolutional neural network, 250, 327, 422, Diï¬€erential entropy, 73, 641
Dirac delta function, 64
456
Directed graphical model, 76, 503, 559, 685
Coordinate descent, 319, 665
Directional derivative, 84
Correlation, 60
Discriminative ï¬?ne-tuning, see supervised
Cost function, see objective function
ï¬?ne-tuning
Covariance, xiii, 60
Discriminative
RBM, 680
Covariance matrix, 61
Distributed representation, 17, 149, 542
Coverage, 421
Domain adaptation, 532
779

INDEX

Dot product, 33, 139
Double backprop, 268
Doubly block circulant matrix, 330
Dream sleep, 605, 647
DropConnect, 263
Dropout, 255, 422, 427, 428, 666, 683
Dynamic structure, 445

F-score, 420
Factor (graphical model), 563
Factor analysis, 486
Factor graph, 575
Factors of variation, 4
Feature, 98
Feature selection, 234
Feedforward neural network, 166
E-step, 629
Fine-tuning, 321
Early stopping, 244, 246, 270, 271, 422
Finite diï¬€erences, 436
EBM, see energy-based model
Forget gate, 304
Echo state network, 23, 26, 401
Forward propagation, 201
Eï¬€ective capacity, 113
Fourier transform, 357, 359
Eigendecomposition, 41
Fovea, 363
Eigenvalue, 41
FPCD, 610
Eigenvector, 41
Free energy, 567, 674
ELBO, see evidence lower bound
Freebase, 479
Element-wise product, see Hadamard prod- Frequentist probability, 54
uct, see Hadamard product
Frequentist statistics, 134
EM, see expectation maximization
Frobenius norm, 45
Embedding, 512
Fully-visible Bayes network, 699
Empirical distribution, 65
Functional derivatives, 640
Empirical risk, 274
FVBN, see fully-visible Bayes network
Empirical risk minimization, 274
Gabor function, 365
Encoder, 4
Energy function, 565
GANs, see generative adversarial networks
Energy-based model, 565, 591, 648, 657
Gated recurrent unit, 422
Ensemble methods, 252
Gaussian distribution, see normal distribuEpoch, 244
tion
Gaussian kernel, 140
Equality constraint, 93
Gaussian mixture, 66, 187
Equivariance, 335
GCN, see global contrast normalization
Error function, see objective function
GeneOntology, 479
ESN, see echo state network
Generalization, 109
Euclidean norm, 38
Generalized Lagrange function, see generalEuler-Lagrange equation, 641
ized Lagrangian
Evidence lower bound, 628, 655
Generalized Lagrangian, 93
Example, 98
Generative adversarial networks, 683, 693
Expectation, 59
Generative moment matching networks, 696
Expectation maximization, 629
Generator network, 687
Expected value, see expectation
Gibbs distribution, 564
Explaining away, 570, 626, 639
Gibbs sampling, 577, 595
Exploitation, 477
Exploration, 477
Global contrast normalization, 451
Exponential distribution, 64
GPU, see graphics processing unit
Gradient, 83
780

INDEX

Gradient clipping, 287, 411
Information retrieval, 520
Gradient descent, 82, 84
Initialization, 298
Graph, xii
Integral, xiii
Graphical model, see structured probabilis- Invariance, 339
tic model
Isotropic, 64
Graphics processing unit, 441
Jacobian matrix, xiii, 71, 85
Greedy algorithm, 321
Greedy layer-wise unsupervised pretraining, Joint probability, 56
524
k-means, 361, 542
Greedy supervised pretraining, 321
k-nearest neighbors, 141, 544
Grid search, 429
Karush-Kuhn-Tucker conditions, 94, 235
Karushâ€“Kuhnâ€“Tucker, 93
Hadamard product, xii, 33
Kernel (convolution), 328, 329
Hard tanh, 195
Harmonium, see restricted Boltzmann ma- Kernel machine, 544
Kernel trick, 139
chine
KKT, see Karushâ€“Kuhnâ€“Tucker
Harmony theory, 567
Helmholtz free energy, see evidence lower KKT conditions, see Karush-Kuhn-Tucker
conditions
bound
KL
divergence,
see Kullback-Leibler diverHessian, 221
gence
Hessian matrix, xiii, 86
Knowledge
base, 2, 479
Heteroscedastic, 186
Krylov methods, 222
Hidden layer, 6, 166
Kullback-Leibler divergence, xiii, 73
Hill climbing, 85
Hyperparameter optimization, 429
Label smoothing, 241
Hyperparameters, 119, 427
Lagrange multipliers, 93, 641
Hypothesis space, 111, 117
Lagrangian, see generalized Lagrangian
LAPGAN, 695
i.i.d. assumptions, 110, 121, 265
Laplace distribution, 64, 492
Identity matrix, 35
ILSVRC, see ImageNet Large Scale Visual Latent variable, 66
Layer (neural network), 166
Recognition Challenge
ImageNet Large Scale Visual Recognition LCN, see local contrast normalization
Leaky ReLU, 191
Challenge, 22
Leaky units, 404
Immorality, 573
Learning rate, 84
Importance sampling, 588, 620, 691
Line search, 84, 85, 92
Importance weighted autoencoder, 691
Linear combination, 36
Independence, xiii, 59
Independent and identically distributed, see Linear dependence, 37
Linear factor models, 485
i.i.d. assumptions
Linear regression, 106, 109, 138
Independent component analysis, 487
Link prediction, 480
Independent subspace analysis, 489
Lipschitz constant, 91
Inequality constraint, 93
Inference, 558, 579, 626, 628, 630, 633, 643, Lipschitz continuous, 91
Liquid state machine, 401
646
781

INDEX

Local conditional probability distribution,
560
Local contrast normalization, 452
Logistic regression, 3, 138, 139
Logistic sigmoid, 7, 66
Long short-term memory, 18, 24, 304, 407,
422
Loop, 575
Loopy belief propagation, 581
Loss function, see objective function
Lp norm, 38
LSTM, see long short-term memory
M-step, 629
Machine learning, 2
Machine translation, 100
Main diagonal, 32
Manifold, 159
Manifold hypothesis, 160
Manifold learning, 160
Manifold tangent classiï¬?er, 268
MAP approximation, 137, 501
Marginal probability, 57
Markov chain, 591
Markov chain Monte Carlo, 591
Markov network, see undirected model
Markov random ï¬?eld, see undirected model
Matrix, xi, xii, 31
Matrix inverse, 35
Matrix product, 33
Max norm, 39
Max pooling, 336
Maximum likelihood, 130
Maxout, 191, 422
MCMC, see Markov chain Monte Carlo
Mean ï¬?eld, 633, 634, 666
Mean squared error, 107
Measure theory, 70
Measure zero, 70
Memory network, 413, 415
Method of steepest descent, see gradient
descent
Minibatch, 277
Missing inputs, 99
Mixing (Markov chain), 597

Mixture density networks, 187
Mixture distribution, 65
Mixture model, 187, 506
Mixture of experts, 446, 544
MLP, see multilayer perception
MNIST, 20, 21, 666
Model averaging, 252
Model compression, 444
Model identiï¬?ability, 282
Model parallelism, 444
Moment matching, 696
Moore-Penrose pseudoinverse, 44, 237
Moralized graph, 573
MP-DBM, see multi-prediction DBM
MRF (Markov Random Field), see undirected model
MSE, see mean squared error
Multi-modal learning, 535
Multi-prediction DBM, 668
Multi-task learning, 242, 533
Multilayer perception, 5
Multilayer perceptron, 26
Multinomial distribution, 61
Multinoulli distribution, 61
n-gram, 458
NADE, 702
Naive Bayes, 3
Nat, 72
Natural image, 555
Natural language processing, 457
Nearest neighbor regression, 114
Negative deï¬?nite, 88
Negative phase, 466, 602, 604
Neocognitron, 16, 23, 26, 364
Nesterov momentum, 298
Netï¬‚ix Grand Prize, 255, 475
Neural language model, 460, 472
Neural network, 13
Neural Turing machine, 415
Neuroscience, 15
Newtonâ€™s method, 88, 309
NLM, see neural language model
NLP, see natural language processing
No free lunch theorem, 115

782

INDEX

Noise-contrastive estimation, 616
Non-parametric model, 113
Norm, xiv, 38
Normal distribution, 62, 63, 124
Normal equations, 108, 108, 111, 232
Normalized initialization, 301
Numerical diï¬€erentiation, see ï¬?nite diï¬€erences

Preprocessing, 450
Pretraining, 320, 524
Primary visual cortex, 362
Principal components analysis, 47, 145, 146,
486, 626
Prior probability distribution, 134
Probabilistic max pooling, 677
Probabilistic PCA, 486, 487, 627
Probability density function, 57
Object detection, 449
Probability distribution, 55
Object recognition, 449
Probability mass function, 55
Objective function, 81
Probability mass function estimation, 102
OMP-k , see orthogonal matching pursuit
Product of experts, 566
One-shot learning, 534
Product rule of probability, see chain rule
Operation, 202
of probability
Optimization, 79, 81
PSD, see predictive sparse decomposition
Orthodox statistics, see frequentist statistics Pseudolikelihood, 611
Orthogonal matching pursuit, 26, 252
Orthogonal matrix, 41
Quadrature pair, 366
Orthogonality, 40
Quasi-Newton methods, 314
Output layer, 166
Radial basis function, 195
Parallel distributed processing, 17
Random search, 431
Parameter initialization, 298, 403
Random variable, 55
Parameter sharing, 249, 332, 370, 372, 386 Ratio matching, 614
Parameter tying, see Parameter sharing
RBF, 195
Parametric model, 113
RBM, see restricted Boltzmann machine
Parametric ReLU, 191
Recall, 420
Partial derivative, 83
Receptive ï¬?eld, 334
Partition function, 564, 601, 663
Recommender Systems, 474
PCA, see principal components analysis
Rectiï¬?ed linear unit, 170, 191, 422, 503
PCD, see stochastic maximum likelihood
Recurrent network, 26
Perceptron, 15, 26
Recurrent neural network, 375
Persistent contrastive divergence, see stochas- Regression, 99
tic maximum likelihood
Regularization, 119, 119, 176, 226, 427
Perturbation analysis, see reparametrization Regularizer, 118
trick
REINFORCE, 683
Point estimator, 121
Reinforcement learning, 24, 105, 476, 683
Policy, 476
Relational database, 479
Pooling, 327, 677
Relations, 478
Positive deï¬?nite, 88
Reparametrization trick, 682
Positive phase, 466, 602, 604, 650, 662
Representation learning, 3
Precision, 420
Representational capacity, 113
Precision (of a normal distribution), 62, 64 Restricted Boltzmann machine, 353, 456,
Predictive sparse decomposition, 519
475, 583, 626, 650, 651, 666, 670,
783

INDEX

672, 674, 677
Ridge regression, see weight decay
Risk, 273
RNN-RBM, 679

Square matrix, 37
ssRBM, see spike and slab restricted Boltzmann machine
Standard deviation, 60
Standard error, 126
Saddle points, 283
Standard error of the mean, 126, 276
Sample mean, 124
Statistic, 121
Scalar, xi, xii, 30
Statistical learni
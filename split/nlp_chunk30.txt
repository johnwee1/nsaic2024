G United] , a unit of [ORG United Airlines
Holding] , said the fare applies to the [LOC Chicago ] route.

Figure 8.7 shows the same excerpt represented with BIO tagging, as well as
variants called IO tagging and BIOES tagging. In BIO tagging we label any token
that begins a span of interest with the label B, tokens that occur inside a span are
tagged with an I, and any tokens outside of any span of interest are labeled O. While
there is only one O tag, we’ll have distinct B and I tags for each named entity class.
The number of tags is thus 2n + 1 tags, where n is the number of entity types. BIO
tagging can represent exactly the same information as the bracketed notation, but has
the advantage that we can represent the task in the same simple sequence modeling
way as part-of-speech tagging: assigning a single label yi to each input word xi:

Words
Jane
Villanueva
of
United
Airlines
Holding
discussed
the
Chicago
route
.
Figure 8.7 NER as a sequence model, showing IO, BIO, and BIOES taggings.

BIO Label
B-PER
I-PER
O
B-ORG
I-ORG
I-ORG
O
O
B-LOC
O
O

IO Label
I-PER
I-PER
O
I-ORG
I-ORG
I-ORG
O
O
I-LOC
O
O

BIOES Label
B-PER
E-PER
O
B-ORG
I-ORG
E-ORG
O
O
S-LOC
O
O

We’ve also shown two variant tagging schemes: IO tagging, which loses some
information by eliminating the B tag, and BIOES tagging, which adds an end tag
E for the end of a span, and a span tag S for a span consisting of only one word.
A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each
token in a text with tags that indicate the presence (or absence) of particular kinds
of named entities.

8.4 HMM Part-of-Speech Tagging

8.4

• HMM PART-OF-SPEECH TAGGING

169

In this section we introduce our ﬁrst sequence labeling algorithm, the Hidden Markov
Model, and show how to apply it to part-of-speech tagging. Recall that a sequence
labeler is a model whose job is to assign a label to each unit in a sequence, thus
mapping a sequence of observations to a sequence of labels of the same length.
The HMM is a classic model that introduces many of the key concepts of sequence
modeling that we will see again in more modern models.

An HMM is a probabilistic sequence model: given a sequence of units (words,
letters, morphemes, sentences, whatever), it computes a probability distribution over
possible sequences of labels and chooses the best label sequence.

8.4.1 Markov Chains

The HMM is based on augmenting the Markov chain. A Markov chain is a model
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. These sets can be words, or
tags, or symbols representing anything, for example the weather. A Markov chain
makes a very strong assumption that if we want to predict the future in the sequence,
all that matters is the current state. All the states before the current state have no im-
pact on the future except via the current state. It’s as if to predict tomorrow’s weather
you could examine today’s weather but you weren’t allowed to look at yesterday’s
weather.

Markov chain

(a)

(b)

Figure 8.8 A Markov chain for weather (a) and one for words (b), showing states and
transitions. A start distribution π is required; setting π = [0.1, 0.7, 0.2] for (a) would mean a
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.

Markov
assumption

More formally, consider a sequence of state variables q1, q2, ..., qi. A Markov
model embodies the Markov assumption on the probabilities of this sequence: that
when predicting the future, the past doesn’t matter, only the present.

q1...qi
Markov Assumption: P(qi = a
|

1) = P(qi = a

−

qi
|

−

1)

(8.3)

Figure 8.8a shows a Markov chain for assigning a probability to a sequence of
weather events, for which the vocabulary consists of HOT, COLD, and WARM. The
states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. The transitions are probabilities: the values of arcs leaving a given
state must sum to 1. Figure 8.8b shows a Markov chain for assigning a probability to
a sequence of words w1...wt . This Markov chain should be familiar; in fact, it repre-
w j)!
sents a bigram language model, with each edge expressing the probability p(wi|
Given the two models in Fig. 8.8, we can assign a probability to any sequence from
our vocabulary.

WARM3HOT1COLD2.8.6.1.1.3.6.1.1.3charminguniformlyare.1.4.5.5.5.2.6.2170 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

Formally, a Markov chain is speciﬁed by the following components:

hidden

hidden Markov
model

Q = q1q2 . . . qN
A = a11a12 . . . aN1 . . . aNN

π = π1, π2, ..., πN

a set of N states
a transition probability matrix A, each ai j represent-
ing the probability of moving from state i to state j, s.t.

n
j=1 ai j = 1

i
∀

an initial probability distribution over states. πi is the
(cid:80)
probability that the Markov chain will start in state i.
Some states j may have π j = 0, meaning that they cannot
be initial states. Also,

n
i=1 πi = 1

Before you go on, use the sample probabilities in Fig. 8.8a (with π = [0.1, 0.7, 0.2])

(cid:80)

to compute the probability of each of the following sequences:

(8.4) hot hot hot hot
(8.5) cold hot cold hot

What does the difference in these probabilities tell you about a real-world weather
fact encoded in Fig. 8.8a?

8.4.2 The Hidden Markov Model

A Markov chain is useful when we need to compute a probability for a sequence
of observable events. In many cases, however, the events we are interested in are
hidden: we don’t observe them directly. For example we don’t normally observe
part-of-speech tags in a text. Rather, we see words, and must infer the tags from the
word sequence. We call the tags hidden because they are not observed.

A hidden Markov model (HMM) allows us to talk about both observed events
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. An HMM is speciﬁed by
the following components:
a set of N states

Q = q1q2 . . . qN
A = a11 . . . ai j . . . aNN a transition probability matrix A, each ai j representing the probability

B = bi(ot )

π = π1, π2, ..., πN

N
j=1 ai j = 1

of moving from state i to state j, s.t.
a sequence of observation likelihoods, also called emission probabili-
ties, each expressing the probability of an observation ot (drawn from a
vocabulary V = v1, v2, ..., vV ) being generated from a state qi
an initial probability distribution over states. πi is the probability that
the Markov chain will start in state i. Some states j may have π j = 0,
meaning that they cannot be initial states. Also,

i
∀

(cid:80)

n
i=1 πi = 1

The HMM is given as input O = o1o2 . . . oT : a sequence of T observations, each

(cid:80)

one drawn from the vocabulary V .

A ﬁrst-order hidden Markov model instantiates two simplifying assumptions.
First, as with a ﬁrst-order Markov chain, the probability of a particular state depends
only on the previous state:

Markov Assumption: P(qi|

q1, ..., qi

1) = P(qi|
qi

−

−

1)

(8.6)

Second, the probability of an output observation oi depends only on the state that

produced the observation qi and not on any other states or any other observations:

Output Independence: P(oi|

q1, . . . qi, . . . , qT , o1, . . . , oi, . . . , oT ) = P(oi|

qi)

(8.7)

8.4

• HMM PART-OF-SPEECH TAGGING

171

8.4.3 The components of an HMM tagger

Let’s start by looking at the pieces of an HMM tagger, and then we’ll see how to use
it to tag. An HMM has two components, the A and B probabilities.
The A matrix contains the tag transition probabilities P(ti|
ti
−

1) which represent
the probability of a tag occurring given the previous tag. For example, modal verbs
like will are very likely to be followed by a verb in the base form, a VB, like race, so
we expect this probability to be high. We compute the maximum likelihood estimate
of this transition probability by counting, out of the times we see the ﬁrst tag in a
labeled corpus, how often the ﬁrst tag is followed by the second:

P(ti|
ti

1) =

−

C(ti
−
C(ti

1,ti)
1)

−

(8.8)

In the WSJ corpus, for example, MD occurs 13124 times of which it is followed

by VB 10471, for an MLE estimate of

MD) =
P(V B
|

C(MD,V B)
C(MD)

=

10471
13124

= .80

(8.9)

Let’s walk through an example, seeing how these probabilities are estimated and

used in a sample tagging task, before we return to the algorithm for decoding.

In HMM tagging, the probabilities are estimated by counting on a tagged training

corpus. For this example we’ll use the tagged WSJ corpus.

The B emission probabilities, P(wi|

ti), represent the probability, given a tag (say
MD), that it will be associated with a given word (say will). The MLE of the emis-
sion probability is

P(wi|

ti) =

C(ti, wi)
C(ti)

(8.10)

Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046
times:

P(will

MD) =
|

C(MD, will)
C(MD)

=

4046
13124

= .31

(8.11)

We saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood
term is not asking “which is the most likely tag for the word will?” That would be
the posterior P(MD
MD) answers the slightly counterintuitive
will). Instead, P(will
|
|
question “If we were going to generate a MD, how likely is it that this modal would
be will?”

The A transition probabilities, and B observation likelihoods of the HMM are
illustrated in Fig. 8.9 for three states in an HMM part-of-speech tagger; the full
tagger would have one state for each tag.

8.4.4 HMM tagging as decoding

For any model, such as an HMM, that contains hidden variables, the task of deter-
mining the hidden variables sequence corresponding to the sequence of observations
is called decoding. More formally,

decoding

Decoding: Given as input an HMM λ = (A, B) and a sequence of ob-
servations O = o1, o2, ..., oT , ﬁnd the most probable sequence of states
Q = q1q2q3 . . . qT .

172 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

the A transition
Figure 8.9 An illustration of the two parts of an HMM representation:
probabilities used to compute the prior probability, and the B observation likelihoods that are
associated with each state, one likelihood for each possible observation word.

For part-of-speech tagging, the goal of HMM decoding is to choose the tag
sequence t1 . . .tn that is most probable given the observation sequence of n words
w1 . . . wn:

ˆt1:n = argmax

t1... tn

P(t1 . . .tn|

w1 . . . wn)

The way we’ll do this in the HMM is to use Bayes’ rule to instead compute:
P(w1 . . . wn|

t1 . . .tn)P(t1 . . .tn)

ˆt1:n = argmax

t1... tn

P(w1 . . . wn)

Furthermore, we simplify Eq. 8.13 by dropping the denominator P(wn

1):

ˆt1:n = argmax

t1... tn

P(w1 . . . wn|

t1 . . .tn)P(t1 . . .tn)

(8.12)

(8.13)

(8.14)

HMM taggers make two further simplifying assumptions. The ﬁrst is that the
probability of a word appearing depends only on its own tag and is independent of
neighboring words and tags:

P(w1 . . . wn|

t1 . . .tn)

≈

n

(cid:89)i=1

P(wi|

ti)

(8.15)

The second assumption, the bigram assumption, is that the probability of a tag is
dependent only on the previous tag, rather than the entire tag sequence;

n

P(t1 . . .tn)

≈

P(ti|
ti
−

1)

(8.16)

(cid:89)i=1

Plugging the simplifying assumptions from Eq. 8.15 and Eq. 8.16 into Eq. 8.14
results in the following equation for the most probable tag sequence from a bigram
tagger:

ˆt1:n = argmax

t1... tn

P(t1 . . .tn|

w1 . . . wn)

argmax
t1... tn

≈

emission
P(wi|
(cid:122) (cid:125)(cid:124) (cid:123)

ti)

transition
P(ti|
ti
−
(cid:122)
(cid:125)(cid:124)

1)
(cid:123)

(8.17)

n

(cid:89)i=1

The two parts of Eq. 8.17 correspond neatly to the B emission probability and A
transition probability that we just deﬁned above!

NN3VB1MD2a22a11a12a21a13a33a32a23a31P("aardvark" | NN)...P(“will” | NN)...P("the" | NN)...P(“back” | NN)...P("zebra" | NN)B3P("aardvark" | VB)...P(“will” | VB)...P("the" | VB)...P(“back” | VB)...P("zebra" | VB)B1P("aardvark" | MD)...P(“will” | MD)...P("the" | MD)...P(“back” | MD)...P("zebra" | MD)B28.4

• HMM PART-OF-SPEECH TAGGING

173

8.4.5 The Viterbi Algorithm

Viterbi
algorithm

The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 8.10.
As an instance of dynamic programming, Viterbi resembles the dynamic program-
ming minimum edit distance algorithm of Chapter 2.

function VITERBI(observations of len T,state-graph of len N) returns best-path, path-prob

; initialization step

; recursion step

create a path probability matrix viterbi[N,T]
for each state s from 1 to N do
bs(o1)
0

viterbi[s,1]
backpointer[s,1]

←

πs ∗
←

for each time step t from 2 to T do
for each state s from 1 to N do
viterbi[s(cid:48),t

viterbi[s,t]

N
max
s(cid:48) =1

←

backpointer[s,t]

←

N
argmax
s(cid:48) =1
viterbi[s, T ]

N
max
s=1

bestpathprob

←
bestpathpointer

bs(ot )

1]

−

∗

viterbi[s(cid:48),t

as(cid:48),s ∗
1]

−

∗

bs(ot )

as(cid:48),s ∗
; termination step

N
argmax
s=1

←

viterbi[s, T ]

; termination step

bestpath
return bestpath, bestpathprob

←

the path starting at state bestpathpointer, that follows backpointer[] to states back in time

Figure 8.10 Viterbi algorithm for ﬁnding the optimal sequence of tags. Given an observation sequence and
an HMM λ = (A, B), the algorithm returns the state path through the HMM that assigns maximum likelihood
to the observation sequence.

The Viterbi algorithm ﬁrst sets up a probability matrix or lattice, with one col-
umn for each observation ot and one row for each state in the state graph. Each col-
umn thus has a cell for each state qi in the single combined automaton. Figure 8.11
shows an intuition of this lattice for the sentence Janet will back the bill.

Each cell of the lattice, vt ( j), represents the probability that the HMM is in state
j after seeing the ﬁrst t observations and passing through the most probable state
1, given the HMM λ . The value of each cell vt ( j) is computed
sequence q1, ..., qt
by recursively taking the most probable path that could lead us to this cell. Formally,
each cell expresses the probability

−

vt ( j) = max
q1,...,qt
1
−

P(q1...qt

1, o1, o2 . . . ot , qt = j

−

λ )
|

(8.18)

We represent the most probable path by taking the maximum over all possible
. Like other dynamic programming algorithms,

previous state sequences max
q1,...,qt
1
−

Viterbi ﬁlls each cell recursively. Given that we had already computed the probabil-
ity of being in every state at time t
1, we compute the Viterbi probability by taking
the most probable of the extensions of the paths that lead to the current cell. For a
given state q j at time t, the value vt ( j) is computed as

−

vt ( j) =

N
max
i=1

vt

1(i) ai j b j(ot )

−

(8.19)

The three factors that are multiplied in Eq. 8.19 for extending the previous paths to
compute the Viterbi probability at time t are

174 CHAPTER 8

• SEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES

Figure 8.11 A sketch of the lattice for Janet will back the bill, showing the possible tags (qi)
for each word and highlighting the path corresponding to the correct tag sequence through the
hidden states. States (parts of speech) which have a zero probability of generating a particular
word according to the B matrix (such as the probability that a determiner DT will be realized
as Janet) are greyed out.

1(i)

vt
−
ai j
b j(
e NP, “Richard
Godown, president of the Industrial Biotechnology Association” the mention is the
entire phrase. Prenominal modiﬁers are annotated as separate entities only if they
are proper nouns. Thus wheat is not an entity in wheat ﬁelds, but UN is an entity in
UN policy (but not adjectives like American in American policy).

A number of corpora mark richer discourse phenomena. The ISNotes corpus
annotates a portion of OntoNotes for information status, include bridging examples
(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains
coreference annotations for 210,532 tokens from 100 different literary novels, in-
cluding singletons and quantiﬁed and negated noun phrases. The AnCora-CO coref-
erence corpus (Recasens and Mart´ı, 2010) contains 400,000 words each of Spanish
(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for
complex phenomena like discourse deixis in both languages. The ARRAU corpus
(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which
means singleton clusters are available. ARRAU includes diverse genres like dialog
(the TRAINS data) and ﬁction (the Pear Stories), and has labels for bridging refer-
ences, discourse deixis, generics, and ambiguous anaphoric relations.

22.3 Mention Detection

mention
detection

The ﬁrst stage of coreference is mention detection: ﬁnding the spans of text that
constitute each mention. Mention detection algorithms are usually very liberal in
proposing candidate mentions (i.e., emphasizing recall), and only ﬁltering later. For
example many systems run parsers and named entity taggers on the text and extract
every span that is either an NP, a possessive pronoun, or a named entity.

Doing so from our sample text repeated in (22.44):

(22.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3

22.3

• MENTION DETECTION

491

million, as the 38-year-old also became the company’s president. It is
widely known that she came to Megabucks from rival Lotsabucks.

might result in the following list of 13 potential mentions:

$2.3 million

Victoria Chen
CFO of Megabucks Banking the 38-year-old
Megabucks Banking
her
her pay

the company
the company’s president
It

she
Megabucks
Lotsabucks

More recent mention detection systems are even more generous; the span-based
algorithm we will describe in Section 22.6 ﬁrst extracts literally all n-gram spans
of words up to N=10. Of course recall from Section 22.1.3 that many NPs—and
the overwhelming majority of random n-gram spans—are not referring expressions.
Therefore all such mention detection systems need to eventually ﬁlter out pleonas-
tic/expletive pronouns like It above, appositives like CFO of Megabucks Banking
Inc, or predicate nominals like the company’s president or $2.3 million.

Some of this ﬁltering can be done by rules. Early rule-based systems designed
regular expressions to deal with pleonastic it, like the following rules from Lappin
and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe, know, antic-
ipate) to capture pleonastic it in “It is thought that ketchup...”, or modal adjectives
(e.g., necessary, possible, certain, important), for, e.g., “It is likely that I...”. Such
rules are sometimes used as part of modern systems:

It is Modaladjective that S
It is Modaladjective (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S

Mention-detection rules are sometimes designed speciﬁcally for particular eval-
uation campaigns. For OntoNotes, for example, mentions are not embedded within
larger mentions, and while numeric quantities are annotated, they are rarely coref-
erential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a
common ﬁrst pass rule-based mention detection algorithm (Lee et al., 2013) is:

1. Take all NPs, possessive pronouns, and named entities.
2. Remove numeric quantities (100 dollars, 8%), mentions embedded in
larger mentions, adjectival forms of nations, and stop words (like there).

3. Remove pleonastic it based on regular expression patterns.

Rule-based systems, however, are generally insufﬁcient to deal with mention-
detection, and so modern systems incorporate some sort of learned mention detec-
tion component, such as a referentiality classiﬁer, an anaphoricity classiﬁer—
detecting whether an NP is an anaphor—or a discourse-new classiﬁer— detecting
whether a mention is discourse-new and a potential antecedent for a future anaphor.
An anaphoricity detector, for example, can draw its positive training examples
from any span that is labeled as an anaphoric referring expression in hand-labeled
datasets like OntoNotes, ARRAU, or AnCora. Any other NP or named entity can be
marked as a negative training example. Anaphoricity classiﬁers use features of the
candidate mention such as its head word, surrounding words, deﬁniteness, animacy,
length, position in the sentence/discourse, many of which were ﬁrst proposed in
early work by Ng and Cardie (2002a); see Section 22.5 for more on features.

anaphoricity
detector

492 CHAPTER 22

• COREFERENCE RESOLUTION AND ENTITY LINKING

Referentiality or anaphoricity detectors can be run as ﬁlters, in which only men-
tions that are classiﬁed as anaphoric or referential are passed on to the coreference
system. The end result of such a ﬁltering mention detection system on our example
above might be the following ﬁltered set of 9 potential mentions:

Victoria Chen
her pay
Megabucks Bank the 38-year-old Megabucks
Lotsabucks
her

the company

she

It turns out, however, that hard ﬁltering of mentions based on an anaphoricity
or referentiality classiﬁer leads to poor performance. If the anaphoricity classiﬁer
threshold is set too high, too many mentions are ﬁltered out and recall suffers. If the
classiﬁer threshold is set too low, too many pleonastic or non-referential mentions
are included and precision suffers.

The modern approach is instead to perform mention detection, anaphoricity, and
coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge
2007, Rahman and Ng 2009). For example mention detection in the Lee et al.
(2017b),2018 system is based on a single end-to-end neural network that computes
a score for each mention being referential, a score for two mentions being corefer-
ence, and combines them to make a decision, training all these scores with a single
end-to-end loss. We’ll describe this method in detail in Section 22.6. 7

Despite these advances, correctly detecting referential mentions seems to still be
an unsolved problem, since systems incorrectly marking pleonastic pronouns like
it and other non-referential NPs as coreferent is a large source of errors of modern
coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube
2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).

Mention, referentiality, or anaphoricity detection is thus an important open area
of investigation. Other sources of knowledge may turn out to be helpful, especially
in combination with unsupervised and semisupervised algorithms, which also mit-
igate the expense of labeled datasets. In early work, for example Bean and Riloff
(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-
tracting and generalizing over the ﬁrst NPs in a text, which are guaranteed to be
non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in
the training data but never appear as gold mentions to help ﬁnd non-referential NPs.
Bergsma et al. (2008b) use web counts as a semisupervised way to augment standard
features for anaphoricity detection for English it, an important task because it is both
common and ambiguous; between a quarter and half it examples are non-anaphoric.
Consider the following two examples:

(22.45) You can make [it] in advance. [anaphoric]
(22.46) You can make [it] in Hollywood. [non-anaphoric]

The it in make it is non-anaphoric, part of the idiom make it. Bergsma et al. (2008b)
turn the context around each example into patterns, like “make * in advance” from
(22.45), and “make * in Hollywood” from (22.46). They then use Google n-grams to
enumerate all the words that can replace it in the patterns. Non-anaphoric contexts
tend to only have it in the wildcard positions, while anaphoric contexts occur with
many other NPs (for example make them in advance is just as frequent in their data

7 Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like
OntoNotes which don’t label singletons, an alternative to ﬁltering out non-referential mentions is to run
coreference resolution, and then simply delete any candidate mentions which were not corefered with
another mention. This likely doesn’t work as well as explicitly modeling referentiality, and cannot solve
the problem of detecting singletons, which is important for tasks like entity linking.

22.4

• ARCHITECTURES FOR COREFERENCE ALGORITHMS

493

as make it in advance, but make them in Hollywood did not occur at all). These
n-gram contexts can be used as features in a supervised anaphoricity classiﬁer.

22.4 Architectures for Coreference Algorithms

Modern systems for coreference are based on supervised neural machine learning,
supervised from hand-labeled datasets like OntoNotes. In this section we overview
the various architecture of modern systems, using the categorization of Ng (2010),
which distinguishes algorithms based on whether they make each coreference deci-
sion in a way that is entity-based—representing each entity in the discourse model—
or only mention-based—considering each mention independently, and whether they
use ranking models to directly compare potential antecedents. Afterwards, we go
into more detail on one state-of-the-art algorithm in Section 22.6.

22.4.1 The Mention-Pair Architecture

mention-pair

mention-pair

We begin with the mention-pair architecture, the simplest and most inﬂuential
coreference architecture, which introduces many of the features of more complex
algorithms, even though other architectures perform better. The mention-pair ar-
chitecture is based around a classiﬁer that— as its name suggests—is given a pair
of mentions, a candidate anaphor and a candidate antecedent, and makes a binary
classiﬁcation decision: coreferring or not.

Let’s consider the task of this classiﬁer for the pronoun she in our example, and

assume the slightly simpliﬁed set of potential antecedents in Fig. 22.2.

Figure 22.2 For each pair of a mention (like she), and a potential antecedent mention (like
Victoria Chen or her), the mention-pair classiﬁer assigns a probability of a coreference link.

For each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary
classiﬁer computes a probability: whether or not the mention is the antecedent of
she. We want this probability to be high for actual antecedents (Victoria Chen, her,
the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay).

Early classiﬁers used hand-built features (Section 22.5); more recent classiﬁers

use neural representation learning (Section 22.6)

For training, we need a heuristic for selecting training samples; since most pairs
of mentions in a document are not coreferent, selecting every pair would lead to
a massive overabundance of negative samples. The most common heuristic, from
(Soon et al., 2001), is to choose the closest antecedent as a positive example, and all
pairs in between as the negative examples. More formally, for each anaphor mention
mi we create

• one positive instance (mi, m j) where m j is the closest antecedent to mi, and

Victoria ChenMegabucks Bankingherher paythe 37-year-oldshep(coref|”Victoria Chen”,”she”)p(coref|”Megabucks Banking”,”she”)494 CHAPTER 22

• COREFERENCE RESOLUTION AND ENTITY LINKING

• a negative instance (mi, mk) for each mk between m j and mi
Thus for the anaphor she, we would choose (she, her) as the positive example
and no negative examples. Similarly, for the anaphor the company we would choose
(the company, Megabucks) as the positive example and (the company, she) (the com-
pany, the 38-year-old) (the company, her pay) and (the company, her) as negative
examples.

Once the classiﬁer is trained, it is applied to each test sentence in a clustering
step. For each mention i in a document, the classiﬁer considers each of the prior i
1
mentions. In closest-ﬁrst clustering (Soon et al., 2001), the classiﬁer is run right to
left (from mention i
1 down to mention 1) and the ﬁrst antecedent with probability
> .5 is linked to i. If no antecedent has probably > 0.5, no antecedent is selected for
i. In best-ﬁrst clustering, the classiﬁer is run on all i
1 antecedents and the most
probable preceding mention is chosen as the antecedent for i. The transitive closure
of the pairwise relation is taken as the cluster.

−

−

−

While the mention-pair model has the advantage of simplicity, it has two main
problems. First, the classiﬁer doesn’t directly compare candidate antecedents to
each other, so it’s not trained to decide, between two likely antecedents, which one
is in fact better. Second, it ignores the discourse model, looking only at mentions,
not entities. Each classiﬁer decision is made completely locally to the pair, without
being able to take into account other mentions of the same entity. The next two
models each address one of these two ﬂaws.

22.4.2 The Mention-Rank Architecture

The mention ranking model directly compares candidate antecedents to each other,
choosing the highest-scoring antecedent for each anaphor.

}

In early formulations, for mention i, the classiﬁer decides which of the
−
prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose i is
1
in fact not anaphoric, and none of the antecedents should be chosen? Such a model
would need to run a separate anaphoricity classiﬁer on i. Instead, it turns out to be
better to jointly learn anaphoricity detection and coreference together with a single
loss (Rahman and Ng, 2009).

1, ..., i
{

So in modern mention-ranking systems, for the ith mention (anaphor), we have
an associated random variable yi ranging over the values Y (i) =
. The
}
value (cid:15) is a special dummy mention meaning that i does not have an antecedent (i.e.,
is either discourse-new and starts a new coref chain, or is non-anaphoric).

1, ..., i
{

1, (cid:15)

−

Figure 22.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a proba-
bility distribution over all previous mentions plus the special dummy mention (cid:15).

At test time, for a given mention i the model computes one softmax over all the
antecedents (plus (cid:15)) giving a probability for each candidate antecedent (or none).

Victoria ChenMegabucks Bankingherher paythe 37-year-oldshep(”Victoria Chen”|”she”)p(ϵ|”she”)ϵOne or more of theseshould be highAll of theseshould be low}p(”her pay”|she”)p(”her”|she”)p(”the 37-year-old”|she”)p(”Megabucks Banking”|she”)}22.5

• CLASSIFIERS USING HAND-BUILT FEATURES

495

Fig. 22.3 shows an example of the computation for the single candidate anaphor
she.

Once the antecedent is classiﬁed for each anaphor, transitive closure can be run

over the pairwise decisions to get a complete clustering.

Training is trickier in the mention-ranking model than the mention-pair model,
because for each anaphor we don’t know which of all the possible gold antecedents
Instead, the best antecedent for each mention is la
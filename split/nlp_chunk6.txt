0)

We can simplify this equation, since the sum of all bigram counts that start with
(cid:80)
1 must be equal to the unigram count for that word wn
1 (the reader

a given word wn
should take a moment to be convinced of this):

−

−

P(wn|

wn

−

1) =

C(wn
−
C(wn

1wn)
1)

−

(3.11)

Let’s work through an example using a mini-corpus of three sentences. We’ll
ﬁrst need to augment each sentence with a special symbol <s> at the beginning
of the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a
special end-symbol. </s>2

<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>

Here are the calculations for some of the bigram probabilities from this corpus

P(I|<s>) = 2
P(</s>|Sam) = 1

3 = .67

2 = 0.5

P(Sam|<s>) = 1
P(Sam|am) = 1

3 = .33
2 = .5

P(am|I) = 2
P(do|I) = 1

3 = .67
3 = .33

1 For probabilistic models, normalizing means dividing by some total count so that the resulting proba-
bilities fall between 0 and 1.
2 We need the end-symbol to make the bigram grammar a true probability distribution. Without an end-
symbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities
for all sentences of a given length would sum to one. This model would deﬁne an inﬁnite set of probability
distributions, with one distribution per sentence length. See Exercise 3.5.

36 CHAPTER 3

• N-GRAM LANGUAGE MODELS

For the general case of MLE n-gram parameter estimation:

P(wn|

wn

−

N+1:n

−

1) =

C(wn

−
C(wn

N+1:n

−
N+1:n

−

1 wn)
1)

−

(3.12)

relative
frequency

Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the
observed frequency of a particular sequence by the observed frequency of a preﬁx.
This ratio is called a relative frequency. We said above that this use of relative
frequencies as a way to estimate probabilities is an example of maximum likelihood
estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood
M)). For example, suppose the
of the training set T given the model M (i.e., P(T
|
word Chinese occurs 400 times in a corpus of a million words like the Brown corpus.
What is the probability that a random word selected from some other text of, say,
400
a million words will be the word Chinese? The MLE of its probability is
1000000
or .0004. Now .0004 is not the best possible estimate of the probability of Chinese
occurring in all situations; it might turn out that in some other corpus or context
Chinese is a very unlikely word. But it is the probability that makes it most likely
that Chinese will occur 400 times in a million-word corpus. We present ways to
modify the MLE estimates slightly to get better probability estimates in Section 3.6.
Let’s move on to some examples from a slightly larger corpus than our 14-word
example above. We’ll use data from the now-defunct Berkeley Restaurant Project,
a dialogue system from the last century that answered questions about a database
of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-
normalized sample user queries (a sample of 9332 sentences is on the website):

can you tell me about any good cantonese restaurants close by
mid priced thai food is what i’m looking for
tell me about chez panisse
can you give me a listing of the kinds of food that are available
i’m looking for a good place to eat breakfast
when is caffe venezia open during the day

Figure 3.1 shows the bigram counts from a piece of a bigram grammar from the
Berkeley Restaurant Project. Note that the majority of the values are zero. In fact,
we have chosen the sample words to cohere with each other; a matrix selected from
a random set of eight words would be even more sparse.

i
want
to
eat
chinese
food
lunch
spend

i
5
2
2
0
1
15
2
1

want
827
0
0
0
0
0
0
0

to
0
608
4
2
0
15
0
1

eat
9
1
686
0
0
0
0
0

chinese
0
6
2
16
0
1
0
0

food
0
6
0
2
82
4
1
0

lunch
0
5
6
42
1
0
0
0

spend
2
1
211
0
0
0
0
0

Figure 3.1 Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-
rant Project corpus of 9332 sentences. Zero counts are in gray.

Figure 3.2 shows the bigram probabilities after normalization (dividing each cell
in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of
unigram probabilities):

3.1

• N-GRAMS

37

eat chinese food lunch spend

want to

i
2533 927

2417 746 158

1093 341

278

chinese

lunch
0

eat
0.0036 0
0.0011 0.0065

spend
food
0.00079
0
0.0065 0.0054 0.0011
0.0025 0.087

want
i
0.33
0.002
0.0022
0
0.00083 0
0
0
0
0
0
0

i
want
to
eat
chinese 0.0063
0.014
food
0.0059
lunch
0.0036
spend
Figure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus
of 9332 sentences. Zero probabilities are in gray.

0.00083 0
0.021
0
0.00092 0.0037 0
0.0029 0
0
0
0
0

0
0.0063 0
0
0
0

to
0
0.66
0.0017 0.28
0.0027 0
0
0
0
0.014
0
0
0.0036 0

0.0027 0.056
0.52

Here are a few other useful probabilities:

P(i|<s>) = 0.25
P(food|english) = 0.5

P(english|want) = 0.0011
P(</s>|food) = 0.68

Now we can compute the probability of sentences like I want English food or
I want Chinese food by simply multiplying the appropriate bigram probabilities to-
gether, as follows:

P(<s> i want english food </s>)

= P(i|<s>)P(want|i)P(english|want)

P(food|english)P(</s>|food)
0.68

.0011

0.5

×

×

= .25
.33
= .000031

×

×

We leave it as Exercise 3.2 to compute the probability of i want chinese food.
What kinds of linguistic phenomena are captured in these bigram statistics?
Some of the bigram probabilities above encode some facts that we think of as strictly
syntactic in nature, like the fact that what comes after eat is usually a noun or an
adjective, or that what comes after to is usually a verb. Others might be a fact about
the personal assistant task, like the high probability of sentences beginning with
the words I. And some might even be cultural rather than linguistic, like the higher
probability that people are looking for Chinese versus English food.

Some practical issues: Although for pedagogical purposes we have only described
bigram models, in practice we might use trigram models, which condition on the
previous two words rather than the previous word, or 4-gram or even 5-gram mod-
els, when there is sufﬁcient training data. Note that for these larger n-grams, we’ll
need to assume extra contexts to the left and right of the sentence end. For example,
to compute trigram probabilities at the very beginning of the sentence, we use two
pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s>).

We always represent and compute language model probabilities in log format
as log probabilities. Since probabilities are (by deﬁnition) less than or equal to
1, the more probabilities we multiply together, the smaller the product becomes.
Multiplying enough n-grams together would result in numerical underﬂow. By using
log probabilities instead of raw probabilities, we get numbers that are not as small.

trigram

4-gram

5-gram

log
probabilities

38 CHAPTER 3

• N-GRAM LANGUAGE MODELS

Adding in log space is equivalent to multiplying in linear space, so we combine log
probabilities by adding them. The result of doing all computation and storage in log
space is that we only need to convert back into probabilities if we need to report
them at the end; then we can just take the exp of the logprob:

p1 ×

p2 ×

p3 ×

p4 = exp(log p1 + log p2 + log p3 + log p4)

(3.13)

In practice throughout this book, we’ll use log to mean natural log (ln) when the

base is not speciﬁed.

3.2 Evaluating Language Models: Training and Test Sets

extrinsic
evaluation

intrinsic
evaluation

training set
development
set
test set

The best way to evaluate the performance of a language model is to embed it in
an application and measure how much the application improves. Such end-to-end
evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to
know if a particular improvement in the language model (or any component) is really
going to help the task at hand. Thus for evaluating n-gram language models that are
a component of some task like speech recognition or machine translation, we can
compare the performance of two candidate language models by running the speech
recognizer or machine translator twice, once with each language model, and seeing
which gives the more accurate transcription.

Unfortunately, running big NLP systems end-to-end is often very expensive. In-
stead, it’s helpful to have a metric that can be used to quickly evaluate potential
improvements in a language model. An intrinsic evaluation metric is one that mea-
sures the quality of a model independent of any application. In the next section we’ll
introduce perplexity, which is the standard intrinsic metric for measuring language
model performance, both for simple n-gram language models and for the more so-
phisticated neural large language models of Chapter 10.

In order to evaluate any machine learning model, we need to have at least three

distinct data sets: the training set, the development set, and the test set.

The training set is the data we use to learn the parameters of our model; for
simple n-gram language models it’s the corpus from which we get the counts that
we normalize into the probabilities of the n-gram language model.

The test set is a different, held-out set of data, not overlapping with the training
set, that we use to evaluate the model. We need a separate test set to give us an
unbiased estimate of how well the model we trained can generalize when we apply
it to some new unknown dataset. A machine learning model that perfectly captured
the training data, but performed terribly on any other data, wouldn’t be much use
when it comes time to apply it to any new data or problem! We thus measure the
quality of an n-gram model by its performance on this unseen test set or test corpus.
How should we choose a training and test set? The test set should reﬂect the
language we want to use the model for. If we’re going to use our language model
for speech recognition of chemistry lectures, the test set should be text of chemistry
lectures. If we’re going to use it as part of a system for translating hotel booking re-
quests from Chinese to English, the test set should be text of hotel booking requests.
If we want our language model to be general purpose, then the test test should be
drawn from a wide variety of texts. In such cases we might collect a lot of texts
from different sources, and then divide it up into a training set and a test set. It’s
important to do the dividing carefully; if we’re building a general purpose model,

3.3

• EVALUATING LANGUAGE MODELS: PERPLEXITY

39

we don’t want the test set to consist of only text from one document, or one author,
since that wouldn’t be a good measure of general performance.

Thus if we are given a corpus of text and want to compare the performance of
two different n-gram models, we divide the data into training and test sets, and train
the parameters of both models on the training set. We can then compare how well
the two trained models ﬁt the test set.

But what does it mean to “ﬁt the test set”? The standard answer is simple:
whichever language model assigns a higher probability to the test set—which
means it more accurately predicts the test set—is a better model. Given two proba-
bilistic models, the better model is the one that has a tighter ﬁt to the test data or that
better predicts the details of the test data, and hence will assign a higher probability
to the test data.

Since our evaluation metric is based on test set probability, it’s important not to
let the test sentences into the training set. Suppose we are trying to compute the
probability of a particular “test” sentence. If our test sentence is part of the training
corpus, we will mistakenly assign it an artiﬁcially high probability when it occurs
in the test set. We call this situation training on the test set. Training on the test
set introduces a bias that makes the probabilities all look too high, and causes huge
inaccuracies in perplexity, the probability-based metric we introduce below.

Even if we don’t train on the test set, if we test our language model on it many
times after making different changes, we might implicitly tune to its characteristics,
by noticing which changes seem to make the model better. For this reason, we only
want to run our model on the test set once, or a very few number of times, once we
are sure our model is ready.

For this reason we normally instead have a third dataset called a development
test set or, devset. We do all our testing on this dataset until the very end, and then
we test on the test once to see how good our model is.

How do we divide our data into training, development, and test sets? We want
our test set to be as large as possible, since a small test set may be accidentally un-
representative, but we also want as much training data as possible. At the minimum,
we would want to pick the smallest test set that gives us enough statistical power
to measure a statistically signiﬁcant difference between two potential models. It’s
important that the dev set be drawn from the same kind of text as the test set, since
its goal is to measure how we would do on the test set.

development
test

3.3 Evaluating Language Models: Perplexity

perplexity

In practice we don’t use raw probability as our metric for evaluating language mod-
els, but a function of probability called perplexity. Perplexity is one of the most
important metrics in natural language processing, and we use it to evaluate neural
language models as well.

The perplexity (sometimes abbreviated as PP or PPL) of a language model on a
test set is the inverse probability of the test set (one over the probability of the test
set), normalized by the number of words. For this reason it’s sometimes called the
per-word perplexity. For a test set W = w1w2 . . . wN,:

40 CHAPTER 3

• N-GRAM LANGUAGE MODELS

perplexity(W ) = P(w1w2 . . . wN)−

1
N

(3.14)

= N
(cid:115)

1
P(w1w2 . . . wN)

Or we can use the chain rule to expand the probability of W :

(3.15)

perplexity(W ) = N
(cid:118)
(cid:117)
(cid:117)
(cid:116)

N

1
w1 . . . wi

1)

P(wi|

−

(cid:89)i=1
Note that because of the inverse in Eq. 3.15, the higher the probability of the
word sequence, the lower the perplexity. Thus the lower the perplexity of a model on
the data, the better the model, and minimizing perplexity is equivalent to maximizing
the test set probability according to the language model. Why does perplexity use
the inverse probability? It turns out the inverse arises from the original deﬁnition
of perplexity from cross-entropy rate in information theory; for those interested,
the explanation is in the advanced section Section 3.9. Meanwhile, we just have to
remember that perplexity has an inverse relationship with probability.

The details of computing the perplexity of a test set W depends on which lan-
guage model we use. Here’s the perplexity of W with a unigram language model
(just the geometric mean of the unigram probabilities):

perplexity(W ) = N
(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:89)i=1

N

1
P(wi)

(3.16)

The perplexity of W computed with a bigram language model is still a geometric
mean, but now of the bigram probabilities:

N

1
P(wi|
wi

perplexity(W ) = N
(cid:118)
(cid:117)
(cid:117)
(cid:116)
What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire
sequence 
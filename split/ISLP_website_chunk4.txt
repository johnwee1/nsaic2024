ions. For m of the observations, where m < n, we have both predictor measurements and a response

2.2 Assessing Model Accuracy

27

measurement. For the remaining n − m observations, we have predictor
measurements but no response measurement. Such a scenario can arise if
the predictors can be measured relatively cheaply but the corresponding
responses are much more expensive to collect. We refer to this setting as
a semi-supervised learning problem. In this setting, we wish to use a stasemitistical learning method that can incorporate the m observations for which supervised
response measurements are available as well as the n − m observations for learning
which they are not. Although this is an interesting topic, it is beyond the
scope of this book.

2.1.5

Regression Versus Classification Problems

Variables can be characterized as either quantitative or qualitative (also
quantitative
known as categorical). Quantitative variables take on numerical values. Exqualitative
amples include a person’s age, height, or income, the value of a house, and
categorical
the price of a stock. In contrast, qualitative variables take on values in
one of K different classes, or categories. Examples of qualitative variables
class
include a person’s marital status (married or not), the brand of product
purchased (brand A, B, or C), whether a person defaults on a debt (yes
or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). We tend to refer to problems with
a quantitative response as regression problems, while those involving a
regression
qualitative response are often referred to as classification problems. Howclassification
ever, the distinction is not always that crisp. Least squares linear regression
(Chapter 3) is used with a quantitative response, whereas logistic regression
(Chapter 4) is typically used with a qualitative (two-class, or binary) rebinary
sponse. Thus, despite its name, logistic regression is a classification method.
But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K-nearest neighbors
(Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of
either quantitative or qualitative responses.
We tend to select statistical learning methods on the basis of whether
the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However,
whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in
this book can be applied regardless of the predictor variable type, provided
that any qualitative predictors are properly coded before the analysis is
performed. This is discussed in Chapter 3.

2.2

Assessing Model Accuracy

One of the key aims of this book is to introduce the reader to a wide range
of statistical learning methods that extend far beyond the standard linear
regression approach. Why is it necessary to introduce so many different
statistical learning approaches, rather than just a single best method? There
is no free lunch in statistics: no one method dominates all others over all
possible data sets. On a particular data set, one specific method may work

28

2. Statistical Learning

best, but some other method may work better on a similar but different
data set. Hence it is an important task to decide for any given set of data
which method produces the best results. Selecting the best approach can
be one of the most challenging parts of performing statistical learning in
practice.
In this section, we discuss some of the most important concepts that
arise in selecting a statistical learning procedure for a specific data set. As
the book progresses, we will explain how the concepts presented here can
be applied in practice.

2.2.1

Measuring the Quality of Fit

In order to evaluate the performance of a statistical learning method on
a given data set, we need some way to measure how well its predictions
actually match the observed data. That is, we need to quantify the extent
to which the predicted response value for a given observation is close to
the true response value for that observation. In the regression setting, the
most commonly-used measure is the mean squared error (MSE), given by mean
n

10
MSE =
(yi − fˆ(xi ))2 ,
n i=1

(2.5)

squared
error

where fˆ(xi ) is the prediction that fˆ gives for the ith observation. The MSE
will be small if the predicted responses are very close to the true responses,
and will be large if for some of the observations, the predicted and true
responses differ substantially.
The MSE in (2.5) is computed using the training data that was used to
fit the model, and so should more accurately be referred to as the training
MSE. But in general, we do not really care how well the method works
training
on the training data. Rather, we are interested in the accuracy of the pre- MSE
dictions that we obtain when we apply our method to previously unseen
test data. Why is this what we care about? Suppose that we are interested
test data
in developing an algorithm to predict a stock’s price based on previous
stock returns. We can train the method using stock returns from the past
6 months. But we don’t really care how well our method predicts last week’s
stock price. We instead care about how well it will predict tomorrow’s price
or next month’s price. On a similar note, suppose that we have clinical
measurements (e.g. weight, blood pressure, height, age, family history of
disease) for a number of patients, as well as information about whether each
patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In
practice, we want this method to accurately predict diabetes risk for future
patients based on their clinical measurements. We are not very interested
in whether or not the method accurately predicts diabetes risk for patients
used to train the model, since we already know which of those patients
have diabetes.
To state it more mathematically, suppose that we fit our statistical learning method on our training observations {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )},
and we obtain the estimate fˆ. We can then compute fˆ(x1 ), fˆ(x2 ), . . . , fˆ(xn ).

29

1.5
1.0
0.0

2

4

0.5

6

Y

8

Mean Squared Error

10

2.0

12

2.5

2.2 Assessing Model Accuracy

0

20

40

60
X

80

100

2

5

10

20

Flexibility

FIGURE 2.9. Left: Data simulated from f , shown in black. Three estimates of
f are shown: the linear regression line (orange curve), and two smoothing spline
fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red
curve), and minimum possible test MSE over all methods (dashed line). Squares
represent the training and test MSEs for the three fits shown in the left-hand
panel.

If these are approximately equal to y1 , y2 , . . . , yn , then the training MSE
given by (2.5) is small. However, we are really not interested in whether
fˆ(xi ) ≈ yi ; instead, we want to know whether fˆ(x0 ) is approximately equal
to y0 , where (x0 , y0 ) is a previously unseen test observation not used to train
the statistical learning method. We want to choose the method that gives
the lowest test MSE, as opposed to the lowest training MSE. In other words,
test MSE
if we had a large number of test observations, we could compute
Ave(y0 − fˆ(x0 ))2 ,
(2.6)
the average squared prediction error for these test observations (x0 , y0 ).
We’d like to select the model for which this quantity is as small as possible.
How can we go about trying to select a method that minimizes the test
MSE? In some settings, we may have a test data set available—that is,
we may have access to a set of observations that were not used to train
the statistical learning method. We can then simply evaluate (2.6) on the
test observations, and select the learning method for which the test MSE is
smallest. But what if no test observations are available? In that case, one
might imagine simply selecting a statistical learning method that minimizes
the training MSE (2.5). This seems like it might be a sensible approach,
since the training MSE and the test MSE appear to be closely related.
Unfortunately, there is a fundamental problem with this strategy: there
is no guarantee that the method with the lowest training MSE will also
have the lowest test MSE. Roughly speaking, the problem is that many
statistical methods specifically estimate coefficients so as to minimize the
training set MSE. For these methods, the training set MSE can be quite
small, but the test MSE is often much larger.
Figure 2.9 illustrates this phenomenon on a simple example. In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with

30

2. Statistical Learning

the true f given by the black curve. The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing
levels of flexibility. The orange line is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing
splines, discussed in Chapter 7, with different levels of smoothness. It is
smoothing
clear that as the level of flexibility increases, the curves fit the observed spline
data more closely. The green curve is the most flexible and matches the
data very well; however, we observe that it fits the true f (shown in black)
poorly because it is too wiggly. By adjusting the level of flexibility of the
smoothing spline fit, we can produce many different fits to this data.
We now move on to the right-hand panel of Figure 2.9. The grey curve
displays the average training MSE as a function of flexibility, or more
formally the degrees of freedom, for a number of smoothing splines. The
degrees of
degrees of freedom is a quantity that summarizes the flexibility of a curve; freedom
it is discussed more fully in Chapter 7. The orange, blue and green squares
indicate the MSEs associated with the corresponding curves in the lefthand panel. A more restricted and hence smoother curve has fewer degrees
of freedom than a wiggly curve—note that in Figure 2.9, linear regression
is at the most restrictive end, with two degrees of freedom. The training
MSE declines monotonically as flexibility increases. In this example the
true f is non-linear, and so the orange linear fit is not flexible enough to
estimate f well. The green curve has the lowest training MSE of all three
methods, since it corresponds to the most flexible of the three curves fit in
the left-hand panel.
In this example, we know the true function f , and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of
course, in general f is unknown, so this will not be possible.) The test MSE
is displayed using the red curve in the right-hand panel of Figure 2.9. As
with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then
starts to increase again. Consequently, the orange and green curves both
have high test MSE. The blue curve minimizes the test MSE, which should
not be surprising given that visually it appears to estimate f the best in the
left-hand panel of Figure 2.9. The horizontal dashed line indicates Var("),
the irreducible error in (2.3), which corresponds to the lowest achievable
test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.
In the right-hand panel of Figure 2.9, as the flexibility of the statistical
learning method increases, we observe a monotone decrease in the training
MSE and a U-shape in the test MSE. This is a fundamental property of
statistical learning that holds regardless of the particular data set at hand
and regardless of the statistical method being used. As model flexibility
increases, the training MSE will decrease, but the test MSE may not. When
a given method yields a small training MSE but a large test MSE, we are
said to be overfitting the data. This happens because our statistical learning
procedure is working too hard to find patterns in the training data, and
may be picking up some patterns that are just caused by random chance
rather than by true properties of the unknown function f . When we overfit
the training data, the test MSE will be very large because the supposed

31

1.5
1.0
0.0

2

4

0.5

6

Y

8

Mean Squared Error

10

2.0

12

2.5

2.2 Assessing Model Accuracy

0

20

40

60

80

100

X

2

5

10

20

Flexibility

FIGURE 2.10. Details are as in Figure 2.9, using a different true f that is
much closer to linear. In this setting, linear regression provides a very good fit to
the data.

patterns that the method found in the training data simply don’t exist
in the test data. Note that regardless of whether or not overfitting has
occurred, we almost always expect the training MSE to be smaller than
the test MSE because most statistical learning methods either directly or
indirectly seek to minimize the training MSE. Overfitting refers specifically
to the case in which a less flexible model would have yielded a smaller
test MSE.
Figure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in
the test MSE. However, because the truth is close to linear, the test MSE
only decreases slightly before increasing again, so that the orange least
squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays an example in which f is highly non-linear. The
training and test MSE curves still exhibit the same general patterns, but
now there is a rapid decrease in both curves before the test MSE starts to
increase slowly.
In practice, one can usually compute the training MSE with relative
ease, but estimating the test MSE is considerably more difficult because
usually no test data are available. As the previous three examples illustrate,
the flexibility level corresponding to the model with the minimal test MSE
can vary considerably among data sets. Throughout this book, we discuss a
variety of approaches that can be used in practice to estimate this minimum
point. One important method is cross-validation (Chapter 5), which is a crossmethod for estimating the test MSE using the training data.
validation

2.2.2

The Bias-Variance Trade-Off

The U-shape observed in the test MSE curves (Figures 2.9–2.11) turns out
to be the result of two competing properties of statistical learning methods.

2. Statistical Learning

15
10
5
0

−10

0

Y

10

Mean Squared Error

20

20

32

0

20

40

60
X

80

100

2

5

10

20

Flexibility

FIGURE 2.11. Details are as in Figure 2.9, using a different f that is far from
linear. In this setting, linear regression provides a very poor fit to the data.

Though the mathematical proof is beyond the scope of this book, it is
possible to show that the expected test MSE, for a given value x0 , can
always be decomposed into the sum of three fundamental quantities: the
variance of fˆ(x0 ), the squared bias of fˆ(x0 ) and the variance of the error
variance
terms ". That is,
bias
1
22
(2.7)
E y0 − fˆ(x0 ) = Var(fˆ(x0 )) + [Bias(fˆ(x0 ))]2 + Var(").

1
22
Here the notation E y0 − fˆ(x0 ) defines th
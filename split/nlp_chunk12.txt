art
0
pathetic 0
1
plot
1
satire
1
scenes
0
the
1
twists
0
was
0
worst

+
1
0
1
2
0
0
0
0
0
1
1
1
0
1
0
0

−
0
1
0
1
1
1
1
1
1
1
0
2
2
1
2
1

−
0
1
0
1
1
1
1
1
1
1
0
2
1
1
1
1

Figure 4.3 An example of binarization for the binary naive Bayes algorithm.

A second important addition commonly made when doing text classiﬁcation for
sentiment is to deal with negation. Consider the difference between I really like this
movie (positive) and I didn’t like this movie (negative). The negation expressed by
didn’t completely alters the inferences we draw from the predicate like. Similarly,
negation can modify a negative word to produce a positive review (don’t dismiss this
ﬁlm, doesn’t let us get bored).

A very simple baseline that is commonly used in sentiment analysis to deal with
negation is the following: during text normalization, prepend the preﬁx NOT to
every word after a token of logical negation (n’t, not, no, never) until the next punc-
tuation mark. Thus the phrase

didn’t like this movie , but I

becomes

didn’t NOT_like NOT_this NOT_movie , but I

Newly formed ‘words’ like NOT like, NOT recommend will thus occur more of-
ten in negative document and act as cues for negative sentiment, while words like
NOT bored, NOT dismiss will acquire positive associations. We will return in Chap-
ter 20 to the use of parsing to deal more accurately with the scope relationship be-
tween these negation words and the predicates they modify, but this simple baseline
works quite well in practice.

68 CHAPTER 4

• NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

sentiment
lexicons

General
Inquirer
LIWC

Finally, in some situations we might have insufﬁcient labeled training data to
train accurate naive Bayes classiﬁers using all words in the training set to estimate
positive and negative sentiment. In such cases we can instead derive the positive
and negative word features from sentiment lexicons, lists of words that are pre-
annotated with positive or negative sentiment. Four popular lexicons are the General
Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon
of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).

For example the MPQA subjectivity lexicon has 6885 words each marked for

whether it is strongly or weakly biased positive or negative. Some examples:

+ : admirable, beautiful, conﬁdent, dazzling, ecstatic, favor, glee, great
: awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate

−

A common way to use lexicons in a naive Bayes classiﬁer is to add a feature
that is counted whenever a word from that lexicon occurs. Thus we might add a
feature called ‘this word occurs in the positive lexicon’, and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. Similarly, we might add as a second feature ‘this word occurs in the
negative lexicon’ of words in the negative lexicon. If we have lots of training data,
and if the test data matches the training data, using just two features won’t work as
well as using all the words. But when training data is sparse or not representative of
the test set, using dense lexicon features instead of sparse individual-word features
may generalize better.

We’ll return to this use of lexicons in Chapter 25, showing how these lexicons
can be learned automatically, and how they can be applied to many other tasks be-
yond sentiment classiﬁcation.

4.5 Naive Bayes for other text classiﬁcation tasks

spam detection

In the previous section we pointed out that naive Bayes doesn’t require that our
classiﬁer use all the words in the training data as features. In fact features in naive
Bayes can express any property of the input text we want.

Consider the task of spam detection, deciding if a particular piece of email is
an example of spam (unsolicited bulk email)—one of the ﬁrst applications of naive
Bayes to text classiﬁcation (Sahami et al., 1998).

A common solution here, rather than using all the words as individual features,
is to predeﬁne likely sets of words or phrases as features, combined with features
that are not purely linguistic. For example the open-source SpamAssassin tool2
predeﬁnes features like the phrase “one hundred percent guaranteed”, or the feature
mentions millions of dollars, which is a regular expression that matches suspiciously
large sums of money. But it also includes features like HTML has a low ratio of text
to image area, that aren’t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
took to arrive. More sample SpamAssassin features:

• Email subject line is all capital letters
• Contains phrases of urgency like “urgent reply”
• Email subject line contains “online pharmaceutical”
• HTML has unbalanced “head” tags

2 https://spamassassin.apache.org

language id

4.6

• NAIVE BAYES AS A LANGUAGE MODEL

69

• Claims you can be removed from the list
For other tasks, like language id—determining what language a given piece
of text is written in—the most effective naive Bayes features are not words at all,
but character n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie z’,
‘thei’), or, even simpler byte n-grams, where instead of using the multibyte Unicode
character representations called codepoints, we just pretend everything is a string of
raw bytes. Because spaces count as a byte, byte n-grams can model statistics about
the beginning or ending of words. A widely used naive Bayes system, langid.py
(Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using fea-
ture selection to winnow down to the most informative 7000 ﬁnal features.

Language ID systems are trained on multilingual text, such as Wikipedia (Wiki-
pedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire.
To make sure that this multilingual text correctly reﬂects different regions, dialects,
and socioeconomic classes, systems also add Twitter text in many languages geo-
tagged to many regions (important for getting world English dialects from countries
with large Anglophone populations like Nigeria or India), Bible and Quran transla-
tions, slang websites like Urban Dictionary, corpora of African American Vernacular
English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).

4.6 Naive Bayes as a Language Model

As we saw in the previous section, naive Bayes classiﬁers can use any sort of fea-
ture: dictionaries, URLs, email addresses, network features, phrases, and so on. But
if, as in the previous section, we use only individual word features, and we use all
of the words in the text (not a subset), then naive Bayes has an important similar-
ity to language modeling. Speciﬁcally, a naive Bayes model can be viewed as a
set of class-speciﬁc unigram language models, in which the model for each class
instantiates a unigram language model.

Since the likelihood features from the naive Bayes model assign a probability to

each word P(word

c), the model also assigns a probability to each sentence:
|

P(s

c) =
|

c)
P(wi|

(4.15)

positions
Thus consider a naive Bayes model with the classes positive (+) and negative (-)

(cid:89)i
∈

and the following model parameters:

-)
|

P(w
w
0.1
I
love 0.1
this 0.01
fun 0.05
ﬁlm 0.1
...
...

+) P(w
|
0.2
0.001
0.01
0.005
0.1
...

Each of the two columns above instantiates a language model that can assign a

probability to the sentence “I love this fun ﬁlm”:

+) = 0.1
P(“I love this fun ﬁlm”
|
) = 0.2
P(“I love this fun ﬁlm”

|−

0.1
×
0.001

×

×

×
0.01

×
0.005

×

×

0.01

0.05

0.1 = 0.0000005

0.1 = .0000000010

×

70 CHAPTER 4

• NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

As it happens, the positive model assigns a higher probability to the sentence:
neg). Note that this is just the likelihood part of the naive Bayes
pos) > P(s
P(s
|
|
model; once we multiply in the prior a full naive Bayes model might well make a
different classiﬁcation decision.

4.7 Evaluation: Precision, Recall, F-measure

gold labels

confusion
matrix

To introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some
simple binary detection tasks. For example, in spam detection, our goal is to label
every text as being in the spam category (“positive”) or not in the spam category
(“negative”). For each item (email document) we therefore need to know whether
our system called it spam or not. We also need to know whether the email is actually
spam or not, i.e. the human-deﬁned labels for each document that we are trying to
match. We will refer to these human labels as the gold labels.

Or imagine you’re the CEO of the Delicious Pie Company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning Delicious Pie. Here the positive class is tweets about
Delicious Pie and the negative class is all other tweets.

In both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. To evaluate any system for detecting things, we start
by building a confusion matrix like the one shown in Fig. 4.4. A confusion matrix
is a table for visualizing how an algorithm performs with respect to the human gold
labels, using two dimensions (system output and gold labels), and each cell labeling
a set of possible outcomes. In the spam detection case, for example, true positives
are documents that are indeed spam (indicated by human-created gold labels) that
our system correctly said were spam. False negatives are documents that are indeed
spam but our system incorrectly labeled as non-spam.

To the bottom right of the table is the equation for accuracy, which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. Although accuracy might seem a natural
metric, we generally don’t use it for text classiﬁcation tasks. That’s because accuracy
doesn’t work well when the classes are unbalanced (as indeed they are with spam,
which is a large majority of email, or with tweets, which are mainly not about pie).

Figure 4.4 A confusion matrix for visualizing how well a binary classiﬁcation system per-
forms against gold standard labels.

To make this more explicit, imagine that we looked at a million tweets, and
let’s say that only 100 of them are discussing their love (or hatred) for our pie,

true positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn4.7

• EVALUATION: PRECISION, RECALL, F-MEASURE

71

while the other 999,900 are tweets about something completely unrelated. Imagine a
simple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should
be happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would
be completely useless, since it wouldn’t ﬁnd a single one of the customer comments
we are looking for. In other words, accuracy is not a good metric when the goal is
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.

That’s why instead of accuracy we generally turn to two other metrics shown in
Fig. 4.4: precision and recall. Precision measures the percentage of the items that
the system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,
are positive according to the human gold labels). Precision is deﬁned as

precision

Precision =

true positives
true positives + false positives

recall

Recall measures the percentage of items actually present in the input that were

correctly identiﬁed by the system. Recall is deﬁned as

Recall =

true positives
true positives + false negatives

Precision and recall will help solve the problem with the useless “nothing is
pie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). You should convince yourself that the precision at ﬁnding relevant
tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize
true positives: ﬁnding the things that we are supposed to be looking for.

There are many ways to deﬁne a single metric that incorporates aspects of both
precision and recall. The simplest of these combinations is the F-measure (van
Rijsbergen, 1975) , deﬁned as:

F-measure

Fβ =

(β 2 + 1)PR
β 2P + R

The β parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β > 1 favor recall, while
values of β < 1 favor precision. When β = 1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called Fβ =1 or just F1:

F1

2PR
P + R
F-measure comes from a weighted harmonic mean of precision and recall. The
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:

(4.16)

F1 =

HarmonicMean(a1, a2, a3, a4, ..., an) =

n
+ 1
a3

1
a1

+ 1
a2

+ ... + 1
an

(4.17)

and hence F-measure is

F =

1
P + (1

α 1

α) 1
R

−

or

with β 2 =

(cid:18)

α

1

−
α

(cid:19)

F =

(β 2 + 1)PR
β 2P + R

(4.18)

72 CHAPTER 4

• NAIVE BAYES, TEXT CLASSIFICATION, AND SENTIMENT

Harmonic mean is used because the harmonic mean of two values is closer to the
minimum of the two values than the arithmetic mean is. Thus it weighs the lower of
the two numbers more heavily, which is more conservative in this situation.

4.7.1 Evaluating with more than two classes

Up to now we have been describing text classiﬁcation tasks with only two classes.
But lots of classiﬁcation tasks in language processing have more than two classes.
For sentiment analysis we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, semantic role labeling, emotion detection, and so on. Luckily the
naive Bayes algorithm is already a multi-class classiﬁcation algorithm.

Figure 4.5 Confusion matrix for a three-class categorization task, showing for each pair of
classes (c1, c2), how many documents from c1 were (in)correctly assigned to c2.

But we’ll need to slightly modify our deﬁnitions of precision and recall. Con-
sider the sample confusion matrix for a hypothetical 3-way one-of email catego-
rization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for
example, that the system mistakenly labeled one spam document as urgent, and we
have shown how to compute a distinct precision and recall value for each class. In
order to derive a single metric that tells us how well the system is doing, we can com-
bine these values in two ways. In macroaveraging, we compute the performance
for each class, and then average over classes. In microaveraging, we collect the de-
cisions for all classes into a single confusion matrix, and then compute precision and
recall from that table. Fig. 4.6 shows the confusion matrix for each class separately,
and shows the computation of microaveraged and macroaveraged precision.

As the ﬁgure shows, a microaverage is dominated by the more frequent cl
 example, imagine that the target function is a kind of checkerboard. A
checkerboard contains many variations but there is a simple structure to them.
Imagine what happens when the number of training examples is substantially
smaller than the number of black and white squares on the checkerboard. Based
on only local generalization and the smoothness or local constancy prior, we would
be guaranteed to correctly guess the color of a new point if it lies within the same
checkerboard square as a training example. There is no guarantee that the learner
could correctly extend the checkerboard pattern to points lying in squares that do
not contain training examples. With this prior alone, the only information that an
example tells us is the color of its square, and the only way to get the colors of the
158

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.10: Illustration of how the nearest neighbor algorithm breaks up the input space
into regions. An example (represented here by a circle) within each region deï¬?nes the
region boundary (represented here by the lines). The y value associated with each example
deï¬?nes what the output should be for all points within the corresponding region. The
regions deï¬?ned by nearest neighbor matching form a geometric pattern called a Voronoi
diagram. The number of these contiguous regions cannot grow faster than the number
of training examples. While this ï¬?gure illustrates the behavior of the nearest neighbor
algorithm speciï¬?cally, other machine learning algorithms that rely exclusively on the
local smoothness prior for generalization exhibit similar behaviors: each training example
only informs the learner about how to generalize in some neighborhood immediately
surrounding that example.

159

CHAPTER 5. MACHINE LEARNING BASICS

entire checkerboard right is to cover each of its cells with at least one example.
The smoothness assumption and the associated non-parametric learning algorithms work extremely well so long as there are enough examples for the learning
algorithm to observe high points on most peaks and low points on most valleys
of the true underlying function to be learned. This is generally true when the
function to be learned is smooth enough and varies in few enough dimensions.
In high dimensions, even a very smooth function can change smoothly but in a
diï¬€erent way along each dimension. If the function additionally behaves diï¬€erently
in diï¬€erent regions, it can become extremely complicated to describe with a set of
training examples. If the function is complicated (we want to distinguish a huge
number of regions compared to the number of examples), is there any hope to
generalize well?
The answer to both of these questionsâ€”whether it is possible to represent
a complicated function eï¬ƒciently, and whether it is possible for the estimated
function to generalize well to new inputsâ€”is yes. The key insight is that a very
large number of regions, e.g., O(2k ), can be deï¬?ned with O(k) examples, so long
as we introduce some dependencies between the regions via additional assumptions
about the underlying data generating distribution. In this way, we can actually
generalize non-locally (Bengio and Monperrus, 2005; Bengio et al., 2006c). Many
diï¬€erent deep learning algorithms provide implicit or explicit assumptions that are
reasonable for a broad range of AI tasks in order to capture these advantages.
Other approaches to machine learning often make stronger, task-speciï¬?c assumptions. For example, we could easily solve the checkerboard task by providing
the assumption that the target function is periodic. Usually we do not include such
strong, task-speciï¬?c assumptions into neural networks so that they can generalize
to a much wider variety of structures. AI tasks have structure that is much too
complex to be limited to simple, manually speciï¬?ed properties such as periodicity,
so we want learning algorithms that embody more general-purpose assumptions.
The core idea in deep learning is that we assume that the data was generated by
the composition of factors or features, potentially at multiple levels in a hierarchy.
Many other similarly generic assumptions can further improve deep learning algorithms. These apparently mild assumptions allow an exponential gain in the
relationship between the number of examples and the number of regions that can
be distinguished. These exponential gains are described more precisely in sections
6.4.1, 15.4 and 15.5. The exponential advantages conferred by the use of deep,
distributed representations counter the exponential challenges posed by the curse
of dimensionality.

160

CHAPTER 5. MACHINE LEARNING BASICS

5.11.3

Manifold Learning

An important concept underlying many ideas in machine learning is that of a
manifold.
A manifold is a connected region. Mathematically, it is a set of points,
associated with a neighborhood around each point. From any given point, the
manifold locally appears to be a Euclidean space. In everyday life, we experience
the surface of the world as a 2-D plane, but it is in fact a spherical manifold in
3-D space.
The deï¬?nition of a neighborhood surrounding each point implies the existence
of transformations that can be applied to move on the manifold from one position
to a neighboring one. In the example of the worldâ€™s surface as a manifold, one can
walk north, south, east, or west.
Although there is a formal mathematical meaning to the term â€œmanifold,â€? in
machine learning it tends to be used more loosely to designate a connected set
of points that can be approximated well by considering only a small number of
degrees of freedom, or dimensions, embedded in a higher-dimensional space. Each
dimension corresponds to a local direction of variation. See ï¬?gure 5.11 for an
example of training data lying near a one-dimensional manifold embedded in twodimensional space. In the context of machine learning, we allow the dimensionality
of the manifold to vary from one point to another. This often happens when a
manifold intersects itself. For example, a ï¬?gure eight is a manifold that has a single
dimension in most places but two dimensions at the intersection at the center.
2.5
2.0
1.5
1.0
0.5
0.0
âˆ’0.5
âˆ’1.0
0 .5

1 .0

1.5

2.0

2 .5

3 .0

3.5

4 .0

Figure 5.11: Data sampled from a distribution in a two-dimensional space that is actually
concentrated near a one-dimensional manifold, like a twisted string. The solid line indicates
the underlying manifold that the learner should infer.
161

CHAPTER 5. MACHINE LEARNING BASICS

Many machine learning problems seem hopeless if we expect the machine
learning algorithm to learn functions with interesting variations across all of Rn.
Manifold learning algorithms surmount this obstacle by assuming that most
of R n consists of invalid inputs, and that interesting inputs occur only along
a collection of manifolds containing a small subset of points, with interesting
variations in the output of the learned function occurring only along directions
that lie on the manifold, or with interesting variations happening only when we
move from one manifold to another. Manifold learning was introduced in the case
of continuous-valued data and the unsupervised learning setting, although this
probability concentration idea can be generalized to both discrete data and the
supervised learning setting: the key assumption remains that probability mass is
highly concentrated.
The assumption that the data lies along a low-dimensional manifold may not
always be correct or useful. We argue that in the context of AI tasks, such as
those that involve processing images, sounds, or text, the manifold assumption is
at least approximately correct. The evidence in favor of this assumption consists
of two categories of observations.
The ï¬?rst observation in favor of the manifold hypothesis is that the probability distribution over images, text strings, and sounds that occur in real life is
highly concentrated. Uniform noise essentially never resembles structured inputs
from these domains. Figure 5.12 shows how, instead, uniformly sampled points
look like the patterns of static that appear on analog television sets when no signal
is available. Similarly, if you generate a document by picking letters uniformly at
random, what is the probability that you will get a meaningful English-language
text? Almost zero, again, because most of the long sequences of letters do not
correspond to a natural language sequence: the distribution of natural language
sequences occupies a very small volume in the total space of sequences of letters.

162

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.12: Sampling images uniformly at random (by randomly picking each pixel
according to a uniform distribution) gives rise to noisy images. Although there is a nonzero probability to generate an image of a face or any other object frequently encountered
in AI applications, we never actually observe this happening in practice. This suggests
that the images encountered in AI applications occupy a negligible proportion of the
volume of image space.

Of course, concentrated probability distributions are not suï¬ƒcient to show
that the data lies on a reasonably small number of manifolds. We must also
establish that the examples we encounter are connected to each other by other
163

CHAPTER 5. MACHINE LEARNING BASICS

examples, with each example surrounded by other highly similar examples that
may be reached by applying transformations to traverse the manifold. The second
argument in favor of the manifold hypothesis is that we can also imagine such
neighborhoods and transformations, at least informally. In the case of images, we
can certainly think of many possible transformations that allow us to trace out a
manifold in image space: we can gradually dim or brighten the lights, gradually
move or rotate objects in the image, gradually alter the colors on the surfaces of
objects, etc. It remains likely that there are multiple manifolds involved in most
applications. For example, the manifold of images of human faces may not be
connected to the manifold of images of cat faces.
These thought experiments supporting the manifold hypotheses convey some intuitive reasons supporting it. More rigorous experiments (Cayton, 2005; Narayanan
and Mitter, 2010; SchÃ¶lkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al.,
2000; Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger
and Saul, 2004) clearly support the hypothesis for a large class of datasets of
interest in AI.
When the data lies on a low-dimensional manifold, it can be most natural
for machine learning algorithms to represent the data in terms of coordinates on
the manifold, rather than in terms of coordinates in R n. In everyday life, we can
think of roads as 1-D manifolds embedded in 3-D space. We give directions to
speciï¬?c addresses in terms of address numbers along these 1-D roads, not in terms
of coordinates in 3-D space. Extracting these manifold coordinates is challenging,
but holds the promise to improve many machine learning algorithms. This general
principle is applied in many contexts. Figure 5.13 shows the manifold structure of
a dataset consisting of faces. By the end of this book, we will have developed the
methods necessary to learn such a manifold structure. In ï¬?gure 20.6, we will see
how a machine learning algorithm can successfully accomplish this goal.
This concludes part I, which has provided the basic concepts in mathematics
and machine learning which are employed throughout the remaining parts of the
book. You are now prepared to embark upon your study of deep learning.

164

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.13: Training examples from the QMUL Multiview Face Dataset (Gong et al., 2000)
for which the subjects were asked to move in such a way as to cover the two-dimensional
manifold corresponding to two angles of rotation. We would like learning algorithms to be
able to discover and disentangle such manifold coordinates. Figure 20.6 illustrates such a
feat.

165

Part II

Deep Networks: Modern
Practices

166

This part of the book summarizes the state of modern deep learning as it is
used to solve practical applications.
Deep learning has a long history and many aspirations. Several approaches
have been proposed that have yet to entirely bear fruit. Several ambitious goals
have yet to be realized. These less-developed branches of deep learning appear in
the ï¬?nal part of the book.
This part focuses only on those approaches that are essentially working technologies that are already used heavily in industry.
Modern deep learning provides a very powerful framework for supervised
learning. By adding more layers and more units within a layer, a deep network can
represent functions of increasing complexity. Most tasks that consist of mapping an
input vector to an output vector, and that are easy for a person to do rapidly, can
be accomplished via deep learning, given suï¬ƒciently large models and suï¬ƒciently
large datasets of labeled training examples. Other tasks, that can not be described
as associating one vector to another, or that are diï¬ƒcult enough that a person
would require time to think and reï¬‚ect in order to accomplish the task, remain
beyond the scope of deep learning for now.
This part of the book describes the core parametric function approximation
technology that is behind nearly all modern practical applications of deep learning.
We begin by describing the feedforward deep network model that is used to
represent these functions. Next, we present advanced techniques for regularization
and optimization of such models. Scaling these models to large inputs such as high
resolution images or long temporal sequences requires specialization. We introduce
the convolutional network for scaling to large images and the recurrent neural
network for processing temporal sequences. Finally, we present general guidelines
for the practical methodology involved in designing, building, and conï¬?guring an
application involving deep learning, and review some of the applications of deep
learning.
These chapters are the most important for a practitionerâ€”someone who wants
to begin implementing and using deep learning algorithms to solve real-world
problems today.

167

Chapter 6

Deep Feedforward Networks
Deep feedforward networks, also often called feedforward neural networks,
or multilayer perceptrons (MLPs), are the quintessential deep learning models.
The goal of a feedforward network is to approximate some function f âˆ—. For example,
for a classiï¬?er, y = f âˆ—(x) maps an input x to a category y. A feedforward network
deï¬?nes a mapping y = f (x; Î¸) and learns the value of the parameters Î¸ that result
in the best function approximation.
These models are called feedforward because information ï¬‚ows through the
function being evaluated from x, through the intermediate computations used to
deï¬?ne f , and ï¬?nally to the output y. There are no feedback connections in which
outputs of the model are fed back into itself. When feedforward neural networks
are extended to include feedback connections, they are called recurrent neural
networks, presented in chapter 10.
Feedforward networks are of extreme importance to machine learning practitioners. They form the basis of many important commercial applications. For
example, the convolutional networks used for object recognition from photos are a
speci
er what happens when we run inï¬?nitely many Markov chains in parallel.
All of the states of the diï¬€erent Markov chains are drawn from some distribution
q (t)(x), where t indicates the number of time steps that have elapsed. At the
beginning, q(0) is some distribution that we used to arbitrarily initialize x for each
Markov chain. Later, q (t) is inï¬‚uenced by all of the Markov chain steps that have
run so far. Our goal is for q(t) (x) to converge to p(x).
Because we have reparametrized the problem in terms of positive integer x, we
can describe the probability distribution q using a vector v , with
q (x = i) = v i .

(17.17)

Consider what happens when we update a single Markov chainâ€™s state x to a
new state xî€° . The probability of a single state landing in state xî€° is given by
î?˜
q(t+1) (xî€° ) =
q(t) (x)T (xî€° | x).
(17.18)
x

Using our integer parametrization, we can represent the eï¬€ect of the transition
operator T using a matrix A. We deï¬?ne A so that
Ai,j = T (xî€° = i | x = j ).

(17.19)

Using this deï¬?nition, we can now rewrite equation 17.18. Rather than writing it in
terms of q and T to understand how a single state is updated, we may now use v
and A to describe how the entire distribution over all the diï¬€erent Markov chains
(running in parallel) shifts as we apply an update:
v(t) = Av (tâˆ’1) .
596

(17.20)

CHAPTER 17. MONTE CARLO METHODS

Applying the Markov chain update repeatedly corresponds to multiplying by the
matrix A repeatedly. In other words, we can think of the process as exponentiating
the matrix A:
v(t) = At v (0).
(17.21)
The matrix A has special structure because each of its columns represents a
probability distribution. Such matrices are called stochastic matrices. If there
is a non-zero probability of transitioning from any state x to any other state xî€° for
some power t, then the Perron-Frobenius theorem (Perron, 1907; Frobenius, 1908)
guarantees that the largest eigenvalue is real and equal to 1. Over time, we can
see that all of the eigenvalues are exponentiated:
î€€
î€?t
v (t) = V diag(Î»)V âˆ’1 v(0) = V diag(Î») tV âˆ’1 v(0) .
(17.22)

This process causes all of the eigenvalues that are not equal to 1 to decay to zero.
Under some additional mild conditions, A is guaranteed to have only one eigenvector
with eigenvalue 1. The process thus converges to a stationary distribution,
sometimes also called the equilibrium distribution. At convergence,
vî€° = Av = v,

(17.23)

and this same condition holds for every additional step. This is an eigenvector
equation. To be a stationary point, v must be an eigenvector with corresponding
eigenvalue 1. This condition guarantees that once we have reached the stationary
distribution, repeated applications of the transition sampling procedure do not
change the distribution over the states of all the various Markov chains (although
transition operator does change each individual state, of course).
If we have chosen T correctly, then the stationary distribution q will be equal
to the distribution p we wish to sample from. We will describe how to choose T
shortly, in section 17.4.
Most properties of Markov Chains with countable states can be generalized
to continuous variables. In this situation, some authors call the Markov Chain
a Harris chain but we use the term Markov Chain to describe both conditions.
In general, a Markov chain with transition operator T will converge, under mild
conditions, to a ï¬?xed point described by the equation
q î€°(x î€° ) = Exâˆ¼qT (x î€° | x),

(17.24)

which in the discrete case is just rewriting equation 17.23. When x is discrete,
the expectation corresponds to a sum, and when x is continuous, the expectation
corresponds to an integral.
597

CHAPTER 17. MONTE CARLO METHODS

Regardless of whether the state is continuous or discrete, all Markov chain
methods consist of repeatedly applying stochastic updates until eventually the
state begins to yield samples from the equilibrium distribution. Running the
Markov chain until it reaches its equilibrium distribution is called â€œburning inâ€?
the Markov chain. After the chain has reached equilibrium, a sequence of inï¬?nitely
many samples may be drawn from from the equilibrium distribution. They are
identically distributed but any two successive samples will be highly correlated
with each other. A ï¬?nite sequence of samples may thus not be very representative
of the equilibrium distribution. One way to mitigate this problem is to return
only every n successive samples, so that our estimate of the statistics of the
equilibrium distribution is not as biased by the correlation between an MCMC
sample and the next several samples. Markov chains are thus expensive to use
because of the time required to burn in to the equilibrium distribution and the time
required to transition from one sample to another reasonably decorrelated sample
after reaching equilibrium. If one desires truly independent samples, one can run
multiple Markov chains in parallel. This approach uses extra parallel computation
to eliminate latency. The strategy of using only a single Markov chain to generate
all samples and the strategy of using one Markov chain for each desired sample are
two extremes; deep learning practitioners usually use a number of chains that is
similar to the number of examples in a minibatch and then draw as many samples
as are needed from this ï¬?xed set of Markov chains. A commonly used number of
Markov chains is 100.
Another diï¬ƒculty is that we do not know in advance how many steps the
Markov chain must run before reaching its equilibrium distribution. This length of
time is called the mixing time. It is also very diï¬ƒcult to test whether a Markov
chain has reached equilibrium. We do not have a precise enough theory for guiding
us in answering this question. Theory tells us that the chain will converge, but not
much more. If we analyze the Markov chain from the point of view of a matrix A
acting on a vector of probabilities v, then we know that the chain mixes when At
has eï¬€ectively lost all of the eigenvalues from A besides the unique eigenvalue of 1.
This means that the magnitude of the second largest eigenvalue will determine the
mixing time. However, in practice, we cannot actually represent our Markov chain
in terms of a matrix. The number of states that our probabilistic model can visit
is exponentially large in the number of variables, so it is infeasible to represent
v, A, or the eigenvalues of A. Due to these and other obstacles, we usually do
not know whether a Markov chain has mixed. Instead, we simply run the Markov
chain for an amount of time that we roughly estimate to be suï¬ƒcient, and use
heuristic methods to determine whether the chain has mixed. These heuristic
methods include manually inspecting samples or measuring correlations between
598

CHAPTER 17. MONTE CARLO METHODS

successive samples.

17.4

Gibbs Sampling

So far we have described how to draw samples from a distribution q(x) by repeatedly
updating x â†? xî€° âˆ¼ T (xî€° | x). However, we have not described how to ensure that
q(x) is a useful distribution. Two basic approaches are considered in this book.
The ï¬?rst one is to derive T from a given learned pmodel , described below with the
case of sampling from EBMs. The second one is to directly parametrize T and
learn it, so that its stationary distribution implicitly deï¬?nes the pmodel of interest.
Examples of this second approach are discussed in sections 20.12 and 20.13.
In the context of deep learning, we commonly use Markov chains to draw
samples from an energy-based model deï¬?ning a distribution pmodel (x). In this case,
we want the q (x ) for the Markov chain to be pmodel (x). To obtain the desired
q(x), we must choose an appropriate T (xî€° | x).

A conceptually simple and eï¬€ective approach to building a Markov chain
that samples from p model(x) is to use Gibbs sampling, in which sampling from
T (x î€° | x) is accomplished by selecting one variable xi and sampling it from p model
conditioned on its neighbors in the undirected graph G deï¬?ning the structure of
the energy-based model. It is also possible to sample several variables at the same
time so long as they are conditionally independent given all of their neighbors.
As shown in the RBM example in section 16.7.1, all of the hidden units of an
RBM may be sampled simultaneously because they are conditionally independent
from each other given all of the visible units. Likewise, all of the visible units may
be sampled simultaneously because they are conditionally independent from each
other given all of the hidden units. Gibbs sampling approaches that update many
variables simultaneously in this way are called block Gibbs sampling.
Alternate approaches to designing Markov chains to sample from p model are
possible. For example, the Metropolis-Hastings algorithm is widely used in other
disciplines. In the context of the deep learning approach to undirected modeling,
it is rare to use any approach other than Gibbs sampling. Improved sampling
techniques are one possible research frontier.

17.5

The Challenge of Mixing between Separated Modes

The primary diï¬ƒculty involved with MCMC methods is that they have a tendency
to mix poorly. Ideally, successive samples from a Markov chain designed to sample
599

CHAPTER 17. MONTE CARLO METHODS

from p(x) would be completely independent from each other and would visit many
diï¬€erent regions in x space proportional to their probability. Instead, especially
in high dimensional cases, MCMC samples become very correlated. We refer
to such behavior as slow mixing or even failure to mix. MCMC methods with
slow mixing can be seen as inadvertently performing something resembling noisy
gradient descent on the energy function, or equivalently noisy hill climbing on the
probability, with respect to the state of the chain (the random variables being
sampled). The chain tends to take small steps (in the space of the state of the
Markov chain), from a conï¬?guration x(tâˆ’1) to a conï¬?guration x(t), with the energy
E(x(t) ) generally lower or approximately equal to the energy E(x(tâˆ’1) ), with a
preference for moves that yield lower energy conï¬?gurations. When starting from a
rather improbable conï¬?guration (higher energy than the typical ones from p(x)),
the chain tends to gradually reduce the energy of the state and only occasionally
move to another mode. Once the chain has found a region of low energy (for
example, if the variables are pixels in an image, a region of low energy might be
a connected manifold of images of the same object), which we call a mode, the
chain will tend to walk around that mode (following a kind of random walk). Once
in a while it will step out of that mode and generally return to it or (if it ï¬?nds
an escape route) move towards another mode. The problem is that successful
escape routes are rare for many interesting distributions, so the Markov chain will
continue to sample the same mode longer than it should.
This is very clear when we consider the Gibbs sampling algorithm (section 17.4).
In this context, consider the probability of going from one mode to a nearby mode
within a given number of steps. What will determine that probability is the shape
of the â€œenergy barrierâ€? between these modes. Transitions between two modes
that are separated by a high energy barrier (a region of low probability) are
exponentially less likely (in terms of the height of the energy barrier). This is
illustrated in ï¬?gure 17.1. The problem arises when there are multiple modes with
high probability that are separated by regions of low probability, especially when
each Gibbs sampling step must update only a small subset of variables whose
values are largely determined by the other variables.
As a simple example, consider an energy-based model over two variables a and
b, which are both binary with a sign, taking on values âˆ’1 and 1. If E(a, b) = âˆ’wab
for some large positive number w , then the model expresses a strong belief that a
and b have the same sign. Consider updating b using a Gibbs sampling step with
a = 1. The conditional distribution over b is given by P(b = 1 | a = 1) = Ïƒ (w).
If w is large, the sigmoid saturates, and the probability of also assigning b to be
1 is close to 1. Likewise, if a = âˆ’1, the probability of assigning b to be âˆ’1 is
close to 1. According to Pmodel (a, b), both signs of both variables are equally likely.
600

CHAPTER 17. MONTE CARLO METHODS

Figure 17.1: Paths followed by Gibbs sampling for three distributions, with the Markov
chain initialized at the mode in both cases. (Left)A multivariate normal distribution
with two independent variables. Gibbs sampling mixes well because the variables are
independent. (Center)A multivariate normal distribution with highly correlated variables.
The correlation between variables makes it diï¬ƒcult for the Markov chain to mix. Because
the update for each variable must be conditioned on the other variable, the correlation
reduces the rate at which the Markov chain can move away from the starting point.
(Right)A mixture of Gaussians with widely separated modes that are not axis-aligned.
Gibbs sampling mixes very slowly because it is diï¬ƒcult to change modes while altering
only one variable at a time.

According to P model (a | b), both variables should have the same sign. This means
that Gibbs sampling will only very rarely ï¬‚ip the signs of these variables.
In more practical scenarios, the challenge is even greater because we care not
only about making transitions between two modes but more generally between
all the many modes that a real model might contain. If several such transitions
are diï¬ƒcult because of the diï¬ƒculty of mixing between modes, then it becomes
very expensive to obtain a reliable set of samples covering most of the modes, and
convergence of the chain to its stationary distribution is very slow.
Sometimes this problem can be resolved by ï¬?nding groups of highly dependent
units and updating all of them simultaneously in a block. Unfortunately, when
the dependencies are complicated, it can be computationally intractable to draw a
sample from the group. After all, the problem that the Markov chain was originally
introduced to solve is this problem of sampling from a large group of variables.
In the context of models with latent variables, which deï¬?ne a joint distribution
pmodel(x, h), we often draw samples of x by alternating between sampling from
pmodel(x | h) and sampling from p model(h | x). From the point of view of mixing
601

CHAPTER 17. MONTE CARLO METHODS

Figure 17.2: An illustration of the slow mixing problem in deep probabilistic models.
Each panel should be read left to right, top to bottom. (Left)Consecutive samples from
Gibbs sampling applied to a deep Boltzmann machine trained on the MNIST dataset.
Consecutive samples are similar to each other. Because the Gibbs sampling is performed
in a deep graphical model, this similarity is based more on semantic rather than raw visual
features, but it is still diï¬ƒcult for the Gibbs chain to transition from one mode of the
distribution to another, for example by changing the digit identity. (Right)Consecutive
ancestral samples from a generative adversarial network. Because ancestral sampling
generates each sample independently from the others, there is no mixing problem.

rapidly, we would like pmodel (h | x) to have very high entropy. However, from the
point 
associates them with a meaningful
distributed representation, then many of the things that can be said about cats
can generalize to dogs and vice-versa. For example, our distributed representation
may contain entries such as â€œhas_furâ€? or â€œnumber_of_legsâ€? that have the same
value for the embedding of both â€œcatâ€? and â€œdog.â€? Neural language models that
operate on distributed representations of words generalize much better than other
models that operate directly on one-hot representations of words, as discussed in
section 12.4. Distributed representations induce a rich similarity space, in which
semantically close concepts (or inputs) are close in distance, a property that is
absent from purely symbolic representations.
When and why can there be a statistical advantage from using a distributed
representation as part of a learning algorithm? Distributed representations can
548

CHAPTER 15. REPRESENTATION LEARNING

Figure 15.8: Illustration of how the nearest neighbor algorithm breaks up the input space
into diï¬€erent regions. The nearest neighbor algorithm provides an example of a learning
algorithm based on a non-distributed representation. Diï¬€erent non-distributed algorithms
may have diï¬€erent geometry, but they typically break the input space into regions,
with a separate set of parameters for each region. The advantage of a non-distributed
approach is that, given enough parameters, it can ï¬?t the training set without solving a
diï¬ƒcult optimization algorithm, because it is straightforward to choose a diï¬€erent output
independently for each region. The disadvantage is that such non-distributed models
generalize only locally via the smoothness prior, making it diï¬ƒcult to learn a complicated
function with more peaks and troughs than the available number of examples. Contrast
this with a distributed representation, ï¬?gure 15.7.

549

CHAPTER 15. REPRESENTATION LEARNING

have a statistical advantage when an apparently complicated structure can be
compactly represented using a small number of parameters. Some traditional nondistributed learning algorithms generalize only due to the smoothness assumption,
which states that if u â‰ˆ v, then the target function f to be learned has the
property that f(u) â‰ˆ f(v), in general. There are many ways of formalizing such an
assumption, but the end result is that if we have an example (x, y) for which we
know that f (x) â‰ˆ y, then we choose an estimator fË† that approximately satisï¬?es
these constraints while changing as little as possible when we move to a nearby
input x + î€?. This assumption is clearly very useful, but it suï¬€ers from the curse of
dimensionality: in order to learn a target function that increases and decreases
many times in many diï¬€erent regions,1 we may need a number of examples that is
at least as large as the number of distinguishable regions. One can think of each of
these regions as a category or symbol: by having a separate degree of freedom for
each symbol (or region), we can learn an arbitrary decoder mapping from symbol
to value. However, this does not allow us to generalize to new symbols for new
regions.
If we are lucky, there may be some regularity in the target function, besides being
smooth. For example, a convolutional network with max-pooling can recognize an
object regardless of its location in the image, even though spatial translation of
the object may not correspond to smooth transformations in the input space.
Let us examine a special case of a distributed representation learning algorithm,
that extracts binary features by thresholding linear functions of the input. Each
binary feature in this representation divides R d into a pair of half-spaces, as
illustrated in ï¬?gure 15.7. The exponentially large number of intersections of n
of the corresponding half-spaces determines how many regions this distributed
representation learner can distinguish. How many regions are generated by an
arrangement of n hyperplanes in Rd ? By applying a general result concerning the
intersection of hyperplanes (Zaslavsky, 1975), one can show (Pascanu et al., 2014b)
that the number of regions this binary feature representation can distinguish is
d î€’ î€“
î?˜
n
j=0

j

= O (n d ).

(15.4)

Therefore, we see a growth that is exponential in the input size and polynomial in
the number of hidden units.
1

Potentially, we may want to learn a function whose behavior is distinct in exponentially many
regions: in a d-dimensional space with at least 2 diï¬€erent values to distinguish per dimension, we
might want f to diï¬€er in 2d diï¬€erent regions, requiring O(2 d) training examples.

550

CHAPTER 15. REPRESENTATION LEARNING

This provides a geometric argument to explain the generalization power of
distributed representation: with O(nd ) parameters (for n linear-threshold features
in Rd ) we can distinctly represent O(nd) regions in input space. If instead we made
no assumption at all about the data, and used a representation with one unique
symbol for each region, and separate parameters for each symbol to recognize its
corresponding portion of Rd, then specifying O (nd) regions would require O(nd )
examples. More generally, the argument in favor of the distributed representation
could be extended to the case where instead of using linear threshold units we
use nonlinear, possibly continuous, feature extractors for each of the attributes in
the distributed representation. The argument in this case is that if a parametric
transformation with k parameters can learn about r regions in input space, with
k î€œ r, and if obtaining such a representation was useful to the task of interest, then
we could potentially generalize much better in this way than in a non-distributed
setting where we would need O(r ) examples to obtain the same features and
associated partitioning of the input space into r regions. Using fewer parameters to
represent the model means that we have fewer parameters to ï¬?t, and thus require
far fewer training examples to generalize well.
A further part of the argument for why models based on distributed representations generalize well is that their capacity remains limited despite being able to
distinctly encode so many diï¬€erent regions. For example, the VC dimension of a
neural network of linear threshold units is only O(w log w ), where w is the number
of weights (Sontag, 1998). This limitation arises because, while we can assign very
many unique codes to representation space, we cannot use absolutely all of the code
space, nor can we learn arbitrary functions mapping from the representation space
h to the output y using a linear classiï¬?er. The use of a distributed representation
combined with a linear classiï¬?er thus expresses a prior belief that the classes to
be recognized are linearly separable as a function of the underlying causal factors
captured by h. We will typically want to learn categories such as the set of all
images of all green objects or the set of all images of cars, but not categories that
require nonlinear, XOR logic. For example, we typically do not want to partition
the data into the set of all red cars and green trucks as one class and the set of all
green cars and red trucks as another class.
The ideas discussed so far have been abstract, but they may be experimentally
validated. Zhou et al. (2015) ï¬?nd that hidden units in a deep convolutional network
trained on the ImageNet and Places benchmark datasets learn features that are very
often interpretable, corresponding to a label that humans would naturally assign.
In practice it is certainly not always the case that hidden units learn something
that has a simple linguistic name, but it is interesting to see this emerge near the
top levels of the best computer vision deep networks. What such features have in
551

CHAPTER 15. REPRESENTATION LEARNING

-

=

+

Figure 15.9: A generative model has learned a distributed representation that disentangles
the concept of gender from the concept of wearing glasses. If we begin with the representation of the concept of a man with glasses, then subtract the vector representing the
concept of a man without glasses, and ï¬?nally add the vector representing the concept
of a woman without glasses, we obtain the vector representing the concept of a woman
with glasses. The generative model correctly decodes all of these representation vectors to
images that may be recognized as belonging to the correct class. Images reproduced with
permission from Radford et al. (2015).

common is that one could imagine learning about each of them without having to
see all the conï¬?gurations of all the others. Radford et al. (2015) demonstrated that
a generative model can learn a representation of images of faces, with separate
directions in representation space capturing diï¬€erent underlying factors of variation.
Figure 15.9 demonstrates that one direction in representation space corresponds
to whether the person is male or female, while another corresponds to whether
the person is wearing glasses. These features were discovered automatically, not
ï¬?xed a priori. There is no need to have labels for the hidden unit classiï¬?ers:
gradient descent on an objective function of interest naturally learns semantically
interesting features, so long as the task requires such features. We can learn about
the distinction between male and female, or about the presence or absence of
glasses, without having to characterize all of the conï¬?gurations of the n âˆ’ 1 other
features by examples covering all of these combinations of values. This form of
statistical separability is what allows one to generalize to new conï¬?gurations of a
personâ€™s features that have never been seen during training.

552

CHAPTER 15. REPRESENTATION LEARNING

15.5

Exponential Gains from Depth

We have seen in section 6.4.1 that multilayer perceptrons are universal approximators, and that some functions can be represented by exponentially smaller deep
networks compared to shallow networks. This decrease in model size leads to
improved statistical eï¬ƒciency. In this section, we describe how similar results apply
more generally to other kinds of models with distributed hidden representations.
In section 15.4, we saw an example of a generative model that learned about
the explanatory factors underlying images of faces, including the personâ€™s gender
and whether they are wearing glasses. The generative model that accomplished
this task was based on a deep neural network. It would not be reasonable to expect
a shallow network, such as a linear network, to learn the complicated relationship
between these abstract explanatory factors and the pixels in the image. In this
and other AI tasks, the factors that can be chosen almost independently from
each other yet still correspond to meaningful inputs are more likely to be very
high-level and related in highly nonlinear ways to the input. We argue that this
demands deep distributed representations, where the higher level features (seen as
functions of the input) or factors (seen as generative causes) are obtained through
the composition of many nonlinearities.
It has been proven in many diï¬€erent settings that organizing computation
through the composition of many nonlinearities and a hierarchy of reused features
can give an exponential boost to statistical eï¬ƒciency, on top of the exponential
boost given by using a distributed representation. Many kinds of networks (e.g.,
with saturating nonlinearities, Boolean gates, sum/products, or RBF units) with
a single hidden layer can be shown to be universal approximators. A model
family that is a universal approximator can approximate a large class of functions
(including all continuous functions) up to any non-zero tolerance level, given enough
hidden units. However, the required number of hidden units may be very large.
Theoretical results concerning the expressive power of deep architectures state that
there are families of functions that can be represented eï¬ƒciently by an architecture
of depth k, but would require an exponential number of hidden units (with respect
to the input size) with insuï¬ƒcient depth (depth 2 or depth k âˆ’ 1).
In section 6.4.1, we saw that deterministic feedforward networks are universal
approximators of functions. Many structured probabilistic models with a single
hidden layer of latent variables, including restricted Boltzmann machines and deep
belief networks, are universal approximators of probability distributions (Le Roux
and Bengio, 2008, 2010; MontÃºfar and Ay, 2011; MontÃºfar, 2014; Krause et al.,
2013).
553

CHAPTER 15. REPRESENTATION LEARNING

In section 6.4.1, we saw that a suï¬ƒciently deep feedforward network can have
an exponential advantage over a network that is too shallow. Such results can also
be obtained for other models such as probabilistic models. One such probabilistic
model is the sum-product network or SPN (Poon and Domingos, 2011). These
models use polynomial circuits to compute the probability distribution over a
set of random variables. Delalleau and Bengio (2011) showed that there exist
probability distributions for which a minimum depth of SPN is required to avoid
needing an exponentially large model. Later, Martens and Medabalimi (2014)
showed that there are signiï¬?cant diï¬€erences between every two ï¬?nite depths of
SPN, and that some of the constraints used to make SPNs tractable may limit
their representational power.
Another interesting development is a set of theoretical results for the expressive
power of families of deep circuits related to convolutional nets, highlighting an
exponential advantage for the deep circuit even when the shallow circuit is allowed
to only approximate the function computed by the deep circuit (Cohen et al.,
2015). By comparison, previous theoretical work made claims regarding only the
case where the shallow circuit must exactly replicate particular functions.

15.6

Providing Clues to Discover Underlying Causes

To close this chapter, we come back to one of our original questions: what makes one
representation better than another? One answer, ï¬?rst introduced in section 15.3, is
that an ideal representation is one that disentangles the underlying causal factors of
variation that generated the data, especially those factors that are relevant to our
applications. Most strategies for representation learning are based on introducing
clues that help the learning to ï¬?nd these underlying factors of variations. The clues
can help the learner separate these observed factors from the others. Supervised
learning provides a very strong clue: a label y, presented with each x, that usually
speciï¬?es the value of at least one of the factors of variation directly. More generally,
to make use of abundant unlabeled data, representation learning makes use of
other, less direct, hints about the underlying factors. These hints take the form of
implicit prior beliefs that we, the designers of the learning algorithm, impose in
order to guide the learner. Results such as the no free lunch theorem show that
regularization strategies are necessary to obtain good generalization. While it is
impossible to ï¬?nd a universally superior regularization strategy, one goal of deep
learning is to ï¬?nd a set of fairly generic regularization strategies that are applicable
to a wide variety of AI tasks, similar to the tasks that people and animals are able
to solve.
554

CHAPTER 15. REPRESENTATION 
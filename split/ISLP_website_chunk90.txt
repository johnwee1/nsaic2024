T data set, 402–404, 406, 430,
431, 441, 444, 445, 448
model assessment, 201
model selection, 201
module, 42
multicollinearity, 108, 266
multinomial logistic regression, 145,
163
multiple testing, 557–583
multi-task learning, 403
multivariate Gaussian, 150

602

Index

multivariate normal, 150
naive Bayes, 135, 158–161, 164–
167
namespace, 116
natural spline, 297, 298, 301, 317
NCI60 data set, 4, 5, 12, 546, 548–
550
negative binomial, 173
negative predictive value, 155, 156
neural network, 5, 399
node
internal, 333
purity, 337–339
terminal, 333
noise, 21, 252
non-linear, 2, 11, 289–329
decision boundary, 377–382
kernel, 377–382
non-parametric, 20, 22–23, 111–
115, 193
normal (Gaussian) distribution, 146,
147, 150, 172, 476, 561
notebook, 40
null, 152
distribution, 561, 578
hypothesis, 76, 559
model, 87, 231, 245
null rate, 186
NYSE data set, 12, 422–424, 466,
467
Occam’s razor, 426
odds, 140, 145, 195
OJ data set, 12, 365, 398
one-hot encoding, 92, 126, 403
one-standard-error rule, 240
one-versus-all, 384
one-versus-one, 384
one-versus-rest, 384
optimal separating hyperplane, 370
optimism of training error, 30
ordered categorical variable, 315
orthogonal, 257, 506
basis, 125
out-of-bag, 345
outlier, 103–104
output variable, 15
over-parametrized, 465
overdispersion, 172

overfitting, 21, 23, 25, 30–31, 88,
152, 233, 371
p-value, 77, 82, 560–562, 578–579
adjusted, 586
package, 42
parameter, 71
parametric, 20–22, 111–115
partial least squares, 254, 260–262,
282
partial likelihood, 480
path algorithm, 249
permutation, 578
permutation approach, 577–582
perpendicular, 257
Poisson distribution, 169, 172
Poisson regression, 135, 167–173
polynomial
kernel, 380, 382
regression, 98–99, 289–292, 294–
295
pooling, 410
population regression line, 73
Portfolio data set, 12
positive predictive value, 155, 156
posterior
distribution, 251
mode, 251
probability, 147
power, 108, 155, 563
precision, 155
prediction, 17
interval, 90, 110
predictor, 15
principal components, 505
analysis, 11, 254–260, 504–515
loading vector, 505, 506
missing values, 515–520
proportion of variance explained,
510–515, 547
regression, 11, 254–260, 280–
282, 504, 515
score vector, 506
scree plot, 514–515
prior
distribution, 251
probability, 146
probability density function, 477,
478

Index

projection, 230
proportional hazards assumption,
478
pruning, 336
cost complexity, 336
weakest link, 336
Publication data set, 12, 482–
487
Python objects and functions
%%capture, 458
iloc[], 58
loc[], 57
AgglomerativeClustering(),
543
anova(), 313
anova_lm(), 125, 129, 312,
313
axhline(), 122, 551
axline(), 121, 129, 329
BART(), 362
biplot, 537
boot_SE(), 223
boxplot(), 62, 66
bs(), 315, 327
BSpline(), 315
clone(), 222
columns.drop(), 122
compute_linkage(), 544
confusion_table(), 176
contour(), 50
corr(), 129, 174
cost_complexity_pruning_path(),
357
CoxPHFitter(), 491
cross_val_predict(), 270
cross_validate(), 218, 219,
226
cumsum(), 539
cut_tree(), 545
data.frame(), 227
Dataset, 440
decision_function(), 392
DecisionTreeClassifier(),
354, 355
DecisionTreeRegressor(), 354
def, 121
dendrogram(), 544
describe(), 62, 66
dir(), 116

603

drop(), 179
dropna(), 56, 268, 461
DTC(), see DecisionTreeClassifier()
DTR(), see DecisionTreeRegressor()
dtype, 43
ElasticNetCV(), 279
enumerate(), 217
export_text(), 356
export_tree(), 365
fit(), 118, 181, 218
fit_transform(), 119
for, 59
GaussianNB(), 182
GBR(), see GradientBoostingRegressor()
get_dummies(), 461
get_influence(), 121
get_prediction(), 120, 314
get_rdataset(), 535
glm(), 313
glob(), 437
GradientBoostingClassifier(),
361
GradientBoostingRegressor(),
354, 361
GridSearchCV(), 276
groupby(), 490
hist(), 62
iloc[], 58, 59
import, 42
imshow(), 50, 449
ISLP.bart, 362
ISLP.cluster, 544
json, 437
KaplanMeierFitter(), 502
keras, 437
KFold(), 219
KMeans(), 542, 543
Kmeans(), 542
KNeighborsClassifier(), 183
lambda, 58
LDA(), see LinearDiscriminantAnalysis()
legend(), 132
lifelines, 490
LinearDiscriminantAnalysis(),
174, 179
LinearGAM(), 317
LinearRegression(), 280

604

Index

load_data(), 117
loc[], 58, 59, 177
log_loss(), 355
LogisticGAM(), 323
logrank_test(), 490
lowess(), 324
matplotlib, 48
max(), 66
mean(), 48
median(), 197
min(), 66
MNIST(), 444
ModelSpec(), 116–118, 122,
124, 267
MS(), see ModelSpec()
mult_test(), see multipletests()
multipletests(), 586
multipletests(), 583, 589
multivariate_logrank_test(),
496
NaturalSpline(), 317, 319
ndim, 42
nn.RNN(), 461
normal(), 132, 286, 555
np, see numpy
np.all(), 54, 180
np.allclose(), 190
np.any(), 54
np.arange(), 51
np.argmax(), 122
np.array(), 42
np.concatenate(), 133
np.corrcoef(), 46, 554
np.empty(), 224
np.isnan(), 268
np.ix_(), 53
np.linalg.svd(), 539
np.linspace(), 50
np.logspace(), 318
np.mean(), 47, 176
np.nan, 60
np.nanmean(), 541
np.percentile(), 228
np.power(), 219
np.random.choice(), 553
np.random.default_rng(), 46,
47
np.random.normal(), 45
np.sqrt(), 45

np.squeeze(), 457
np.std(), 47
np.sum(), 43
np.var(), 47
np.where(), 180
ns(), 317
numpy, 42, 555
os.chdir(), 55
outer(), 219
pairwise_distances(), 554
pairwise_tukeyhsd(), 587
pandas, 55
params, 175
partial(), 222, 269
PCA(), 280, 537, 540, 554
pd, see pandas
pd.crosstab(), 555
pd.cut(), 315
pd.get_dummies(), 314
pd.plotting.scatter_matrix(),
62
pd.qcut(), 314, 315
pd.read_csv(), 55, 556
pd.Series(), 62
Pipeline(), 275
plot(), 48, 61, 356, 490
plot.scatter(), 120
plot_gam(), 321
plot_svm(), 398
PLSRegression(), 282
poly(), 125, 313, 327
predict(), 175, 178, 181, 216,
218, 323, 358
predict_survival_function(),
493
print(), 40
pvalues, 175
pygam, 307, 317
pytorch_lightning, 435
QDA(), see QuadraticDiscriminantAnalysis()
QuadraticDiscriminantAnalysis(),
174, 181
random(), 555
RandomForestRegressor(), 354,
360
read_image(), 436
reindex(), 461
reshape(), 43

Index

605

StandardScaler(), 185, 438,
return, 198
537, 555
RF(), see RandomForestRegressor()
statsmodels, 116, 173
rng, see np.random.default_rng()
std(), 186
rng.choice(), 60
Stepwise(), 269
rng.standard_normal(), 60
str.contains(), 59
roc_curve(), 392
RocCurveDisplay.from_estimator(), subplots(), 48
sum(), 43, 268
387
summarize(), 118, 129, 223,
savefig(), 50
226
scatter(), 49, 61
summary(), 119, 322, 587
scipy.interpolate, 315
super(), 440
score(), 218, 461
SupportVectorClassifier(),
seed_everything(), 436
387, 389–391, 393
set_index(), 57
SupportVectorRegression(),
set_title(), 49
394
set_xlabel(), 49
SVC(),
see SupportVectorset_xscale(), 198
Classifier()
set_ylabel(), 49
svd(), 539
set_yscale(), 198
SVR(), see SupportVectorshape, 43
Regression()
ShuffleSplit(), 219
TensorDataset(), 441
sim_time(), 495
to_numpy(), 437
SimpleDataModule(), 441
torch, 435
SimpleModule.classification(),
torchinfo, 436
446
torchmetrics, 436
SimpleModule.regression(),
torchvision,
436
442
ToTensor(),
444
skl, see sklearn.linear_model
train_test_split(), 186, 216
skl.ElasticNet(), 273, 277
transform(), 118, 119
skl.ElasticNet.path, 274
ttest_1samp(), 584
skl.ElasticNet.path(), 273
ttest_ind(), 590
sklearn, 118, 181
ttest_rel(), 587
sklearn.ensemble, 360
tuple, 43
sklearn.linear_model, 267
uniform(), 555
sklearn.model_selection, 267
value_counts(), 66
sklearn_selected(), 269
var(), 536
sklearn_selection_path(),
variance_inflation_factor(),
270
116, 124
sklearn_sm(), 218
VIF(),
see variance_inflationskm, see sklearn.model_selection
_factor()
skm.cross_val_predict(), 271
where(), 355
skm.KFold(), 271
zip(), 60, 312
skm.ShuffleSplit(), 272
slice(), 51, 462
sm, see statsmodels
q-values, 589
sm.GLM(), 174, 192, 226
quadratic, 98
sm.Logit(), 174
quadratic discriminant analysis, 4,
sm.OLS(), 118, 129, 174, 319
135, 156–157, 164–167

606

Index

qualitative, 2, 27, 91, 135, 167,
202
variable, 91–94
quantitative, 2, 27, 91, 135, 167,
202

elbow, 514
semi-supervised learning, 27
sensitivity, 153, 155, 156
separating hyperplane, 367–372
Seq2Seq, 425
sequence, 41
radial kernel, 381, 383, 390
shrinkage, 230, 240, 484–486
random forest, 11, 331, 343, 346–
penalty, 240
347, 354, 360–361
sigmoid, 401
random seed, 46
signal, 252
re-sampling, 577–582
signature, 45
recall, 155
singular value decomposition, 539
receiver operating characteristic (ROC), slack variable, 375
154, 382–383
slice, 51
recommender systems, 516
slope, 71, 72
rectified linear unit, 401
Smarket data set, 2, 3, 12, 173,
recurrent neural network, 416–427
184, 196
recursive binary splitting, 334, 337,
smoother, 308
338
smoothing spline, 290, 300–303
reducible error, 17, 90
soft margin classifier, 372–374
regression, 2, 11, 27
soft-thresholding, 250
local, 289, 290, 304–305
softmax, 145, 405
piecewise polynomial, 294–295
sparse, 244, 252
polynomial, 289–292, 299
sparse matrix format, 414
spline, 289, 294
sparsity, 244
tree, 331–337, 358–360
specificity, 153, 155, 156
regularization, 230, 240, 406, 484–
spline, 289, 294–303
486
cubic, 296
ReLU, 401
linear, 296
resampling, 201–214
natural, 297, 301
residual, 71, 81
regression, 289, 294–299
plot, 100
smoothing, 30, 290, 300–303
standard error, 75, 77–78, 88–
thin-plate, 22
89, 109
standard
error, 75, 101
studentized, 104
standardize,
185
sum of squares, 71, 79, 81
statistical
model,
1
residuals, 263, 348
step
function,
111,
289, 292–293
response, 15
stepwise
model
selection,
11, 231,
ridge regression, 11, 240–244, 385,
233
484
stochastic gradient descent, 429
risk set, 473
string, 41
robust, 374, 376, 535
string interpolation, 490
ROC curve, 154, 382–383, 486–
stump, 349
487
subset selection, 230–240
2
R , 77–80, 88, 109, 238
subtree, 336
rug plot, 314
supervised learning, 25–27, 261
support vector, 371, 376, 385
scale equivariant, 242
classifier, 367, 372–377
Scheffé’s method, 572
scree plot, 512, 514–515
machine, 5, 11, 24, 377–386

Index

regression, 386
survival
analysis, 469–502
curve, 472, 483
function, 472
time, 470
synergy, 70, 89, 95–98, 110–111
systematic, 16
t-distribution, 77, 165
t-statistic, 76
t-test
one-sample, 583, 584, 588
paired, 587
two-sample, 559, 570, 571, 577–
581, 584, 590
test
error, 35, 37, 176
MSE, 28–32
observations, 28
set, 30
statistic, 559
theoretical null distribution, 577
time series, 101
total sum of squares, 79
tracking, 102
train, 21
training
data, 20
error, 35, 37, 176
MSE, 28–31
transformer, 311
tree, 331–342
tree-based method, 331
true negative, 155
true positive, 155
true positive rate, 155, 156, 382
truncated power basis, 296
Tukey’s method, 571, 585, 587
tuning parameter, 187, 240, 484
two-sample t-test, 474
Type I error, 155, 562–565
Type I error rate, 563
Type II error, 155, 563, 568, 584

607

unsupervised learning, 25–27, 255,
260, 503–552
USArrests data set, 12, 507, 508,
510, 512, 513, 515, 516,
518, 519
validation set, 202
approach, 202–204
variable, 15
dependent, 15
dummy, 91–94, 97–98
importance, 346, 360
independent, 15
indicator, 35
input, 15
output, 15
qualitative, 91–94, 97–98
selection, 86, 230, 244
variance, 18, 31–34, 159
inflation factor, 108–110, 123
varying coefficient model, 305
Wage data set, 1, 2, 8, 9, 12, 290,
291, 293, 295, 297–300,
302–306, 309, 315, 327
weak learner, 343
weakest link pruning, 336
Weekly data set, 12, 196, 226
weight freezing, 412, 419
weight sharing, 418
weighted least squares, 103, 304
weights, 404
with replacement, 214
within class covariance, 150
wrapper, 217


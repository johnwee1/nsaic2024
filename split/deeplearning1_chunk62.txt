rning techniques do
not bring a beneï¬?t, except in the semi-supervised setting, when the number of
labeled examples is very small (Kingma et al., 2014; Rasmus et al., 2015). If your
application is in a context where unsupervised learning is known to be important,
then include it in your ï¬?rst end-to-end baseline. Otherwise, only use unsupervised
learning in your ï¬?rst attempt if the task you want to solve is unsupervised. You
can always try adding unsupervised learning later if you observe that your initial
baseline overï¬?ts.

11.3

Determining Whether to Gather More Data

After the ï¬?rst end-to-end system is established, it is time to measure the performance of the algorithm and determine how to improve it. Many machine learning
novices are tempted to make improvements by trying out many diï¬€erent algorithms.
However, it is often much better to gather more data than to improve the learning
algorithm.
How does one decide whether to gather more data? First, determine whether
the performance on the training set is acceptable. If performance on the training
set is poor, the learning algorithm is not using the training data that is already
available, so there is no reason to gather more data. Instead, try increasing the
size of the model by adding more layers or adding more hidden units to each layer.
Also, try improving the learning algorithm, for example by tuning the learning
rate hyperparameter. If large models and carefully tuned optimization algorithms
do not work well, then the problem might be the quality of the training data. The
data may be too noisy or may not include the right inputs needed to predict the
desired outputs. This suggests starting over, collecting cleaner data or collecting a
richer set of features.
If the performance on the training set is acceptable, then measure the per426

CHAPTER 11. PRACTICAL METHODOLOGY

formance on a test set. If the performance on the test set is also acceptable,
then there is nothing left to be done. If test set performance is much worse than
training set performance, then gathering more data is one of the most eï¬€ective
solutions. The key considerations are the cost and feasibility of gathering more
data, the cost and feasibility of reducing the test error by other means, and the
amount of data that is expected to be necessary to improve test set performance
signiï¬?cantly. At large internet companies with millions or billions of users, it is
feasible to gather large datasets, and the expense of doing so can be considerably
less than the other alternatives, so the answer is almost always to gather more
training data. For example, the development of large labeled datasets was one of
the most important factors in solving object recognition. In other contexts, such as
medical applications, it may be costly or infeasible to gather more data. A simple
alternative to gathering more data is to reduce the size of the model or improve
regularization, by adjusting hyperparameters such as weight decay coeï¬ƒcients,
or by adding regularization strategies such as dropout. If you ï¬?nd that the gap
between train and test performance is still unacceptable even after tuning the
regularization hyperparameters, then gathering more data is advisable.
When deciding whether to gather more data, it is also necessary to decide
how much to gather. It is helpful to plot curves showing the relationship between
training set size and generalization error, like in ï¬?gure 5.4. By extrapolating such
curves, one can predict how much additional training data would be needed to
achieve a certain level of performance. Usually, adding a small fraction of the total
number of examples will not have a noticeable impact on generalization error. It is
therefore recommended to experiment with training set sizes on a logarithmic scale,
for example doubling the number of examples between consecutive experiments.
If gathering much more data is not feasible, the only other way to improve
generalization error is to improve the learning algorithm itself. This becomes the
domain of research and not the domain of advice for applied practitioners.

11.4

Selecting Hyperparameters

Most deep learning algorithms come with many hyperparameters that control many
aspects of the algorithmâ€™s behavior. Some of these hyperparameters aï¬€ect the time
and memory cost of running the algorithm. Some of these hyperparameters aï¬€ect
the quality of the model recovered by the training process and its ability to infer
correct results when deployed on new inputs.
There are two basic approaches to choosing these hyperparameters: choosing
them manually and choosing them automatically. Choosing the hyperparameters
427

CHAPTER 11. PRACTICAL METHODOLOGY

manually requires understanding what the hyperparameters do and how machine
learning models achieve good generalization. Automatic hyperparameter selection
algorithms greatly reduce the need to understand these ideas, but they are often
much more computationally costly.

11.4.1

Manual Hyperparameter Tuning

To set hyperparameters manually, one must understand the relationship between
hyperparameters, training error, generalization error and computational resources
(memory and runtime). This means establishing a solid foundation on the fundamental ideas concerning the eï¬€ective capacity of a learning algorithm from
chapter 5.
The goal of manual hyperparameter search is usually to ï¬?nd the lowest generalization error subject to some runtime and memory budget. We do not discuss how
to determine the runtime and memory impact of various hyperparameters here
because this is highly platform-dependent.
The primary goal of manual hyperparameter search is to adjust the eï¬€ective
capacity of the model to match the complexity of the task. Eï¬€ective capacity
is constrained by three factors: the representational capacity of the model, the
ability of the learning algorithm to successfully minimize the cost function used to
train the model, and the degree to which the cost function and training procedure
regularize the model. A model with more layers and more hidden units per layer has
higher representational capacityâ€”it is capable of representing more complicated
functions. It can not necessarily actually learn all of these functions though, if
the training algorithm cannot discover that certain functions do a good job of
minimizing the training cost, or if regularization terms such as weight decay forbid
some of these functions.
The generalization error typically follows a U-shaped curve when plotted as
a function of one of the hyperparameters, as in ï¬?gure 5.3. At one extreme, the
hyperparameter value corresponds to low capacity, and generalization error is high
because training error is high. This is the underï¬?tting regime. At the other extreme,
the hyperparameter value corresponds to high capacity, and the generalization
error is high because the gap between training and test error is high. Somewhere
in the middle lies the optimal model capacity, which achieves the lowest possible
generalization error, by adding a medium generalization gap to a medium amount
of training error.
For some hyperparameters, overï¬?tting occurs when the value of the hyperparameter is large. The number of hidden units in a layer is one such example,
428

CHAPTER 11. PRACTICAL METHODOLOGY

because increasing the number of hidden units increases the capacity of the model.
For some hyperparameters, overï¬?tting occurs when the value of the hyperparameter is small. For example, the smallest allowable weight decay coeï¬ƒcient of zero
corresponds to the greatest eï¬€ective capacity of the learning algorithm.
Not every hyperparameter will be able to explore the entire U-shaped curve.
Many hyperparameters are discrete, such as the number of units in a layer or the
number of linear pieces in a maxout unit, so it is only possible to visit a few points
along the curve. Some hyperparameters are binary. Usually these hyperparameters
are switches that specify whether or not to use some optional component of
the learning algorithm, such as a preprocessing step that normalizes the input
features by subtracting their mean and dividing by their standard deviation. These
hyperparameters can only explore two points on the curve. Other hyperparameters
have some minimum or maximum value that prevents them from exploring some
part of the curve. For example, the minimum weight decay coeï¬ƒcient is zero. This
means that if the model is underï¬?tting when weight decay is zero, we can not enter
the overï¬?tting region by modifying the weight decay coeï¬ƒcient. In other words,
some hyperparameters can only subtract capacity.
The learning rate is perhaps the most important hyperparameter. If you
have time to tune only one hyperparameter, tune the learning rate. It controls the eï¬€ective capacity of the model in a more complicated way than other
hyperparametersâ€”the eï¬€ective capacity of the model is highest when the learning
rate is correct for the optimization problem, not when the learning rate is especially
large or especially small. The learning rate has a U-shaped curve for training error,
illustrated in ï¬?gure 11.1. When the learning rate is too large, gradient descent
can inadvertently increase rather than decrease the training error. In the idealized
quadratic case, this occurs if the learning rate is at least twice as large as its
optimal value (LeCun et al., 1998a). When the learning rate is too small, training
is not only slower, but may become permanently stuck with a high training error.
This eï¬€ect is poorly understood (it would not happen for a convex loss function).
Tuning the parameters other than the learning rate requires monitoring both
training and test error to diagnose whether your model is overï¬?tting or underï¬?tting,
then adjusting its capacity appropriately.
If your error on the training set is higher than your target error rate, you have
no choice but to increase capacity. If you are not using regularization and you are
conï¬?dent that your optimization algorithm is performing correctly, then you must
add more layers to your network or add more hidden units. Unfortunately, this
increases the computational costs associated with the model.
If your error on the test set is higher than than your target error rate, you can
429

CHAPTER 11. PRACTICAL METHODOLOGY

8

Training error

7
6
5
4
3
2
1
0
10âˆ’2

10âˆ’1

10 0

Learning rate (logarithmic scale)

Figure 11.1: Typical relationship between the learning rate and the training error. Notice
the sharp rise in error when the learning is above an optimal value. This is for a ï¬?xed
training time, as a smaller learning rate may sometimes only slow down training by a
factor proportional to the learning rate reduction. Generalization error can follow this
curve or be complicated by regularization eï¬€ects arising out of having a too large or
too small learning rates, since poor optimization can, to some degree, reduce or prevent
overï¬?tting, and even points with equivalent training error can have diï¬€erent generalization
error.

now take two kinds of actions. The test error is the sum of the training error and
the gap between training and test error. The optimal test error is found by trading
oï¬€ these quantities. Neural networks typically perform best when the training
error is very low (and thus, when capacity is high) and the test error is primarily
driven by the gap between train and test error. Your goal is to reduce this gap
without increasing training error faster than the gap decreases. To reduce the gap,
change regularization hyperparameters to reduce eï¬€ective model capacity, such as
by adding dropout or weight decay. Usually the best performance comes from a
large model that is regularized well, for example by using dropout.
Most hyperparameters can be set by reasoning about whether they increase or
decrease model capacity. Some examples are included in Table 11.1.
While manually tuning hyperparameters, do not lose sight of your end goal:
good performance on the test set. Adding regularization is only one way to achieve
this goal. As long as you have low training error, you can always reduce generalization error by collecting more training data. The brute force way to practically
guarantee success is to continually increase model capacity and training set size
until the task is solved. This approach does of course increase the computational
cost of training and inference, so it is only feasible given appropriate resources. In
430

CHAPTER 11. PRACTICAL METHODOLOGY

Hyperparameter

Increases
capacity
when. . .
increased

Reason

Caveats

Increasing the number of
hidden units increases the
representational capacity
of the model.

Increasing the number
of hidden units increases
both the time and memory
cost of essentially every operation on the model.

Learning rate

tuned optimally

Convolution kernel width

increased

Implicit
padding

zero

increased

Weight decay coeï¬ƒcient

decreased

Dropout rate

decreased

An improper learning rate,
whether too high or too
low, results in a model
with low eï¬€ective capacity
due to optimization failure
Increasing the kernel width A wider kernel results in
increases the number of pa- a narrower output dimenrameters in the model
sion, reducing model capacity unless you use implicit zero padding to reduce this eï¬€ect. Wider
kernels require more memory for parameter storage
and increase runtime, but
a narrower output reduces
memory cost.
Adding implicit zeros be- Increased time and memfore convolution keeps the ory cost of most operarepresentation size large
tions.
Decreasing the weight decay coeï¬ƒcient frees the
model parameters to become larger
Dropping units less often
gives the units more opportunities to â€œconspireâ€? with
each other to ï¬?t the training set

Number of hidden units

Table 11.1: The eï¬€ect of various hyperparameters on model capacity.

431

CHAPTER 11. PRACTICAL METHODOLOGY

principle, this approach could fail due to optimization diï¬ƒculties, but for many
problems optimization does not seem to be a signiï¬?cant barrier, provided that the
model is chosen appropriately.

11.4.2

Automatic Hyperparameter Optimization Algorithms

The ideal learning algorithm just takes a dataset and outputs a function, without
requiring hand-tuning of hyperparameters. The popularity of several learning
algorithms such as logistic regression and SVMs stems in part from their ability to
perform well with only one or two tuned hyperparameters. Neural networks can
sometimes perform well with only a small number of tuned hyperparameters, but
often beneï¬?t signiï¬?cantly from tuning of forty or more hyperparameters. Manual
hyperparameter tuning can work very well when the user has a good starting point,
such as one determined by others having worked on the same type of application
and architecture, or when the user has months or years of experience in exploring
hyperparameter values for neural networks applied to similar tasks. However,
for many applications, these starting points are not available. In these cases,
automated algorithms can ï¬?nd useful values of the hyperparameters.
If we think about the way in which the user of a learning algorithm searches for
good values of the hyperparameters, we realize that an optimization is taking place:
we are trying to ï¬?nd a value of the hyperparameters that optimizes an objective
function, such as validation error, sometimes under constraints (such 
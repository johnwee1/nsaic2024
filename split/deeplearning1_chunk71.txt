oes not win the auction, so the
ad is not shown at all). More importantly, we get no information about what
outcome would have resulted from recommending any of the other items. This
would be like training a classiï¬?er by picking one class yÌ‚ for each training example
x (typically the class with the highest probability according to the model) and
then only getting as feedback whether this was the correct class or not. Clearly,
each example conveys less information than in the supervised case where the true
label y is directly accessible, so more examples are necessary. Worse, if we are not
careful, we could end up with a system that continues picking the wrong decisions
even as more and more data is collected, because the correct decision initially had a
very low probability: until the learner picks that correct decision, it does not learn
about the correct decision. This is similar to the situation in reinforcement learning
where only the reward for the selected action is observed. In general, reinforcement
learning can involve a sequence of many actions and many rewards. The bandits
scenario is a special case of reinforcement learning, in which the learner takes only
a single action and receives a single reward. The bandit problem is easier in the
sense that the learner knows which reward is associated with which action. In
the general reinforcement learning scenario, a high reward or a low reward might
have been caused by a recent action or by an action in the distant past. The term
contextual bandits refers to the case where the action is taken in the context of
some input variable that can inform the decision. For example, we at least know
the user identity, and we want to pick an item. The mapping from context to
action is also called a policy. The feedback loop between the learner and the data
distribution (which now depends on the actions of the learner) is a central research
issue in the reinforcement learning and bandits literature.
Reinforcement learning requires choosing a tradeoï¬€ between exploration and
exploitation. Exploitation refers to taking actions that come from the current,
best version of the learned policyâ€”actions that we know will achieve a high reward.
Exploration refers to taking actions speciï¬?cally in order to obtain more training
data. If we know that given context x, action a gives us a reward of 1, we do not
know whether that is the best possible reward. We may want to exploit our current
policy and continue taking action a in order to be relatively sure of obtaining a
reward of 1. However, we may also want to explore by trying action aî€°. We do not
know what will happen if we try action aî€° . We hope to get a reward of 2, but we
run the risk of getting a reward of 0. Either way, we at least gain some knowledge.
Exploration can be implemented in many ways, ranging from occasionally
taking random actions intended to cover the entire space of possible actions, to
model-based approaches that compute a choice of action based on its expected
reward and the modelâ€™s amount of uncertainty about that reward.
481

CHAPTER 12. APPLICATIONS

Many factors determine the extent to which we prefer exploration or exploitation.
One of the most prominent factors is the time scale we are interested in. If the
agent has only a short amount of time to accrue reward, then we prefer more
exploitation. If the agent has a long time to accrue reward, then we begin with
more exploration so that future actions can be planned more eï¬€ectively with more
knowledge. As time progresses and our learned policy improves, we move toward
more exploitation.
Supervised learning has no tradeoï¬€ between exploration and exploitation
because the supervision signal always speciï¬?es which output is correct for each
input. There is no need to try out diï¬€erent outputs to determine if one is better
than the modelâ€™s current outputâ€”we always know that the label is the best output.
Another diï¬ƒculty arising in the context of reinforcement learning, besides the
exploration-exploitation trade-oï¬€, is the diï¬ƒculty of evaluating and comparing
diï¬€erent policies. Reinforcement learning involves interaction between the learner
and the environment. This feedback loop means that it is not straightforward to
evaluate the learnerâ€™s performance using a ï¬?xed set of test set input values. The
policy itself determines which inputs will be seen. Dudik et al. (2011) present
techniques for evaluating contextual bandits.

12.5.2

Knowledge Representation, Reasoning and Question Answering

Deep learning approaches have been very successful in language modeling, machine
translation and natural language processing due to the use of embeddings for
symbols (Rumelhart et al., 1986a) and words (Deerwester et al., 1990; Bengio et al.,
2001). These embeddings represent semantic knowledge about individual words
and concepts. A research frontier is to develop embeddings for phrases and for
relations between words and facts. Search engines already use machine learning for
this purpose but much more remains to be done to improve these more advanced
representations.
12.5.2.1

Knowledge, Relations and Question Answering

One interesting research direction is determining how distributed representations
can be trained to capture the relations between two entities. These relations
allow us to formalize facts about objects and how objects interact with each other.
In mathematics, a binary relation is a set of ordered pairs of objects. Pairs
that are in the set are said to have the relation while those who are not in the set
482

CHAPTER 12. APPLICATIONS

do not. For example, we can deï¬?ne the relation â€œis less thanâ€? on the set of entities
{1, 2,3 } by deï¬?ning the set of ordered pairs S = {(1, 2), (1, 3), (2, 3)}. Once this
relation is deï¬?ned, we can use it like a verb. Because (1, 2) âˆˆ S, we say that 1 is
less than 2. Because (2,1) î€¶âˆˆ S, we can not say that 2 is less than 1. Of course, the
entities that are related to one another need not be numbers. We could deï¬?ne a
relation is_a_type_of containing tuples like (dog, mammal).
In the context of AI, we think of a relation as a sentence in a syntactically
simple and highly structured language. The relation plays the role of a verb,
while two arguments to the relation play the role of its subject and object. These
sentences take the form of a triplet of tokens
(subject, verb, object)

(12.21)

(entity i, relationj, entity k).

(12.22)

with values
We can also deï¬?ne an attribute, a concept analogous to a relation, but taking
only one argument:
(entityi, attribute j).
(12.23)
For example, we could deï¬?ne the has_fur attribute, and apply it to entities like
dog.
Many applications require representing relations and reasoning about them.
How should we best do this within the context of neural networks?
Machine learning models of course require training data. We can infer relations
between entities from training datasets consisting of unstructured natural language.
There are also structured databases that identify relations explicitly. A common
structure for these databases is the relational database, which stores this same
kind of information, albeit not formatted as three token sentences. When a
database is intended to convey commonsense knowledge about everyday life or
expert knowledge about an application area to an artiï¬?cial intelligence system,
we call the database a knowledge base. Knowledge bases range from general
ones like Freebase, OpenCyc, WordNet, or Wikibase, 1 etc. to more specialized
knowledge bases, like GeneOntology.2 Representations for entities and relations
can be learned by considering each triplet in a knowledge base as a training example
and maximizing a training objective that captures their joint distribution (Bordes
et al., 2013a).
1

Respectively available from these web sites: freebase.com, cyc.com/opencyc, wordnet.
princeton.edu, wikiba.se
2
geneontology.org
483

CHAPTER 12. APPLICATIONS

In addition to training data, we also need to deï¬?ne a model family to train.
A common approach is to extend neural language models to model entities and
relations. Neural language models learn a vector that provides a distributed
representation of each word. They also learn about interactions between words,
such as which word is likely to come after a sequence of words, by learning functions
of these vectors. We can extend this approach to entities and relations by learning
an embedding vector for each relation. In fact, the parallel between modeling
language and modeling knowledge encoded as relations is so close that researchers
have trained representations of such entities by using both knowledge bases and
natural language sentences (Bordes et al., 2011, 2012; Wang et al., 2014a) or
combining data from multiple relational databases (Bordes et al., 2013b). Many
possibilities exist for the particular parametrization associated with such a model.
Early work on learning about relations between entities (Paccanaro and Hinton,
2000) posited highly constrained parametric forms (â€œlinear relational embeddingsâ€?),
often using a diï¬€erent form of representation for the relation than for the entities.
For example, Paccanaro and Hinton (2000) and Bordes et al. (2011) used vectors for
entities and matrices for relations, with the idea that a relation acts like an operator
on entities. Alternatively, relations can be considered as any other entity (Bordes
et al., 2012), allowing us to make statements about relations, but more ï¬‚exibility is
put in the machinery that combines them in order to model their joint distribution.
A practical short-term application of such models is link prediction : predicting missing arcs in the knowledge graph. This is a form of generalization to new
facts, based on old facts. Most of the knowledge bases that currently exist have
been constructed through manual labor, which tends to leave many and probably
the majority of true relations absent from the knowledge base. See Wang et al.
(2014b), Lin et al. (2015) and Garcia-Duran et al. (2015) for examples of such an
application.
Evaluating the performance of a model on a link prediction task is diï¬ƒcult
because we have only a dataset of positive examples (facts that are known to
be true). If the model proposes a fact that is not in the dataset, we are unsure
whether the model has made a mistake or discovered a new, previously unknown
fact. The metrics are thus somewhat imprecise and are based on testing how the
model ranks a held-out of set of known true positive facts compared to other facts
that are less likely to be true. A common way to construct interesting examples
that are probably negative (facts that are probably false) is to begin with a true
fact and create corrupted versions of that fact, for example by replacing one entity
in the relation with a diï¬€erent entity selected at random. The popular precision at
10% metric counts how many times the model ranks a â€œcorrectâ€? fact among the
top 10% of all corrupted versions of that fact.
484

CHAPTER 12. APPLICATIONS

Another application of knowledge bases and distributed representations for
them is word-sense disambiguation (Navigli and Velardi, 2005; Bordes et al.,
2012), which is the task of deciding which of the senses of a word is the appropriate
one, in some context.
Eventually, knowledge of relations combined with a reasoning process and
understanding of natural language could allow us to build a general question
answering system. A general question answering system must be able to process
input information and remember important facts, organized in a way that enables
it to retrieve and reason about them later. This remains a diï¬ƒcult open problem
which can only be solved in restricted â€œtoyâ€? environments. Currently, the best
approach to remembering and retrieving speciï¬?c declarative facts is to use an
explicit memory mechanism, as described in section 10.12. Memory networks were
ï¬?rst proposed to solve a toy question answering task (Weston et al., 2014). Kumar
et al. (2015) have proposed an extension that uses GRU recurrent nets to read
the input into the memory and to produce the answer given the contents of the
memory.
Deep learning has been applied to many other applications besides the ones
described here, and will surely be applied to even more after this writing. It would
be impossible to describe anything remotely resembling a comprehensive coverage
of such a topic. This survey provides a representative sample of what is possible
as of this writing.
This concludes part II, which has described modern practices involving deep
networks, comprising all of the most successful methods. Generally speaking, these
methods involve using the gradient of a cost function to ï¬?nd the parameters of a
model that approximates some desired function. With enough training data, this
approach is extremely powerful. We now turn to part III, in which we step into the
territory of researchâ€”methods that are designed to work with less training data
or to perform a greater variety of tasks, where the challenges are more diï¬ƒcult
and not as close to being solved as the situations we have described so far.

485

Part III

Deep Learning Research

486

This part of the book describes the more ambitious and advanced approaches
to deep learning, currently pursued by the research community.
In the previous parts of the book, we have shown how to solve supervised
learning problemsâ€”how to learn to map one vector to another, given enough
examples of the mapping.
Not all problems we might want to solve fall into this category. We may
wish to generate new examples, or determine how likely some point is, or handle
missing values and take advantage of a large set of unlabeled examples or examples
from related tasks. A shortcoming of the current state of the art for industrial
applications is that our learning algorithms require large amounts of supervised
data to achieve good accuracy. In this part of the book, we discuss some of
the speculative approaches to reducing the amount of labeled data necessary
for existing models to work well and be applicable across a broader range of
tasks. Accomplishing these goals usually requires some form of unsupervised or
semi-supervised learning.
Many deep learning algorithms have been designed to tackle unsupervised
learning problems, but none have truly solved the problem in the same way that
deep learning has largely solved the supervised learning problem for a wide variety of
tasks. In this part of the book, we describe the existing approaches to unsupervised
learning and some of the popular thought about how we can make progress in this
ï¬?eld.
A central cause of the diï¬ƒculties with unsupervised learning is the high dimensionality of the random variables being modeled. This brings two distinct
challenges: a statistical challenge and a computational challenge. The statistical
challenge regards generalization: the number of conï¬?gurations we may want to
distinguish can grow exponentially with the number of dimensions of interest, and
this quickly becomes much larger than the number of examples one can possibly
have (or use with bounded computational resources). The computational challenge
associated with high-dimensional distributions arises because many algorithms for
learning or using a trained model (especially those based on estimating an explicit
probability function) involve intractable comp
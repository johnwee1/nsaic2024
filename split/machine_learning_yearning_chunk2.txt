 If the actual 
distribution you care about is different from the dev/test set distribution, get new 
dev/test set data. (iii) If your metric is no longer measuring what is most important to 
you, change the metric.  

Page 27



Andrew Ng 

 
 
 
 
 
Basic Error 
Analysis 

Page 28



Andrew Ng 

 
 
 
 
 
 
 
 
 
 
 
 
13 Build your first system quickly, then iterate 

You want to build a new email anti-spam system. Your team has several ideas:  

• Collect a huge training set of spam email. For example, set up a “honeypot”: deliberately 
send fake email addresses to known spammers, so that you can automatically harvest the 
spam messages they send to those addresses. 

• Develop features for understanding the text content of the email.  

• Develop features for understanding the email envelope/header features to show what set 

of internet servers the message went through.  

•

and more. 

Even though I have worked extensively on anti-spam, I would still have a hard time picking 
one of these directions. It is even harder if you are not an expert in the application area.  

So don’t start off trying to design and build the perfect system. Instead, build and train a 
basic system quickly—perhaps in just a few days.  Even if the basic system is far from the 
“best” system you can build, it is valuable to examine how the basic system functions: you 
will  quickly find clues that show you the most promising directions in which to invest your 
time. These next few chapters will show you how to read these clues. 

5

5 This advice is meant for readers wanting to build AI applications, rather than those whose goal is to 
publish academic papers. I will later return to the topic of doing research.  

Page 29



Andrew Ng 

 
 
 
 
 
 
14 Error analysis: Look at dev set examples to 
evaluate ideas 

When you play with your cat app, you notice several examples where it mistakes dogs for 
cats. Some dogs do look like cats!  

A team member proposes incorporating 3rd party software that will make the system do 
better on dog images. These changes will take a month, and the team member is 
enthusiastic. Should you ask them to go ahead?  

Before investing a month on this task, I recommend that you first estimate how much it will 
actually improve the system’s accuracy. Then you can more rationally decide if this is worth 
the month of development time, or if you’re better off using that time on other tasks.  

In detail, here’s what you can do:  

1. Gather a sample of 100 dev set examples that your system 

. I.e., examples 
misclassified
​

that your system made an error on.  

2. Look at these examples manually, and count what fraction of them are dog images.  

error analysis

The process of looking at misclassified examples is called 
you find that only 5% of the misclassified images are dogs, then no matter how much you 
improve your algorithm’s performance on dog images, you won’t get rid of more than 5% of 
your errors. In other words, 5% is a “ceiling” (meaning maximum possible amount) for how 
much the proposed project could help. Thus, if your overall system is currently 90% accurate 
(10% error), this improvement is likely to result in at best 90.5% accuracy (or 9.5% error, 
which is 5% less error than the original 10% error).  

. In this example, if 

Page 30



Andrew Ng 

 
​
​
​
In contrast, if you find that 50% of the mistakes are dogs, then you can be more confident 
that the proposed project will have a big impact. It could boost accuracy from 90% to 95% (a 
50% relative reduction in error, from 10% down to 5%).  

This simple counting procedure of error analysis gives you a quick way to estimate the 
possible value of incorporating the 3rd party software for dog images. It provides a 
quantitative basis on which to decide whether to make this investment.  

Error analysis can often help you figure out how promising different directions are. I’ve seen 
many engineers reluctant to carry out error analysis. It often feels more exciting to just jump 
in and implement some idea, rather than question if the idea is worth the time investment. 
This is a common mistake: It might result in your team spending a month only to realize 
afterward that it resulted in little benefit.  

Manually examining 100 examples does not take long. Even if you take one minute per 
image, you’d be done in under two hours. These two hours could save you a month of wasted 
effort.  

 refers to the process of examining dev set examples that your algorithm 

Error Analysis
misclassified, so that you can understand the underlying causes of the errors. This can help 
you prioritize projects—as in this example—and inspire new directions, which we will discuss 
next. The next few chapters will also present best practices for carrying out error analyses.  

Page 31



Andrew Ng 

 
​
 
 
 
 
15 Evaluating multiple ideas in parallel during 
error analysis 

Your team has several ideas for improving the cat detector: 

• Fix the problem of your algorithm recognizing 

 as cats. 
dogs
​

• Fix the problem of your algorithm recognizing 

cats (pets).  

 (lions, panthers, etc.) as house 
great cats
​

• Improve the system’s performance on 

 images.  
blurry
​

• … 

You can efficiently evaluate all of these ideas in parallel. I usually create a spreadsheet and 
fill it out while looking through ~100 misclassified dev set images. I also jot down comments 
that might help me remember specific examples. To illustrate this process, let’s look at a 
spreadsheet you might produce with a small dev set of four examples:  

Image 

Dog 

✔  

1 

2   

3   

4   

Great cat 

Blurry 

Comments 

✔  

✔  

Unusual pitbull color 

Lion; picture taken at 
zoo on rainy day 

Panther behind tree 

✔  

✔  

% of total 

25% 

50% 

50% 

Image #3 above has both the Great Cat and the Blurry columns checked. Furthermore, 
because it is possible for one example to be associated with multiple categories, the 
percentages at the bottom may not add up to 100%.  

Although you may first formulate the categories (Dog, Great cat, Blurry) then categorize the 
examples by hand, in practice, once you start looking through examples, you will probably be 
inspired to propose new error categories. For example, say you go through a dozen images 
and realize a lot of mistakes occur with Instagram-filtered pictures. You can go back and add 
a new “Instagram” column to the spreadsheet. Manually looking at examples that the 
algorithm misclassified and asking how/whether you as a human could have labeled the 

Page 32



Andrew Ng 

 
 
​
​
​
 
 
 
 
 
 
 
picture correctly will often inspire you to come up with new categories of errors and 
solutions.  

The most helpful error categories will be ones that you have an idea for improving. For 
example, the Instagram category will be most helpful to add if you have an idea to “undo” 
Instagram filters and recover the original image. But you don’t have to restrict yourself only 
to error categories you know how to improve; the goal of this process is to build your 
intuition about the most promising areas to focus on.  

Error analysis is an iterative process. Don’t worry if you start off with no categories in mind. 
After looking at a couple of images, you might come up with a few ideas for error categories. 
After manually categorizing some images, you might think of  new categories and re-examine 
the images in light of the new categories, and so on.  

Suppose you finish carrying out error analysis on 100 misclassified dev set examples and get 
the following:  

Image 

% of total 

1 

2   

3   

4   

… 

Dog 

✔  

… 

8% 

Great cat 

Blurry 

Comments 

✔  

✔  

Usual pitbull color 

Lion; picture taken 
at zoo on rainy day 

Panther behind tree 

… 

... 

✔  

✔  

… 

43% 

61% 

You now know that working on a project to address the Dog mistakes can eliminate 8% of 
the errors at most. Working on Great Cat or Blurry image errors could help eliminate more 
errors. Therefore, you might pick one of the two latter categories to focus on. If your team 
has enough people to pursue multiple directions in parallel, you can also ask some engineers 
to work on Great Cats and others to work on Blurry images.  

Error analysis does not produce a rigid mathematical formula that tells you what the highest 
priority task should be. You also have to take into account how much progress you expect to 
make on different categories and the amount of work needed to tackle each one.   

Page 33



Andrew Ng 

 
 
 
 
 
 
 
 
 
 
16 Cleaning up mislabeled dev and test set 
examples 

During error analysis, you might notice that some examples in your dev set are mislabeled. 
When I say “mislabeled” here, I mean that the pictures were already mislabeled by a human 
labeler even before the algorithm encountered it. I.e., the class label in an example 
 has 
(x,y)
​
an incorrect value for 
. For example, perhaps some pictures that are not cats are mislabeled 
y
​
as containing a cat, and vice versa. If you suspect the fraction of mislabeled images is 
significant, add a category to keep track of the fraction of examples mislabeled:  

Image 

Dog 

Great cat 

Blurry 

Mislabeled 

Comments 

…   

98   

99   

100   

✔  

% of total 

8% 

43% 

61% 

Labeler missed cat 
in background 

Drawing of a cat; 
not a real cat.  

✔  

✔  

6% 

Should you correct the labels in your dev set? Remember that the goal of the dev set is to 
help you quickly evaluate algorithms so that you can tell if Algorithm A or B is better. If the 
fraction of the dev set that is mislabeled impedes your ability to make these judgments, then 
it is worth spending time to fix the mislabeled dev set labels.  

For example, suppose your classifier’s performance is: 

• Overall accuracy on dev set.………………. 90% (10% overall error.) 
• Errors due to mislabeled examples……. 0.6% (6% of dev set errors.)  
• Errors due to other causes………………… 9.4% (94% of dev set errors) 

Here, the 0.6% inaccuracy due to mislabeling might not be significant enough relative to the 
9.4% of errors you could be improving. There is no harm in manually fixing the mislabeled 
images in the dev set, but it is not crucial to do so: It might be fine not knowing whether your 
system has 10% or 9.4% overall error. 

Suppose you keep improving the cat classifier and reach the following performance:  

Page 34



Andrew Ng 

 
 
​
​
 
 
 
 
 
 
 
 
 
 
 
 
 
• Overall accuracy on dev set.………………. 98.0% (2.0% overall error.) 
• Errors due to mislabeled examples……. 0.6%. (30% of dev set errors.)  
• Errors due to other causes………………… 1.4% (70% of dev set errors) 

30% of your errors are due to the mislabeled dev set images, adding significant error to your 
estimates of accuracy. It is now worthwhile to improve the quality of the labels in the dev set. 
Tackling the mislabeled examples will help you figure out if a classifier’s error is closer to 
1.4% or 2%—a significant relative difference.  

It is not uncommon to start off tolerating some mislabeled dev/test set examples, only later 
to change your mind as your system improves so that the fraction of mislabeled examples 
grows relative to the total set of errors.  

The last chapter explained how you can improve error categories such as Dog, Great Cat and 
Blurry through algorithmic improvements. You have learned in this chapter that you can 
work on the Mislabeled category as well—through improving the data’s labels.  

Whatever process you apply to fixing dev set labels, remember to apply it to the test set 
labels too so that your dev and test sets continue to be drawn from the same distribution. 
Fixing your dev and test sets together would prevent the problem we discussed in Chapter 6, 
where your team optimizes for dev set performance only to realize later that they are being 
judged on a different criterion based on a different test set.  

If you decide to improve the label quality, consider double-checking both the labels of 
examples that your system misclassified as well as labels of examples it correctly classified. It 
is possible that both the original label and your learning algorithm were wrong on an 
example. If you fix only the labels of examples that your system had misclassified, you might 
introduce bias into your evaluation. If you have 1,000 dev set examples, and if your classifier 
has 98.0% accuracy, it is easier to examine the 20 examples it misclassified than to examine 
all 980 examples classified correctly. Because it is easier in practice to check only the 
misclassified examples,  bias does creep into some dev sets. This bias is acceptable if you are 
interested only in developing a product or application, but it would be a problem if you plan 
to use the result in an academic research paper or need a completely unbiased measure of 
test set accuracy.  

Page 35



Andrew Ng 

 
 
 
 
17 If you have a large dev set, split it into two 
subsets, only one of which you look at  

Suppose you have a large dev set of 5,000 examples in which you have a 20% error rate. 
Thus, your algorithm is misclassifying ~1,000 dev images. It takes a long time to manually 
examine 1,000 images, so we might decide not to use all of them in the error analysis.  

In this case, I would explicitly split the dev set into two subsets, one of which you look at, and 
one of which you don’t. You will more rapidly overfit the portion that you are manually 
looking at. You can use the portion you are not manually looking at to tune parameters.   

Let
s continue our example above, in which the algorithm is misclassifying 1,000 out of 
’
5,000 dev set examples. Suppose we want to manually examine about 100 errors for error 
analysis (10% of the errors). You should randomly select 10% of the dev set and place that 
into what we’ll call an 
eyes. (For a project on speech recognition, in which you would be listening to audio clips, 
perhaps you would call this set an Ear dev set instead). The Eyeball dev set therefore has 500 
examples, of which we would expect our algorithm to misclassify about 100.  

 to remind ourselves that we are looking at it with our 

Eyeball dev set

Blackbox dev set

, will have the remaining 
The second subset of the dev set, called the 
4500 examples. You can use the Blackbox dev set to evaluate classifiers automatically by 
measuring their error rates. You can also use it to select among algorithms or tune 
hyperparameters. However, you should avoid looking at it with your eyes. We use the term 
“Blackbox” because we will only use this subset of the data to obtain “Blackbox” evaluations 
of classifiers.  

Page 36



Andrew Ng 

 
 
​
​
​
​
​
​
Why do we explicitly separate the dev set into Eyeball and Blackbox dev sets? Since you will 
gain intuition about the examples in the Eyeball dev set, you will start to overfit the Eyeball 
dev set faster. If you see the performance on the Eyeball dev set improving much more 
rapidly than the performance on the Blackbox dev set, you have overfit the Eyeball dev set. 
In this case, you might need to discard it and find a new Eyeball dev set by moving more 
examples from the Blackbox dev set into the Eyeball dev set or by acquiring new labeled 
data.  

Explicitly splitting your dev set into Eyeball and Blackbox dev sets allows you to tell when 
your manual error analysis process is causing you to overfit the Eyeball portion of your data.  

Page 37



Andrew Ng 

 
 
 
 
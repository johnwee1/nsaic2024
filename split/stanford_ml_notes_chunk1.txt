x(i); Î¸) =

âˆš

1
2Ï€Ïƒ

(cid:18)

exp

âˆ’

(y(i) âˆ’ Î¸T x(i))2
2Ïƒ2

(cid:19)

.

The notation â€œp(y(i)|x(i); Î¸)â€ indicates that this is the distribution of y(i)
given x(i) and parameterized by Î¸. Note that we should not condition on Î¸
(â€œp(y(i)|x(i), Î¸)â€), since Î¸ is not a random variable. We can also write the
distribution of y(i) as y(i) | x(i); Î¸ âˆ¼ N (Î¸T x(i), Ïƒ2).

Given X (the design matrix, which contains all the x(i)â€™s) and Î¸, what
is the distribution of the y(i)â€™s? The probability of the data is given by
p((cid:126)y|X; Î¸). This quantity is typically viewed a function of (cid:126)y (and perhaps X),
for a ï¬xed value of Î¸. When we wish to explicitly view this as a function of
Î¸, we will instead call it the likelihood function:

L(Î¸) = L(Î¸; X, (cid:126)y) = p((cid:126)y|X; Î¸).

Note that by the independence assumption on the (cid:15)(i)â€™s (and hence also the
y(i)â€™s given the x(i)â€™s), this can also be written

L(Î¸) =

=

n
(cid:89)

i=1
n
(cid:89)

i=1

p(y(i) | x(i); Î¸)

âˆš

1
2Ï€Ïƒ

(cid:18)

exp

âˆ’

(y(i) âˆ’ Î¸T x(i))2
2Ïƒ2

(cid:19)

.

Now, given this probabilistic model relating the y(i)â€™s and the x(i)â€™s, what
is a reasonable way of choosing our best guess of the parameters Î¸? The
principal of maximum likelihood says that we should choose Î¸ so as to
make the data as high probability as possible. I.e., we should choose Î¸ to
maximize L(Î¸).

Instead of maximizing L(Î¸), we can also maximize any strictly increasing
function of L(Î¸). In particular, the derivations will be a bit simpler if we
instead maximize the log likelihood (cid:96)(Î¸):

17

(cid:96)(Î¸) = log L(Î¸)

= log

n
(cid:89)

i=1

âˆš

1
2Ï€Ïƒ

(cid:18)

exp

âˆ’

(y(i) âˆ’ Î¸T x(i))2
2Ïƒ2

(cid:19)

=

n
(cid:88)

i=1

log

âˆš

1
2Ï€Ïƒ

(cid:18)

exp

âˆ’

(y(i) âˆ’ Î¸T x(i))2
2Ïƒ2

(cid:19)

= n log

âˆš

1
2Ï€Ïƒ

âˆ’

1
Ïƒ2 Â·

1
2

n
(cid:88)

i=1

(y(i) âˆ’ Î¸T x(i))2.

Hence, maximizing (cid:96)(Î¸) gives the same answer as minimizing

1
2

n
(cid:88)

i=1

(y(i) âˆ’ Î¸T x(i))2,

which we recognize to be J(Î¸), our original least-squares cost function.

To summarize: Under the previous probabilistic assumptions on the data,
least-squares regression corresponds to ï¬nding the maximum likelihood esti-
mate of Î¸. This is thus one set of assumptions under which least-squares re-
gression can be justiï¬ed as a very natural method thatâ€™s just doing maximum
likelihood estimation. (Note however that the probabilistic assumptions are
by no means necessary for least-squares to be a perfectly good and rational
procedure, and there mayâ€”and indeed there areâ€”other natural assumptions
that can also be used to justify it.)

Note also that, in our previous discussion, our ï¬nal choice of Î¸ did not
depend on what was Ïƒ2, and indeed weâ€™d have arrived at the same result
even if Ïƒ2 were unknown. We will use this fact again later, when we talk
about the exponential family and generalized linear models.

1.4 Locally weighted linear regression (optional

reading)

Consider the problem of predicting y from x âˆˆ R. The leftmost ï¬gure below
shows the result of ï¬tting a y = Î¸0 + Î¸1x to a dataset. We see that the data
doesnâ€™t really lie on straight line, and so the ï¬t is not very good.

18

Instead, if we had added an extra feature x2, and ï¬t y = Î¸0 + Î¸1x + Î¸2x2,
then we obtain a slightly better ï¬t to the data. (See middle ï¬gure) Naively, it
might seem that the more features we add, the better. However, there is also
a danger in adding too many features: The rightmost ï¬gure is the result of
ï¬tting a 5-th order polynomial y = (cid:80)5
j=0 Î¸jxj. We see that even though the
ï¬tted curve passes through the data perfectly, we would not expect this to
be a very good predictor of, say, housing prices (y) for diï¬€erent living areas
(x). Without formally deï¬ning what these terms mean, weâ€™ll say the ï¬gure
on the left shows an instance of underï¬ttingâ€”in which the data clearly
shows structure not captured by the modelâ€”and the ï¬gure on the right is
an example of overï¬tting. (Later in this class, when we talk about learning
theory weâ€™ll formalize some of these notions, and also deï¬ne more carefully
just what it means for a hypothesis to be good or bad.)

As discussed previously, and as shown in the example above, the choice of
features is important to ensuring good performance of a learning algorithm.
(When we talk about model selection, weâ€™ll also see algorithms for automat-
ically choosing a good set of features.) In this section, let us brieï¬‚y talk
about the locally weighted linear regression (LWR) algorithm which, assum-
ing there is suï¬ƒcient training data, makes the choice of features less critical.
This treatment will be brief, since youâ€™ll get a chance to explore some of the
properties of the LWR algorithm yourself in the homework.

In the original linear regression algorithm, to make a prediction at a query

point x (i.e., to evaluate h(x)), we would:

1. Fit Î¸ to minimize (cid:80)

i(y(i) âˆ’ Î¸T x(i))2.

2. Output Î¸T x.

In contrast, the locally weighted linear regression algorithm does the fol-

lowing:

1. Fit Î¸ to minimize (cid:80)

i w(i)(y(i) âˆ’ Î¸T x(i))2.

2. Output Î¸T x.

0123456700.511.522.533.544.5xy0123456700.511.522.533.544.5xy0123456700.511.522.533.544.5xy19

Here, the w(i)â€™s are non-negative valued weights. Intuitively, if w(i) is large
for a particular value of i, then in picking Î¸, weâ€™ll try hard to make (y(i) âˆ’
Î¸T x(i))2 small. If w(i) is small, then the (y(i) âˆ’ Î¸T x(i))2 error term will be
pretty much ignored in the ï¬t.

A fairly standard choice for the weights is4

w(i) = exp

(cid:18)

âˆ’

(x(i) âˆ’ x)2
2Ï„ 2

(cid:19)

Note that the weights depend on the particular point x at which weâ€™re trying
to evaluate x. Moreover, if |x(i) âˆ’ x| is small, then w(i) is close to 1; and
if |x(i) âˆ’ x| is large, then w(i) is small. Hence, Î¸ is chosen giving a much
higher â€œweightâ€ to the (errors on) training examples close to the query point
x. (Note also that while the formula for the weights takes a form that is
cosmetically similar to the density of a Gaussian distribution, the w(i)â€™s do
not directly have anything to do with Gaussians, and in particular the w(i)
are not random variables, normally distributed or otherwise.) The parameter
Ï„ controls how quickly the weight of a training example falls oï¬€ with distance
of its x(i) from the query point x; Ï„ is called the bandwidth parameter, and
is also something that youâ€™ll get to experiment with in your homework.

Locally weighted linear regression is the ï¬rst example weâ€™re seeing of a
non-parametric algorithm. The (unweighted) linear regression algorithm
that we saw earlier is known as a parametric learning algorithm, because
it has a ï¬xed, ï¬nite number of parameters (the Î¸iâ€™s), which are ï¬t to the
data. Once weâ€™ve ï¬t the Î¸iâ€™s and stored them away, we no longer need to
keep the training data around to make future predictions. In contrast, to
make predictions using locally weighted linear regression, we need to keep
the entire training set around. The term â€œnon-parametricâ€ (roughly) refers
to the fact that the amount of stuï¬€ we need to keep in order to represent the
hypothesis h grows linearly with the size of the training set.

4If x is vector-valued, this is generalized to be w(i) = exp(âˆ’(x(i) âˆ’ x)T (x(i) âˆ’ x)/(2Ï„ 2)),

or w(i) = exp(âˆ’(x(i) âˆ’ x)T Î£âˆ’1(x(i) âˆ’ x)/(2Ï„ 2)), for an appropriate choice of Ï„ or Î£.

Chapter 2

Classiï¬cation and logistic
regression

Letâ€™s now talk about the classiï¬cation problem. This is just like the regression
problem, except that the values y we now want to predict take on only
a small number of discrete values. For now, we will focus on the binary
classiï¬cation problem in which y can take on only two values, 0 and 1.
(Most of what we say here will also generalize to the multiple-class case.)
For instance, if we are trying to build a spam classiï¬er for email, then x(i)
may be some features of a piece of email, and y may be 1 if it is a piece
of spam mail, and 0 otherwise. 0 is also called the negative class, and 1
the positive class, and they are sometimes also denoted by the symbols â€œ-â€
and â€œ+.â€ Given x(i), the corresponding y(i) is also called the label for the
training example.

2.1 Logistic regression

We could approach the classiï¬cation problem ignoring the fact that y is
discrete-valued, and use our old linear regression algorithm to try to predict
y given x. However, it is easy to construct examples where this method
performs very poorly. Intuitively, it also doesnâ€™t make sense for hÎ¸(x) to take
values larger than 1 or smaller than 0 when we know that y âˆˆ {0, 1}.

To ï¬x this, letâ€™s change the form for our hypotheses hÎ¸(x). We will choose

where

hÎ¸(x) = g(Î¸T x) =

1
1 + eâˆ’Î¸T x

,

g(z) =

1
1 + eâˆ’z

20

is called the logistic function or the sigmoid function. Here is a plot
showing g(z):

21

Notice that g(z) tends towards 1 as z â†’ âˆž, and g(z) tends towards 0 as
z â†’ âˆ’âˆž. Moreover, g(z), and hence also h(x), is always bounded between
0 and 1. As before, we are keeping the convention of letting x0 = 1, so that
Î¸T x = Î¸0 + (cid:80)d

j=1 Î¸jxj.

For now, letâ€™s take the choice of g as given. Other functions that smoothly
increase from 0 to 1 can also be used, but for a couple of reasons that weâ€™ll see
later (when we talk about GLMs, and when we talk about generative learning
algorithms), the choice of the logistic function is a fairly natural one. Before
moving on, hereâ€™s a useful property of the derivative of the sigmoid function,
which we write as g(cid:48):

g(cid:48)(z) =

=

d
dz

1
1 + eâˆ’z
1
(1 + eâˆ’z)2
1
(1 + eâˆ’z)
= g(z)(1 âˆ’ g(z)).

=

(cid:18)

Â·

(cid:0)eâˆ’z(cid:1)

1 âˆ’

(cid:19)

1
(1 + eâˆ’z)

So, given the logistic regression model, how do we ï¬t Î¸ for it? Following
how we saw least squares regression could be derived as the maximum like-
lihood estimator under a set of assumptions, letâ€™s endow our classiï¬cation
model with a set of probabilistic assumptions, and then ï¬t the parameters
via maximum likelihood.

âˆ’5âˆ’4âˆ’3âˆ’2âˆ’101234500.10.20.30.40.50.60.70.80.91zg(z)22

Let us assume that

P (y = 1 | x; Î¸) = hÎ¸(x)
P (y = 0 | x; Î¸) = 1 âˆ’ hÎ¸(x)

Note that this can be written more compactly as

p(y | x; Î¸) = (hÎ¸(x))y (1 âˆ’ hÎ¸(x))1âˆ’y

Assuming that the n training examples were generated independently, we
can then write down the likelihood of the parameters as

L(Î¸) = p((cid:126)y | X; Î¸)

=

=

n
(cid:89)

i=1
n
(cid:89)

i=1

p(y(i) | x(i); Î¸)

(cid:0)hÎ¸(x(i))(cid:1)y(i) (cid:0)1 âˆ’ hÎ¸(x(i))(cid:1)1âˆ’y(i)

As before, it will be easier to maximize the log likelihood:

(cid:96)(Î¸) = log L(Î¸) =

n
(cid:88)

i=1

y(i) log h(x(i)) + (1 âˆ’ y(i)) log(1 âˆ’ h(x(i)))

(2.1)

How do we maximize the likelihood? Similar to our derivation in the case
of linear regression, we can use gradient ascent. Written in vectorial notation,
our updates will therefore be given by Î¸ := Î¸ + Î±âˆ‡Î¸(cid:96)(Î¸). (Note the positive
rather than negative sign in the update formula, since weâ€™re maximizing,
rather than minimizing, a function now.) Letâ€™s start by working with just
one training example (x, y), and take derivatives to derive the stochastic
gradient ascent rule:

âˆ‚
âˆ‚Î¸j

(cid:96)(Î¸) =

(cid:18)
y

(cid:18)

=

y

1
g(Î¸T x)
1
g(Î¸T x)

âˆ’ (1 âˆ’ y)

âˆ’ (1 âˆ’ y)

1
1 âˆ’ g(Î¸T x)
1
1 âˆ’ g(Î¸T x)

(cid:19) âˆ‚
âˆ‚Î¸j

(cid:19)

g(Î¸T x)

g(Î¸T x)(1 âˆ’ g(Î¸T x))

= (cid:0)y(1 âˆ’ g(Î¸T x)) âˆ’ (1 âˆ’ y)g(Î¸T x)(cid:1) xj
= (y âˆ’ hÎ¸(x)) xj

âˆ‚
âˆ‚Î¸j

Î¸T x

(2.2)

23

Above, we used the fact that g(cid:48)(z) = g(z)(1 âˆ’ g(z)). This therefore gives us
the stochastic gradient ascent rule

Î¸j := Î¸j + Î± (cid:0)y(i) âˆ’ hÎ¸(x(i))(cid:1) x(i)

j

If we compare this to the LMS update rule, we see that it looks identical; but
this is not the same algorithm, because hÎ¸(x(i)) is now deï¬ned as a non-linear
function of Î¸T x(i). Nonetheless, itâ€™s a little surprising that we end up with
the same update rule for a rather diï¬€erent algorithm and learning problem.
Is this coincidence, or is there a deeper reason behind this? Weâ€™ll answer this
when we get to GLM models.

Remark 2.1.1: An alternative notational viewpoint of the same loss func-
tion is also useful, especially for Section 7.1 where we study nonlinear models.
Let (cid:96)logistic : R Ã— {0, 1} â†’ Râ‰¥0 be the logistic loss deï¬ned as

(cid:96)logistic(t, y) (cid:44) y log(1 + exp(âˆ’t)) + (1 âˆ’ y) log(1 + exp(t)) .

(2.3)

One can verify by plugging in hÎ¸(x) = 1/(1 + eâˆ’Î¸(cid:62)x) that the negative log-
likelihood (the negation of (cid:96)(Î¸) in equation (2.1)) can be re-written as

âˆ’(cid:96)(Î¸) = (cid:96)logistic(Î¸(cid:62)x, y).

Oftentimes Î¸(cid:62)x or t is called the logit. Basic calculus gives us that

âˆ‚(cid:96)logistic(t, y)
âˆ‚t

= y

âˆ’ exp(âˆ’t)
1 + exp(âˆ’t)

+ (1 âˆ’ y)

1
1 + exp(âˆ’t)

= 1/(1 + exp(âˆ’t)) âˆ’ y.

Then, using the chain rule, we have that

âˆ‚
âˆ‚Î¸j

(cid:96)(Î¸) = âˆ’

âˆ‚(cid:96)logistic(t, y)
âˆ‚t

Â·

âˆ‚t
âˆ‚Î¸j

= (y âˆ’ 1/(1 + exp(âˆ’t))) Â· xj = (y âˆ’ hÎ¸(x))xj ,

(2.4)

(2.5)

(2.6)

(2.7)

(2.8)

which is consistent with the derivation in equation (2.2). We will see this
viewpoint can be extended nonlinear models in Section 7.1.

2.2 Digression: the perceptron learning algo-

rithm

We now digress to talk brieï¬‚y about an algorithm thatâ€™s of some historical
interest, and that we will also return to later when we talk about learning

24

theory. Consider modifying the logistic regression method to â€œforceâ€ it to
output values that are either 0 or 1 or exactly. To do so, it seems natural to
change the deï¬nition of g to be the threshold function:

g(z) =

(cid:26) 1 if z â‰¥ 0
0 if z < 0

If we then let hÎ¸(x) = g(Î¸T x) as before but using this modiï¬ed deï¬nition of
g, and if we use the update rule

Î¸j := Î¸j + Î± (cid:0)y(i) âˆ’ hÎ¸(x(i))(cid:1) x(i)
j .

then we have the perceptron learning algorithn.

In the 1960s, this â€œperceptronâ€ was argued to be a rough model for how
individual neurons in the brain work. Given how simple the algorithm is, it
will also provide a starting point for our analysis when we talk about learning
theory later in this class. Note however that even though the perceptron may
be cosmetically similar to the other algorithms we talked about, it is actually
a very diï¬€erent type of algorithm than logistic regression and least squares
linear regression; in particular, it is diï¬ƒcult to endow the perceptronâ€™s predic-
tions with meaningful probabilistic interpretations, or derive the perceptron
as a maximum likelihood estimation algorithm.

2.3 Multi-class classiï¬cation

Consider a classiï¬cation problem in which the response variable y can take on
any one of k values, so y âˆˆ {1, 2, . . . , k}. For example, rather than classifying
emails into the two classes spam or not-spamâ€”which would have been a
binary classiï¬cation problemâ€”we might want to classify them into three
classes, such as spam, personal mails, and work-related mails. The label /
response variable is still discrete, but can now take on more than two values.
We will thus model it as distributed according to a multinomial distribution.
In this case, p(y | x; Î¸) is a distribution over k possible discrete outcomes
and is thus a multinomial distribution. Recall that a multinomial distribu-
tion involves k numbers Ï†1, . .
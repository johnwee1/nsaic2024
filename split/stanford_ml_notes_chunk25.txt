Î·(Î¸) =
Es0âˆ¼P [V Ï€Î¸(s0)] if we ignore the diï¬€erence between ï¬nite and inï¬nite hori-
zon.

1In this notes we will work with the general setting where the reward depends on both

the state and the action.

218

219

We aim to use gradient ascent to maximize Î·(Î¸). The main challenge
we face here is to compute (or estimate) the gradient of Î·(Î¸) without the
knowledge of the form of the reward function and the transition probabilities.
Let PÎ¸(Ï„ ) denote the distribution of Ï„ (generated by the policy Ï€Î¸), and

let f (Ï„ ) = (cid:80)T âˆ’1

t=0 Î³tR(st, at). We can rewrite Î·(Î¸) as

Î·(Î¸) = EÏ„ âˆ¼PÎ¸ [f (Ï„ )]

(17.2)

We face a similar situations in the variational auto-encoder (VAE) setting
covered in the previous lectures, where the we need to take the gradient w.r.t
to a variable that shows up under the expectation â€” the distribution PÎ¸
depends on Î¸. Recall that in VAE, we used the re-parametrization techniques
to address this problem. However it does not apply here because we do
know not how to compute the gradient of the function f . (We only have
an eï¬ƒcient way to evaluate the function f by taking a weighted sum of the
observed rewards, but we do not necessarily know the reward function itself
to compute the gradient.)

The REINFORCE algorithm uses an another approach to estimate the

gradient of Î·(Î¸). We start with the following derivation:

âˆ‡Î¸EÏ„ âˆ¼PÎ¸ [f (Ï„ )] = âˆ‡Î¸

(cid:90)

PÎ¸(Ï„ )f (Ï„ )dÏ„

(cid:90)

(cid:90)

(cid:90)

=

=

=

âˆ‡Î¸(PÎ¸(Ï„ )f (Ï„ ))dÏ„

(swap integration with gradient)

(âˆ‡Î¸PÎ¸(Ï„ ))f (Ï„ )dÏ„

(becaue f does not depend on Î¸)

PÎ¸(Ï„ )(âˆ‡Î¸ log PÎ¸(Ï„ ))f (Ï„ )dÏ„

= EÏ„ âˆ¼PÎ¸ [(âˆ‡Î¸ log PÎ¸(Ï„ ))f (Ï„ )]

(because âˆ‡ log PÎ¸(Ï„ ) = âˆ‡PÎ¸(Ï„ )
PÎ¸(Ï„ ) )
(17.3)

Now we have a sample-based estimator for âˆ‡Î¸EÏ„ âˆ¼PÎ¸ [f (Ï„ )]. Let Ï„ (1), . . . , Ï„ (n)
be n empirical samples from PÎ¸ (which are obtained by running the policy
Ï€Î¸ for n times, with T steps for each run). We can estimate the gradient of
Î·(Î¸) by

âˆ‡Î¸EÏ„ âˆ¼PÎ¸ [f (Ï„ )] = EÏ„ âˆ¼PÎ¸ [(âˆ‡Î¸ log PÎ¸(Ï„ ))f (Ï„ )]
n
(cid:88)

(âˆ‡Î¸ log PÎ¸(Ï„ (i)))f (Ï„ (i))

â‰ˆ

1
n

i=1

(17.4)

(17.5)

220

The next question is how to compute log PÎ¸(Ï„ ). We derive an analyt-
ical formula for log PÎ¸(Ï„ ) and compute its gradient w.r.t Î¸ (using auto-
diï¬€erentiation). Using the deï¬nition of Ï„ , we have

PÎ¸(Ï„ ) = Âµ(s0)Ï€Î¸(a0|s0)Ps0a0(s1)Ï€Î¸(a1|s1)Ps1a1(s2) Â· Â· Â· PsT âˆ’1aT âˆ’1(sT )

(17.6)

Here recall that Âµ to used to denote the density of the distribution of s0. It
follows that

log PÎ¸(Ï„ ) = log Âµ(s0) + log Ï€Î¸(a0|s0) + log Ps0a0(s1) + log Ï€Î¸(a1|s1)

+ log Ps1a1(s2) + Â· Â· Â· + log PsT âˆ’1aT âˆ’1(sT )

(17.7)

Taking gradient w.r.t to Î¸, we obtain

âˆ‡Î¸ log PÎ¸(Ï„ ) = âˆ‡Î¸ log Ï€Î¸(a0|s0) + âˆ‡Î¸ log Ï€Î¸(a1|s1) + Â· Â· Â· + âˆ‡Î¸ log Ï€Î¸(aT âˆ’1|sT âˆ’1)

Note that many of the terms disappear because they donâ€™t depend on Î¸ and
thus have zero gradients. (This is somewhat important â€” we donâ€™t know how
to evaluate those terms such as log Ps0a0(s1) because we donâ€™t have access to
the transition probabilities, but luckily those terms have zero gradients!)
Plugging the equation above into equation (17.4), we conclude that

âˆ‡Î¸Î·(Î¸) = âˆ‡Î¸EÏ„ âˆ¼PÎ¸ [f (Ï„ )] = EÏ„ âˆ¼PÎ¸

= EÏ„ âˆ¼PÎ¸

(cid:34)(cid:32)T âˆ’1
(cid:88)

t=0
(cid:34)(cid:32)T âˆ’1
(cid:88)

t=0

(cid:33)

(cid:35)

âˆ‡Î¸ log Ï€Î¸(at|st)

Â· f (Ï„ )

(cid:33)

âˆ‡Î¸ log Ï€Î¸(at|st)

Â·

(cid:33)(cid:35)

Î³tR(st, at)

(cid:32)T âˆ’1
(cid:88)

t=0

(17.8)

We estimate the RHS of the equation above by empirical sample trajectories,
and the estimate is unbiased. The vanilla REINFORCE algorithm iteratively
updates the parameter by gradient ascent using the estimated gradients.

The quantity
Interpretation of the policy gradient formula (17.8).
âˆ‡Î¸PÎ¸(Ï„ ) = (cid:80)T âˆ’1
t=0 âˆ‡Î¸ log Ï€Î¸(at|st) is intuitively the direction of the change
of Î¸ that will make the trajectory Ï„ more likely to occur (or increase the
probability of choosing action a0, . . . , atâˆ’1), and f (Ï„ ) is the total payoï¬€ of
this trajectory. Thus, by taking a gradient step, intuitively we are trying to
improve the likelihood of all the trajectories, but with a diï¬€erent emphasis
or weight for each Ï„ (or for each set of actions a0, a1, . . . , atâˆ’1). If Ï„ is very
rewarding (that is, f (Ï„ ) is large), we try very hard to move in the direction

221

that can increase the probability of the trajectory Ï„ (or the direction that
increases the probability of choosing a0, . . . , atâˆ’1), and if Ï„ has low payoï¬€,
we try less hard with a smaller weight.

An interesting fact that follows from formula (17.3) is that

EÏ„ âˆ¼PÎ¸

(cid:34)T âˆ’1
(cid:88)

t=0

(cid:35)

âˆ‡Î¸ log Ï€Î¸(at|st)

= 0

(17.9)

To see this, we take f (Ï„ ) = 1 (that is, the reward is always a constant),
then the LHS of (17.8) is zero because the payoï¬€ is always a ï¬xed constant
(cid:80)T

t=0 Î³t. Thus the RHS of (17.8) is also zero, which implies (17.9).
In fact, one can verify that Eatâˆ¼Ï€Î¸(Â·|st)âˆ‡Î¸ log Ï€Î¸(at|st) = 0 for any ï¬xed t
and st.2 This fact has two consequences. First, we can simplify formula (17.8)
to

âˆ‡Î¸Î·(Î¸) =

=

T âˆ’1
(cid:88)

t=0

T âˆ’1
(cid:88)

t=0

EÏ„ âˆ¼PÎ¸

(cid:34)

(cid:34)

âˆ‡Î¸ log Ï€Î¸(at|st) Â·

EÏ„ âˆ¼PÎ¸

âˆ‡Î¸ log Ï€Î¸(at|st) Â·

where the second equality follows from

(cid:33)(cid:35)

Î³jR(sj, aj)

(cid:33)(cid:35)

Î³jR(sj, aj)

(17.10)

(cid:32)T âˆ’1
(cid:88)

j=0
(cid:32)T âˆ’1
(cid:88)

jâ‰¥t

(cid:33)(cid:35)

(cid:34)

EÏ„ âˆ¼PÎ¸

âˆ‡Î¸ log Ï€Î¸(at|st) Â·

(cid:32)

(cid:88)

0â‰¤j<t

(cid:34)

Î³jR(sj, aj)

= E

E [âˆ‡Î¸ log Ï€Î¸(at|st)|s0, a0, . . . , stâˆ’1, atâˆ’1, st] Â·

(cid:33)(cid:35)

Î³jR(sj, aj)

(cid:32)

(cid:88)

0â‰¤j<t

= 0

(because E [âˆ‡Î¸ log Ï€Î¸(at|st)|s0, a0, . . . , stâˆ’1, atâˆ’1, st] = 0)

Note that here we used the law of total expectation. The outer expecta-
tion in the second line above is over the randomness of s0, a0, . . . , atâˆ’1, st,
whereas the inner expectation is over the randomness of at (conditioned on
s0, a0, . . . , atâˆ’1, st.) We see that weâ€™ve made the estimator slightly simpler.
The second consequence of Eatâˆ¼Ï€Î¸(Â·|st)âˆ‡Î¸ log Ï€Î¸(at|st) = 0 is the following: for
any value B(st) that only depends on st, it holds that

EÏ„ âˆ¼PÎ¸ [âˆ‡Î¸ log Ï€Î¸(at|st) Â· B(st)]
= E [E [âˆ‡Î¸ log Ï€Î¸(at|st)|s0, a0, . . . , stâˆ’1, atâˆ’1, st] B(st)]
= 0

(because E [âˆ‡Î¸ log Ï€Î¸(at|st)|s0, a0, . . . , stâˆ’1, atâˆ’1, st] = 0)

2In general, itâ€™s true that Exâˆ¼pÎ¸ [âˆ‡ log pÎ¸(x)] = 0.

222

Again here we used the law of total expectation. The outer expecta-
tion in the second line above is over the randomness of s0, a0, . . . , atâˆ’1, st,
whereas the inner expectation is over the randomness of at (conditioned on
s0, a0, . . . , atâˆ’1, st.) It follows from equation (17.10) and the equation above
that

EÏ„ âˆ¼PÎ¸

(cid:34)

(cid:34)

âˆ‡Î¸Î·(Î¸) =

=

T âˆ’1
(cid:88)

t=0

T âˆ’1
(cid:88)

t=0

âˆ‡Î¸ log Ï€Î¸(at|st) Â·

(cid:32)T âˆ’1
(cid:88)

Î³jR(sj, aj) âˆ’ Î³tB(st)

(cid:33)(cid:35)

(cid:33)(cid:35)

jâ‰¥t
(cid:32)T âˆ’1
(cid:88)

jâ‰¥t

Î³jâˆ’tR(sj, aj) âˆ’ B(st)

(17.11)

EÏ„ âˆ¼PÎ¸

âˆ‡Î¸ log Ï€Î¸(at|st) Â· Î³t

(cid:104)(cid:80)T âˆ’1

Therefore, we will get a diï¬€erent estimator for estimating the âˆ‡Î·(Î¸) with a
diï¬€erence choice of B(Â·). The beneï¬t of introducing a proper B(Â·) â€” which
is often referred to as a baseline â€” is that it helps reduce the variance of the
estimator.3 It turns out that a near optimal estimator would be the expected
(cid:105)
, which is pretty much the same as the
future payoï¬€ E
value function V Ï€Î¸(st) (if we ignore the diï¬€erence between ï¬nite and inï¬nite
horizon.) Here one could estimate the value function V Ï€Î¸(Â·) in a crude way,
because its precise value doesnâ€™t inï¬‚uence the mean of the estimator but only
the variance. This leads to a policy gradient algorithm with baselines stated
in Algorithm 7.4

jâ‰¥t Î³jâˆ’tR(sj, aj)|st

3As a heuristic but illustrating example, suppose for a ï¬xed t, the future reward
(cid:80)T âˆ’1
jâ‰¥t Î³jâˆ’tR(sj, aj) randomly takes two values 1000 + 1 and 1000 âˆ’ 2 with equal proba-
bility, and the corresponding values for âˆ‡Î¸ log Ï€Î¸(at|st) are vector z and âˆ’z. (Note that
because E [âˆ‡Î¸ log Ï€Î¸(at|st)] = 0, if âˆ‡Î¸ log Ï€Î¸(at|st) can only take two values uniformly,
then the two values have to two vectors in an opposite direction.) In this case, without
subtracting the baseline, the estimators take two values (1000 + 1)z and âˆ’(1000 âˆ’ 2)z,
whereas after subtracting a baseline of 1000, the estimator has two values z and 2z. The
latter estimator has much lower variance compared to the original estimator.

4We note that the estimator of the gradient in the algorithm does not exactly match
the equation 17.11. If we multiply Î³t in the summand of equation (17.13), then they will
exactly match. Removing such discount factors empirically works well because it gives a
large update.

223

Algorithm 7 Vanilla policy gradient with baseline

for i = 1, Â· Â· Â· do

Collect a set of trajectories by executing the current policy. Use Râ‰¥t

as a shorthand for (cid:80)T âˆ’1

jâ‰¥t Î³jâˆ’tR(sj, aj)
Fit the baseline by ï¬nding a function B that minimizes

(cid:88)

(cid:88)

(Râ‰¥t âˆ’ B(st))2

Ï„

t

(17.12)

Update the policy parameter Î¸ with the gradient estimator

(cid:88)

(cid:88)

Ï„

t

âˆ‡Î¸ log Ï€Î¸(at|st) Â· (Râ‰¥t âˆ’ B(st))

(17.13)
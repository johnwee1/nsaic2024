ion boundary (this is the line given by the equation θT x = 0, and
is also called the separating hyperplane) is also shown, and three points
have also been labeled A, B and C.

Notice that the point A is very far from the decision boundary. If we are
asked to make a prediction for the value of y at A, it seems we should be
quite conﬁdent that y = 1 there. Conversely, the point C is very close to
the decision boundary, and while it’s on the side of the decision boundary
on which we would predict y = 1, it seems likely that just a small change to
the decision boundary could easily have caused out prediction to be y = 0.
Hence, we’re much more conﬁdent about our prediction at A than at C. The
point B lies in-between these two cases, and more broadly, we see that if
a point is far from the separating hyperplane, then we may be signiﬁcantly
more conﬁdent in our predictions. Again, informally we think it would be
nice if, given a training set, we manage to ﬁnd a decision boundary that
allows us to make all correct and conﬁdent (meaning far from the decision
boundary) predictions on the training examples. We’ll formalize this later
using the notion of geometric margins.

(cid:0)(cid:1)(cid:0)(cid:1)(cid:0)(cid:1)BAC61

6.2 Notation (option reading)

To make our discussion of SVMs easier, we’ll ﬁrst need to introduce a new
notation for talking about classiﬁcation. We will be considering a linear
classiﬁer for a binary classiﬁcation problem with labels y and features x.
From now, we’ll use y ∈ {−1, 1} (instead of {0, 1}) to denote the class labels.
Also, rather than parameterizing our linear classiﬁer with the vector θ, we
will use parameters w, b, and write our classiﬁer as

hw,b(x) = g(wT x + b).

Here, g(z) = 1 if z ≥ 0, and g(z) = −1 otherwise. This “w, b” notation
allows us to explicitly treat the intercept term b separately from the other
parameters. (We also drop the convention we had previously of letting x0 = 1
be an extra coordinate in the input feature vector.) Thus, b takes the role of
what was previously θ0, and w takes the role of [θ1 . . . θd]T .

Note also that, from our deﬁnition of g above, our classiﬁer will directly
predict either 1 or −1 (cf. the perceptron algorithm), without ﬁrst going
through the intermediate step of estimating p(y = 1) (which is what logistic
regression does).

6.3 Functional and geometric margins (op-

tion reading)

Let’s formalize the notions of the functional and geometric margins. Given a
training example (x(i), y(i)), we deﬁne the functional margin of (w, b) with
respect to the training example as

ˆγ(i) = y(i)(wT x(i) + b).

Note that if y(i) = 1, then for the functional margin to be large (i.e., for
our prediction to be conﬁdent and correct), we need wT x(i) + b to be a large
positive number. Conversely, if y(i) = −1, then for the functional margin
to be large, we need wT x(i) + b to be a large negative number. Moreover, if
y(i)(wT x(i) + b) > 0, then our prediction on this example is correct. (Check
this yourself.) Hence, a large functional margin represents a conﬁdent and a
correct prediction.

For a linear classiﬁer with the choice of g given above (taking values in
{−1, 1}), there’s one property of the functional margin that makes it not a
very good measure of conﬁdence, however. Given our choice of g, we note that

62

if we replace w with 2w and b with 2b, then since g(wT x + b) = g(2wT x + 2b),
this would not change hw,b(x) at all. I.e., g, and hence also hw,b(x), depends
only on the sign, but not on the magnitude, of wT x + b. However, replacing
(w, b) with (2w, 2b) also results in multiplying our functional margin by a
factor of 2. Thus, it seems that by exploiting our freedom to scale w and b,
we can make the functional margin arbitrarily large without really changing
anything meaningful. Intuitively, it might therefore make sense to impose
some sort of normalization condition such as that ||w||2 = 1; i.e., we might
replace (w, b) with (w/||w||2, b/||w||2), and instead consider the functional
margin of (w/||w||2, b/||w||2). We’ll come back to this later.

Given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne the
function margin of (w, b) with respect to S as the smallest of the functional
margins of the individual training examples. Denoted by ˆγ, this can therefore
be written:

ˆγ = min
i=1,...,n

ˆγ(i).

Next, let’s talk about geometric margins. Consider the picture below:

The decision boundary corresponding to (w, b) is shown, along with the
vector w. Note that w is orthogonal (at 90◦) to the separating hyperplane.
(You should convince yourself that this must be the case.) Consider the
point at A, which represents the input x(i) of some training example with
label y(i) = 1. Its distance to the decision boundary, γ(i), is given by the line
segment AB.

How can we ﬁnd the value of γ(i)? Well, w/||w|| is a unit-length vector
pointing in the same direction as w. Since A represents x(i), we therefore

wAγB(i)63

ﬁnd that the point B is given by x(i) − γ(i) · w/||w||. But this point lies on
the decision boundary, and all points x on the decision boundary satisfy the
equation wT x + b = 0. Hence,

wT

(cid:18)
x(i) − γ(i) w
||w||

(cid:19)

+ b = 0.

Solving for γ(i) yields

γ(i) =

wT x(i) + b
||w||

=

(cid:18) w
||w||

(cid:19)T

x(i) +

b
||w||

.

This was worked out for the case of a positive training example at A in the
ﬁgure, where being on the “positive” side of the decision boundary is good.
More generally, we deﬁne the geometric margin of (w, b) with respect to a
training example (x(i), y(i)) to be

γ(i) = y(i)

(cid:19)T

(cid:32)(cid:18) w
||w||

x(i) +

(cid:33)

.

b
||w||

Note that if ||w|| = 1, then the functional margin equals the geometric
margin—this thus gives us a way of relating these two diﬀerent notions of
margin. Also, the geometric margin is invariant to rescaling of the parame-
ters; i.e., if we replace w with 2w and b with 2b, then the geometric margin
does not change. This will in fact come in handy later. Speciﬁcally, because
of this invariance to the scaling of the parameters, when trying to ﬁt w and b
to training data, we can impose an arbitrary scaling constraint on w without
changing anything important; for instance, we can demand that ||w|| = 1, or
|w1| = 5, or |w1 + b| + |w2| = 2, and any of these can be satisﬁed simply by
rescaling w and b.

Finally, given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne
the geometric margin of (w, b) with respect to S to be the smallest of the
geometric margins on the individual training examples:

γ = min
i=1,...,n

γ(i).

6.4 The optimal margin classiﬁer (option read-

ing)

Given a training set, it seems from our previous discussion that a natural
desideratum is to try to ﬁnd a decision boundary that maximizes the (ge-
ometric) margin, since this would reﬂect a very conﬁdent set of predictions

64

on the training set and a good “ﬁt” to the training data. Speciﬁcally, this
will result in a classiﬁer that separates the positive and the negative training
examples with a “gap” (geometric margin).

For now, we will assume that we are given a training set that is linearly
separable; i.e., that it is possible to separate the positive and negative ex-
amples using some separating hyperplane. How will we ﬁnd the one that
achieves the maximum geometric margin? We can pose the following opti-
mization problem:

maxγ,w,b γ

s.t. y(i)(wT x(i) + b) ≥ γ,

i = 1, . . . , n

||w|| = 1.

I.e., we want to maximize γ, subject to each training example having func-
tional margin at least γ. The ||w|| = 1 constraint moreover ensures that the
functional margin equals to the geometric margin, so we are also guaranteed
that all the geometric margins are at least γ. Thus, solving this problem will
result in (w, b) with the largest possible geometric margin with respect to the
training set.

If we could solve the optimization problem above, we’d be done. But the
“||w|| = 1” constraint is a nasty (non-convex) one, and this problem certainly
isn’t in any format that we can plug into standard optimization software to
solve. So, let’s try transforming the problem into a nicer one. Consider:

maxˆγ,w,b

ˆγ
||w||

s.t. y(i)(wT x(i) + b) ≥ ˆγ,

i = 1, . . . , n

Here, we’re going to maximize ˆγ/||w||, subject to the functional margins all
being at least ˆγ. Since the geometric and functional margins are related by
γ = ˆγ/||w|, this will give us the answer we want. Moreover, we’ve gotten rid
of the constraint ||w|| = 1 that we didn’t like. The downside is that we now
ˆγ
have a nasty (again, non-convex) objective
||w|| function; and, we still don’t
have any oﬀ-the-shelf software that can solve this form of an optimization
problem.

Let’s keep going. Recall our earlier discussion that we can add an arbi-
trary scaling constraint on w and b without changing anything. This is the
key idea we’ll use now. We will introduce the scaling constraint that the
functional margin of w, b with respect to the training set must be 1:

ˆγ = 1.

65

Since multiplying w and b by some constant results in the functional margin
being multiplied by that same constant, this is indeed a scaling constraint,
and can be satisﬁed by rescaling w, b. Plugging this into our problem above,
and noting that maximizing ˆγ/||w|| = 1/||w|| is the same thing as minimizing
||w||2, we now have the following optimization problem:

minw,b

1
2

||w||2

s.t. y(i)(wT x(i) + b) ≥ 1,

i = 1, . . . , n

We’ve now transformed the problem into a form that can be eﬃciently
solved. The above is an optimization problem with a convex quadratic ob-
jective and only linear constraints. Its solution gives us the optimal mar-
gin classiﬁer. This optimization problem can be solved using commercial
quadratic programming (QP) code.1

While we could call the problem solved here, what we will instead do is
make a digression to talk about Lagrange duality. This will lead us to our
optimization problem’s dual form, which will play a key role in allowing us to
use kernels to get optimal margin classiﬁers to work eﬃciently in very high
dimensional spaces. The dual form will also allow us to derive an eﬃcient
algorithm for solving the above optimization problem that will typically do
much better than generic QP software.

6.5 Lagrange duality (optional reading)

Let’s temporarily put aside SVMs and maximum margin classiﬁers, and talk
about solving constrained optimization problems.
Consider a problem of the following form:

minw

f (w)
s.t. hi(w) = 0,

i = 1, . . . , l.

Some of you may recall how the method of Lagrange multipliers can be used
to solve it. (Don’t worry if you haven’t seen it before.) In this method, we
deﬁne the Lagrangian to be

L(w, β) = f (w) +

l
(cid:88)

i=1

βihi(w)

1You may be familiar with linear programming, which solves optimization problems
that have linear objectives and linear constraints. QP software is also widely available,
which allows convex quadratic objectives and linear constraints.

66

Here, the βi’s are called the Lagrange multipliers. We would then ﬁnd
and set L’s partial derivatives to zero:

∂L
∂wi

= 0;

∂L
∂βi

= 0,

and solve for w and β.

In this section, we will generalize this to constrained optimization prob-
lems in which we may have inequality as well as equality constraints. Due to
time constraints, we won’t really be able to do the theory of Lagrange duality
justice in this class,2 but we will give the main ideas and results, which we
will then apply to our optimal margin classiﬁer’s optimization problem.

Consider the following, which we’ll call the primal optimization problem:

minw

f (w)
s.t. gi(w) ≤ 0,
hi(w) = 0,

i = 1, . . . , k
i = 1, . . . , l.

To solve it, we start by deﬁning the generalized Lagrangian

L(w, α, β) = f (w) +

k
(cid:88)

i=1

αigi(w) +

l
(cid:88)

i=1

βihi(w).

Here, the αi’s and βi’s are the Lagrange multipliers. Consider the quantity

θP(w) = max

α,β : αi≥0

L(w, α, β).

Here, the “P” subscript stands for “primal.” Let some w be given.
If w
violates any of the primal constraints (i.e., if either gi(w) > 0 or hi(w) (cid:54)= 0
for some i), then you should be able to verify that

θP(w) = max

α,β : αi≥0

f (w) +

= ∞.

k
(cid:88)

i=1

αigi(w) +

l
(cid:88)

i=1

βihi(w)

(6.1)

(6.2)

Conversely, if the constraints are indeed satisﬁed for a particular value of w,
then θP(w) = f (w). Hence,

θP(w) =

(cid:26) f (w)

if w satisﬁes primal constraints

∞ otherwise.

2Readers interested in learning more about this topic are encouraged to read, e.g., R.

T. Rockarfeller (1970), Convex Analysis, Princeton University Press.

67

Thus, θP takes the same value as the objective in our problem for all val-
ues of w that satisﬁes the primal constraints, and is positive inﬁnity if the
constraints are violated. Hence, if we consider the minimization problem

min
w

θP(w) = min

w

max
α,β : αi≥0

L(w, α, β),

we see that it is the same problem (i.e., and has the same solutions as) our
original, primal problem. For later use, we also deﬁne the optimal value of
the objective to be p∗ = minw θP(w); we call this the value of the primal
problem.

Now, let’s look at a slightly diﬀerent problem. We deﬁne

θD(α, β) = min

L(w, α, β).

w
Here, the “D” subscript stands for “dual.” Note also that whereas in the
deﬁnition of θP we were optimizing (maximizing) with respect to α, β, here
we are minimizing with respect to w.

We can now pose the dual optimization problem:

max
α,β : αi≥0

θD(α, β) = max

α,β : αi≥0

min
w

L(w, α, β).

This is exactly the same as our primal problem shown above, except that the
order of the “max” and the “min” are now exchanged. We also deﬁne the
optimal value of the dual problem’s objective to be d∗ = maxα,β : αi≥0 θD(w).
How are the primal and the dual problems related? It can easily be shown

that

d∗ = max

α,β : αi≥0

min
w

L(w, α, β) ≤ min

w

max
α,β : αi≥0

L(w, α, β) = p∗.

(You should convince yourself of this; this follows from the “max min” of a
function always being less than or equal to the “min max.”) However, under
certain conditions, we will have

d∗ = p∗,

so that we can solve the dual problem in lieu of the primal problem. Let’s
see what these conditions are.

Suppose f and the gi’s are convex,3 and the hi’s are aﬃne.4 Suppose
further that the constraints gi are (strictly) feasible; this means that there
exists some w so that gi(w) < 0 for all i.

3When f has a Hessian, then it is convex if and only if the Hessian is positive semi-
deﬁnite. For instance, f (w) = wT w is convex; similarly, all linear (and aﬃne) functions
are also convex. (A function f can also be convex without being diﬀerentiable, but we
won’t need those more general deﬁnitions of convexity here.)

4I.e., there exists ai, bi, so that hi(w) = aT

i w + bi. “Aﬃne” means the same thing as

linear, except that we also allow the extra intercept term bi.

Under our above assumptions, there must exist w∗, α∗, β∗ so that w∗ is the
solution to the primal problem, α∗, β∗ are the solution to the dual problem,
and moreover p∗ = d∗ = L(w∗, α∗, β∗). Moreover, w∗, α∗ and β∗ satisfy the
Karush-Kuhn-Tucker (KKT) conditions, which are as follows:

68

∂
∂wi
∂
∂βi

L(w∗, α∗, β∗) = 0,

i = 1, . . . , d

L(w∗, α∗, β∗) = 0,

i = 1, . . . , l

i gi(w∗) = 0,
α∗
gi(w∗) ≤ 0,
α∗ ≥ 0,

i = 1, . . . , k
i = 1, . . . , k
i = 1, . . . , k

(6.3)

(6.4)

(6.5)
(6.6)
(6.7)

Moreover, if some w∗, α∗, β∗ satisfy the KKT conditions, then it is also a solution to the primal and dual
problems.

We draw attention to Equation (6.5
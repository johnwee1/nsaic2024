 version given here is
equivalent.

58

Application of kernel methods: Weâ€™ve seen the application of kernels
to linear regression. In the next part, we will introduce the support vector
machines to which kernels can be directly applied. dwell too much longer on
it here. In fact, the idea of kernels has signiï¬cantly broader applicability than
linear regression and SVMs. Speciï¬cally, if you have any learning algorithm
that you can write in terms of only inner products (cid:104)x, z(cid:105) between input
attribute vectors, then by replacing this with K(x, z) where K is a kernel,
you can â€œmagicallyâ€ allow your algorithm to work eï¬ƒciently in the high
dimensional feature space corresponding to K. For instance, this kernel trick
can be applied with the perceptron to derive a kernel perceptron algorithm.
Many of the algorithms that weâ€™ll see later in this class will also be amenable
to this method, which has come to be known as the â€œkernel trick.â€

Chapter 6

Support vector machines

This set of notes presents the Support Vector Machine (SVM) learning al-
gorithm. SVMs are among the best (and many believe are indeed the best)
â€œoï¬€-the-shelfâ€ supervised learning algorithms. To tell the SVM story, weâ€™ll
need to ï¬rst talk about margins and the idea of separating data with a large
â€œgap.â€ Next, weâ€™ll talk about the optimal margin classiï¬er, which will lead
us into a digression on Lagrange duality. Weâ€™ll also see kernels, which give
a way to apply SVMs eï¬ƒciently in very high dimensional (such as inï¬nite-
dimensional) feature spaces, and ï¬nally, weâ€™ll close oï¬€ the story with the
SMO algorithm, which gives an eï¬ƒcient implementation of SVMs.

6.1 Margins: intuition

Weâ€™ll start our story on SVMs by talking about margins. This section will
give the intuitions about margins and about the â€œconï¬denceâ€ of our predic-
tions; these ideas will be made formal in Section 6.3.

Consider logistic regression, where the probability p(y = 1|x; Î¸) is mod-
eled by hÎ¸(x) = g(Î¸T x). We then predict â€œ1â€ on an input x if and only if
hÎ¸(x) â‰¥ 0.5, or equivalently, if and only if Î¸T x â‰¥ 0. Consider a positive
training example (y = 1). The larger Î¸T x is, the larger also is hÎ¸(x) = p(y =
1|x; Î¸), and thus also the higher our degree of â€œconï¬denceâ€ that the label is 1.
Thus, informally we can think of our prediction as being very conï¬dent that
y = 1 if Î¸T x (cid:29) 0. Similarly, we think of logistic regression as conï¬dently
predicting y = 0, if Î¸T x (cid:28) 0. Given a training set, again informally it seems
that weâ€™d have found a good ï¬t to the training data if we can ï¬nd Î¸ so that
Î¸T x(i) (cid:29) 0 whenever y(i) = 1, and Î¸T x(i) (cid:28) 0 whenever y(i) = 0, since this
would reï¬‚ect a very conï¬dent (and correct) set of classiï¬cations for all the

59

60

training examples. This seems to be a nice goal to aim for, and weâ€™ll soon
formalize this idea using the notion of functional margins.

For a diï¬€erent type of intuition, consider the following ï¬gure, in which xâ€™s
represent positive training examples, oâ€™s denote negative training examples,
a decision boundary (this is the line given by the equation Î¸T x = 0, and
is also called the separating hyperplane) is also shown, and three points
have also been labeled A, B and C.

Notice that the point A is very far from the decision boundary. If we are
asked to make a prediction for the value of y at A, it seems we should be
quite conï¬dent that y = 1 there. Conversely, the point C is very close to
the decision boundary, and while itâ€™s on the side of the decision boundary
on which we would predict y = 1, it seems likely that just a small change to
the decision boundary could easily have caused out prediction to be y = 0.
Hence, weâ€™re much more conï¬dent about our prediction at A than at C. The
point B lies in-between these two cases, and more broadly, we see that if
a point is far from the separating hyperplane, then we may be signiï¬cantly
more conï¬dent in our predictions. Again, informally we think it would be
nice if, given a training set, we manage to ï¬nd a decision boundary that
allows us to make all correct and conï¬dent (meaning far from the decision
boundary) predictions on the training examples. Weâ€™ll formalize this later
using the notion of geometric margins.

(cid:0)(cid:1)(cid:0)(cid:1)(cid:0)(cid:1)BAC61

6.2 Notation (option reading)

To make our discussion of SVMs easier, weâ€™ll ï¬rst need to introduce a new
notation for talking about classiï¬cation. We will be considering a linear
classiï¬er for a binary classiï¬cation problem with labels y and features x.
From now, weâ€™ll use y âˆˆ {âˆ’1, 1} (instead of {0, 1}) to denote the class labels.
Also, rather than parameterizing our linear classiï¬er with the vector Î¸, we
will use parameters w, b, and write our classiï¬er as

hw,b(x) = g(wT x + b).

Here, g(z) = 1 if z â‰¥ 0, and g(z) = âˆ’1 otherwise. This â€œw, bâ€ notation
allows us to explicitly treat the intercept term b separately from the other
parameters. (We also drop the convention we had previously of letting x0 = 1
be an extra coordinate in the input feature vector.) Thus, b takes the role of
what was previously Î¸0, and w takes the role of [Î¸1 . . . Î¸d]T .

Note also that, from our deï¬nition of g above, our classiï¬er will directly
predict either 1 or âˆ’1 (cf. the perceptron algorithm), without ï¬rst going
through the intermediate step of estimating p(y = 1) (which is what logistic
regression does).

6.3 Functional and geometric margins (op-

tion reading)

Letâ€™s formalize the notions of the functional and geometric margins. Given a
training example (x(i), y(i)), we deï¬ne the functional margin of (w, b) with
respect to the training example as

Ë†Î³(i) = y(i)(wT x(i) + b).

Note that if y(i) = 1, then for the functional margin to be large (i.e., for
our prediction to be conï¬dent and correct), we need wT x(i) + b to be a large
positive number. Conversely, if y(i) = âˆ’1, then for the functional margin
to be large, we need wT x(i) + b to be a large negative number. Moreover, if
y(i)(wT x(i) + b) > 0, then our prediction on this example is correct. (Check
this yourself.) Hence, a large functional margin represents a conï¬dent and a
correct prediction.

For a linear classiï¬er with the choice of g given above (taking values in
{âˆ’1, 1}), thereâ€™s one property of the functional margin that makes it not a
very good measure of conï¬dence, however. Given our choice of g, we note that

62

if we replace w with 2w and b with 2b, then since g(wT x + b) = g(2wT x + 2b),
this would not change hw,b(x) at all. I.e., g, and hence also hw,b(x), depends
only on the sign, but not on the magnitude, of wT x + b. However, replacing
(w, b) with (2w, 2b) also results in multiplying our functional margin by a
factor of 2. Thus, it seems that by exploiting our freedom to scale w and b,
we can make the functional margin arbitrarily large without really changing
anything meaningful. Intuitively, it might therefore make sense to impose
some sort of normalization condition such as that ||w||2 = 1; i.e., we might
replace (w, b) with (w/||w||2, b/||w||2), and instead consider the functional
margin of (w/||w||2, b/||w||2). Weâ€™ll come back to this later.

Given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deï¬ne the
function margin of (w, b) with respect to S as the smallest of the functional
margins of the individual training examples. Denoted by Ë†Î³, this can therefore
be written:

Ë†Î³ = min
i=1,...,n

Ë†Î³(i).

Next, letâ€™s talk about geometric margins. Consider the picture below:

The decision boundary corresponding to (w, b) is shown, along with the
vector w. Note that w is orthogonal (at 90â—¦) to the separating hyperplane.
(You should convince yourself that this must be the case.) Consider the
point at A, which represents the input x(i) of some training example with
label y(i) = 1. Its distance to the decision boundary, Î³(i), is given by the line
segment AB.

How can we ï¬nd the value of Î³(i)? Well, w/||w|| is a unit-length vector
pointing in the same direction as w. Since A represents x(i), we therefore

wAÎ³B(i)63

ï¬nd that the point B is given by x(i) âˆ’ Î³(i) Â· w/||w||. But this point lies on
the decision boundary, and all points x on the decision boundary satisfy the
equation wT x + b = 0. Hence,

wT

(cid:18)
x(i) âˆ’ Î³(i) w
||w||

(cid:19)

+ b = 0.

Solving for Î³(i) yields

Î³(i) =

wT x(i) + b
||w||

=

(cid:18) w
||w||

(cid:19)T

x(i) +

b
||w||

.

This was worked out for the case of a positive training example at A in the
ï¬gure, where being on the â€œpositiveâ€ side of the decision boundary is good.
More generally, we deï¬ne the geometric margin of (w, b) with respect to a
training example (x(i), y(i)) to be

Î³(i) = y(i)

(cid:19)T

(cid:32)(cid:18) w
||w||

x(i) +

(cid:33)

.

b
||w||

Note that if ||w|| = 1, then the functional margin equals the geometric
marginâ€”this thus gives us a way of relating these two diï¬€erent notions of
margin. Also, the geometric margin is invariant to rescaling of the parame-
ters; i.e., if we replace w with 2w and b with 2b, then the geometric margin
does not change. This will in fact come in handy later. Speciï¬cally, because
of this invariance to the scaling of the parameters, when trying to ï¬t w and b
to training data, we can impose an arbitrary scaling constraint on w without
changing anything important; for instance, we can demand that ||w|| = 1, or
|w1| = 5, or |w1 + b| + |w2| = 2, and any of these can be satisï¬ed simply by
rescaling w and b.

Finally, given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deï¬ne
the geometric margin of (w, b) with respect to S to be the smallest of the
geometric margins on the individual training examples:

Î³ = min
i=1,...,n

Î³(i).

6.4 The optimal margin classiï¬er (option read-

ing)

Given a training set, it seems from our previous discussion that a natural
desideratum is to try to ï¬nd a decision boundary that maximizes the (ge-
ometric) margin, since this would reï¬‚ect a very conï¬dent set of predictions

64

on the training set and a good â€œï¬tâ€ to the training data. Speciï¬cally, this
will result in a classiï¬er that separates the positive and the negative training
examples with a â€œgapâ€ (geometric margin).

For now, we will assume that we are given a training set that is linearly
separable; i.e., that it is possible to separate the positive and negative ex-
amples using some separating hyperplane. How will we ï¬nd the one that
achieves the maximum geometric margin? We can pose the following opti-
mization problem:

maxÎ³,w,b Î³

s.t. y(i)(wT x(i) + b) â‰¥ Î³,

i = 1, . . . , n

||w|| = 1.

I.e., we want to maximize Î³, subject to each training example having func-
tional margin at least Î³. The ||w|| = 1 constraint moreover ensures that the
functional margin equals to the geometric margin, so we are also guaranteed
that all the geometric margins are at least Î³. Thus, solving this problem will
result in (w, b) with the largest possible geometric margin with respect to the
training set.

If we could solve the optimization problem above, weâ€™d be done. But the
â€œ||w|| = 1â€ constraint is a nasty (non-convex) one, and this problem certainly
isnâ€™t in any format that we can plug into standard optimization software to
solve. So, letâ€™s try transforming the problem into a nicer one. Consider:

maxË†Î³,w,b

Ë†Î³
||w||

s.t. y(i)(wT x(i) + b) â‰¥ Ë†Î³,

i = 1, . . . , n

Here, weâ€™re going to maximize Ë†Î³/||w||, subject to the functional margins all
being at least Ë†Î³. Since the geometric and functional margins are related by
Î³ = Ë†Î³/||w|, this will give us the answer we want. Moreover, weâ€™ve gotten rid
of the constraint ||w|| = 1 that we didnâ€™t like. The downside is that we now
Ë†Î³
have a nasty (again, non-convex) objective
||w|| function; and, we still donâ€™t
have any oï¬€-the-shelf software that can solve this form of an optimization
problem.

Letâ€™s keep going. Recall our earlier discussion that we can add an arbi-
trary scaling constraint on w and b without changing anything. This is the
key idea weâ€™ll use now. We will introduce the scaling constraint that the
functional margin of w, b with respect to the training set must be 1:

Ë†Î³ = 1.

65

Since multiplying w and b by some constant results in the functional margin
being multiplied by that same constant, this is indeed a scaling constraint,
and can be satisï¬ed by rescaling w, b. Plugging this into our problem above,
and noting that maximizing Ë†Î³/||w|| = 1/||w|| is the same thing as minimizing
||w||2, we now have the following optimization problem:

minw,b

1
2

||w||2

s.t. y(i)(wT x(i) + b) â‰¥ 1,

i = 1, . . . , n

Weâ€™ve now transformed the problem into a form that can be eï¬ƒciently
solved. The above is an optimization problem with a convex quadratic ob-
jective and only linear constraints. Its solution gives us the optimal mar-
gin classiï¬er. This optimization problem can be solved using commercial
quadratic programming (QP) code.1

While we could call the problem solved here, what we will instead do is
make a digression to talk about Lagrange duality. This will lead us to our
optimization problemâ€™s dual form, which will play a key role in allowing us to
use kernels to get optimal margin classiï¬ers to work eï¬ƒciently in very high
dimensional spaces. The dual form will also allow us to derive an eï¬ƒcient
algorithm for solving the above optimization problem that will typically do
much better than generic QP software.

6.5 Lagrange duality (optional reading)

Letâ€™s temporarily put aside SVMs and maximum margin classiï¬ers, and talk
about solving constrained optimization problems.
Consider a problem of the following form:

minw

f (w)
s.t. hi(w) = 0,

i = 1, . . . , l.

Some of you may recall how the method of Lagrange multipliers can be used
to solve it. (Donâ€™t worry if you havenâ€™t seen it before.) In this method, we
deï¬ne the Lagrangian to be

L(w, Î²) = f (w) +

l
(cid:88)

i=1

Î²ihi(w)

1You may be familiar with linear programming, which solves optimization problems
that have linear objectives and linear constraints. QP software is also widely available,
which allows convex quadratic objectives and linear constraints.

66

Here, the Î²iâ€™s are called the Lagrange multipliers. We would then ï¬nd
and set Lâ€™s partial derivatives to zero:

âˆ‚L
âˆ‚wi

= 0;

âˆ‚L
âˆ‚Î²i

= 0,

and solve for w and Î².

In this section, we will generalize this to constrained optimization prob-
lems in which we may have inequality as well as equality constraints. Due to
time constraints, we wonâ€™t really be able to do the theory of Lagrange duality
justice in this class,2 but we will give the main ideas and results, which we
will then apply to our optimal margin classiï¬erâ€™s optimization problem.

Consider the following, which weâ€™ll call the primal optimization problem:

minw

f (w)
s.t. gi(w) â‰¤ 0,
hi(w) = 0,

i = 1, . . . , k
i = 1, . . . , l.

To solve it, we start by deï¬ning the generalized Lagrangian

L(w, Î±, Î²) = f (w) +

k
(cid:88)

i=1

Î±igi(w) +

l
(cid:88)

i=1

Î²ihi(w).

Here, the Î±iâ€™s and Î²iâ€™s are the Lagrange multipliers. Consider the quantity

Î¸P(w) = max

Î±,Î² : Î±iâ‰¥0

L(w, Î±, Î²).

Here, the â€œPâ€ subscript stands for â€œprimal.â€ Let some w be given.
If w
violates any of the primal constraints (i.e., if either gi(w) > 0 or hi(w) (cid:54)= 0
for some i), then you shou
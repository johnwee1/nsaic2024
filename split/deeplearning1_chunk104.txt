14c) or Dosovitskiy
et al. (2015)). To do so, we use the â€œtransposeâ€? of the convolution operator,
described in section 9.5. This approach often yields more realistic images and does
so using fewer parameters than using fully connected layers without parameter
sharing.
Convolutional networks for recognition tasks have information ï¬‚ow from the
image to some summarization layer at the top of the network, often a class label.
704

CHAPTER 20. DEEP GENERATIVE MODELS

As this image ï¬‚ows upward through the network, information is discarded as the
representation of the image becomes more invariant to nuisance transformations.
In a generator network, the opposite is true. Rich details must be added as
the representation of the image to be generated propagates through the network,
culminating in the ï¬?nal representation of the image, which is of course the image
itself, in all of its detailed glory, with object positions and poses and textures and
lighting. The primary mechanism for discarding information in a convolutional
recognition network is the pooling layer. The generator network seems to need to
add information. We cannot put the inverse of a pooling layer into the generator
network because most pooling functions are not invertible. A simpler operation is
to merely increase the spatial size of the representation. An approach that seems
to perform acceptably is to use an â€œun-poolingâ€? as introduced by Dosovitskiy et al.
(2015). This layer corresponds to the inverse of the max-pooling operation under
certain simplifying conditions. First, the stride of the max-pooling operation is
constrained to be equal to the width of the pooling region. Second, the maximum
input within each pooling region is assumed to be the input in the upper-left
corner. Finally, all non-maximal inputs within each pooling region are assumed to
be zero. These are very strong and unrealistic assumptions, but they do allow the
max-pooling operator to be inverted. The inverse un-pooling operation allocates
a tensor of zeros, then copies each value from spatial coordinate i of the input
to spatial coordinate i Ã— k of the output. The integer value k deï¬?nes the size
of the pooling region. Even though the assumptions motivating the deï¬?nition of
the un-pooling operator are unrealistic, the subsequent layers are able to learn to
compensate for its unusual output, so the samples generated by the model as a
whole are visually pleasing.

20.10.7

Auto-Regressive Networks

Auto-regressive networks are directed probabilistic models with no latent random
variables. The conditional probability distributions in these models are represented
by neural networks (sometimes extremely simple neural networks such as logistic
regression). The graph structure of these models is the complete graph. They
decompose a joint probability over the observed variables using the chain rule of
probability to obtain a product of conditionals of the form P(x d | xdâˆ’1 , . . . , x 1).
Such models have been called fully-visible Bayes networks (FVBNs) and used
successfully in many forms, ï¬?rst with logistic regression for each conditional
distribution (Frey, 1998) and then with neural networks with hidden units (Bengio
and Bengio, 2000b; Larochelle and Murray, 2011).
In some forms of autoregressive networks, such as NADE (Larochelle and Murray, 2011), described
705

CHAPTER 20. DEEP GENERATIVE MODELS

in section 20.10.10 below, we can introduce a form of parameter sharing that
brings both a statistical advantage (fewer unique parameters) and a computational
advantage (less computation). This is one more instance of the recurring deep
learning motif of reuse of features.

x1

x2

x3

x4

P (x3 | x1 , x2 )

P (x 1)

P (x 4 | x1 , x 2, x 3 )

P (x2 | x1 )

x1

x2

x3

x4

Figure 20.8: A fully visible belief network predicts the i -th variable from the i âˆ’ 1
previous ones. (Top)The directed graphical model for an FVBN. (Bottom)Corresponding
computational graph, in the case of the logistic FVBN, where each prediction is made by
a linear predictor.

20.10.8

Linear Auto-Regressive Networks

The simplest form of auto-regressive network has no hidden units and no sharing
of parameters or features. Each P (xi | x iâˆ’1 , . . . , x1) is parametrized as a linear
model (linear regression for real-valued data, logistic regression for binary data,
softmax regression for discrete data). This model was introduced by Frey (1998)
and has O(d2 ) parameters when there are d variables to model. It is illustrated in
ï¬?gure 20.8.
If the variables are continuous, a linear auto-regressive model is merely another
way to formulate a multivariate Gaussian distribution, capturing linear pairwise
interactions between the observed variables.
Linear auto-regressive networks are essentially the generalization of linear
classiï¬?cation methods to generative modeling. They therefore have the same
706

CHAPTER 20. DEEP GENERATIVE MODELS

advantages and disadvantages as linear classiï¬?ers. Like linear classiï¬?ers, they may
be trained with convex loss functions, and sometimes admit closed form solutions
(as in the Gaussian case). Like linear classiï¬?ers, the model itself does not oï¬€er
a way of increasing its capacity, so capacity must be raised using techniques like
basis expansions of the input or the kernel trick.
P (x3 | x1, x 2 )

P (x1 )
P (x2 | x1 )

P (x4 | x1 , x2 , x 3 )

h1

h2

h3

x1

x2

x3

x4

Figure 20.9: A neural auto-regressive network predicts the i -th variable xi from the i âˆ’ 1
previous ones, but is parametrized so that features (groups of hidden units denoted hi )
that are functions of x1 , . . . , xi can be reused in predicting all of the subsequent variables
x i+1, x i+2 , . . . , xd.

20.10.9

Neural Auto-Regressive Networks

Neural auto-regressive networks (Bengio and Bengio, 2000a,b) have the same
left-to-right graphical model as logistic auto-regressive networks (ï¬?gure 20.8) but
employ a diï¬€erent parametrization of the conditional distributions within that
graphical model structure. The new parametrization is more powerful in the sense
that its capacity can be increased as much as needed, allowing approximation of
any joint distribution. The new parametrization can also improve generalization
by introducing a parameter sharing and feature sharing principle common to deep
learning in general. The models were motivated by the objective of avoiding the
curse of dimensionality arising out of traditional tabular graphical models, sharing
the same structure as ï¬?gure 20.8. In tabular discrete probabilistic models, each
conditional distribution is represented by a table of probabilities, with one entry
and one parameter for each possible conï¬?guration of the variables involved. By
using a neural network instead, two advantages are obtained:
707

CHAPTER 20. DEEP GENERATIVE MODELS

1. The parametrization of each P (xi | x iâˆ’1 , . . . , x1 ) by a neural network with
(i âˆ’ 1) Ã— k inputs and k outputs (if the variables are discrete and take k
values, encoded one-hot) allows one to estimate the conditional probability
without requiring an exponential number of parameters (and examples), yet
still is able to capture high-order dependencies between the random variables.
2. Instead of having a diï¬€erent neural network for the prediction of each xi ,
a left-to-right connectivity illustrated in ï¬?gure 20.9 allows one to merge all
the neural networks into one. Equivalently, it means that the hidden layer
features computed for predicting xi can be reused for predicting xi+k (k > 0).
The hidden units are thus organized in groups that have the particularity
that all the units in the i-th group only depend on the input values x 1, . . . , xi.
The parameters used to compute these hidden units are jointly optimized
to improve the prediction of all the variables in the sequence. This is
an instance of the reuse principle that recurs throughout deep learning in
scenarios ranging from recurrent and convolutional network architectures to
multi-task and transfer learning.
Each P(xi | xiâˆ’1 , . . . , x1) can represent a conditional distribution by having
outputs of the neural network predict parameters of the conditional distribution
of xi , as discussed in section 6.2.1.1. Although the original neural auto-regressive
networks were initially evaluated in the context of purely discrete multivariate
data (with a sigmoid output for a Bernoulli variable or softmax output for a
multinoulli variable) it is natural to extend such models to continuous variables or
joint distributions involving both discrete and continuous variables.

20.10.10

NADE

The neural autoregressive density estimator (NADE) is a very successful
recent form of neural auto-regressive network (Larochelle and Murray, 2011). The
connectivity is the same as for the original neural auto-regressive network of Bengio
and Bengio (2000b) but NADE introduces an additional parameter sharing scheme,
as illustrated in ï¬?gure 20.10. The parameters of the hidden units of diï¬€erent groups
j are shared.
î€°
The weights Wj,k,i
from the i-th input xi to the k-th element of the j -th group

of hidden unit h(kj ) (j â‰¥ i) are shared among the groups:
î€°
W j,k,i
= Wk,i.

The remaining weights, where j < i, are zero.
708

(20.83)

CHAPTER 20. DEEP GENERATIVE MODELS

P (x3 | x1 , x2 )

P (x 1)
P (x2 | x1 )

P (x 4 | x1 , x 2, x 3 )

h1

W :,1

h2

W:,1

W:,1
W :,2

x1

h3

W:,2

x2

W :,3
x3

x4

Figure 20.10: An illustration of the neural autoregressive density estimator (NADE). The
hidden units are organized in groups h (j) so that only the inputs x 1 , . . . , x i participate
in computing h(i) and predicting P (xj | xjâˆ’1 , . . . , x1 ), for j > i. NADE is diï¬€erentiated
from earlier neural auto-regressive networks by the use of a particular weight sharing
î€°
= W k,i is shared (indicated in the ï¬?gure by the use of the same line pattern
pattern: Wj,k,i
for every instance of a replicated weight) for all the weights going out from x i to the k -th
unit of any group j â‰¥ i. Recall that the vector (W1,i , W 2,i, . . . , Wn,i ) is denoted W:,i .

Larochelle and Murray (2011) chose this sharing scheme so that forward
propagation in a NADE model loosely resembles the computations performed in
mean ï¬?eld inference to ï¬?ll in missing inputs in an RBM. This mean ï¬?eld inference
corresponds to running a recurrent network with shared weights and the ï¬?rst step
of that inference is the same as in NADE. The only diï¬€erence is that with NADE,
the output weights connecting the hidden units to the output are parametrized
independently from the weights connecting the input units to the hidden units. In
the RBM, the hidden-to-output weights are the transpose of the input-to-hidden
weights. The NADE architecture can be extended to mimic not just one time step
of the mean ï¬?eld recurrent inference but to mimic k steps. This approach is called
NADE-k (Raiko et al., 2014).
As mentioned previously, auto-regressive networks may be extend to process
continuous-valued data. A particularly powerful and generic way of parametrizing
a continuous density is as a Gaussian mixture (introduced in section 3.9.6) with
mixture weights Î±i (the coeï¬ƒcient or prior probability for component i), percomponent conditional mean Âµi and per-component conditional variance Ïƒi2 . A
model called RNADE (Uria et al., 2013) uses this parametrization to extend NADE
to real values. As with other mixture density networks, the parameters of this
709

CHAPTER 20. DEEP GENERATIVE MODELS

distribution are outputs of the network, with the mixture weight probabilities
produced by a softmax unit, and the variances parametrized so that they are
positive. Stochastic gradient descent can be numerically ill-behaved due to the
interactions between the conditional means Âµi and the conditional variances Ïƒi2.
To reduce this diï¬ƒculty, Uria et al. (2013) use a pseudo-gradient that replaces the
gradient on the mean, in the back-propagation phase.
Another very interesting extension of the neural auto-regressive architectures
gets rid of the need to choose an arbitrary order for the observed variables (Murray
and Larochelle, 2014). In auto-regressive networks, the idea is to train the network
to be able to cope with any order by randomly sampling orders and providing the
information to hidden units specifying which of the inputs are observed (on the
right side of the conditioning bar) and which are to be predicted and are thus
considered missing (on the left side of the conditioning bar). This is nice because
it allows one to use a trained auto-regressive network to perform any inference
problem (i.e. predict or sample from the probability distribution over any subset
of variables given any subset) extremely eï¬ƒciently. Finally, since many orders of
variables are possible (n! for n variables) and each order o of variables yields a
diï¬€erent p(x | o), we can form an ensemble of models for many values of o:
k

1î?˜
pensemble(x) =
p(x | o(i) ).
k i=1

(20.84)

This ensemble model usually generalizes better and assigns higher probability to
the test set than does an individual model deï¬?ned by a single ordering.
In the same paper, the authors propose deep versions of the architecture, but
unfortunately that immediately makes computation as expensive as in the original
neural auto-regressive neural network (Bengio and Bengio, 2000b). The ï¬?rst layer
and the output layer can still be computed in O(nh) multiply-add operations,
as in the regular NADE, where h is the number of hidden units (the size of the
groups h i , in ï¬?gures 20.10 and 20.9), whereas it is O(n2h) in Bengio and Bengio
(2000b). However, for the other hidden layers, the computation is O (n2h2) if every
â€œpreviousâ€? group at layer l participates in predicting the â€œnextâ€? group at layer l + 1,
assuming n groups of h hidden units at each layer. Making the i-th group at layer
l + 1 only depend on the i-th group, as in Murray and Larochelle (2014) at layer l
reduces it to O (nh2), which is still h times worse than the regular NADE.

710

CHAPTER 20. DEEP GENERATIVE MODELS

20.11

Drawing Samples from Autoencoders

In chapter 14, we saw that many kinds of autoencoders learn the data distribution.
There are close connections between score matching, denoising autoencoders, and
contractive autoencoders. These connections demonstrate that some kinds of
autoencoders learn the data distribution in some way. We have not yet seen how
to draw samples from such models.
Some kinds of autoencoders, such as the variational autoencoder, explicitly
represent a probability distribution and admit straightforward ancestral sampling.
Most other kinds of autoencoders require MCMC sampling.
Contractive autoencoders are designed to recover an estimate of the tangent
plane of the data manifold. This means that repeated encoding and decoding with
injected noise will induce a random walk along the surface of the manifold (Rifai
et al., 2012; Mesnil et al., 2012). This manifold diï¬€usion technique is a kind of
Markov chain.
There is also a more general Markov chain that can sample from any denoising
autoencoder.

20.11.1

Markov Chain Associated with any Denoising Autoencoder

The above discussion left open the question of what noise to inject and where,
in order to obtain a Markov chain that would generate from the distribution
estimated by the autoencoder. Bengio et al. (2013c) showed how to construct
such a Markov chain for generalized den
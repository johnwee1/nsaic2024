the subword level
in  a  language-independent  way,  treating  spaces  like  other  characters.  With  this
approach, even if your model encounters a word it has never seen before, it can still
reasonably  guess  what  it  means.  For  example,  it  may  never  have  seen  the  word
“smartest”  during  training,  but  perhaps  it  learned  the  word  “smart”  and  it  also
learned  that  the  suffix  “est”  means  “the  most,”  so  it  can  infer  the  meaning  of

4 Taku Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword

Candidates,” arXiv preprint arXiv:1804.10959 (2018).

Sentiment Analysis 

| 

535

“smartest.”  Google’s  SentencePiece  project  provides  an  open  source  implementation,
described in a paper5 by Taku Kudo and John Richardson.

Another  option  was  proposed  in  an  earlier  paper6  by  Rico  Sennrich  et  al.  that
explored  other  ways  of  creating  subword  encodings  (e.g.,  using  byte  pair  encoding).
Last  but  not  least,  the  TensorFlow  team  released  the  TF.Text  library  in  June  2019,
which implements various tokenization strategies, including WordPiece7 (a variant of
byte pair encoding).

If you want to deploy your model to a mobile device or a web browser, and you don’t
want  to  have  to  write  a  different  preprocessing  function  every  time,  then  you  will
want to handle preprocessing using only TensorFlow operations, so it can be included
in  the  model  itself.  Let’s  see  how.  First,  let’s  load  the  original  IMDb  reviews,  as  text
(byte strings), using TensorFlow Datasets (introduced in Chapter 13):

import tensorflow_datasets as tfds

datasets, info = tfds.load("imdb_reviews", as_supervised=True, with_info=True)
train_size = info.splits["train"].num_examples

Next, let’s write the preprocessing function:

def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, b"<br\\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch

It starts by truncating the reviews, keeping only the first 300 characters of each: this
will  speed  up  training,  and  it  won’t  impact  performance  too  much  because  you  can
generally tell whether a review is positive or not in the first sentence or two. Then it
uses regular expressions to replace <br /> tags with spaces, and to replace any charac‐
ters  other  than  letters  and  quotes  with  spaces.  For  example,  the  text  "Well,  I
can't<br  />"  will  become  "Well  I  can't".  Finally,  the  preprocess()  function
splits  the  reviews  by  the  spaces,  which  returns  a  ragged  tensor,  and  it  converts  this
ragged tensor to a dense tensor, padding all reviews with the padding token "<pad>"
so that they all have the same length.

5 Taku Kudo and John Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer

and Detokenizer for Neural Text Processing,” arXiv preprint arXiv:1808.06226 (2018).

6 Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units,” Proceedings of the 54th

Annual Meeting of the Association for Computational Linguistics 1 (2016): 1715–1725.

7 Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap Between Human and

Machine Translation,” arXiv preprint arXiv:1609.08144 (2016).

536 

| 

Chapter 16: Natural Language Processing with RNNs and Attention

Next,  we  need  to  construct  the  vocabulary.  This  requires  going  through  the  whole
training set once, applying our preprocess() function, and using a Counter to count
the number of occurrences of each word:

from collections import Counter
vocabulary = Counter()
for X_batch, y_batch in datasets["train"].batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))

Let’s look at the three most common words:

>>> vocabulary.most_common()[:3]
[(b'<pad>', 215797), (b'the', 61137), (b'a', 38564)]

Great! We probably don’t need our model to know all the words in the dictionary to
get  good  performance,  though,  so  let’s  truncate  the  vocabulary,  keeping  only  the
10,000 most common words:

vocab_size = 10000
truncated_vocabulary = [
    word for word, count in vocabulary.most_common()[:vocab_size]]

Now  we  need  to  add  a  preprocessing  step  to  replace  each  word  with  its  ID  (i.e.,  its
index in the vocabulary). Just like we did in Chapter 13, we will create a lookup table
for this, using 1,000 out-of-vocabulary (oov) buckets:

words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)

We can then use this table to look up the IDs of a few words:

>>> table.lookup(tf.constant([b"This movie was faaaaaantastic".split()]))
<tf.Tensor: [...], dtype=int64, numpy=array([[   22,    12,    11, 10054]])>

Note that the words “this,” “movie,” and “was” were found in the table, so their IDs
are lower than 10,000, while the word “faaaaaantastic” was not found, so it was map‐
ped to one of the oov buckets, with an ID greater than or equal to 10,000.

TF  Transform  (introduced  in  Chapter  13)  provides  some  useful
functions to handle such vocabularies. For example, check out the
tft.compute_and_apply_vocabulary() 
it  will  go
through the dataset to find all distinct words and build the vocabu‐
lary, and it will generate the TF operations required to encode each
word using this vocabulary.

function: 

Now we are ready to create the final training set. We batch the reviews, then convert
them  to  short  sequences  of  words  using  the  preprocess()  function,  then  encode

Sentiment Analysis 

| 

537

these words using a simple encode_words() function that uses the table we just built,
and finally prefetch the next batch:

def encode_words(X_batch, y_batch):
    return table.lookup(X_batch), y_batch

train_set = datasets["train"].batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)

At last we can create the model and train it:

embed_size = 128
model = keras.models.Sequential([
    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
                           input_shape=[None]),
    keras.layers.GRU(128, return_sequences=True),
    keras.layers.GRU(128),
    keras.layers.Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", optimizer="adam",
              metrics=["accuracy"])
history = model.fit(train_set, epochs=5)

The first layer is an  Embedding layer, which will convert word IDs into embeddings
(introduced in Chapter 13). The embedding matrix needs to have one row per word
ID  (vocab_size  +  num_oov_buckets)  and  one  column  per  embedding  dimension
(this  example  uses  128  dimensions,  but  this  is  a  hyperparameter  you  could  tune).
Whereas the inputs of the model will be 2D tensors of shape [batch size, time steps],
the output of the Embedding layer will be a 3D tensor of shape [batch size, time steps,
embedding size].

The rest of the model is fairly straightforward: it is composed of two GRU layers, with
the second one returning only the output of the last time step. The output layer is just
a single neuron using the sigmoid activation function to output the estimated proba‐
bility  that  the  review  expresses  a  positive  sentiment  regarding  the  movie.  We  then
compile the model quite simply, and we fit it on the dataset we prepared earlier, for a
few epochs.

Masking
As it stands, the model will need to learn that the padding tokens should be ignored.
But we already know that! Why don’t we tell the model to ignore the padding tokens,
so that it can focus on the data that actually matters? It’s actually trivial: simply add

538 

| 

Chapter 16: Natural Language Processing with RNNs and Attention

mask_zero=True when creating the Embedding layer. This means that padding tokens
(whose ID is 0)8 will be ignored by all downstream layers. That’s all!

The  way  this  works  is  that  the  Embedding  layer  creates  a  mask  tensor  equal  to
K.not_equal(inputs,  0)  (where  K  =  keras.backend):  it  is  a  Boolean  tensor  with
the same shape as the inputs, and it is equal to False anywhere the word IDs are 0, or
True otherwise. This mask tensor is then automatically propagated by the model to
all subsequent layers, as long as the time dimension is preserved. So in this example,
both  GRU  layers  will  receive  this  mask  automatically,  but  since  the  second  GRU  layer
does not return sequences (it only returns the output of the last time step), the mask
will not be transmitted to the Dense layer. Each layer may handle the mask differently,
but  in  general  they  simply  ignore  masked  time  steps  (i.e.,  time  steps  for  which  the
mask is False). For example, when a recurrent layer encounters a masked time step,
it simply copies the output from the previous time step. If the mask propagates all the
way  to  the  output  (in  models  that  output  sequences,  which  is  not  the  case  in  this
example), then it will be applied to the losses as well, so the masked time steps will
not contribute to the loss (their loss will be 0).

The  LSTM  and  GRU  layers  have  an  optimized  implementation  for
GPUs, based on Nvidia’s cuDNN library. However, this implemen‐
tation does not support masking. If your model uses a mask, then
these layers will fall back to the (much slower) default implementa‐
tion. Note that the optimized implementation also requires you to
use  the  default  values  for  several  hyperparameters:  activation,
recurrent_activation,  recurrent_dropout,  unroll,  use_bias,
and reset_after.

All layers that receive the mask must support masking (or else an exception will be
raised). This includes all recurrent layers, as well as the TimeDistributed layer and a
few  other  layers.  Any  layer  that  supports  masking  must  have  a  supports_masking
attribute equal to True. If you want to implement your own custom layer with mask‐
ing support, you should add a mask argument to the call() method (and obviously
make  the  method  use  the  mask  somehow).  Additionally,  you  should  set
self.supports_masking = True in the constructor. If your layer does not start with
an Embedding layer, you may use the keras.layers.Masking layer instead: it sets the
mask to K.any(K.not_equal(inputs, 0), axis=-1), meaning that time steps where
the last dimension is full of zeros will be masked out in subsequent layers (again, as
long as the time dimension exists).

8 Their ID is 0 only because they are the most frequent “words” in the dataset. It would probably be a good idea

to ensure that the padding tokens are always encoded as 0, even if they are not the most frequent.

Sentiment Analysis 

| 

539

Using  masking  layers  and  automatic  mask  propagation  works  best  for  simple
Sequential models. It will not always work for more complex models, such as when
you need to mix Conv1D layers with recurrent layers. In such cases, you will need to
explicitly  compute  the  mask  and  pass  it  to  the  appropriate  layers,  using  either  the
Functional API or the Subclassing API. For example, the following model is identical
to the previous model, except it is built using the Functional API and handles mask‐
ing manually:

K = keras.backend
inputs = keras.layers.Input(shape=[None])
mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)
z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)
z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)
z = keras.layers.GRU(128)(z, mask=mask)
outputs = keras.layers.Dense(1, activation="sigmoid")(z)
model = keras.Model(inputs=[inputs], outputs=[outputs])

After training for a few epochs, this model will become quite good at judging whether
a review is positive or not. If you use the TensorBoard() callback, you can visualize
the  embeddings  in  TensorBoard  as  they  are  being  learned:  it  is  fascinating  to  see
words like “awesome” and “amazing” gradually cluster on one side of the embedding
space, while words like “awful” and “terrible” cluster on the other side. Some words
are  not  as  positive  as  you  might  expect  (at  least  with  this  model),  such  as  the  word
“good,” presumably because many negative reviews contain the phrase “not good.” It’s
impressive  that  the  model  is  able  to  learn  useful  word  embeddings  based  on  just
25,000 movie reviews. Imagine how good the embeddings would be if we had billions
of  reviews  to  train  on!  Unfortunately  we  don’t,  but  perhaps  we  can  reuse  word
embeddings trained on some other large text corpus (e.g., Wikipedia articles), even if
it is not composed of movie reviews? After all, the word “amazing” generally has the
same  meaning  whether  you  use  it  to  talk  about  movies  or  anything  else.  Moreover,
perhaps embeddings would be useful for sentiment analysis even if they were trained
on another task: since words like “awesome” and “amazing” have a similar meaning,
they  will  likely  cluster  in  the  embedding  space  even  for  other  tasks  (e.g.,  predicting
the next word in a sentence). If all positive words and all negative words form clus‐
ters,  then  this  will  be  helpful  for  sentiment  analysis.  So  instead  of  using  so  many
parameters  to  learn  word  embeddings,  let’s  see  if  we  can’t  just  reuse  pretrained
embeddings.

Reusing Pretrained Embeddings
The TensorFlow Hub project makes it easy to reuse pretrained model components in
your  own  models.  These  model  components  are  called  modules.  Simply  browse  the
TF  Hub  repository,  find  the  one  you  need,  and  copy  the  code  example  into  your
project, and the module will be automatically downloaded, along with its pretrained
weights, and included in your model. Easy!

540 

| 

Chapter 16: Natural Language Processing with RNNs and Attention

For example, let’s use the nnlm-en-dim50 sentence embedding module, version 1, in
our sentiment analysis model:

import tensorflow_hub as hub

model = keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1",
                   dtype=tf.string, input_shape=[], output_shape=[50]),
    keras.layers.Dense(128, activation="relu"),
    keras.layers.Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", optimizer="adam",
              metrics=["accuracy"])

The hub.KerasLayer layer downloads the module from the given URL. This particu‐
lar module is a sentence encoder: it takes strings as input and encodes each one as a
single  vector  (in  this  case,  a  50-dimensional  vector).  Internally,  it  parses  the  string
(splitting words on spaces) and embeds each word using an embedding matrix that
was  pretrained  on  a  huge  corpus:  the  Google  News  7B  corpus  (seven  billion  words
long!). Then it computes the mean of all the word embeddings, and the result is the
sentence embedding.9 We can then add two simple Dense layers to create a good sen‐
timent analysis model. By default, a hub.KerasLayer is not trainable, but you can set
trainable=True when creating it to change that so that you can fine-tune it for y
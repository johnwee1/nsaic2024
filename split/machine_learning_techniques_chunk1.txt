rs and restricted Boltzmann

machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.

Types of Machine Learning Systems 

| 

9

Figure 1-7. An unlabeled training set for unsupervised learning

Here  are  some  of  the  most  important  unsupervised  learning  algorithms  (most  of
these are covered in Chapters 8 and 9):

• Clustering

— K-Means

— DBSCAN

— Hierarchical Cluster Analysis (HCA)

• Anomaly detection and novelty detection

— One-class SVM

— Isolation Forest

• Visualization and dimensionality reduction

— Principal Component Analysis (PCA)

— Kernel PCA

— Locally Linear Embedding (LLE)

— t-Distributed Stochastic Neighbor Embedding (t-SNE)

• Association rule learning

— Apriori

— Eclat

For example, say you have a lot of data about your blog’s visitors. You may want to
run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At
no  point  do  you  tell  the  algorithm  which  group  a  visitor  belongs  to:  it  finds  those
connections without your help. For example, it might notice that 40% of your visitors
are males who love comic books and generally read your blog in the evening, while
20% are young sci-fi lovers who visit during the weekends. If you use a hierarchical
clustering algorithm, it may also subdivide each group into smaller groups. This may
help you target your posts for each group.

10 

| 

Chapter 1: The Machine Learning Landscape

Figure 1-8. Clustering

Visualization algorithms are also good examples of unsupervised learning algorithms:
you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐
resentation of your data that can easily be plotted (Figure 1-9). These algorithms try
to preserve as much structure as they can (e.g., trying to keep separate clusters in the
input  space  from  overlapping  in  the  visualization)  so  that  you  can  understand  how
the data is organized and perhaps identify unsuspected patterns.

Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3

3 Notice how animals are rather well separated from vehicles and how horses are close to deer but far from

birds. Figure reproduced with permission from Richard Socher et al., “Zero-Shot Learning Through Cross-
Modal Transfer,” Proceedings of the 26th International Conference on Neural Information Processing Systems 1
(2013): 935–943.

Types of Machine Learning Systems 

| 

11

A  related  task  is  dimensionality  reduction,  in  which  the  goal  is  to  simplify  the  data
without losing too much information. One way to do this is to merge several correla‐
ted features into one. For example, a car’s mileage may be strongly correlated with its
age, so the dimensionality reduction algorithm will merge them into one feature that
represents the car’s wear and tear. This is called feature extraction.

It is often a good idea to try to reduce the dimension of your train‐
ing  data  using  a  dimensionality  reduction  algorithm  before  you
feed  it  to  another  Machine  Learning  algorithm  (such  as  a  super‐
vised learning algorithm). It will run much faster, the data will take
up less disk and memory space, and in some cases it may also per‐
form better.

Yet another important unsupervised task is anomaly detection—for example, detect‐
ing unusual credit card transactions to prevent fraud, catching manufacturing defects,
or automatically removing outliers from a dataset before feeding it to another learn‐
ing  algorithm.  The  system  is  shown  mostly  normal  instances  during  training,  so  it
learns to recognize them; then, when it sees a new instance, it can tell whether it looks
like a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar
task  is  novelty  detection:  it  aims  to  detect  new  instances  that  look  different  from  all
instances in the training set. This requires having a very “clean” training set, devoid of
any  instance  that  you  would  like  the  algorithm  to  detect.  For  example,  if  you  have
thousands of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a
novelty detection algorithm should not treat new pictures of Chihuahuas as novelties.
On the other hand, anomaly detection algorithms may consider these dogs as so rare
and so different from other dogs that they would likely classify them as anomalies (no
offense to Chihuahuas).

Figure 1-10. Anomaly detection

Finally, another common unsupervised task is association rule learning, in which the
goal  is  to  dig  into  large  amounts  of  data  and  discover  interesting  relations  between

12 

| 

Chapter 1: The Machine Learning Landscape

attributes. For example, suppose you own a supermarket. Running an association rule
on  your  sales  logs  may  reveal  that  people  who  purchase  barbecue  sauce  and  potato
chips  also  tend  to  buy  steak.  Thus,  you  may  want  to  place  these  items  close  to  one
another.

Semisupervised learning

Since labeling data is usually time-consuming and costly, you will often have plenty of
unlabeled  instances,  and  few  labeled  instances.  Some  algorithms  can  deal  with  data
that’s partially labeled. This is called semisupervised learning (Figure 1-11).

Figure 1-11. Semisupervised learning with two classes (triangles and squares): the unla‐
beled examples (circles) help classify a new instance (the cross) into the triangle class
rather than the square class, even though it is closer to the labeled squares

Some photo-hosting services, such as Google Photos, are good examples of this. Once
you upload all your family photos to the service, it automatically recognizes that the
same person A shows up in photos 1, 5, and 11, while another person B shows up in
photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all
the system needs is for you to tell it who these people are. Just add one label per per‐
son4  and  it  is  able  to  name  everyone  in  every  photo,  which  is  useful  for  searching
photos.

Most  semisupervised  learning  algorithms  are  combinations  of  unsupervised  and
supervised algorithms. For example, deep belief networks (DBNs) are based on unsu‐
pervised components called restricted Boltzmann machines (RBMs) stacked on top of
one another. RBMs are trained sequentially in an unsupervised manner, and then the
whole system is fine-tuned using supervised learning techniques.

4 That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes
mixes up two people who look alike, so you may need to provide a few labels per person and manually clean
up some clusters.

Types of Machine Learning Systems 

| 

13

Reinforcement Learning

Reinforcement Learning is a very different beast. The learning system, called an agent
in  this  context,  can  observe  the  environment,  select  and  perform  actions,  and  get
rewards  in  return  (or  penalties  in  the  form  of  negative  rewards,  as  shown  in
Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get
the  most  reward  over  time.  A  policy  defines  what  action  the  agent  should  choose
when it is in a given situation.

Figure 1-12. Reinforcement Learning

For  example,  many  robots  implement  Reinforcement  Learning  algorithms  to  learn
how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement
Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie
at the game of Go. It learned its winning policy by analyzing millions of games, and
then playing many games against itself. Note that learning was turned off during the
games against the champion; AlphaGo was just applying the policy it had learned.

Batch and Online Learning
Another  criterion  used  to  classify  Machine  Learning  systems  is  whether  or  not  the
system can learn incrementally from a stream of incoming data.

14 

| 

Chapter 1: The Machine Learning Landscape

Batch learning

In batch learning, the system is incapable of learning incrementally: it must be trained
using  all  the  available  data.  This  will  generally  take  a  lot  of  time  and  computing
resources,  so  it  is  typically  done  offline.  First  the  system  is  trained,  and  then  it  is
launched into production and runs without learning anymore; it just applies what it
has learned. This is called offline learning.

If you want a batch learning system to know about new data (such as a new type of
spam), you need to train a new version of the system from scratch on the full dataset
(not just the new data, but also the old data), then stop the old system and replace it
with the new one.

Fortunately,  the  whole  process  of  training,  evaluating,  and  launching  a  Machine
Learning  system  can  be  automated  fairly  easily  (as  shown  in  Figure  1-3),  so  even  a
batch learning system can adapt to change. Simply update the data and train a new
version of the system from scratch as often as needed.

This solution is simple and often works fine, but training using the full set of data can
take many hours, so you would typically train a new system only every 24 hours or
even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐
dict stock prices), then you need a more reactive solution.

Also,  training  on  the  full  set  of  data  requires  a  lot  of  computing  resources  (CPU,
memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and
you automate your system to train from scratch every day, it will end up costing you a
lot of money. If the amount of data is huge, it may even be impossible to use a batch
learning algorithm.

Finally,  if  your  system  needs  to  be  able  to  learn  autonomously  and  it  has  limited
resources (e.g., a smartphone application or a rover on Mars), then carrying around
large  amounts  of  training  data  and  taking  up  a  lot  of  resources  to  train  for  hours
every day is a showstopper.

Fortunately, a better option in all these cases is to use algorithms that are capable of
learning incrementally.

Online learning

In  online  learning,  you  train  the  system  incrementally  by  feeding  it  data  instances
sequentially, either individually or in small groups called mini-batches. Each learning
step is fast and cheap, so the system can learn about new data on the fly, as it arrives
(see Figure 1-13).

Types of Machine Learning Systems 

| 

15

Figure 1-13. In online learning, a model is trained and launched into production, and
then it keeps learning as new data comes in

Online learning is great for systems that receive data as a continuous flow (e.g., stock
prices) and need to adapt to change rapidly or autonomously. It is also a good option
if you have limited computing resources: once an online learning system has learned
about new data instances, it does not need them anymore, so you can discard them
(unless you want to be able to roll back to a previous state and “replay” the data). This
can save a huge amount of space.

Online  learning  algorithms  can  also  be  used  to  train  systems  on  huge  datasets  that
cannot  fit  in  one  machine’s  main  memory  (this  is  called  out-of-core  learning).  The
algorithm  loads  part  of  the  data,  runs  a  training  step  on  that  data,  and  repeats  the
process until it has run on all of the data (see Figure 1-14).

Out-of-core  learning  is  usually  done  offline  (i.e.,  not  on  the  live
system), so online learning can be a confusing name. Think of it as
incremental learning.

One important parameter of online learning systems is how fast they should adapt to
changing data: this is called the learning rate. If you set a high learning rate, then your
system will rapidly adapt to new data, but it will also tend to quickly forget the old
data (you don’t want a spam filter to flag only the latest kinds of spam it was shown).
Conversely, if you set a low learning rate, the system will have more inertia; that is, it
will learn more slowly, but it will also be less sensitive to noise in the new data or to
sequences of nonrepresentative data points (outliers).

16 

| 

Chapter 1: The Machine Learning Landscape

Figure 1-14. Using online learning to handle huge datasets

A big challenge with online learning is that if bad data is fed to the system, the sys‐
tem’s performance will gradually decline. If it’s a live system, your clients will notice.
For example, bad data could come from a malfunctioning sensor on a robot, or from
someone spamming a search engine to try to rank high in search results. To reduce
this risk, you need to monitor your system closely and promptly switch learning off
(and  possibly  revert  to  a  previously  working  state)  if  you  detect  a  drop  in  perfor‐
mance. You may also want to monitor the input data and react to abnormal data (e.g.,
using an anomaly detection algorithm).

Instance-Based Versus Model-Based Learning
One  more  way  to  categorize  Machine  Learning  systems  is  by  how  they  generalize.
Most Machine Learning tasks are about making predictions. This means that given a
number of training examples, the system needs to be able to make good predictions
for  (generalize  to)  examples  it  has  never  seen  before.  Having  a  good  performance
measure on the training data is good, but insufficient; the true goal is to perform well
on new instances.

There  are  two  main  approaches  to  generalization:  instance-based  learning  and
model-based learning.

Instance-based learning

Possibly the most trivial form of learning is simply to learn by heart. If you were to
create a spam filter this way, it would just flag all emails that are identical to emails
that have already been flagged by users—not the worst solution, but certainly not the
best.

Types of Machine Learning Systems 

| 

17

Instead  of  just  flagging  emails  that  are  identical  to  known  spam  emails,  your  spam
filter could be programmed to also flag emails that are very similar to known spam
emails. This requires a measure of similarity between two emails. A (very basic) simi‐
larity measure between two emails could be to count the number of words they have
in common. The system would flag an email as spam if it has many words in com‐
mon with a known spam email.

This is called instance-based learning: the system learns the examples by heart, then
generalizes  to  new  cases  by  using  a  similarity  measure  to  compare  them  to  the
learned examples (or a subset of them). For example, in Figure 1-15 the new instance
would  be  classified  as  a  triangle  because  the  majority  of  the  most  similar  instances
belong to that class.

Figure 1-15. Instance-based learning

Model-based learning

Another way to generalize from a set of examples is to build a model of these exam‐
ples and then use that model to make predictions. This is called model-based learning
(Figure 1-16).

Figure 1-16. Model-based learning

18 

| 

Chapter 1: The Machine Learning Landscape

For example, suppose you want to know if money makes people happy, so you down‐
load the Better Life Index data from the OECD’s website and stats about gross domes‐
tic  product  (GDP)  per  capita  from  the  IMF’s  website.  Then  you  join  the  tables  and
sort by GDP per capita. Table 1-1 shows an excerpt of what you get.

Table 1-1. Does money mak
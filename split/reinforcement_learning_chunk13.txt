n a table. In these
cases the functions must be approximated, using some sort of more compact
parameterized function representation.

Our framing of the reinforcement learning problem forces us to settle for
approximations. However, it also presents us with some unique opportunities
for achieving useful approximations.
For example, in approximating opti-
mal behavior, there may be many states that the agent faces with such a low
probability that selecting suboptimal actions for them has little impact on the
amount of reward the agent receives. Tesauro’s backgammon player, for exam-
ple, plays with exceptional skill even though it might make very bad decisions
on board conﬁgurations that never occur in games against experts. In fact, it
is possible that TD-Gammon makes bad decisions for a large fraction of the
game’s state set. The on-line nature of reinforcement learning makes it possi-
ble to approximate optimal policies in ways that put more eﬀort into learning
to make good decisions for frequently encountered states, at the expense of
less eﬀort for infrequently encountered states. This is one key property that
distinguishes reinforcement learning from other approaches to approximately
solving MDPs.

3.10 Summary

Let us summarize the elements of the reinforcement learning problem that
we have presented in this chapter. Reinforcement learning is about learning
from interaction how to behave in order to achieve a goal. The reinforcement
learning agent and its environment interact over a sequence of discrete time
steps. The speciﬁcation of their interface deﬁnes a particular task: the actions
are the choices made by the agent; the states are the basis for making the
choices; and the rewards are the basis for evaluating the choices. Everything

3.10. SUMMARY

81

inside the agent is completely known and controllable by the agent; everything
outside is incompletely controllable but may or may not be completely known.
A policy is a stochastic rule by which the agent selects actions as a function of
states. The agent’s objective is to maximize the amount of reward it receives
over time.

The return is the function of future rewards that the agent seeks to max-
imize. It has several diﬀerent deﬁnitions depending upon the nature of the
task and whether one wishes to discount delayed reward. The undiscounted
formulation is appropriate for episodic tasks, in which the agent–environment
interaction breaks naturally into episodes; the discounted formulation is appro-
priate for continuing tasks, in which the interaction does not naturally break
into episodes but continues without limit.

An environment satisﬁes the Markov property if its state signal compactly
summarizes the past without degrading the ability to predict the future. This
is rarely exactly true, but often nearly so; the state signal should be chosen or
constructed so that the Markov property holds as nearly as possible. In this
book we assume that this has already been done and focus on the decision-
making problem: how to decide what to do as a function of whatever state
signal is available. If the Markov property does hold, then the environment is
called a Markov decision process (MDP). A ﬁnite MDP is an MDP with ﬁnite
state and action sets. Most of the current theory of reinforcement learning is
restricted to ﬁnite MDPs, but the methods and ideas apply more generally.

A policy’s value functions assign to each state, or state–action pair, the
expected return from that state, or state–action pair, given that the agent uses
the policy. The optimal value functions assign to each state, or state–action
pair, the largest expected return achievable by any policy. A policy whose value
functions are optimal is an optimal policy. Whereas the optimal value functions
for states and state–action pairs are unique for a given MDP, there can be many
optimal policies. Any policy that is greedy with respect to the optimal value
functions must be an optimal policy. The Bellman optimality equations are
special consistency condition that the optimal value functions must satisfy and
that can, in principle, be solved for the optimal value functions, from which
an optimal policy can be determined with relative ease.

A reinforcement learning problem can be posed in a variety of diﬀerent ways
depending on assumptions about the level of knowledge initially available to
the agent. In problems of complete knowledge, the agent has a complete and
accurate model of the environment’s dynamics. If the environment is an MDP,
then such a model consists of the one-step transition probabilities and expected
rewards for all states and their allowable actions. In problems of incomplete
knowledge, a complete and perfect model of the environment is not available.

82

CHAPTER 3. FINITE MARKOV DECISION PROCESSES

Even if the agent has a complete and accurate environment model, the
agent is typically unable to perform enough computation per time step to fully
use it. The memory available is also an important constraint. Memory may
be required to build up accurate approximations of value functions, policies,
and models. In most cases of practical interest there are far more states than
could possibly be entries in a table, and approximations must be made.

A well-deﬁned notion of optimality organizes the approach to learning we
describe in this book and provides a way to understand the theoretical prop-
erties of various learning algorithms, but it is an ideal that reinforcement
learning agents can only approximate to varying degrees.
In reinforcement
learning we are very much concerned with cases in which optimal solutions
cannot be found but must be approximated in some way.

Bibliographical and Historical Remarks

The reinforcement learning problem is deeply indebted to the idea of Markov
decision processes (MDPs) from the ﬁeld of optimal control. These histor-
ical inﬂuences and other major inﬂuences from psychology are described in
the brief history given in Chapter 1. Reinforcement learning adds to MDPs a
focus on approximation and incomplete information for realistically large prob-
lems. MDPs and the reinforcement learning problem are only weakly linked
to traditional learning and decision-making problems in artiﬁcial intelligence.
However, artiﬁcial intelligence is now vigorously exploring MDP formulations
for planning and decision-making from a variety of perspectives. MDPs are
more general than previous formulations used in artiﬁcial intelligence in that
they permit more general kinds of goals and uncertainty.

Our presentation of the reinforcement learning problem was inﬂuenced by

Watkins (1989).

3.1

The bioreactor example is based on the work of Ungar (1990) and Miller
and Williams (1992). The recycling robot example was inspired by the
can-collecting robot built by Jonathan Connell (1989).

3.3–4 The terminology of episodic and continuing tasks is diﬀerent from that
usually used in the MDP literature.
In that literature it is common
to distinguish three types of tasks: (1) ﬁnite-horizon tasks, in which
interaction terminates after a particular ﬁxed number of time steps; (2)
indeﬁnite-horizon tasks, in which interaction can last arbitrarily long
but must eventually terminate; and (3) inﬁnite-horizon tasks, in which
interaction does not terminate. Our episodic and continuing tasks are

3.10. SUMMARY

83

similar to indeﬁnite-horizon and inﬁnite-horizon tasks, respectively, but
we prefer to emphasize the diﬀerence in the nature of the interaction.
This diﬀerence seems more fundamental than the diﬀerence in the ob-
jective functions emphasized by the usual terms. Often episodic tasks
use an indeﬁnite-horizon objective function and continuing tasks an
inﬁnite-horizon objective function, but we see this as a common coin-
cidence rather than a fundamental diﬀerence.

The pole-balancing example is from Michie and Chambers (1968) and
Barto, Sutton, and Anderson (1983).

3.5

For further discussion of the concept of state, see Minsky (1967).

3.6

The theory of MDPs is treated by, e.g., Bertsekas (1995), Ross (1983),
White (1969), and Whittle (1982, 1983). This theory is also studied
under the heading of stochastic optimal control, where adaptive optimal
control methods are most closely related to reinforcement learning (e.g.,
Kumar, 1985; Kumar and Varaiya, 1986).

The theory of MDPs evolved from eﬀorts to understand the problem
of making sequences of decisions under uncertainty, where each deci-
sion can depend on the previous decisions and their outcomes.
It is
sometimes called the theory of multistage decision processes, or se-
quential decision processes, and has roots in the statistical literature
on sequential sampling beginning with the papers by Thompson (1933,
1934) and Robbins (1952) that we cited in Chapter 2 in connection
with bandit problems (which are prototypical MDPs if formulated as
multiple-situation problems).

The earliest instance of which we are aware in which reinforcement
learning was discussed using the MDP formalism is Andreae’s (1969b)
description of a uniﬁed view of learning machines. Witten and Corbin
(1973) experimented with a reinforcement learning system later ana-
lyzed by Witten (1977) using the MDP formalism. Although he did not
explicitly mention MDPs, Werbos (1977) suggested approximate solu-
tion methods for stochastic optimal control problems that are related to
modern reinforcement learning methods (see also Werbos, 1982, 1987,
1988, 1989, 1992). Although Werbos’s ideas were not widely recognized
at the time, they were prescient in emphasizing the importance of ap-
proximately solving optimal control problems in a variety of domains,
including artiﬁcial intelligence. The most inﬂuential integration of rein-
forcement learning and MDPs is due to Watkins (1989). His treatment
of reinforcement learning using the MDP formalism has been widely
adopted.

84

CHAPTER 3. FINITE MARKOV DECISION PROCESSES

Our characterization of the dynamics of an MDP in terms of p(s(cid:48), r
s, a)
|
is slightly unusual. It is more common in the MDP literature to describe
the dynamics in terms of the state transition probabilities p(s(cid:48)
s, a) and
|
expected next rewards r(s, a). In reinforcement learning, however, we
more often have to refer to individual actual or sample rewards (rather
than just their expected values). Our notation also makes it plainer that
St and Rt are in general jointly determined, and thus must have the
same time index. In teaching reinforcement learning, we have found
our notation to be more straightforward conceptually and easier to
understand.

3.7–8 Assigning value on the basis of what is good or bad in the long run has
ancient roots.
In control theory, mapping states to numerical values
representing the long-term consequences of control decisions is a key
part of optimal control theory, which was developed in the 1950s by ex-
tending nineteenth century state-function theories of classical mechan-
ics (see, e.g., Schultz and Melsa, 1967). In describing how a computer
could be programmed to play chess, Shannon (1950) suggested using
an evaluation function that took into account the long-term advantages
and disadvantages of chess positions.

∗

Watkins’s (1989) Q-learning algorithm for estimating q
(Chapter 6)
made action-value functions an important part of reinforcement learn-
ing, and consequently these functions are often called Q-functions. But
the idea of an action-value function is much older than this. Shannon
(1950) suggested that a function h(P, M ) could be used by a chess-
playing program to decide whether a move M in position P is worth
exploring. Michie’s (1961, 1963) MENACE system and Michie and
Chambers’s (1968) BOXES system can be understood as estimating
action-value functions. In classical physics, Hamilton’s principal func-
tion is an action-value function; Newtonian dynamics are greedy with
respect to this function (e.g., Goldstein, 1957). Action-value functions
also played a central role in Denardo’s (1967) theoretical treatment of
DP in terms of contraction mappings.

What we call the Bellman equation for v
was ﬁrst introduced by
∗
Richard Bellman (1957a), who called it the “basic functional equa-
tion.” The counterpart of the Bellman optimality equation for continu-
ous time and state problems is known as the Hamilton–Jacobi–Bellman
equation (or often just the Hamilton–Jacobi equation), indicating its
roots in classical physics (e.g., Schultz and Melsa, 1967).

The golf example was suggested by Chris Watkins.

3.10. SUMMARY

Exercises

85

Exercise 3.1 Devise three example tasks of your own that ﬁt into the re-
inforcement learning framework, identifying for each its states, actions, and
rewards. Make the three examples as diﬀerent from each other as possible.
The framework is abstract and ﬂexible and can be applied in many diﬀerent
ways. Stretch its limits in some way in at least one of your examples.

Exercise 3.2 Is the reinforcement learning framework adequate to usefully
represent all goal-directed learning tasks? Can you think of any clear excep-
tions?

Exercise 3.3 Consider the problem of driving. You could deﬁne the actions
in terms of the accelerator, steering wheel, and brake, that is, where your
body meets the machine. Or you could deﬁne them farther out—say, where
the rubber meets the road, considering your actions to be tire torques. Or
you could deﬁne them farther in—say, where your brain meets your body, the
actions being muscle twitches to control your limbs. Or you could go to a
really high level and say that your actions are your choices of where to drive.
What is the right level, the right place to draw the line between agent and
environment? On what basis is one location of the line to be preferred over
another? Is there any fundamental reason for preferring one location over
another, or is it a free choice?

Exercise 3.4 Suppose you treated pole-balancing as an episodic task but
also used discounting, with all rewards zero except for
1 upon failure. What
then would the return be at each time? How does this return diﬀer from that
in the discounted, continuing formulation of this task?

−

Exercise 3.5 Imagine that you are designing a robot to run a maze. You de-
cide to give it a reward of +1 for escaping from the maze and a reward of zero
at all other times. The task seems to break down naturally into episodes—the
successive runs through the maze—so you decide to treat it as an episodic task,
where the goal is to maximize expected total reward (3.1). After running the
learning agent for a while, you ﬁnd that it is showing no improvement in escap-
ing from the maze. What is going wrong? Have you eﬀectively communicated
to the agent what you want it to achieve?

Exercise 3.6: Broken Vision System Imagine that you are a vision
system. When you are ﬁrst turned on for the day, an image ﬂoods into your
camera. You can see lots of things, but not all things. You can’t see objects
that are occluded, and of course you can’t see objects that are behind you.
After seeing that ﬁrst scene, do you have access to the Markov state of the
environment? Suppose your camera was broken that day and you received no

86

CHAPTER 3. FINITE MARKOV DECISION PROCESSES

images at all, all day. Would you have access to the Markov state then?

Exercise 3.7 There is no exercise 3.7.

Exercise 3.8 What is the Bellman equation for action values, that is, for qπ?
It must give the action value qπ(s, a) in terms of the ac